# 7ì¥ Part 3: ìë™í™”ì™€ ìˆ˜ë™ ì‘ì—…ì˜ ê· í˜• ì°¾ê¸°
**ë¶€ì œ: íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆì˜ ìµœì  ê· í˜•ì ì„ ì°¾ì•„ì„œ**

## í•™ìŠµ ëª©í‘œ
ì´ Partë¥¼ ì™„ë£Œí•œ í›„, ì—¬ëŸ¬ë¶„ì€ ë‹¤ìŒì„ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤:
- ë°ì´í„° ë¶„ì„ ì‘ì—…ì—ì„œ ìë™í™”ì— ì í•©í•œ ì˜ì—­ê³¼ ì¸ê°„ ê°œì…ì´ í•„ìš”í•œ ì˜ì—­ì„ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤
- íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆì„ ë™ì‹œì— ë³´ì¥í•˜ëŠ” ì¸ê°„-AI í˜‘ì—… ì›Œí¬í”Œë¡œìš°ë¥¼ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤
- í’ˆì§ˆ ê´€ë¦¬ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ì™€ ê²€ì¦ ì²´ê³„ë¥¼ ìˆ˜ë¦½í•  ìˆ˜ ìˆë‹¤
- í”„ë¡œì íŠ¸ íŠ¹ì„±ì— ë§ëŠ” ìµœì ì˜ ìë™í™” ì „ëµì„ ì„ íƒí•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆë‹¤

## ì´ë²ˆ Part ë¯¸ë¦¬ë³´ê¸°
AI ì‹œëŒ€ì˜ ë°ì´í„° ë¶„ì„ì€ ë§ˆì¹˜ ì˜¤ì¼€ìŠ¤íŠ¸ë¼ì™€ ê°™ìŠµë‹ˆë‹¤. ê° ì•…ê¸°(AIì™€ ì¸ê°„)ê°€ ìì‹ ì˜ ì¥ì ì„ ì‚´ë ¤ ì—°ì£¼í•  ë•Œ ì•„ë¦„ë‹¤ìš´ í•˜ëª¨ë‹ˆê°€ ë§Œë“¤ì–´ì§‘ë‹ˆë‹¤. ëª¨ë“  ê²ƒì„ AIì—ê²Œ ë§¡ê¸°ê±°ë‚˜, ë°˜ëŒ€ë¡œ ëª¨ë“  ê²ƒì„ ìˆ˜ë™ìœ¼ë¡œ í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì…ë‹ˆë‹¤.

í•µì‹¬ì€ **ì–¸ì œ AIë¥¼ í™œìš©í•˜ê³ , ì–¸ì œ ì¸ê°„ì´ ê°œì…í•´ì•¼ í•˜ëŠ”ì§€**ë¥¼ ì •í™•íˆ íŒë‹¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¨ìˆœ ë°˜ë³µ ì‘ì—…ì€ AIê°€, ì°½ì˜ì  í•´ì„ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ íŒë‹¨ì€ ì¸ê°„ì´ ë§¡ëŠ” ê²ƒì´ ê¸°ë³¸ ì›ì¹™ì´ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•˜ê³  ë¯¸ë¬˜í•œ ê· í˜•ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì´ë²ˆ Partì—ì„œëŠ” ë°ì´í„° ë¶„ì„ í”„ë¡œì íŠ¸ì˜ ê° ë‹¨ê³„ë³„ë¡œ ìµœì ì˜ ìë™í™” ì „ëµì„ ìˆ˜ë¦½í•˜ê³ , í’ˆì§ˆì„ ë³´ì¥í•˜ë©´ì„œë„ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì‹¤ì œì ì¸ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ê²½í—˜ì„ ìŒ“ì•„ë³´ê² ìŠµë‹ˆë‹¤.

---

> ğŸ“ **ì¤‘ìš” ìš©ì–´**: **í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš°(Hybrid Workflow)**
> 
> ì¸ê°„ì˜ ì°½ì˜ì„±ê³¼ íŒë‹¨ë ¥, AIì˜ ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ì¼ê´€ì„±ì„ ì¡°í•©í•˜ì—¬ ì„¤ê³„ëœ ì‘ì—… íë¦„ì…ë‹ˆë‹¤. ê° ë‹¨ê³„ì—ì„œ ê°€ì¥ ì í•©í•œ ì£¼ì²´(ì¸ê°„ ë˜ëŠ” AI)ê°€ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë˜, ìƒí˜¸ ê²€ì¦ê³¼ í”¼ë“œë°±ì„ í†µí•´ í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” í˜‘ì—… ë°©ì‹ì…ë‹ˆë‹¤.

## 1. ìë™í™” ì í•©ì„± í‰ê°€ ê¸°ì¤€

ë°ì´í„° ë¶„ì„ ì‘ì—…ì„ ìë™í™”í• ì§€ ê²°ì •í•˜ëŠ” ê²ƒì€ ë§ˆì¹˜ ìš”ë¦¬ì—ì„œ ì–´ë–¤ ì¬ë£Œë¥¼ ê¸°ê³„ë¡œ ìë¥´ê³  ì–´ë–¤ ê²ƒì„ ì†ìœ¼ë¡œ ë‹¤ë“¬ì„ì§€ ê²°ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ê° ì‘ì—…ì˜ íŠ¹ì„±ì„ ì •í™•íˆ íŒŒì•…í•´ì•¼ ìµœì ì˜ ì„ íƒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1.1 STAR í”„ë ˆì„ì›Œí¬: ìë™í™” ì í•©ì„± í‰ê°€

**STAR í”„ë ˆì„ì›Œí¬**ëŠ” ì‘ì—…ì˜ ìë™í™” ì í•©ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

#### **S - Standardization (í‘œì¤€í™” ê°€ëŠ¥ì„±)**
ì‘ì—…ì´ ì–¼ë§ˆë‚˜ í‘œì¤€í™”ë˜ì–´ ìˆê³  ì¼ê´€ëœ ê·œì¹™ì„ ë”°ë¥´ëŠ”ê°€?

```python
class TaskStandardizationAnalyzer:
    """ì‘ì—… í‘œì¤€í™” ì •ë„ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.standardization_criteria = {
            'input_consistency': 'ì…ë ¥ ë°ì´í„°ì˜ ì¼ê´€ì„±',
            'process_repeatability': 'í”„ë¡œì„¸ìŠ¤ ë°˜ë³µ ê°€ëŠ¥ì„±', 
            'output_predictability': 'ì¶œë ¥ ê²°ê³¼ì˜ ì˜ˆì¸¡ ê°€ëŠ¥ì„±',
            'rule_clarity': 'ì‘ì—… ê·œì¹™ì˜ ëª…í™•ì„±'
        }
    
    def evaluate_standardization(self, task_description):
        """
        ì‘ì—…ì˜ í‘œì¤€í™” ì •ë„ë¥¼ í‰ê°€
        
        Args:
            task_description: ì‘ì—… ì„¤ëª… ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: í‘œì¤€í™” í‰ê°€ ê²°ê³¼
        """
        
        scores = {}
        
        # ì…ë ¥ ì¼ê´€ì„± í‰ê°€ (1-5ì )
        input_types = task_description.get('input_types', [])
        if len(set(input_types)) == 1:  # ë™ì¼í•œ íƒ€ì…
            scores['input_consistency'] = 5
        elif len(set(input_types)) <= 3:  # ì œí•œì  íƒ€ì…
            scores['input_consistency'] = 3
        else:  # ë‹¤ì–‘í•œ íƒ€ì…
            scores['input_consistency'] = 1
        
        # í”„ë¡œì„¸ìŠ¤ ë°˜ë³µì„± í‰ê°€
        steps_variability = task_description.get('steps_variability', 'high')
        process_scores = {'low': 5, 'medium': 3, 'high': 1}
        scores['process_repeatability'] = process_scores.get(steps_variability, 1)
        
        # ì¶œë ¥ ì˜ˆì¸¡ì„± í‰ê°€
        output_variance = task_description.get('output_variance', 'high')
        output_scores = {'low': 5, 'medium': 3, 'high': 1}
        scores['output_predictability'] = output_scores.get(output_variance, 1)
        
        # ê·œì¹™ ëª…í™•ì„± í‰ê°€
        rule_complexity = task_description.get('rule_complexity', 'complex')
        rule_scores = {'simple': 5, 'moderate': 3, 'complex': 1}
        scores['rule_clarity'] = rule_scores.get(rule_complexity, 1)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        total_score = sum(scores.values()) / len(scores)
        
        # í‘œì¤€í™” ë“±ê¸‰ ê²°ì •
        if total_score >= 4:
            standardization_level = 'high'
            automation_recommendation = 'strongly_recommended'
        elif total_score >= 3:
            standardization_level = 'medium' 
            automation_recommendation = 'recommended'
        elif total_score >= 2:
            standardization_level = 'low'
            automation_recommendation = 'partial_automation'
        else:
            standardization_level = 'very_low'
            automation_recommendation = 'manual_preferred'
        
        return {
            'scores': scores,
            'total_score': total_score,
            'standardization_level': standardization_level,
            'automation_recommendation': automation_recommendation,
            'detailed_analysis': self._generate_analysis(scores)
        }
    
    def _generate_analysis(self, scores):
        """ìƒì„¸ ë¶„ì„ ìƒì„±"""
        analysis = []
        
        for criterion, score in scores.items():
            criterion_name = self.standardization_criteria[criterion]
            
            if score >= 4:
                analysis.append(f"âœ… {criterion_name}: ìë™í™”ì— ë§¤ìš° ì í•©")
            elif score >= 3:
                analysis.append(f"ğŸŸ¡ {criterion_name}: ìë™í™” ê°€ëŠ¥")
            else:
                analysis.append(f"âŒ {criterion_name}: ì¸ê°„ ê°œì… í•„ìš”")
        
        return analysis

# SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ì˜ ì‘ì—…ë“¤ í‰ê°€ ì˜ˆì‹œ
analyzer = TaskStandardizationAnalyzer()

# ë‹¤ì–‘í•œ ì‘ì—…ë“¤ì˜ í‘œì¤€í™” ì •ë„ í‰ê°€
tasks_to_evaluate = {
    'data_cleaning': {
        'description': 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ë°ì´í„° ì •ì œ',
        'input_types': ['csv', 'json'],
        'steps_variability': 'low',
        'output_variance': 'low', 
        'rule_complexity': 'simple'
    },
    'feature_engineering': {
        'description': 'í…ìŠ¤íŠ¸ íŠ¹ì„± ì¶”ì¶œ',
        'input_types': ['text'],
        'steps_variability': 'medium',
        'output_variance': 'medium',
        'rule_complexity': 'moderate'
    },
    'model_interpretation': {
        'description': 'ëª¨ë¸ ê²°ê³¼ í•´ì„ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ',
        'input_types': ['model_output', 'metrics', 'domain_knowledge'],
        'steps_variability': 'high',
        'output_variance': 'high',
        'rule_complexity': 'complex'
    },
    'data_validation': {
        'description': 'ë°ì´í„° í’ˆì§ˆ ê²€ì¦',
        'input_types': ['dataframe'],
        'steps_variability': 'low',
        'output_variance': 'low',
        'rule_complexity': 'simple'
    }
}

print("ğŸ“Š ì‘ì—…ë³„ í‘œì¤€í™” ì •ë„ í‰ê°€")
print("=" * 50)

for task_name, task_info in tasks_to_evaluate.items():
    result = analyzer.evaluate_standardization(task_info)
    
    print(f"\nğŸ” {task_name.upper()}: {task_info['description']}")
    print(f"í‘œì¤€í™” ë“±ê¸‰: {result['standardization_level'].upper()}")
    print(f"ìë™í™” ê¶Œê³ : {result['automation_recommendation']}")
    print(f"ì¢…í•© ì ìˆ˜: {result['total_score']:.1f}/5.0")
    
    print("ìƒì„¸ ë¶„ì„:")
    for analysis in result['detailed_analysis']:
        print(f"  {analysis}")
```

**ì½”ë“œ í•´ì„¤:**
- **ë‹¤ì°¨ì› í‰ê°€**: ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ ê¸°ì¤€ì´ ì•„ë‹Œ 4ê°€ì§€ ê´€ì ì—ì„œ ì¢…í•© í‰ê°€
- **ì ìˆ˜í™” ì‹œìŠ¤í…œ**: ì£¼ê´€ì  íŒë‹¨ì„ ê°ê´€ì  ì ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± í™•ë³´
- **ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ **: í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ìë™í™” ì „ëµ ì œì‹œ

#### **T - Time Sensitivity (ì‹œê°„ ë¯¼ê°ì„±)**
ì‘ì—…ì˜ ì‹œê°„ì  ì œì•½ê³¼ ì¦‰ì‹œì„± ìš”êµ¬ì‚¬í•­ì„ í‰ê°€í•©ë‹ˆë‹¤.

```python
import time
from datetime import datetime, timedelta
from enum import Enum

class TimeSensitivity(Enum):
    """ì‹œê°„ ë¯¼ê°ì„± ìˆ˜ì¤€"""
    REAL_TIME = "real_time"      # ì‹¤ì‹œê°„ (< 1ì´ˆ)
    NEAR_REAL_TIME = "near_real_time"  # ì¤€ì‹¤ì‹œê°„ (< 1ë¶„)
    BATCH_HOURLY = "batch_hourly"      # ì‹œê°„ë³„ ë°°ì¹˜ (< 1ì‹œê°„)
    BATCH_DAILY = "batch_daily"        # ì¼ë³„ ë°°ì¹˜ (< 1ì¼)
    PERIODIC = "periodic"              # ì£¼ê¸°ì  (> 1ì¼)

class TimeSensitivityAnalyzer:
    """ì‹œê°„ ë¯¼ê°ì„± ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.automation_recommendations = {
            TimeSensitivity.REAL_TIME: {
                'automation_priority': 'critical',
                'human_involvement': 'minimal',
                'strategy': 'full_automation_with_monitoring'
            },
            TimeSensitivity.NEAR_REAL_TIME: {
                'automation_priority': 'high', 
                'human_involvement': 'oversight',
                'strategy': 'automated_with_human_oversight'
            },
            TimeSensitivity.BATCH_HOURLY: {
                'automation_priority': 'medium',
                'human_involvement': 'quality_check',
                'strategy': 'automated_with_quality_gates'
            },
            TimeSensitivity.BATCH_DAILY: {
                'automation_priority': 'medium',
                'human_involvement': 'review_and_validate',
                'strategy': 'hybrid_approach'
            },
            TimeSensitivity.PERIODIC: {
                'automation_priority': 'low',
                'human_involvement': 'full_involvement',
                'strategy': 'human_led_with_ai_assistance'
            }
        }
    
    def evaluate_time_sensitivity(self, task_requirements):
        """
        ì‘ì—…ì˜ ì‹œê°„ ë¯¼ê°ì„± í‰ê°€
        
        Args:
            task_requirements: ì‘ì—… ìš”êµ¬ì‚¬í•­ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: ì‹œê°„ ë¯¼ê°ì„± í‰ê°€ ê²°ê³¼
        """
        
        # ìš”êµ¬ ì‘ë‹µ ì‹œê°„ ë¶„ì„
        max_response_time = task_requirements.get('max_response_time_seconds', 86400)
        
        if max_response_time < 1:
            sensitivity = TimeSensitivity.REAL_TIME
        elif max_response_time < 60:
            sensitivity = TimeSensitivity.NEAR_REAL_TIME
        elif max_response_time < 3600:
            sensitivity = TimeSensitivity.BATCH_HOURLY
        elif max_response_time < 86400:
            sensitivity = TimeSensitivity.BATCH_DAILY
        else:
            sensitivity = TimeSensitivity.PERIODIC
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë¶„ì„
        business_impact = task_requirements.get('delay_impact', 'low')
        impact_multiplier = {'low': 1.0, 'medium': 1.5, 'high': 2.0, 'critical': 3.0}
        
        # ìë™í™” ìš°ì„ ìˆœìœ„ ê³„ì‚°
        base_priority = self.automation_recommendations[sensitivity]['automation_priority']
        
        priority_scores = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}
        adjusted_score = priority_scores[base_priority] * impact_multiplier[business_impact]
        
        if adjusted_score >= 6:
            final_priority = 'critical'
        elif adjusted_score >= 4:
            final_priority = 'high'
        elif adjusted_score >= 2:
            final_priority = 'medium'
        else:
            final_priority = 'low'
        
        recommendation = self.automation_recommendations[sensitivity].copy()
        recommendation['automation_priority'] = final_priority
        
        return {
            'time_sensitivity': sensitivity.value,
            'max_response_time': max_response_time,
            'business_impact': business_impact,
            'automation_priority': final_priority,
            'strategy': recommendation['strategy'],
            'human_involvement': recommendation['human_involvement'],
            'implementation_guidelines': self._generate_guidelines(sensitivity, business_impact)
        }
    
    def _generate_guidelines(self, sensitivity, impact):
        """êµ¬í˜„ ê°€ì´ë“œë¼ì¸ ìƒì„±"""
        
        guidelines = []
        
        if sensitivity == TimeSensitivity.REAL_TIME:
            guidelines.extend([
                "ì™„ì „ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í•„ìˆ˜",
                "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ ì‹œìŠ¤í…œ êµ¬ì¶•",
                "ì¥ì•  ë³µêµ¬ ìë™í™” ë©”ì»¤ë‹ˆì¦˜ ì¤€ë¹„",
                "ì¸ê°„ ê°œì… ìµœì†Œí™”, ì‚¬í›„ ê²€í† ë§Œ ìˆ˜í–‰"
            ])
        elif sensitivity == TimeSensitivity.NEAR_REAL_TIME:
            guidelines.extend([
                "ìë™í™” + ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§",
                "ì„ê³„ê°’ ê¸°ë°˜ ìë™ ì•ŒëŒ ì„¤ì •",
                "ë¹ ë¥¸ ì¸ê°„ ê°œì… í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•"
            ])
        else:
            guidelines.extend([
                "ë‹¨ê³„ë³„ í’ˆì§ˆ ê²€ì¦ í¬ì¸íŠ¸ ì„¤ì •",
                "ì •ê¸°ì ì¸ ì¸ê°„ ê²€í†  ë° ìŠ¹ì¸ ê³¼ì •",
                "ì ì§„ì  ìë™í™” í™•ëŒ€ ì „ëµ"
            ])
        
        if impact in ['high', 'critical']:
            guidelines.append("ë‹¤ì¤‘ ê²€ì¦ ì²´ê³„ ë° ë¡¤ë°± ê³„íš í•„ìˆ˜")
        
        return guidelines

# SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ì˜ ì‹œê°„ ë¯¼ê°ì„± í‰ê°€
time_analyzer = TimeSensitivityAnalyzer()

sms_tasks_time_requirements = {
    'real_time_classification': {
        'description': 'ì‹¤ì‹œê°„ SMS ìŠ¤íŒ¸ ë¶„ë¥˜',
        'max_response_time_seconds': 0.5,  # 0.5ì´ˆ
        'delay_impact': 'high'
    },
    'model_retraining': {
        'description': 'ëª¨ë¸ ì¬í•™ìŠµ',
        'max_response_time_seconds': 86400,  # 1ì¼
        'delay_impact': 'medium'
    },
    'performance_monitoring': {
        'description': 'ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ë¦¬í¬íŒ…',
        'max_response_time_seconds': 3600,  # 1ì‹œê°„
        'delay_impact': 'low'
    },
    'business_analysis': {
        'description': 'ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë¶„ì„',
        'max_response_time_seconds': 604800,  # 1ì£¼ì¼
        'delay_impact': 'medium'
    }
}

print("\nâ° ì‘ì—…ë³„ ì‹œê°„ ë¯¼ê°ì„± í‰ê°€")
print("=" * 50)

for task_name, requirements in sms_tasks_time_requirements.items():
    result = time_analyzer.evaluate_time_sensitivity(requirements)
    
    print(f"\nğŸ“‹ {task_name.upper()}: {requirements['description']}")
    print(f"ì‹œê°„ ë¯¼ê°ì„±: {result['time_sensitivity']}")
    print(f"ìë™í™” ìš°ì„ ìˆœìœ„: {result['automation_priority'].upper()}")
    print(f"ê¶Œì¥ ì „ëµ: {result['strategy']}")
    print(f"ì¸ê°„ ê°œì… ìˆ˜ì¤€: {result['human_involvement']}")
    
    print("êµ¬í˜„ ê°€ì´ë“œë¼ì¸:")
    for guideline in result['implementation_guidelines']:
        print(f"  â€¢ {guideline}")
```

**ì½”ë“œ í•´ì„¤:**
- **ì‹œê°„ ê¸°ë°˜ ë¶„ë¥˜**: ì‘ë‹µ ì‹œê°„ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ìë™í™” ì „ëµ ì°¨ë³„í™”
- **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ë°˜ì˜**: ì§€ì—° ì‹œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ë„ë¥¼ ê³ ë ¤í•œ ìš°ì„ ìˆœìœ„ ì¡°ì •
- **ì‹¤ìš©ì  ê°€ì´ë“œë¼ì¸**: ê° ì‹œê°„ ë¯¼ê°ì„± ìˆ˜ì¤€ë³„ êµ¬ì²´ì  êµ¬í˜„ ë°©ì•ˆ ì œì‹œ

#### **A - Accuracy Requirements (ì •í™•ë„ ìš”êµ¬ì‚¬í•­)**
ì‘ì—…ì—ì„œ ìš”êµ¬ë˜ëŠ” ì •í™•ë„ ìˆ˜ì¤€ê³¼ ì˜¤ë¥˜ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

```python
from dataclasses import dataclass
from typing import Dict, List, Tuple
import numpy as np

@dataclass
class AccuracyRequirement:
    """ì •í™•ë„ ìš”êµ¬ì‚¬í•­ ì •ì˜"""
    min_accuracy: float
    error_cost: str  # 'low', 'medium', 'high', 'critical'
    false_positive_tolerance: float
    false_negative_tolerance: float
    
class AccuracyAnalyzer:
    """ì •í™•ë„ ìš”êµ¬ì‚¬í•­ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.error_cost_weights = {
            'low': 1.0,
            'medium': 2.0, 
            'high': 5.0,
            'critical': 10.0
        }
        
        self.automation_thresholds = {
            'accuracy': 0.95,      # 95% ì´ìƒ ì •í™•ë„
            'reliability': 0.99,   # 99% ì´ìƒ ì•ˆì •ì„±
            'consistency': 0.98    # 98% ì´ìƒ ì¼ê´€ì„±
        }
    
    def evaluate_accuracy_requirements(self, task_spec, historical_performance=None):
        """
        ì •í™•ë„ ìš”êµ¬ì‚¬í•­ í‰ê°€
        
        Args:
            task_spec: ì‘ì—… ëª…ì„¸ (AccuracyRequirement ê°ì²´)
            historical_performance: ê³¼ê±° ì„±ëŠ¥ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            dict: ì •í™•ë„ ë¶„ì„ ê²°ê³¼
        """
        
        # 1. ê¸°ë³¸ ìë™í™” ì í•©ì„± í‰ê°€
        automation_score = self._calculate_automation_score(task_spec)
        
        # 2. AI vs ì¸ê°„ ì„±ëŠ¥ ë¹„êµ
        performance_comparison = self._compare_ai_human_performance(task_spec, historical_performance)
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• í‰ê°€
        hybrid_strategy = self._design_hybrid_strategy(task_spec, performance_comparison)
        
        # 4. ìµœì¢… ê¶Œê³ ì‚¬í•­ ìƒì„±
        recommendation = self._generate_recommendation(
            automation_score, 
            performance_comparison, 
            hybrid_strategy
        )
        
        return {
            'accuracy_requirements': {
                'min_accuracy': task_spec.min_accuracy,
                'error_cost': task_spec.error_cost,
                'fp_tolerance': task_spec.false_positive_tolerance,
                'fn_tolerance': task_spec.false_negative_tolerance
            },
            'automation_score': automation_score,
            'performance_comparison': performance_comparison,
            'hybrid_strategy': hybrid_strategy,
            'recommendation': recommendation
        }
    
    def _calculate_automation_score(self, task_spec):
        """ìë™í™” ì ìˆ˜ ê³„ì‚°"""
        
        # ì •í™•ë„ ìš”êµ¬ì‚¬í•­ì´ ë†’ì„ìˆ˜ë¡ ìë™í™” ì–´ë ¤ì›€
        accuracy_penalty = max(0, (task_spec.min_accuracy - 0.8) * 2)
        
        # ì˜¤ë¥˜ ë¹„ìš©ì´ ë†’ì„ìˆ˜ë¡ ìë™í™” ì‹ ì¤‘
        error_cost_penalty = self.error_cost_weights[task_spec.error_cost] * 0.1
        
        # False Positive/Negative í—ˆìš©ë„ê°€ ë‚®ì„ìˆ˜ë¡ ìë™í™” ì–´ë ¤ì›€
        fp_penalty = max(0, (0.1 - task_spec.false_positive_tolerance) * 5)
        fp_penalty = max(0, (0.1 - task_spec.false_negative_tolerance) * 5)
        
        # ê¸°ë³¸ ì ìˆ˜ì—ì„œ í˜ë„í‹° ì°¨ê°
        base_score = 1.0
        total_penalty = accuracy_penalty + error_cost_penalty + fp_penalty + fp_penalty
        
        automation_score = max(0, base_score - total_penalty)
        
        return {
            'score': automation_score,
            'penalties': {
                'accuracy': accuracy_penalty,
                'error_cost': error_cost_penalty,
                'false_positive': fp_penalty,
                'false_negative': fp_penalty
            },
            'suitability': self._classify_automation_suitability(automation_score)
        }
    
    def _classify_automation_suitability(self, score):
        """ìë™í™” ì í•©ì„± ë¶„ë¥˜"""
        if score >= 0.8:
            return 'highly_suitable'
        elif score >= 0.6:
            return 'suitable_with_monitoring'
        elif score >= 0.4:
            return 'partial_automation_recommended'
        else:
            return 'human_led_preferred'
    
    def _compare_ai_human_performance(self, task_spec, historical_data):
        """AI vs ì¸ê°„ ì„±ëŠ¥ ë¹„êµ"""
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ì„±ëŠ¥ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ì‹¤í—˜ ê²°ê³¼ ì‚¬ìš©)
        if historical_data is None:
            # SMS ìŠ¤íŒ¸ ë¶„ë¥˜ ì‘ì—… ê¸°ì¤€ ì‹œë®¬ë ˆì´ì…˜
            ai_performance = {
                'accuracy': 0.92,
                'precision': 0.89,
                'recall': 0.94,
                'consistency': 0.98,  # AIëŠ” ì¼ê´€ì„±ì´ ë†’ìŒ
                'speed': 0.001,       # ì´ˆë‹¹ ì²˜ë¦¬ ì‹œê°„
                'cost_per_task': 0.0001
            }
            
            human_performance = {
                'accuracy': 0.85,
                'precision': 0.88,
                'recall': 0.82,
                'consistency': 0.75,  # ì¸ê°„ì€ ì¼ê´€ì„±ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ
                'speed': 30,          # ì´ˆë‹¹ ì²˜ë¦¬ ì‹œê°„
                'cost_per_task': 0.50
            }
        else:
            ai_performance = historical_data['ai']
            human_performance = historical_data['human']
        
        # ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        comparison = {}
        for metric in ai_performance:
            ai_val = ai_performance[metric]
            human_val = human_performance[metric]
            
            if metric in ['speed', 'cost_per_task']:
                # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ (ì‹œê°„, ë¹„ìš©)
                advantage = 'ai' if ai_val < human_val else 'human'
                ratio = human_val / ai_val if ai_val != 0 else float('inf')
            else:
                # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ (ì •í™•ë„, ì •ë°€ë„ ë“±)
                advantage = 'ai' if ai_val > human_val else 'human'
                ratio = ai_val / human_val if human_val != 0 else float('inf')
            
            comparison[metric] = {
                'ai_value': ai_val,
                'human_value': human_val,
                'advantage': advantage,
                'ratio': ratio
            }
        
        return comparison
    
    def _design_hybrid_strategy(self, task_spec, performance_comparison):
        """í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ì„¤ê³„"""
        
        # AIì˜ ê°•ì ê³¼ ì•½ì  ë¶„ì„
        ai_strengths = []
        ai_weaknesses = []
        
        for metric, comp in performance_comparison.items():
            if comp['advantage'] == 'ai':
                ai_strengths.append(metric)
            else:
                ai_weaknesses.append(metric)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ì„¤ê³„
        if task_spec.error_cost in ['critical', 'high']:
            strategy = {
                'primary_approach': 'human_review_required',
                'ai_role': 'preprocessing_and_flagging',
                'human_role': 'final_decision_and_quality_assurance',
                'workflow': [
                    'AIê°€ 1ì°¨ ë¶„ë¥˜ ìˆ˜í–‰',
                    'ì‹ ë¢°ë„ ë‚®ì€ ì¼€ì´ìŠ¤ ì¸ê°„ì—ê²Œ ì „ë‹¬',
                    'ì¸ê°„ì´ ìµœì¢… ê²€í†  ë° ìŠ¹ì¸',
                    'AI ê²°ê³¼ì™€ ì¸ê°„ íŒë‹¨ ë¹„êµ í•™ìŠµ'
                ]
            }
        elif 'accuracy' in ai_strengths and 'consistency' in ai_strengths:
            strategy = {
                'primary_approach': 'ai_with_human_oversight',
                'ai_role': 'primary_classification',
                'human_role': 'exception_handling_and_monitoring',
                'workflow': [
                    'AIê°€ ëŒ€ë¶€ë¶„ ì‘ì—… ìë™ ì²˜ë¦¬',
                    'ì„ê³„ê°’ ë²—ì–´ë‚œ ì¼€ì´ìŠ¤ë§Œ ì¸ê°„ ê²€í† ',
                    'ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§',
                    'ì ì§„ì ì¸ ìë™í™” í™•ëŒ€'
                ]
            }
        else:
            strategy = {
                'primary_approach': 'collaborative',
                'ai_role': 'data_processing_and_feature_extraction',
                'human_role': 'analysis_and_decision_making',
                'workflow': [
                    'AIê°€ ë°ì´í„° ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì¶”ì¶œ',
                    'ì¸ê°„ì´ íŒ¨í„´ ë¶„ì„ ë° í•´ì„',
                    'AIê°€ ë°˜ë³µ ì‘ì—… ìë™í™”',
                    'ì¸ê°„ì´ ì°½ì˜ì  ë¶„ì„ ë° ì „ëµ ìˆ˜ë¦½'
                ]
            }
        
        return strategy
    
    def _generate_recommendation(self, automation_score, performance_comparison, hybrid_strategy):
        """ìµœì¢… ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        
        score_val = automation_score['score']
        suitability = automation_score['suitability']
        
        if suitability == 'highly_suitable':
            recommendation = {
                'approach': 'full_automation',
                'confidence': 'high',
                'implementation_priority': 'immediate',
                'risk_level': 'low'
            }
        elif suitability == 'suitable_with_monitoring':
            recommendation = {
                'approach': 'automated_with_monitoring',
                'confidence': 'medium-high',
                'implementation_priority': 'near_term',
                'risk_level': 'medium'
            }
        elif suitability == 'partial_automation_recommended':
            recommendation = {
                'approach': 'hybrid',
                'confidence': 'medium',
                'implementation_priority': 'gradual',
                'risk_level': 'medium-high'
            }
        else:
            recommendation = {
                'approach': 'human_led',
                'confidence': 'high',
                'implementation_priority': 'ai_assistance_only',
                'risk_level': 'low'
            }
        
        recommendation['strategy_details'] = hybrid_strategy
        
        return recommendation

# SMS ìŠ¤íŒ¸ ë¶„ë¥˜ ì‘ì—…ì˜ ì •í™•ë„ ìš”êµ¬ì‚¬í•­ í‰ê°€
accuracy_analyzer = AccuracyAnalyzer()

# ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì •í™•ë„ ìš”êµ¬ì‚¬í•­
accuracy_scenarios = {
    'customer_facing_filter': AccuracyRequirement(
        min_accuracy=0.95,
        error_cost='high',  # ê³ ê° ë¶ˆë§Œìœ¼ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŒ
        false_positive_tolerance=0.01,  # ì •ìƒ ë©”ì‹œì§€ ì°¨ë‹¨ ìµœì†Œí™”
        false_negative_tolerance=0.05   # ì¼ë¶€ ìŠ¤íŒ¸ í†µê³¼ëŠ” í—ˆìš©
    ),
    'internal_preprocessing': AccuracyRequirement(
        min_accuracy=0.90,
        error_cost='medium',
        false_positive_tolerance=0.05,
        false_negative_tolerance=0.05
    ),
    'research_analysis': AccuracyRequirement(
        min_accuracy=0.85,
        error_cost='low',
        false_positive_tolerance=0.10,
        false_negative_tolerance=0.10
    )
}

print("\nğŸ¯ ì‘ì—…ë³„ ì •í™•ë„ ìš”êµ¬ì‚¬í•­ ë¶„ì„")
print("=" * 50)

for scenario_name, requirements in accuracy_scenarios.items():
    result = accuracy_analyzer.evaluate_accuracy_requirements(requirements)
    
    print(f"\nğŸ“‹ {scenario_name.upper()}")
    print(f"ìµœì†Œ ì •í™•ë„ ìš”êµ¬: {requirements.min_accuracy:.1%}")
    print(f"ì˜¤ë¥˜ ë¹„ìš©: {requirements.error_cost}")
    print(f"ìë™í™” ì í•©ì„±: {result['automation_score']['suitability']}")
    print(f"ê¶Œì¥ ì ‘ê·¼ë²•: {result['recommendation']['approach']}")
    print(f"êµ¬í˜„ ìš°ì„ ìˆœìœ„: {result['recommendation']['implementation_priority']}")
    
    print("\nê¶Œì¥ ì›Œí¬í”Œë¡œìš°:")
    for step in result['recommendation']['strategy_details']['workflow']:
        print(f"  {step}")
```

**ì½”ë“œ í•´ì„¤:**
- **ë‹¤ì°¨ì› ì •í™•ë„ í‰ê°€**: ë‹¨ìˆœí•œ ì •í™•ë„ë¿ë§Œ ì•„ë‹ˆë¼ FP/FN ë¹„ìœ¨, ì˜¤ë¥˜ ë¹„ìš© ê³ ë ¤
- **ì„±ëŠ¥ ë¹„êµ ë¶„ì„**: AIì™€ ì¸ê°„ì˜ ìƒëŒ€ì  ê°•ì /ì•½ì ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¹„êµ
- **ë§ì¶¤í˜• ì „ëµ ì„¤ê³„**: ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ìµœì  í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ìë™ ìƒì„±

#### **R - Resource Requirements (ìì› ìš”êµ¬ì‚¬í•­)**
ì‘ì—…ì— í•„ìš”í•œ ì¸ì /ê¸°ìˆ ì  ìì›ê³¼ ë¹„ìš©ì„ í‰ê°€í•©ë‹ˆë‹¤.

```python
from datetime import datetime
import json

class ResourceAnalyzer:
    """ìì› ìš”êµ¬ì‚¬í•­ ë¶„ì„ê¸°"""
    
    def __init__(self):
        # ë¹„ìš© ê¸°ì¤€í‘œ (USD ê¸°ì¤€)
        self.cost_benchmarks = {
            'human_analyst_hourly': 50,
            'data_scientist_hourly': 75,
            'cloud_compute_hourly': 0.10,
            'ai_api_calls_per_1k': 0.002,
            'storage_gb_monthly': 0.023
        }
        
        # ìì› ìœ í˜•ë³„ íŠ¹ì„±
        self.resource_characteristics = {
            'human': {
                'setup_time': 'low',
                'scalability': 'limited',
                'consistency': 'variable',
                'expertise_required': 'high'
            },
            'ai_automation': {
                'setup_time': 'high',
                'scalability': 'excellent', 
                'consistency': 'high',
                'expertise_required': 'medium'
            },
            'hybrid': {
                'setup_time': 'medium',
                'scalability': 'good',
                'consistency': 'good',
                'expertise_required': 'high'
            }
        }
    
    def analyze_resource_requirements(self, task_profile):
        """
        ì‘ì—…ì˜ ìì› ìš”êµ¬ì‚¬í•­ ë¶„ì„
        
        Args:
            task_profile: ì‘ì—… í”„ë¡œí•„ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            dict: ìì› ë¶„ì„ ê²°ê³¼
        """
        
        # 1. ì‘ì—…ëŸ‰ ë¶„ì„
        workload_analysis = self._analyze_workload(task_profile)
        
        # 2. ì ‘ê·¼ë²•ë³„ ë¹„ìš© ê³„ì‚°
        cost_analysis = self._calculate_costs(task_profile, workload_analysis)
        
        # 3. ROI ë¶„ì„
        roi_analysis = self._calculate_roi(cost_analysis, task_profile)
        
        # 4. ìì› ìµœì í™” ê¶Œê³ 
        optimization_recommendations = self._generate_optimization_recommendations(
            workload_analysis, cost_analysis, roi_analysis
        )
        
        return {
            'workload_analysis': workload_analysis,
            'cost_analysis': cost_analysis,
            'roi_analysis': roi_analysis,
            'optimization_recommendations': optimization_recommendations,
            'summary': self._generate_summary(cost_analysis, roi_analysis)
        }
    
    def _analyze_workload(self, task_profile):
        """ì‘ì—…ëŸ‰ ë¶„ì„"""
        
        # ê¸°ë³¸ ì‘ì—…ëŸ‰ íŒŒë¼ë¯¸í„°
        volume = task_profile.get('monthly_volume', 1000)
        complexity = task_profile.get('complexity_score', 5)  # 1-10
        variability = task_profile.get('variability', 'medium')  # low, medium, high
        
        # ì²˜ë¦¬ ì‹œê°„ ì¶”ì •
        base_time_per_task = {
            'human': complexity * 2,      # ë¶„ ë‹¨ìœ„
            'ai': complexity * 0.1,       # ë¶„ ë‹¨ìœ„
            'hybrid': complexity * 0.5    # ë¶„ ë‹¨ìœ„
        }
        
        # ë³€ë™ì„±ì— ë”°ë¥¸ ì¡°ì •
        variability_multipliers = {
            'low': 1.0,
            'medium': 1.2,
            'high': 1.5
        }
        
        multiplier = variability_multipliers[variability]
        
        monthly_hours = {}
        for approach in base_time_per_task:
            time_per_task = base_time_per_task[approach] * multiplier
            total_hours = (volume * time_per_task) / 60
            monthly_hours[approach] = total_hours
        
        return {
            'volume': volume,
            'complexity_score': complexity,
            'variability': variability,
            'time_per_task_minutes': {k: v * multiplier for k, v in base_time_per_task.items()},
            'monthly_hours': monthly_hours,
            'peak_capacity_needed': monthly_hours['human'] * 1.5  # í”¼í¬ ì‹œê°„ ëŒ€ë¹„
        }
    
    def _calculate_costs(self, task_profile, workload_analysis):
        """ì ‘ê·¼ë²•ë³„ ë¹„ìš© ê³„ì‚°"""
        
        monthly_volume = workload_analysis['volume']
        monthly_hours = workload_analysis['monthly_hours']
        
        # ì¸ê°„ ì „ìš© ì ‘ê·¼ë²• ë¹„ìš©
        human_costs = {
            'labor': monthly_hours['human'] * self.cost_benchmarks['human_analyst_hourly'],
            'training': 500,  # ì›”ê°„ êµìœ¡ ë¹„ìš©
            'management': 200,  # ê´€ë¦¬ ë¹„ìš©
            'quality_control': 100
        }
        human_total = sum(human_costs.values())
        
        # AI ìë™í™” ì ‘ê·¼ë²• ë¹„ìš©
        ai_costs = {
            'api_calls': (monthly_volume / 1000) * self.cost_benchmarks['ai_api_calls_per_1k'],
            'compute': monthly_hours['ai'] * self.cost_benchmarks['cloud_compute_hourly'],
            'storage': 10 * self.cost_benchmarks['storage_gb_monthly'],  # 10GB ì €ì¥ì†Œ
            'development': 2000,  # ì´ˆê¸° ê°œë°œ ë¹„ìš© (ì›”ê°„ ë¶„í• )
            'maintenance': 300   # ì›”ê°„ ìœ ì§€ë³´ìˆ˜
        }
        ai_total = sum(ai_costs.values())
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• ë¹„ìš©
        hybrid_costs = {
            'labor': monthly_hours['hybrid'] * self.cost_benchmarks['data_scientist_hourly'],
            'ai_infrastructure': ai_costs['api_calls'] + ai_costs['compute'] + ai_costs['storage'],
            'integration': 500,  # í†µí•© ìœ ì§€ë³´ìˆ˜
            'quality_assurance': 300
        }
        hybrid_total = sum(hybrid_costs.values())
        
        return {
            'human': {
                'breakdown': human_costs,
                'total_monthly': human_total,
                'cost_per_task': human_total / monthly_volume
            },
            'ai': {
                'breakdown': ai_costs,
                'total_monthly': ai_total,
                'cost_per_task': ai_total / monthly_volume
            },
            'hybrid': {
                'breakdown': hybrid_costs,
                'total_monthly': hybrid_total,
                'cost_per_task': hybrid_total / monthly_volume
            }
        }
    
    def _calculate_roi(self, cost_analysis, task_profile):
        """ROI ë¶„ì„"""
        
        # ê¸°ì¤€ì„  (í˜„ì¬ ë¹„ìš©) - ë³´í†µ ì¸ê°„ ì „ìš© ë°©ì‹
        baseline_cost = cost_analysis['human']['total_monthly']
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì¶”ì •
        monthly_value = task_profile.get('monthly_business_value', baseline_cost * 2)
        
        roi_analysis = {}
        
        for approach in ['human', 'ai', 'hybrid']:
            monthly_cost = cost_analysis[approach]['total_monthly']
            monthly_profit = monthly_value - monthly_cost
            
            if monthly_cost > 0:
                roi_percent = (monthly_profit / monthly_cost) * 100
            else:
                roi_percent = float('inf')
            
            # íˆ¬ì íšŒìˆ˜ ê¸°ê°„ (ê°œì›”)
            initial_investment = task_profile.get('initial_investment', {}).get(approach, 0)
            if monthly_profit > 0:
                payback_months = initial_investment / monthly_profit
            else:
                payback_months = float('inf')
            
            roi_analysis[approach] = {
                'monthly_cost': monthly_cost,
                'monthly_profit': monthly_profit,
                'roi_percent': roi_percent,
                'payback_months': payback_months,
                'annual_savings_vs_baseline': (baseline_cost - monthly_cost) * 12
            }
        
        return roi_analysis
    
    def _generate_optimization_recommendations(self, workload, costs, roi):
        """ìµœì í™” ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ë¹„ìš© íš¨ìœ¨ì„± ë¶„ì„
        cost_ranking = sorted(costs.items(), key=lambda x: x[1]['total_monthly'])
        most_cost_effective = cost_ranking[0][0]
        
        # ROI ë¶„ì„
        roi_ranking = sorted(roi.items(), key=lambda x: x[1]['roi_percent'], reverse=True)
        best_roi = roi_ranking[0][0]
        
        recommendations.append({
            'type': 'cost_optimization',
            'priority': 'high',
            'description': f"ê°€ì¥ ë¹„ìš© íš¨ìœ¨ì ì¸ ì ‘ê·¼ë²•: {most_cost_effective}",
            'monthly_savings': costs['human']['total_monthly'] - costs[most_cost_effective]['total_monthly']
        })
        
        recommendations.append({
            'type': 'roi_optimization', 
            'priority': 'high',
            'description': f"ê°€ì¥ ë†’ì€ ROI ì ‘ê·¼ë²•: {best_roi}",
            'roi_percentage': roi[best_roi]['roi_percent']
        })
        
        # í™•ì¥ì„± ê³ ë ¤ì‚¬í•­
        if workload['volume'] > 5000:  # ëŒ€ìš©ëŸ‰ ì‘ì—…
            recommendations.append({
                'type': 'scalability',
                'priority': 'medium',
                'description': "ëŒ€ìš©ëŸ‰ ì‘ì—…ìœ¼ë¡œ ì¸í•´ AI ìë™í™” ë˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• ê¶Œì¥",
                'rationale': "ì¸ê°„ ì „ìš© ì ‘ê·¼ë²•ì€ í™•ì¥ì„± ì œí•œ"
            })
        
        # í’ˆì§ˆ ê³ ë ¤ì‚¬í•­
        if workload['complexity_score'] >= 8:  # ë†’ì€ ë³µì¡ë„
            recommendations.append({
                'type': 'quality_assurance',
                'priority': 'high',
                'description': "ë†’ì€ ë³µì¡ë„ë¡œ ì¸í•´ ì¸ê°„ ì „ë¬¸ê°€ ê°œì… í•„ìˆ˜",
                'rationale': "AI ë‹¨ë…ìœ¼ë¡œëŠ” ë³µì¡í•œ íŒë‹¨ ì–´ë ¤ì›€"
            })
        
        return recommendations
    
    def _generate_summary(self, costs, roi):
        """ìš”ì•½ ì •ë³´ ìƒì„±"""
        
        # ìµœì  ì„ íƒì§€ ê²°ì •
        approaches = ['human', 'ai', 'hybrid']
        
        # ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ ì¢…í•© ì ìˆ˜ ê³„ì‚°
        scores = {}
        for approach in approaches:
            cost_score = 1 / (costs[approach]['total_monthly'] / 1000)  # ë¹„ìš©ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            roi_score = max(0, roi[approach]['roi_percent'] / 100)      # ROIê°€ ë†’ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            
            # ê°€ì¤‘ í‰ê·  (ë¹„ìš© 60%, ROI 40%)
            scores[approach] = (cost_score * 0.6) + (roi_score * 0.4)
        
        best_approach = max(scores, key=scores.get)
        
        return {
            'recommended_approach': best_approach,
            'confidence_score': scores[best_approach],
            'key_factors': {
                'most_cost_effective': min(costs, key=lambda x: costs[x]['total_monthly']),
                'highest_roi': max(roi, key=lambda x: roi[x]['roi_percent']),
                'fastest_payback': min(roi, key=lambda x: roi[x]['payback_months'])
            },
            'decision_rationale': self._generate_decision_rationale(best_approach, costs, roi)
        }
    
    def _generate_decision_rationale(self, best_approach, costs, roi):
        """ì˜ì‚¬ê²°ì • ê·¼ê±° ìƒì„±"""
        
        cost = costs[best_approach]['total_monthly']
        roi_pct = roi[best_approach]['roi_percent']
        
        rationale = f"{best_approach} ì ‘ê·¼ë²•ì„ ê¶Œì¥í•˜ëŠ” ì´ìœ :\n"
        rationale += f"- ì›”ê°„ ë¹„ìš©: ${cost:,.0f}\n"
        rationale += f"- ROI: {roi_pct:.1f}%\n"
        
        if best_approach == 'ai':
            rationale += "- ë†’ì€ í™•ì¥ì„±ê³¼ ì¼ê´€ì„±\n- ì¥ê¸°ì  ë¹„ìš© ì ˆê° íš¨ê³¼"
        elif best_approach == 'hybrid':
            rationale += "- ê· í˜•ì¡íŒ ë¹„ìš©ê³¼ í’ˆì§ˆ\n- ì ì§„ì  ìë™í™” í™•ëŒ€ ê°€ëŠ¥"
        else:
            rationale += "- ë†’ì€ í’ˆì§ˆê³¼ ìœ ì—°ì„±\n- ë³µì¡í•œ íŒë‹¨ ìƒí™©ì— ì í•©"
        
        return rationale

# SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ì˜ ìì› ìš”êµ¬ì‚¬í•­ ë¶„ì„
resource_analyzer = ResourceAnalyzer()

sms_project_profile = {
    'monthly_volume': 50000,        # ì›”ê°„ 5ë§Œê°œ SMS ì²˜ë¦¬
    'complexity_score': 6,          # ì¤‘ê°„ ë³µì¡ë„
    'variability': 'medium',        # ì¤‘ê°„ ë³€ë™ì„±
    'monthly_business_value': 10000, # ì›”ê°„ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ $10,000
    'initial_investment': {         # ì´ˆê¸° íˆ¬ìë¹„ìš©
        'human': 0,
        'ai': 15000,
        'hybrid': 8000
    }
}

print("\nğŸ’° ìì› ìš”êµ¬ì‚¬í•­ ë° ë¹„ìš© ë¶„ì„")
print("=" * 50)

resource_result = resource_analyzer.analyze_resource_requirements(sms_project_profile)

# ì‘ì—…ëŸ‰ ë¶„ì„ ì¶œë ¥
workload = resource_result['workload_analysis']
print(f"\nğŸ“Š ì‘ì—…ëŸ‰ ë¶„ì„:")
print(f"ì›”ê°„ ì²˜ë¦¬ëŸ‰: {workload['volume']:,}ê°œ")
print(f"ë³µì¡ë„ ì ìˆ˜: {workload['complexity_score']}/10")
print(f"ë³€ë™ì„±: {workload['variability']}")

# ì ‘ê·¼ë²•ë³„ ì›”ê°„ ì†Œìš” ì‹œê°„
print(f"\nâ±ï¸ ì ‘ê·¼ë²•ë³„ ì›”ê°„ ì†Œìš” ì‹œê°„:")
for approach, hours in workload['monthly_hours'].items():
    print(f"  {approach}: {hours:.1f}ì‹œê°„")

# ë¹„ìš© ë¶„ì„ ì¶œë ¥
costs = resource_result['cost_analysis']
print(f"\nğŸ’¸ ì ‘ê·¼ë²•ë³„ ì›”ê°„ ë¹„ìš©:")
for approach, cost_data in costs.items():
    print(f"\n{approach.upper()} ì ‘ê·¼ë²•:")
    print(f"  ì´ ì›”ê°„ ë¹„ìš©: ${cost_data['total_monthly']:,.2f}")
    print(f"  ì‘ì—…ë‹¹ ë¹„ìš©: ${cost_data['cost_per_task']:.4f}")
    
    print(f"  ë¹„ìš© êµ¬ì„±:")
    for category, amount in cost_data['breakdown'].items():
        print(f"    - {category}: ${amount:,.2f}")

# ROI ë¶„ì„ ì¶œë ¥
roi_data = resource_result['roi_analysis']
print(f"\nğŸ“ˆ ROI ë¶„ì„:")
for approach, roi_info in roi_data.items():
    print(f"\n{approach.upper()}:")
    print(f"  ROI: {roi_info['roi_percent']:.1f}%")
    print(f"  íˆ¬ìíšŒìˆ˜ê¸°ê°„: {roi_info['payback_months']:.1f}ê°œì›”")
    print(f"  ì—°ê°„ ì ˆì•½ì•¡: ${roi_info['annual_savings_vs_baseline']:,.0f}")

# ìµœì í™” ê¶Œê³ ì‚¬í•­
recommendations = resource_result['optimization_recommendations']
print(f"\nğŸ¯ ìµœì í™” ê¶Œê³ ì‚¬í•­:")
for i, rec in enumerate(recommendations, 1):
    print(f"\n{i}. [{rec['priority'].upper()}] {rec['description']}")
    if 'monthly_savings' in rec:
        print(f"   ì›”ê°„ ì ˆì•½ì•¡: ${rec['monthly_savings']:,.2f}")
    if 'roi_percentage' in rec:
        print(f"   ì˜ˆìƒ ROI: {rec['roi_percentage']:.1f}%")

# ì¢…í•© ê²°ë¡ 
summary = resource_result['summary']
print(f"\nğŸ† ì¢…í•© ê²°ë¡ :")
print(f"ê¶Œì¥ ì ‘ê·¼ë²•: {summary['recommended_approach'].upper()}")
print(f"ì‹ ë¢°ë„ ì ìˆ˜: {summary['confidence_score']:.2f}")
print(f"\nê²°ì • ê·¼ê±°:")
print(summary['decision_rationale'])
```

**ì½”ë“œ í•´ì„¤:**
- **ì¢…í•©ì  ë¹„ìš© ë¶„ì„**: ì´ˆê¸° íˆ¬ì, ìš´ì˜ ë¹„ìš©, ROIë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì˜ì‚¬ê²°ì • ì§€ì›
- **í™•ì¥ì„± ê³ ë ¤**: ì‘ì—…ëŸ‰ ì¦ê°€ì— ë”°ë¥¸ ê° ì ‘ê·¼ë²•ì˜ ëŒ€ì‘ ëŠ¥ë ¥ í‰ê°€
- **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì—°ê³„**: ë‹¨ìˆœ ë¹„ìš© ì ˆê°ì„ ë„˜ì–´ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì°½ì¶œ ê´€ì ì—ì„œ ë¶„ì„

> ğŸ’¡ **STAR í”„ë ˆì„ì›Œí¬ í™œìš© íŒ**
> 
> **S (í‘œì¤€í™”)**: ë†’ì„ìˆ˜ë¡ ìë™í™” ìœ ë¦¬
> **T (ì‹œê°„ ë¯¼ê°ì„±)**: ë¹ ë¥¼ìˆ˜ë¡ ìë™í™” í•„ìˆ˜
> **A (ì •í™•ë„)**: ë†’ì„ìˆ˜ë¡ ì‹ ì¤‘í•œ ì ‘ê·¼ í•„ìš”
> **R (ìì›)**: ì œì•½ì´ í´ìˆ˜ë¡ íš¨ìœ¨ì„± ì¤‘ì‹œ
> 
> 4ê°€ì§€ ê´€ì ì„ ì¢…í•©í•˜ì—¬ ìµœì ì˜ ìë™í™” ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”!

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "STAR í”„ë ˆì„ì›Œí¬ë¥¼ ë³´ì—¬ì£¼ëŠ” 4ë¶„ë©´ ë§¤íŠ¸ë¦­ìŠ¤. Standardization(í‘œì¤€í™”), Time sensitivity(ì‹œê°„ë¯¼ê°ì„±), Accuracy requirements(ì •í™•ë„ ìš”êµ¬ì‚¬í•­), Resource requirements(ìì› ìš”êµ¬ì‚¬í•­)ê°€ ê°ê° ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ í‘œí˜„ë˜ê³ , ì¤‘ì•™ì—ëŠ” ìë™í™” ì í•©ì„± ì ìˆ˜ê°€ í‘œì‹œëœ ë¶„ì„ì  ë‹¤ì´ì–´ê·¸ë¨"

## 2. ì¸ê°„-AI í˜‘ì—… ëª¨ë¸ ì„¤ê³„

íš¨ê³¼ì ì¸ ì¸ê°„-AI í˜‘ì—…ì€ ë§ˆì¹˜ ì˜ ì¡°ìœ¨ëœ ë“€ì—£ê³¼ ê°™ìŠµë‹ˆë‹¤. ê°ìì˜ ì¥ì ì„ ì‚´ë¦¬ë©´ì„œ ì„œë¡œì˜ ì•½ì ì„ ë³´ì™„í•˜ëŠ” ì¡°í™”ë¡œìš´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤.

### 2.1 í˜‘ì—… íŒ¨í„´ì˜ 3ê°€ì§€ ìœ í˜•

#### **íŒ¨í„´ 1: ìˆœì°¨ì  í˜‘ì—… (Sequential Collaboration)**
AIì™€ ì¸ê°„ì´ ë‹¨ê³„ë³„ë¡œ ìˆœì°¨ì ìœ¼ë¡œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import time
import logging

class TaskStatus(Enum):
    """ì‘ì—… ìƒíƒœ"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress" 
    COMPLETED = "completed"
    FAILED = "failed"
    REQUIRES_REVIEW = "requires_review"

@dataclass
class WorkflowTask:
    """ì›Œí¬í”Œë¡œìš° ì‘ì—… ì •ì˜"""
    task_id: str
    task_type: str
    assigned_to: str  # 'ai' or 'human'
    input_data: Dict[str, Any]
    output_data: Optional[Dict[str, Any]] = None
    status: TaskStatus = TaskStatus.PENDING
    confidence_score: Optional[float] = None
    review_required: bool = False
    created_at: float = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = time.time()

class WorkflowAgent(ABC):
    """ì›Œí¬í”Œë¡œìš° ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, agent_id: str):
        self.agent_id = agent_id
        self.logger = logging.getLogger(f"{self.__class__.__name__}_{agent_id}")
    
    @abstractmethod
    def can_handle(self, task: WorkflowTask) -> bool:
        """ì‘ì—… ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        pass
    
    @abstractmethod
    def process_task(self, task: WorkflowTask) -> WorkflowTask:
        """ì‘ì—… ì²˜ë¦¬"""
        pass
    
    def validate_input(self, task: WorkflowTask) -> bool:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        return task.input_data is not None

class AIAgent(WorkflowAgent):
    """AI ì—ì´ì „íŠ¸"""
    
    def __init__(self, agent_id: str, capabilities: List[str]):
        super().__init__(agent_id)
        self.capabilities = capabilities
        self.processing_time_per_task = 0.1  # ì´ˆ
    
    def can_handle(self, task: WorkflowTask) -> bool:
        """AIê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì¸ì§€ í™•ì¸"""
        return task.task_type in self.capabilities and task.assigned_to == 'ai'
    
    def process_task(self, task: WorkflowTask) -> WorkflowTask:
        """AI ì‘ì—… ì²˜ë¦¬"""
        
        if not self.validate_input(task):
            task.status = TaskStatus.FAILED
            return task
        
        task.status = TaskStatus.IN_PROGRESS
        self.logger.info(f"AI ì²˜ë¦¬ ì‹œì‘: {task.task_id}")
        
        # ì‘ì—… ìœ í˜•ë³„ ì²˜ë¦¬
        try:
            if task.task_type == 'text_preprocessing':
                result = self._preprocess_text(task.input_data)
            elif task.task_type == 'spam_classification':
                result = self._classify_spam(task.input_data)
            elif task.task_type == 'feature_extraction':
                result = self._extract_features(task.input_data)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—… ìœ í˜•: {task.task_type}")
            
            task.output_data = result
            task.status = TaskStatus.COMPLETED
            
            # ì‹ ë¢°ë„ ê¸°ë°˜ ê²€í†  í•„ìš”ì„± íŒë‹¨
            if task.confidence_score and task.confidence_score < 0.8:
                task.review_required = True
                task.status = TaskStatus.REQUIRES_REVIEW
            
            self.logger.info(f"AI ì²˜ë¦¬ ì™„ë£Œ: {task.task_id}, ì‹ ë¢°ë„: {task.confidence_score}")
            
        except Exception as e:
            self.logger.error(f"AI ì²˜ë¦¬ ì‹¤íŒ¨: {task.task_id}, ì˜¤ë¥˜: {str(e)}")
            task.status = TaskStatus.FAILED
            task.output_data = {'error': str(e)}
        
        return task
    
    def _preprocess_text(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        text = input_data.get('text', '')
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ì „ì²˜ë¦¬
        cleaned_text = text.lower().strip()
        word_count = len(cleaned_text.split())
        
        return {
            'cleaned_text': cleaned_text,
            'word_count': word_count,
            'character_count': len(cleaned_text)
        }
    
    def _classify_spam(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìŠ¤íŒ¸ ë¶„ë¥˜ (ì‹œë®¬ë ˆì´ì…˜)"""
        text = input_data.get('text', '')
        
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜
        spam_indicators = ['free', 'win', 'money', 'prize', 'urgent']
        spam_count = sum(1 for indicator in spam_indicators if indicator in text.lower())
        
        if spam_count >= 2:
            prediction = 'spam'
            confidence = 0.8 + (spam_count * 0.1)
        elif spam_count == 1:
            prediction = 'ham'
            confidence = 0.6
        else:
            prediction = 'ham'
            confidence = 0.9
        
        confidence = min(confidence, 1.0)
        
        return {
            'prediction': prediction,
            'confidence': confidence,
            'spam_indicators_found': spam_count
        }
    
    def _extract_features(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """íŠ¹ì„± ì¶”ì¶œ"""
        text = input_data.get('text', '')
        
        features = {
            'length': len(text),
            'word_count': len(text.split()),
            'exclamation_count': text.count('!'),
            'capital_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0,
            'digit_ratio': sum(1 for c in text if c.isdigit()) / len(text) if text else 0
        }
        
        return {'features': features}

class HumanAgent(WorkflowAgent):
    """ì¸ê°„ ì—ì´ì „íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)"""
    
    def __init__(self, agent_id: str, expertise_areas: List[str]):
        super().__init__(agent_id)
        self.expertise_areas = expertise_areas
        self.processing_time_per_task = 30  # ì´ˆ (ì‹œë®¬ë ˆì´ì…˜)
    
    def can_handle(self, task: WorkflowTask) -> bool:
        """ì¸ê°„ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì¸ì§€ í™•ì¸"""
        return (task.task_type in self.expertise_areas and 
                task.assigned_to == 'human') or task.review_required
    
    def process_task(self, task: WorkflowTask) -> WorkflowTask:
        """ì¸ê°„ ì‘ì—… ì²˜ë¦¬ (ì‹œë®¬ë ˆì´ì…˜)"""
        
        task.status = TaskStatus.IN_PROGRESS
        self.logger.info(f"ì¸ê°„ ì²˜ë¦¬ ì‹œì‘: {task.task_id}")
        
        try:
            if task.task_type == 'quality_review':
                result = self._review_quality(task.input_data)
            elif task.task_type == 'business_interpretation':
                result = self._interpret_business_impact(task.input_data)
            elif task.review_required:
                result = self._review_ai_output(task)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‘ì—… ìœ í˜•: {task.task_type}")
            
            task.output_data = result
            task.status = TaskStatus.COMPLETED
            task.review_required = False
            
            self.logger.info(f"ì¸ê°„ ì²˜ë¦¬ ì™„ë£Œ: {task.task_id}")
            
        except Exception as e:
            self.logger.error(f"ì¸ê°„ ì²˜ë¦¬ ì‹¤íŒ¨: {task.task_id}, ì˜¤ë¥˜: {str(e)}")
            task.status = TaskStatus.FAILED
            task.output_data = {'error': str(e)}
        
        return task
    
    def _review_quality(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²€í† """
        # ì‹œë®¬ë ˆì´ì…˜ëœ í’ˆì§ˆ ê²€í† 
        return {
            'quality_score': 0.85,
            'issues_found': ['minor_inconsistency'],
            'recommendations': ['ì¶”ê°€ ë°ì´í„° ì •ì œ í•„ìš”']
        }
    
    def _interpret_business_impact(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ í•´ì„"""
        results = input_data.get('analysis_results', {})
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„
        interpretation = {
            'key_insights': [
                'ìŠ¤íŒ¸ íƒì§€ìœ¨ 95% ë‹¬ì„±ìœ¼ë¡œ ê³ ê° ë§Œì¡±ë„ í–¥ìƒ ì˜ˆìƒ',
                'ì›”ê°„ ì•½ 500ê±´ì˜ ìŠ¤íŒ¸ ì°¨ë‹¨ìœ¼ë¡œ ì‹œìŠ¤í…œ íš¨ìœ¨ì„± ê°œì„ '
            ],
            'business_value': 'high',
            'recommended_actions': [
                'í˜„ì¬ ëª¨ë¸ì„ í”„ë¡œë•ì…˜ì— ë°°í¬',
                'ì›”ê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•'
            ]
        }
        
        return interpretation
    
    def _review_ai_output(self, task: WorkflowTask) -> Dict[str, Any]:
        """AI ì¶œë ¥ ê²€í† """
        ai_output = task.output_data
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ AI ì¶œë ¥ ê²€í† 
        if task.task_type == 'spam_classification':
            prediction = ai_output.get('prediction', '')
            confidence = ai_output.get('confidence', 0)
            
            if confidence < 0.7:
                # ì¸ê°„ì´ ì¬ë¶„ë¥˜
                reviewed_prediction = 'ham'  # ì‹œë®¬ë ˆì´ì…˜
                return {
                    'original_prediction': prediction,
                    'reviewed_prediction': reviewed_prediction,
                    'review_reason': 'ë‚®ì€ ì‹ ë¢°ë„ë¡œ ì¸í•œ ì¸ê°„ ì¬ê²€í† ',
                    'final_confidence': 0.9
                }
            else:
                return {
                    'prediction': prediction,
                    'confidence': confidence,
                    'human_approval': True
                }
        
        return ai_output

class SequentialWorkflow:
    """ìˆœì°¨ì  ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.agents: Dict[str, WorkflowAgent] = {}
        self.workflow_definition: List[Dict[str, Any]] = []
        self.task_queue: List[WorkflowTask] = []
        self.completed_tasks: List[WorkflowTask] = []
        self.logger = logging.getLogger(__name__)
    
    def register_agent(self, agent: WorkflowAgent):
        """ì—ì´ì „íŠ¸ ë“±ë¡"""
        self.agents[agent.agent_id] = agent
        self.logger.info(f"ì—ì´ì „íŠ¸ ë“±ë¡: {agent.agent_id}")
    
    def define_workflow(self, workflow_steps: List[Dict[str, Any]]):
        """ì›Œí¬í”Œë¡œìš° ì •ì˜"""
        self.workflow_definition = workflow_steps
        self.logger.info(f"ì›Œí¬í”Œë¡œìš° ì •ì˜: {len(workflow_steps)}ë‹¨ê³„")
    
    def execute_workflow(self, initial_data: Dict[str, Any]) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        
        self.logger.info("ìˆœì°¨ì  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘")
        
        current_data = initial_data
        execution_results = []
        
        for step_index, step_config in enumerate(self.workflow_definition):
            step_name = step_config['name']
            task_type = step_config['task_type']
            assigned_to = step_config['assigned_to']
            
            # ì‘ì—… ìƒì„±
            task = WorkflowTask(
                task_id=f"{step_name}_{int(time.time())}",
                task_type=task_type,
                assigned_to=assigned_to,
                input_data=current_data
            )
            
            # ì ì ˆí•œ ì—ì´ì „íŠ¸ ì°¾ê¸°
            assigned_agent = self._find_agent_for_task(task)
            
            if not assigned_agent:
                self.logger.error(f"ì‘ì—… ì²˜ë¦¬ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì—†ìŒ: {task.task_id}")
                break
            
            # ì‘ì—… ì²˜ë¦¬
            processed_task = assigned_agent.process_task(task)
            execution_results.append(processed_task)
            
            if processed_task.status == TaskStatus.FAILED:
                self.logger.error(f"ì›Œí¬í”Œë¡œìš° ì¤‘ë‹¨: {step_name} ì‹¤íŒ¨")
                break
            
            # ê²€í† ê°€ í•„ìš”í•œ ê²½ìš° ì¸ê°„ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬
            if processed_task.review_required:
                review_agent = self._find_human_agent()
                if review_agent:
                    reviewed_task = review_agent.process_task(processed_task)
                    execution_results[-1] = reviewed_task
                    current_data = reviewed_task.output_data
                else:
                    self.logger.warning(f"ê²€í†  ê°€ëŠ¥í•œ ì¸ê°„ ì—ì´ì „íŠ¸ ì—†ìŒ: {task.task_id}")
                    current_data = processed_task.output_data
            else:
                current_data = processed_task.output_data
        
        self.logger.info("ìˆœì°¨ì  ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ")
        
        return {
            'final_result': current_data,
            'execution_steps': execution_results,
            'total_steps': len(execution_results),
            'success_rate': sum(1 for task in execution_results 
                               if task.status == TaskStatus.COMPLETED) / len(execution_results)
        }
    
    def _find_agent_for_task(self, task: WorkflowTask) -> Optional[WorkflowAgent]:
        """ì‘ì—…ì— ì í•©í•œ ì—ì´ì „íŠ¸ ì°¾ê¸°"""
        for agent in self.agents.values():
            if agent.can_handle(task):
                return agent
        return None
    
    def _find_human_agent(self) -> Optional[HumanAgent]:
        """ì¸ê°„ ì—ì´ì „íŠ¸ ì°¾ê¸°"""
        for agent in self.agents.values():
            if isinstance(agent, HumanAgent):
                return agent
        return None

# SMS ìŠ¤íŒ¸ íƒì§€ ìˆœì°¨ì  ì›Œí¬í”Œë¡œìš° êµ¬í˜„ ì˜ˆì‹œ
print("ğŸ”„ ìˆœì°¨ì  í˜‘ì—… ì›Œí¬í”Œë¡œìš° ì‹œì—°")
print("=" * 50)

# ì—ì´ì „íŠ¸ ìƒì„± ë° ë“±ë¡
ai_agent = AIAgent(
    agent_id="sms_ai_processor",
    capabilities=['text_preprocessing', 'spam_classification', 'feature_extraction']
)

human_agent = HumanAgent(
    agent_id="sms_human_analyst", 
    expertise_areas=['quality_review', 'business_interpretation']
)

# ì›Œí¬í”Œë¡œìš° ìƒì„±
workflow = SequentialWorkflow()
workflow.register_agent(ai_agent)
workflow.register_agent(human_agent)

# ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ ì •ì˜
workflow_steps = [
    {
        'name': 'text_preprocessing',
        'task_type': 'text_preprocessing',
        'assigned_to': 'ai'
    },
    {
        'name': 'spam_classification', 
        'task_type': 'spam_classification',
        'assigned_to': 'ai'
    },
    {
        'name': 'business_interpretation',
        'task_type': 'business_interpretation', 
        'assigned_to': 'human'
    }
]

workflow.define_workflow(workflow_steps)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
test_messages = [
    "FREE money! Call now to claim your prize!",
    "Hey, how are you doing today?",
    "URGENT: Your account will be suspended unless you call 123-456-7890"
]

print("\nğŸ“¨ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬:")
for i, message in enumerate(test_messages, 1):
    print(f"\n--- ë©”ì‹œì§€ {i}: {message[:30]}... ---")
    
    initial_data = {'text': message}
    result = workflow.execute_workflow(initial_data)
    
    print(f"ì²˜ë¦¬ ë‹¨ê³„ ìˆ˜: {result['total_steps']}")
    print(f"ì„±ê³µë¥ : {result['success_rate']:.1%}")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    final_result = result['final_result']
    if 'key_insights' in final_result:
        print(f"ì£¼ìš” ì¸ì‚¬ì´íŠ¸: {final_result['key_insights'][0]}")
```

**ì½”ë“œ í•´ì„¤:**
- **ì—ì´ì „íŠ¸ ê¸°ë°˜ ì„¤ê³„**: AIì™€ ì¸ê°„ì„ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ ì¶”ìƒí™”í•˜ì—¬ ìœ ì—°í•œ í˜‘ì—… ê°€ëŠ¥
- **ì‹ ë¢°ë„ ê¸°ë°˜ ê²€í† **: AIì˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ë•Œ ìë™ìœ¼ë¡œ ì¸ê°„ ê²€í†  ìš”ì²­
- **ìƒíƒœ ì¶”ì **: ê° ì‘ì—…ì˜ ì§„í–‰ ìƒíƒœë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬
- **í™•ì¥ ê°€ëŠ¥í•œ êµ¬ì¡°**: ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ë‚˜ ì‘ì—… ìœ í˜•ì„ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥

#### **íŒ¨í„´ 2: ë³‘ë ¬ì  í˜‘ì—… (Parallel Collaboration)**
AIì™€ ì¸ê°„ì´ ë™ì‹œì— ì‘ì—…í•˜ì—¬ ê²°ê³¼ë¥¼ ê²°í•©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

```python
import asyncio
import concurrent.futures
from typing import List, Callable, Tuple
import json

class ParallelWorkflowResult:
    """ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° ê²°ê³¼"""
    
    def __init__(self):
        self.ai_results: List[Dict[str, Any]] = []
        self.human_results: List[Dict[str, Any]] = []
        self.combined_results: Dict[str, Any] = {}
        self.consensus_score: float = 0.0
        self.disagreement_areas: List[str] = []

class ParallelCollaborationEngine:
    """ë³‘ë ¬ í˜‘ì—… ì—”ì§„"""
    
    def __init__(self):
        self.ai_agents: List[AIAgent] = []
        self.human_agents: List[HumanAgent] = []
        self.combination_strategies = {
            'majority_vote': self._majority_vote_combination,
            'weighted_average': self._weighted_average_combination,
            'confidence_based': self._confidence_based_combination,
            'expert_arbitration': self._expert_arbitration_combination
        }
    
    def register_ai_agent(self, agent: AIAgent):
        """AI ì—ì´ì „íŠ¸ ë“±ë¡"""
        self.ai_agents.append(agent)
    
    def register_human_agent(self, agent: HumanAgent):
        """ì¸ê°„ ì—ì´ì „íŠ¸ ë“±ë¡"""
        self.human_agents.append(agent)
    
    async def execute_parallel_workflow(self, 
                                      task_data: Dict[str, Any],
                                      task_type: str,
                                      combination_strategy: str = 'confidence_based') -> ParallelWorkflowResult:
        """ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        
        result = ParallelWorkflowResult()
        
        # AIì™€ ì¸ê°„ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
        ai_tasks = []
        human_tasks = []
        
        # AI ì—ì´ì „íŠ¸ ì‘ì—… ìƒì„±
        for ai_agent in self.ai_agents:
            if task_type in ai_agent.capabilities:
                task = WorkflowTask(
                    task_id=f"ai_{ai_agent.agent_id}_{int(time.time())}",
                    task_type=task_type,
                    assigned_to='ai',
                    input_data=task_data
                )
                ai_tasks.append(self._execute_ai_task_async(ai_agent, task))
        
        # ì¸ê°„ ì—ì´ì „íŠ¸ ì‘ì—… ìƒì„±
        for human_agent in self.human_agents:
            if task_type in human_agent.expertise_areas:
                task = WorkflowTask(
                    task_id=f"human_{human_agent.agent_id}_{int(time.time())}",
                    task_type=task_type,
                    assigned_to='human',
                    input_data=task_data
                )
                human_tasks.append(self._execute_human_task_async(human_agent, task))
        
        # ë³‘ë ¬ ì‹¤í–‰
        if ai_tasks:
            ai_results = await asyncio.gather(*ai_tasks, return_exceptions=True)
            result.ai_results = [r for r in ai_results if not isinstance(r, Exception)]
        
        if human_tasks:
            human_results = await asyncio.gather(*human_tasks, return_exceptions=True)
            result.human_results = [r for r in human_results if not isinstance(r, Exception)]
        
        # ê²°ê³¼ ê²°í•©
        if combination_strategy in self.combination_strategies:
            combination_func = self.combination_strategies[combination_strategy]
            result.combined_results = combination_func(result.ai_results, result.human_results)
            result.consensus_score = self._calculate_consensus_score(result)
            result.disagreement_areas = self._identify_disagreement_areas(result)
        
        return result
    
    async def _execute_ai_task_async(self, agent: AIAgent, task: WorkflowTask) -> WorkflowTask:
        """AI ì‘ì—… ë¹„ë™ê¸° ì‹¤í–‰"""
        loop = asyncio.get_event_loop()
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = loop.run_in_executor(executor, agent.process_task, task)
            return await future
    
    async def _execute_human_task_async(self, agent: HumanAgent, task: WorkflowTask) -> WorkflowTask:
        """ì¸ê°„ ì‘ì—… ë¹„ë™ê¸° ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œë¡œëŠ” ì¸ê°„ì˜ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ í ì‹œìŠ¤í…œ í•„ìš”
        await asyncio.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
        return agent.process_task(task)
    
    def _majority_vote_combination(self, ai_results: List[WorkflowTask], 
                                 human_results: List[WorkflowTask]) -> Dict[str, Any]:
        """ë‹¤ìˆ˜ê²° íˆ¬í‘œ ë°©ì‹ ê²°í•©"""
        
        all_predictions = []
        
        # AI ê²°ê³¼ì—ì„œ ì˜ˆì¸¡ ìˆ˜ì§‘
        for task in ai_results:
            if task.output_data and 'prediction' in task.output_data:
                all_predictions.append(task.output_data['prediction'])
        
        # ì¸ê°„ ê²°ê³¼ì—ì„œ ì˜ˆì¸¡ ìˆ˜ì§‘
        for task in human_results:
            if task.output_data and 'reviewed_prediction' in task.output_data:
                all_predictions.append(task.output_data['reviewed_prediction'])
            elif task.output_data and 'prediction' in task.output_data:
                all_predictions.append(task.output_data['prediction'])
        
        if not all_predictions:
            return {'error': 'No valid predictions found'}
        
        # ë‹¤ìˆ˜ê²° ê³„ì‚°
        from collections import Counter
        vote_counts = Counter(all_predictions)
        final_prediction = vote_counts.most_common(1)[0][0]
        confidence = vote_counts[final_prediction] / len(all_predictions)
        
        return {
            'final_prediction': final_prediction,
            'confidence': confidence,
            'vote_breakdown': dict(vote_counts),
            'combination_method': 'majority_vote'
        }
    
    def _confidence_based_combination(self, ai_results: List[WorkflowTask], 
                                    human_results: List[WorkflowTask]) -> Dict[str, Any]:
        """ì‹ ë¢°ë„ ê¸°ë°˜ ê²°í•©"""
        
        weighted_predictions = []
        
        # AI ê²°ê³¼ ì²˜ë¦¬
        for task in ai_results:
            if task.output_data and 'prediction' in task.output_data:
                prediction = task.output_data['prediction']
                confidence = task.output_data.get('confidence', 0.5)
                weighted_predictions.append((prediction, confidence, 'ai'))
        
        # ì¸ê°„ ê²°ê³¼ ì²˜ë¦¬ (ì¸ê°„ì˜ ê²½ìš° ë†’ì€ ì‹ ë¢°ë„ ë¶€ì—¬)
        for task in human_results:
            if task.output_data and 'reviewed_prediction' in task.output_data:
                prediction = task.output_data['reviewed_prediction']
                confidence = task.output_data.get('final_confidence', 0.9)
                weighted_predictions.append((prediction, confidence, 'human'))
            elif task.output_data and 'prediction' in task.output_data:
                prediction = task.output_data['prediction']
                confidence = 0.85  # ì¸ê°„ íŒë‹¨ì— ëŒ€í•œ ê¸°ë³¸ ì‹ ë¢°ë„
                weighted_predictions.append((prediction, confidence, 'human'))
        
        if not weighted_predictions:
            return {'error': 'No valid predictions found'}
        
        # ì‹ ë¢°ë„ ê°€ì¤‘ í‰ê· 
        spam_weight = sum(conf for pred, conf, source in weighted_predictions if pred == 'spam')
        ham_weight = sum(conf for pred, conf, source in weighted_predictions if pred == 'ham')
        
        total_weight = spam_weight + ham_weight
        
        if total_weight == 0:
            final_prediction = 'unknown'
            final_confidence = 0.0
        elif spam_weight > ham_weight:
            final_prediction = 'spam'
            final_confidence = spam_weight / total_weight
        else:
            final_prediction = 'ham'
            final_confidence = ham_weight / total_weight
        
        return {
            'final_prediction': final_prediction,
            'confidence': final_confidence,
            'spam_weight': spam_weight,
            'ham_weight': ham_weight,
            'combination_method': 'confidence_based',
            'contributor_breakdown': [
                {'source': source, 'prediction': pred, 'confidence': conf}
                for pred, conf, source in weighted_predictions
            ]
        }
    
    def _weighted_average_combination(self, ai_results: List[WorkflowTask], 
                                    human_results: List[WorkflowTask]) -> Dict[str, Any]:
        """ê°€ì¤‘ í‰ê·  ê²°í•© (ìˆ«ì ê²°ê³¼ìš©)"""
        # íŠ¹ì„± ê°’ë“¤ì˜ ê°€ì¤‘ í‰ê·  ê³„ì‚° ì˜ˆì‹œ
        return {'combination_method': 'weighted_average', 'note': 'Numeric results only'}
    
    def _expert_arbitration_combination(self, ai_results: List[WorkflowTask], 
                                      human_results: List[WorkflowTask]) -> Dict[str, Any]:
        """ì „ë¬¸ê°€ ì¤‘ì¬ ê²°í•©"""
        # ë³µì¡í•œ ê²½ìš° ì „ë¬¸ê°€ê°€ ìµœì¢… íŒë‹¨
        return {'combination_method': 'expert_arbitration', 'note': 'Requires human expert decision'}
    
    def _calculate_consensus_score(self, result: ParallelWorkflowResult) -> float:
        """í•©ì˜ ì ìˆ˜ ê³„ì‚°"""
        
        all_predictions = []
        
        # AI ê²°ê³¼ì—ì„œ ì˜ˆì¸¡ ìˆ˜ì§‘
        for task in result.ai_results:
            if task.output_data and 'prediction' in task.output_data:
                all_predictions.append(task.output_data['prediction'])
        
        # ì¸ê°„ ê²°ê³¼ì—ì„œ ì˜ˆì¸¡ ìˆ˜ì§‘
        for task in result.human_results:
            if task.output_data and 'prediction' in task.output_data:
                all_predictions.append(task.output_data['prediction'])
        
        if len(all_predictions) < 2:
            return 1.0  # ë‹¨ì¼ ê²°ê³¼ì¸ ê²½ìš° ì™„ì „ í•©ì˜
        
        # ê°€ì¥ ë§ì€ ì˜ˆì¸¡ê³¼ ì¼ì¹˜í•˜ëŠ” ë¹„ìœ¨ ê³„ì‚°
        from collections import Counter
        most_common_prediction = Counter(all_predictions).most_common(1)[0][0]
        consensus_count = all_predictions.count(most_common_prediction)
        
        return consensus_count / len(all_predictions)
    
    def _identify_disagreement_areas(self, result: ParallelWorkflowResult) -> List[str]:
        """ì˜ê²¬ ë¶ˆì¼ì¹˜ ì˜ì—­ ì‹ë³„"""
        
        disagreements = []
        
        # AI vs ì¸ê°„ ì˜ˆì¸¡ ë¹„êµ
        ai_predictions = [task.output_data.get('prediction') for task in result.ai_results 
                         if task.output_data and 'prediction' in task.output_data]
        human_predictions = [task.output_data.get('prediction') for task in result.human_results
                           if task.output_data and 'prediction' in task.output_data]
        
        if ai_predictions and human_predictions:
            ai_majority = Counter(ai_predictions).most_common(1)[0][0]
            human_majority = Counter(human_predictions).most_common(1)[0][0]
            
            if ai_majority != human_majority:
                disagreements.append(f"AI ë‹¤ìˆ˜ì˜ê²¬({ai_majority}) vs ì¸ê°„ ë‹¤ìˆ˜ì˜ê²¬({human_majority})")
        
        return disagreements

# ë³‘ë ¬ í˜‘ì—… ì‹œì—°
print("\nâš¡ ë³‘ë ¬ í˜‘ì—… ì›Œí¬í”Œë¡œìš° ì‹œì—°")
print("=" * 50)

async def demo_parallel_collaboration():
    """ë³‘ë ¬ í˜‘ì—… ë°ëª¨"""
    
    # ë³‘ë ¬ í˜‘ì—… ì—”ì§„ ìƒì„±
    parallel_engine = ParallelCollaborationEngine()
    
    # ì—¬ëŸ¬ AI ì—ì´ì „íŠ¸ ë“±ë¡ (ë‹¤ì–‘í•œ ì ‘ê·¼ë²•)
    ai_agent1 = AIAgent("rule_based_ai", ['spam_classification'])
    ai_agent2 = AIAgent("ml_based_ai", ['spam_classification'])
    
    # ì¸ê°„ ì „ë¬¸ê°€ ë“±ë¡
    human_expert = HumanAgent("sms_expert", ['spam_classification'])
    
    parallel_engine.register_ai_agent(ai_agent1)
    parallel_engine.register_ai_agent(ai_agent2)
    parallel_engine.register_human_agent(human_expert)
    
    # í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
    test_message = {
        'text': "Congratulations! You've won $1000! Call 555-0123 now to claim your prize!"
    }
    
    print(f"ğŸ“¨ ë¶„ì„ ë©”ì‹œì§€: {test_message['text']}")
    
    # ë‹¤ì–‘í•œ ê²°í•© ì „ëµìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬
    strategies = ['majority_vote', 'confidence_based']
    
    for strategy in strategies:
        print(f"\nğŸ”„ {strategy} ì „ëµ ì‹¤í–‰:")
        
        result = await parallel_engine.execute_parallel_workflow(
            task_data=test_message,
            task_type='spam_classification',
            combination_strategy=strategy
        )
        
        print(f"AI ê²°ê³¼ ìˆ˜: {len(result.ai_results)}")
        print(f"ì¸ê°„ ê²°ê³¼ ìˆ˜: {len(result.human_results)}")
        print(f"í•©ì˜ ì ìˆ˜: {result.consensus_score:.2f}")
        
        if result.combined_results:
            combined = result.combined_results
            print(f"ìµœì¢… ì˜ˆì¸¡: {combined.get('final_prediction', 'N/A')}")
            print(f"ìµœì¢… ì‹ ë¢°ë„: {combined.get('confidence', 0):.3f}")
        
        if result.disagreement_areas:
            print(f"ì˜ê²¬ ë¶ˆì¼ì¹˜: {result.disagreement_areas}")

# ë¹„ë™ê¸° ë°ëª¨ ì‹¤í–‰
import asyncio
asyncio.run(demo_parallel_collaboration())
```

**ì½”ë“œ í•´ì„¤:**
- **ë¹„ë™ê¸° ì²˜ë¦¬**: AIì™€ ì¸ê°„ì˜ ì‘ì—…ì„ ë™ì‹œì— ì‹¤í–‰í•˜ì—¬ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•
- **ë‹¤ì–‘í•œ ê²°í•© ì „ëµ**: ë‹¤ìˆ˜ê²°, ì‹ ë¢°ë„ ê¸°ë°˜, ê°€ì¤‘ í‰ê·  ë“± ìƒí™©ì— ë§ëŠ” ê²°í•© ë°©ì‹ ì„ íƒ
- **í•©ì˜ ì ìˆ˜**: ì°¸ì—¬ìë“¤ ê°„ì˜ ì˜ê²¬ ì¼ì¹˜ ì •ë„ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •
- **ì˜ê²¬ ë¶ˆì¼ì¹˜ ì‹ë³„**: ì¶”ê°€ ë…¼ì˜ë‚˜ ì¡°ì •ì´ í•„ìš”í•œ ì˜ì—­ì„ ìë™ íƒì§€

#### **íŒ¨í„´ 3: ê³„ì¸µì  í˜‘ì—… (Hierarchical Collaboration)**
AIê°€ 1ì°¨ ì²˜ë¦¬ë¥¼ í•˜ê³  ì¸ê°„ì´ ê²€í† /ìŠ¹ì¸í•˜ëŠ” ê³„ì¸µ êµ¬ì¡°ì…ë‹ˆë‹¤.

```python
from typing import Protocol, Dict, Any, List
from enum import Enum

class ReviewDecision(Enum):
    """ê²€í†  ê²°ì •"""
    APPROVE = "approve"
    REJECT = "reject" 
    MODIFY = "modify"
    ESCALATE = "escalate"

class QualityGate(Protocol):
    """í’ˆì§ˆ ê²Œì´íŠ¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def evaluate(self, task_result: WorkflowTask) -> Dict[str, Any]:
        """í’ˆì§ˆ í‰ê°€"""
        ...
    
    def should_escalate(self, task_result: WorkflowTask) -> bool:
        """ìƒìœ„ ë ˆë²¨ë¡œ ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€"""
        ...

class AIQualityGate:
    """AI í’ˆì§ˆ ê²Œì´íŠ¸"""
    
    def __init__(self, thresholds: Dict[str, float]):
        self.thresholds = thresholds
    
    def evaluate(self, task_result: WorkflowTask) -> Dict[str, Any]:
        """AI ê²°ê³¼ í’ˆì§ˆ í‰ê°€"""
        
        if not task_result.output_data:
            return {
                'pass': False,
                'reason': 'No output data',
                'confidence_score': 0.0
            }
        
        confidence = task_result.output_data.get('confidence', 0.0)
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ í‰ê°€
        if confidence >= self.thresholds.get('high_confidence', 0.9):
            return {
                'pass': True,
                'level': 'high_confidence',
                'confidence_score': confidence,
                'auto_approve': True
            }
        elif confidence >= self.thresholds.get('medium_confidence', 0.7):
            return {
                'pass': True, 
                'level': 'medium_confidence',
                'confidence_score': confidence,
                'requires_review': True
            }
        else:
            return {
                'pass': False,
                'level': 'low_confidence',
                'confidence_score': confidence,
                'requires_human_review': True
            }
    
    def should_escalate(self, task_result: WorkflowTask) -> bool:
        """ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€"""
        evaluation = self.evaluate(task_result)
        return evaluation.get('requires_human_review', False)

class HumanReviewGate:
    """ì¸ê°„ ê²€í†  ê²Œì´íŠ¸"""
    
    def __init__(self, reviewer_expertise: List[str]):
        self.reviewer_expertise = reviewer_expertise
        self.review_criteria = {
            'accuracy': 0.8,
            'relevance': 0.8,
            'business_impact': 0.7
        }
    
    def evaluate(self, task_result: WorkflowTask) -> Dict[str, Any]:
        """ì¸ê°„ ê²€í†  í‰ê°€"""
        
        # ì‹œë®¬ë ˆì´ì…˜ëœ ì¸ê°„ ê²€í† 
        accuracy_score = 0.85  # ì‹¤ì œë¡œëŠ” ì¸ê°„ ê²€í† ìê°€ í‰ê°€
        relevance_score = 0.90
        business_impact_score = 0.75
        
        overall_score = (accuracy_score + relevance_score + business_impact_score) / 3
        
        decision = ReviewDecision.APPROVE
        if overall_score < 0.6:
            decision = ReviewDecision.REJECT
        elif overall_score < 0.75:
            decision = ReviewDecision.MODIFY
        
        return {
            'decision': decision,
            'overall_score': overall_score,
            'detailed_scores': {
                'accuracy': accuracy_score,
                'relevance': relevance_score,
                'business_impact': business_impact_score
            },
            'comments': self._generate_review_comments(decision, overall_score)
        }
    
    def should_escalate(self, task_result: WorkflowTask) -> bool:
        """ìƒìœ„ ê´€ë¦¬ì ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš” ì—¬ë¶€"""
        evaluation = self.evaluate(task_result)
        return evaluation['decision'] == ReviewDecision.ESCALATE
    
    def _generate_review_comments(self, decision: ReviewDecision, score: float) -> str:
        """ê²€í†  ì˜ê²¬ ìƒì„±"""
        if decision == ReviewDecision.APPROVE:
            return f"í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•©ë‹ˆë‹¤ (ì ìˆ˜: {score:.2f})"
        elif decision == ReviewDecision.MODIFY:
            return f"ì¼ë¶€ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤ (ì ìˆ˜: {score:.2f})"
        else:
            return f"ê¸°ì¤€ì— ë¯¸ë‹¬í•˜ì—¬ ì¬ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤ (ì ìˆ˜: {score:.2f})"

class HierarchicalWorkflow:
    """ê³„ì¸µì  ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self):
        self.ai_layer: List[AIAgent] = []
        self.quality_gates: List[QualityGate] = []
        self.human_reviewers: List[HumanAgent] = []
        self.escalation_levels = ['junior', 'senior', 'manager']
        self.processing_stats = {
            'total_processed': 0,
            'auto_approved': 0,
            'human_reviewed': 0,
            'escalated': 0,
            'rejected': 0
        }
    
    def add_ai_agent(self, agent: AIAgent):
        """AI ì—ì´ì „íŠ¸ ì¶”ê°€"""
        self.ai_layer.append(agent)
    
    def add_quality_gate(self, gate: QualityGate):
        """í’ˆì§ˆ ê²Œì´íŠ¸ ì¶”ê°€"""
        self.quality_gates.append(gate)
    
    def add_human_reviewer(self, reviewer: HumanAgent, level: str = 'junior'):
        """ì¸ê°„ ê²€í† ì ì¶”ê°€"""
        reviewer.review_level = level
        self.human_reviewers.append(reviewer)
    
    def process_hierarchical_workflow(self, task_data: Dict[str, Any], 
                                    task_type: str) -> Dict[str, Any]:
        """ê³„ì¸µì  ì›Œí¬í”Œë¡œìš° ì²˜ë¦¬"""
        
        self.processing_stats['total_processed'] += 1
        
        # 1ë‹¨ê³„: AI ì²˜ë¦¬
        ai_result = self._process_ai_layer(task_data, task_type)
        if not ai_result:
            return {'error': 'AI ì²˜ë¦¬ ì‹¤íŒ¨', 'stage': 'ai_processing'}
        
        # 2ë‹¨ê³„: í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼
        quality_result = self._evaluate_quality_gates(ai_result)
        
        # 3ë‹¨ê³„: ìë™ ìŠ¹ì¸ or ì¸ê°„ ê²€í† 
        if quality_result.get('auto_approve', False):
            self.processing_stats['auto_approved'] += 1
            return {
                'final_result': ai_result.output_data,
                'processing_path': 'auto_approved',
                'quality_score': quality_result.get('confidence_score', 0),
                'stage': 'completed'
            }
        
        # 4ë‹¨ê³„: ì¸ê°„ ê²€í†  ê³„ì¸µ ì²˜ë¦¬
        if quality_result.get('requires_review', False):
            review_result = self._process_human_review_hierarchy(ai_result)
            return review_result
        
        # 5ë‹¨ê³„: ê±°ë¶€ëœ ê²½ìš°
        self.processing_stats['rejected'] += 1
        return {
            'final_result': None,
            'processing_path': 'rejected',
            'quality_score': quality_result.get('confidence_score', 0),
            'stage': 'rejected'
        }
    
    def _process_ai_layer(self, task_data: Dict[str, Any], 
                         task_type: str) -> Optional[WorkflowTask]:
        """AI ê³„ì¸µ ì²˜ë¦¬"""
        
        for ai_agent in self.ai_layer:
            if ai_agent.can_handle(WorkflowTask('', task_type, 'ai', task_data)):
                task = WorkflowTask(
                    task_id=f"hierarchical_{ai_agent.agent_id}_{int(time.time())}",
                    task_type=task_type,
                    assigned_to='ai',
                    input_data=task_data
                )
                
                result = ai_agent.process_task(task)
                if result.status == TaskStatus.COMPLETED:
                    return result
        
        return None
    
    def _evaluate_quality_gates(self, task_result: WorkflowTask) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²Œì´íŠ¸ í‰ê°€"""
        
        for gate in self.quality_gates:
            evaluation = gate.evaluate(task_result)
            
            # ì²« ë²ˆì§¸ ê²Œì´íŠ¸ê°€ ê²°ì •ì 
            if not evaluation.get('pass', False):
                return evaluation
            
            # ì—ìŠ¤ì»¬ë ˆì´ì…˜ í•„ìš”í•œ ê²½ìš°
            if gate.should_escalate(task_result):
                evaluation['requires_review'] = True
            
            return evaluation
        
        # í’ˆì§ˆ ê²Œì´íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ìŠ¹ì¸
        return {'pass': True, 'auto_approve': True}
    
    def _process_human_review_hierarchy(self, task_result: WorkflowTask) -> Dict[str, Any]:
        """ì¸ê°„ ê²€í†  ê³„ì¸µ ì²˜ë¦¬"""
        
        self.processing_stats['human_reviewed'] += 1
        
        # ê²€í†  ë ˆë²¨ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        for level in self.escalation_levels:
            reviewers = [r for r in self.human_reviewers if r.review_level == level]
            
            if not reviewers:
                continue
            
            # ì²« ë²ˆì§¸ ì í•©í•œ ê²€í† ì ì„ íƒ
            reviewer = reviewers[0]
            
            # ê²€í†  ìˆ˜í–‰
            review_gate = HumanReviewGate(reviewer.expertise_areas)
            review_result = review_gate.evaluate(task_result)
            
            decision = review_result['decision']
            
            if decision == ReviewDecision.APPROVE:
                return {
                    'final_result': task_result.output_data,
                    'processing_path': f'human_approved_{level}',
                    'review_score': review_result['overall_score'],
                    'reviewer_level': level,
                    'stage': 'completed'
                }
            
            elif decision == ReviewDecision.REJECT:
                self.processing_stats['rejected'] += 1
                return {
                    'final_result': None,
                    'processing_path': f'human_rejected_{level}',
                    'review_score': review_result['overall_score'],
                    'rejection_reason': review_result['comments'],
                    'stage': 'rejected'
                }
            
            elif decision == ReviewDecision.MODIFY:
                # ìˆ˜ì • í›„ ì¬ì²˜ë¦¬ (ì‹œë®¬ë ˆì´ì…˜)
                modified_result = self._apply_modifications(task_result, review_result)
                if modified_result:
                    return {
                        'final_result': modified_result.output_data,
                        'processing_path': f'modified_and_approved_{level}',
                        'review_score': review_result['overall_score'],
                        'stage': 'completed'
                    }
            
            elif decision == ReviewDecision.ESCALATE:
                # ë‹¤ìŒ ë ˆë²¨ë¡œ ì—ìŠ¤ì»¬ë ˆì´ì…˜
                self.processing_stats['escalated'] += 1
                continue
        
        # ëª¨ë“  ë ˆë²¨ì„ ê±°ì³ë„ í•´ê²°ë˜ì§€ ì•Šì€ ê²½ìš°
        return {
            'final_result': None,
            'processing_path': 'escalation_failed',
            'stage': 'requires_management_decision'
        }
    
    def _apply_modifications(self, task_result: WorkflowTask, 
                           review_result: Dict[str, Any]) -> Optional[WorkflowTask]:
        """ìˆ˜ì •ì‚¬í•­ ì ìš© (ì‹œë®¬ë ˆì´ì…˜)"""
        
        # ì‹¤ì œë¡œëŠ” ìˆ˜ì • ì§€ì¹¨ì— ë”°ë¼ ì¬ì²˜ë¦¬
        modified_output = task_result.output_data.copy()
        modified_output['human_modified'] = True
        modified_output['modification_reason'] = review_result['comments']
        
        task_result.output_data = modified_output
        return task_result
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        
        total = self.processing_stats['total_processed']
        if total == 0:
            return self.processing_stats
        
        return {
            **self.processing_stats,
            'auto_approval_rate': self.processing_stats['auto_approved'] / total,
            'human_review_rate': self.processing_stats['human_reviewed'] / total,
            'rejection_rate': self.processing_stats['rejected'] / total,
            'escalation_rate': self.processing_stats['escalated'] / total
        }

# ê³„ì¸µì  í˜‘ì—… ì‹œì—°
print("\nğŸ¢ ê³„ì¸µì  í˜‘ì—… ì›Œí¬í”Œë¡œìš° ì‹œì—°")
print("=" * 50)

# ê³„ì¸µì  ì›Œí¬í”Œë¡œìš° ì„¤ì •
hierarchical_workflow = HierarchicalWorkflow()

# AI ì—ì´ì „íŠ¸ ì¶”ê°€
ai_classifier = AIAgent("hierarchical_ai", ['spam_classification'])
hierarchical_workflow.add_ai_agent(ai_classifier)

# í’ˆì§ˆ ê²Œì´íŠ¸ ì¶”ê°€
quality_gate = AIQualityGate({
    'high_confidence': 0.9,
    'medium_confidence': 0.7
})
hierarchical_workflow.add_quality_gate(quality_gate)

# ì¸ê°„ ê²€í† ì ì¶”ê°€ (ë‹¤ë‹¨ê³„)
junior_reviewer = HumanAgent("junior_analyst", ['spam_classification'])
senior_reviewer = HumanAgent("senior_analyst", ['spam_classification'])
manager_reviewer = HumanAgent("manager", ['spam_classification'])

hierarchical_workflow.add_human_reviewer(junior_reviewer, 'junior')
hierarchical_workflow.add_human_reviewer(senior_reviewer, 'senior')
hierarchical_workflow.add_human_reviewer(manager_reviewer, 'manager')

# ë‹¤ì–‘í•œ ì‹ ë¢°ë„ ìˆ˜ì¤€ì˜ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
test_cases = [
    {
        'text': 'Hello, how are you today?',  # ë†’ì€ ì‹ ë¢°ë„ ì˜ˆìƒ (ì •ìƒ)
        'expected_confidence': 'high'
    },
    {
        'text': 'Free money! Call now!',  # ë†’ì€ ì‹ ë¢°ë„ ì˜ˆìƒ (ìŠ¤íŒ¸)
        'expected_confidence': 'high'
    },
    {
        'text': 'Limited time offer for students',  # ì¤‘ê°„ ì‹ ë¢°ë„ ì˜ˆìƒ
        'expected_confidence': 'medium'
    },
    {
        'text': 'Complex message with ambiguous content',  # ë‚®ì€ ì‹ ë¢°ë„ ì˜ˆìƒ
        'expected_confidence': 'low'
    }
]

print(f"\nğŸ“‹ {len(test_cases)}ê°œ ë©”ì‹œì§€ ê³„ì¸µì  ì²˜ë¦¬:")

for i, test_case in enumerate(test_cases, 1):
    print(f"\n--- í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i} ---")
    print(f"ë©”ì‹œì§€: {test_case['text']}")
    print(f"ì˜ˆìƒ ì‹ ë¢°ë„: {test_case['expected_confidence']}")
    
    result = hierarchical_workflow.process_hierarchical_workflow(
        task_data={'text': test_case['text']},
        task_type='spam_classification'
    )
    
    print(f"ì²˜ë¦¬ ê²½ë¡œ: {result.get('processing_path', 'unknown')}")
    print(f"ì²˜ë¦¬ ë‹¨ê³„: {result.get('stage', 'unknown')}")
    
    if result.get('final_result'):
        final_result = result['final_result']
        print(f"ìµœì¢… ê²°ê³¼: {final_result.get('prediction', 'N/A')}")
        if 'confidence' in final_result:
            print(f"ì‹ ë¢°ë„: {final_result['confidence']:.3f}")

# ì²˜ë¦¬ í†µê³„ ì¶œë ¥
print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
stats = hierarchical_workflow.get_processing_statistics()
for key, value in stats.items():
    if key.endswith('_rate'):
        print(f"{key}: {value:.1%}")
    else:
        print(f"{key}: {value}")
```

**ì½”ë“œ í•´ì„¤:**
- **ë‹¤ë‹¨ê³„ í’ˆì§ˆ ê´€ë¦¬**: AI â†’ í’ˆì§ˆ ê²Œì´íŠ¸ â†’ ì¸ê°„ ê²€í† ì˜ ê³„ì¸µì  í’ˆì§ˆ ë³´ì¥
- **ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì²´ê³„**: ë³µì¡ë„ì— ë”°ë¼ ì ì ˆí•œ ë ˆë²¨ì˜ ì „ë¬¸ê°€ì—ê²Œ ìë™ ë°°ì •
- **ì²˜ë¦¬ í†µê³„**: ìë™í™” íš¨ìœ¨ì„±ê³¼ ì¸ê°„ ê°œì… í•„ìš”ì„±ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¶”ì 
- **ìœ ì—°í•œ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤**: ìƒí™©ì— ë”°ë¥¸ ìŠ¹ì¸, ê±°ë¶€, ìˆ˜ì •, ì—ìŠ¤ì»¬ë ˆì´ì…˜ ê²°ì •

> ğŸ’¡ **í˜‘ì—… íŒ¨í„´ ì„ íƒ ê°€ì´ë“œ**
> 
> **ìˆœì°¨ì  í˜‘ì—…**: 
> - ë‹¨ê³„ë³„ ì •í™•ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°
> - ê° ë‹¨ê³„ì˜ ê²°ê³¼ê°€ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ì´ ë˜ëŠ” ê²½ìš°
> 
> **ë³‘ë ¬ì  í˜‘ì—…**:
> - ë¹ ë¥¸ ì²˜ë¦¬ ì‹œê°„ì´ í•„ìš”í•œ ê²½ìš°
> - ë‹¤ì–‘í•œ ê´€ì ì˜ ê²€ì¦ì´ í•„ìš”í•œ ê²½ìš°
> 
> **ê³„ì¸µì  í˜‘ì—…**:
> - í’ˆì§ˆ ë³´ì¥ì´ ìµœìš°ì„ ì¸ ê²½ìš°
> - ë³µì¡ë„ì— ë”°ë¥¸ ì°¨ë³„ì  ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "ì¸ê°„-AI í˜‘ì—…ì˜ 3ê°€ì§€ íŒ¨í„´ì„ ë³´ì—¬ì£¼ëŠ” í”Œë¡œìš° ë‹¤ì´ì–´ê·¸ë¨. ì™¼ìª½ì—ëŠ” ìˆœì°¨ì (Sequential), ê°€ìš´ë°ì—ëŠ” ë³‘ë ¬ì (Parallel), ì˜¤ë¥¸ìª½ì—ëŠ” ê³„ì¸µì (Hierarchical) í˜‘ì—… íŒ¨í„´ì´ ê°ê° ë‹¤ë¥¸ ìƒ‰ìƒê³¼ í™”ì‚´í‘œ ë°©í–¥ìœ¼ë¡œ í‘œí˜„ëœ ëª¨ë˜í•œ ì›Œí¬í”Œë¡œìš° ì°¨íŠ¸"

## 3. í’ˆì§ˆ ê´€ë¦¬ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ì„¤ì •

í’ˆì§ˆ ê´€ë¦¬ëŠ” ë§ˆì¹˜ ì œì¡°ì—…ì˜ í’ˆì§ˆ ê²€ì‚¬ì™€ ê°™ìŠµë‹ˆë‹¤. ì œí’ˆì´ ì™„ì„±ë˜ê¸° ì „ ì—¬ëŸ¬ ë‹¨ê³„ì—ì„œ í’ˆì§ˆì„ í™•ì¸í•˜ì—¬ ìµœì¢… ì œí’ˆì˜ í’ˆì§ˆì„ ë³´ì¥í•©ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ì—ì„œë„ ê° ë‹¨ê³„ë§ˆë‹¤ ì ì ˆí•œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„¤ì •í•˜ì—¬ í’ˆì§ˆì„ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.

### 3.1 ì²´í¬í¬ì¸íŠ¸ ì„¤ê³„ ì›ì¹™

#### **ì›ì¹™ 1: Critical Control Points (CCP) ì‹ë³„**
ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš°ì—ì„œ í’ˆì§ˆì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ í†µì œì ì„ ì‹ë³„í•©ë‹ˆë‹¤.

```python
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import time
import json
import hashlib

class CheckpointType(Enum):
    """ì²´í¬í¬ì¸íŠ¸ ìœ í˜•"""
    DATA_QUALITY = "data_quality"
    ALGORITHM_PERFORMANCE = "algorithm_performance"
    BUSINESS_LOGIC = "business_logic"
    ETHICAL_COMPLIANCE = "ethical_compliance"
    SECURITY_VALIDATION = "security_validation"

class CheckpointSeverity(Enum):
    """ì²´í¬í¬ì¸íŠ¸ ì‹¬ê°ë„"""
    BLOCKER = "blocker"      # ì§„í–‰ ë¶ˆê°€
    CRITICAL = "critical"    # ì¦‰ì‹œ ìˆ˜ì • í•„ìš”
    MAJOR = "major"         # ìˆ˜ì • ê¶Œì¥
    MINOR = "minor"         # ê°œì„  ì œì•ˆ

@dataclass
class CheckpointResult:
    """ì²´í¬í¬ì¸íŠ¸ ê²°ê³¼"""
    checkpoint_id: str
    checkpoint_type: CheckpointType
    passed: bool
    score: float  # 0.0 ~ 1.0
    severity: CheckpointSeverity
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)
    timestamp: float = field(default_factory=time.time)

class QualityCheckpoint:
    """í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 checkpoint_id: str,
                 checkpoint_type: CheckpointType,
                 acceptance_threshold: float = 0.8):
        self.checkpoint_id = checkpoint_id
        self.checkpoint_type = checkpoint_type
        self.acceptance_threshold = acceptance_threshold
        self.validation_rules = []
    
    def add_validation_rule(self, rule_func: Callable, weight: float = 1.0):
        """ê²€ì¦ ê·œì¹™ ì¶”ê°€"""
        self.validation_rules.append({
            'function': rule_func,
            'weight': weight
        })
    
    def evaluate(self, data: Dict[str, Any]) -> CheckpointResult:
        """ì²´í¬í¬ì¸íŠ¸ í‰ê°€ ì‹¤í–‰"""
        
        if not self.validation_rules:
            return CheckpointResult(
                checkpoint_id=self.checkpoint_id,
                checkpoint_type=self.checkpoint_type,
                passed=True,
                score=1.0,
                severity=CheckpointSeverity.MINOR,
                message="ê²€ì¦ ê·œì¹™ì´ ì •ì˜ë˜ì§€ ì•ŠìŒ"
            )
        
        # ê° ê·œì¹™ë³„ ì ìˆ˜ ê³„ì‚°
        rule_scores = []
        rule_details = {}
        
        for i, rule in enumerate(self.validation_rules):
            try:
                rule_result = rule['function'](data)
                if isinstance(rule_result, dict):
                    score = rule_result.get('score', 0.0)
                    details = rule_result.get('details', {})
                else:
                    score = float(rule_result)
                    details = {}
                
                weighted_score = score * rule['weight']
                rule_scores.append(weighted_score)
                rule_details[f'rule_{i}'] = {
                    'score': score,
                    'weight': rule['weight'],
                    'weighted_score': weighted_score,
                    'details': details
                }
                
            except Exception as e:
                rule_scores.append(0.0)
                rule_details[f'rule_{i}'] = {
                    'error': str(e),
                    'score': 0.0
                }
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_weight = sum(rule['weight'] for rule in self.validation_rules)
        overall_score = sum(rule_scores) / total_weight if total_weight > 0 else 0.0
        
        # í•©ê²©/ë¶ˆí•©ê²© íŒì •
        passed = overall_score >= self.acceptance_threshold
        
        # ì‹¬ê°ë„ ê²°ì •
        if overall_score < 0.3:
            severity = CheckpointSeverity.BLOCKER
        elif overall_score < 0.6:
            severity = CheckpointSeverity.CRITICAL
        elif overall_score < 0.8:
            severity = CheckpointSeverity.MAJOR
        else:
            severity = CheckpointSeverity.MINOR
        
        # ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
        message = self._generate_message(passed, overall_score, severity)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(overall_score, rule_details)
        
        return CheckpointResult(
            checkpoint_id=self.checkpoint_id,
            checkpoint_type=self.checkpoint_type,
            passed=passed,
            score=overall_score,
            severity=severity,
            message=message,
            details=rule_details,
            recommendations=recommendations
        )
    
    def _generate_message(self, passed: bool, score: float, severity: CheckpointSeverity) -> str:
        """ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±"""
        status = "í†µê³¼" if passed else "ì‹¤íŒ¨"
        return f"{self.checkpoint_id} {status} (ì ìˆ˜: {score:.2f}, ì‹¬ê°ë„: {severity.value})"
    
    def _generate_recommendations(self, score: float, rule_details: Dict) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if score < 0.5:
            recommendations.append("ì „ì²´ì ì¸ í’ˆì§ˆ ê°œì„ ì´ ì‹œê¸‰í•©ë‹ˆë‹¤")
        
        # ì‹¤íŒ¨í•œ ê·œì¹™ë“¤ì— ëŒ€í•œ êµ¬ì²´ì  ê¶Œì¥ì‚¬í•­
        for rule_name, details in rule_details.items():
            if details.get('score', 0) < 0.6:
                recommendations.append(f"{rule_name} ê°œì„  í•„ìš”")
        
        if score >= 0.8:
            recommendations.append("í˜„ì¬ í’ˆì§ˆ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”")
        
        return recommendations

class DataQualityCheckpoint(QualityCheckpoint):
    """ë°ì´í„° í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸"""
    
    def __init__(self):
        super().__init__("data_quality_check", CheckpointType.DATA_QUALITY, 0.85)
        self._setup_validation_rules()
    
    def _setup_validation_rules(self):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê·œì¹™ ì„¤ì •"""
        
        # ì™„ì „ì„± ê²€ì‚¬
        self.add_validation_rule(self._check_completeness, weight=2.0)
        
        # ì¼ê´€ì„± ê²€ì‚¬
        self.add_validation_rule(self._check_consistency, weight=1.5)
        
        # ì •í™•ì„± ê²€ì‚¬
        self.add_validation_rule(self._check_accuracy, weight=2.0)
        
        # ìœ íš¨ì„± ê²€ì‚¬
        self.add_validation_rule(self._check_validity, weight=1.0)
    
    def _check_completeness(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì™„ì „ì„± ê²€ì‚¬ - ê²°ì¸¡ì¹˜ ë¹„ìœ¨"""
        
        dataset = data.get('dataset')
        if dataset is None:
            return {'score': 0.0, 'details': {'error': 'No dataset provided'}}
        
        try:
            import pandas as pd
            if isinstance(dataset, pd.DataFrame):
                total_cells = dataset.size
                missing_cells = dataset.isnull().sum().sum()
                completeness_ratio = 1 - (missing_cells / total_cells)
                
                return {
                    'score': completeness_ratio,
                    'details': {
                        'total_cells': total_cells,
                        'missing_cells': missing_cells,
                        'completeness_ratio': completeness_ratio
                    }
                }
            else:
                return {'score': 1.0, 'details': {'note': 'Non-dataframe data assumed complete'}}
                
        except Exception as e:
            return {'score': 0.0, 'details': {'error': str(e)}}
    
    def _check_consistency(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¼ê´€ì„± ê²€ì‚¬ - ë°ì´í„° íƒ€ì… ë° í˜•ì‹ ì¼ê´€ì„±"""
        
        dataset = data.get('dataset')
        if dataset is None:
            return {'score': 0.0, 'details': {'error': 'No dataset provided'}}
        
        try:
            import pandas as pd
            if isinstance(dataset, pd.DataFrame):
                consistency_scores = []
                
                for column in dataset.columns:
                    if dataset[column].dtype == 'object':
                        # ë¬¸ìì—´ ì»¬ëŸ¼ì˜ í˜•ì‹ ì¼ê´€ì„± í™•ì¸
                        unique_patterns = len(set(
                            str(val).strip().lower() for val in dataset[column].dropna()
                        ))
                        total_values = len(dataset[column].dropna())
                        
                        if total_values > 0:
                            pattern_ratio = unique_patterns / total_values
                            # íŒ¨í„´ì´ ë„ˆë¬´ ë‹¤ì–‘í•˜ë©´ ì¼ê´€ì„±ì´ ë‚®ìŒ
                            consistency_score = max(0, 1 - pattern_ratio * 2)
                            consistency_scores.append(consistency_score)
                
                overall_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 1.0
                
                return {
                    'score': overall_consistency,
                    'details': {
                        'column_consistency_scores': dict(zip(dataset.columns, consistency_scores)),
                        'overall_consistency': overall_consistency
                    }
                }
            else:
                return {'score': 0.8, 'details': {'note': 'Basic consistency assumed for non-dataframe'}}
                
        except Exception as e:
            return {'score': 0.0, 'details': {'error': str(e)}}
    
    def _check_accuracy(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì •í™•ì„± ê²€ì‚¬ - ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¤€ìˆ˜"""
        
        dataset = data.get('dataset')
        business_rules = data.get('business_rules', {})
        
        if dataset is None:
            return {'score': 0.0, 'details': {'error': 'No dataset provided'}}
        
        try:
            accuracy_scores = []
            
            # SMS ë©”ì‹œì§€ ê¸¸ì´ ê·œì¹™ (ì˜ˆì‹œ)
            if 'message' in dataset.columns:
                message_lengths = dataset['message'].str.len()
                valid_length_ratio = ((message_lengths >= 1) & (message_lengths <= 1600)).mean()
                accuracy_scores.append(valid_length_ratio)
            
            # ë¼ë²¨ ê°’ ìœ íš¨ì„± (ì˜ˆì‹œ)
            if 'label' in dataset.columns:
                valid_labels = business_rules.get('valid_labels', ['spam', 'ham'])
                valid_label_ratio = dataset['label'].isin(valid_labels).mean()
                accuracy_scores.append(valid_label_ratio)
            
            overall_accuracy = sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 0.8
            
            return {
                'score': overall_accuracy,
                'details': {
                    'accuracy_checks': len(accuracy_scores),
                    'overall_accuracy': overall_accuracy
                }
            }
            
        except Exception as e:
            return {'score': 0.0, 'details': {'error': str(e)}}
    
    def _check_validity(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ìœ íš¨ì„± ê²€ì‚¬ - ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜"""
        
        dataset = data.get('dataset')
        expected_schema = data.get('expected_schema', {})
        
        if dataset is None:
            return {'score': 0.0, 'details': {'error': 'No dataset provided'}}
        
        try:
            validity_scores = []
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            required_columns = expected_schema.get('required_columns', [])
            if required_columns:
                present_columns = sum(1 for col in required_columns if col in dataset.columns)
                column_validity = present_columns / len(required_columns)
                validity_scores.append(column_validity)
            
            # ë°ì´í„° íƒ€ì… í™•ì¸
            expected_dtypes = expected_schema.get('dtypes', {})
            if expected_dtypes:
                type_matches = 0
                for col, expected_dtype in expected_dtypes.items():
                    if col in dataset.columns:
                        actual_dtype = str(dataset[col].dtype)
                        if expected_dtype in actual_dtype or actual_dtype in expected_dtype:
                            type_matches += 1
                
                dtype_validity = type_matches / len(expected_dtypes)
                validity_scores.append(dtype_validity)
            
            overall_validity = sum(validity_scores) / len(validity_scores) if validity_scores else 0.9
            
            return {
                'score': overall_validity,
                'details': {
                    'validity_checks': len(validity_scores),
                    'overall_validity': overall_validity
                }
            }
            
        except Exception as e:
            return {'score': 0.0, 'details': {'error': str(e)}}

class AlgorithmPerformanceCheckpoint(QualityCheckpoint):
    """ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸"""
    
    def __init__(self):
        super().__init__("algorithm_performance_check", CheckpointType.ALGORITHM_PERFORMANCE, 0.80)
        self._setup_validation_rules()
    
    def _setup_validation_rules(self):
        """ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ê²€ì¦ ê·œì¹™ ì„¤ì •"""
        
        # ì •í™•ë„ ê²€ì‚¬
        self.add_validation_rule(self._check_accuracy_metrics, weight=2.0)
        
        # ì¼ë°˜í™” ì„±ëŠ¥ ê²€ì‚¬
        self.add_validation_rule(self._check_generalization, weight=1.5)
        
        # í¸í–¥ì„± ê²€ì‚¬
        self.add_validation_rule(self._check_bias, weight=1.0)
        
        # ì•ˆì •ì„± ê²€ì‚¬
        self.add_validation_rule(self._check_stability, weight=1.0)
    
    def _check_accuracy_metrics(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì •í™•ë„ ì§€í‘œ ê²€ì‚¬"""
        
        metrics = data.get('performance_metrics', {})
        
        if not metrics:
            return {'score': 0.0, 'details': {'error': 'No performance metrics provided'}}
        
        # ì£¼ìš” ì§€í‘œë“¤ì˜ ì„ê³„ê°’
        thresholds = {
            'accuracy': 0.85,
            'precision': 0.80,
            'recall': 0.80,
            'f1_score': 0.80
        }
        
        scores = []
        details = {}
        
        for metric_name, threshold in thresholds.items():
            if metric_name in metrics:
                metric_value = metrics[metric_name]
                metric_score = min(1.0, metric_value / threshold)
                scores.append(metric_score)
                details[metric_name] = {
                    'value': metric_value,
                    'threshold': threshold,
                    'score': metric_score
                }
        
        overall_score = sum(scores) / len(scores) if scores else 0.0
        
        return {
            'score': overall_score,
            'details': details
        }
    
    def _check_generalization(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì¼ë°˜í™” ì„±ëŠ¥ ê²€ì‚¬ - ê³¼ì í•© íƒì§€"""
        
        train_metrics = data.get('train_metrics', {})
        test_metrics = data.get('test_metrics', {})
        
        if not train_metrics or not test_metrics:
            return {'score': 0.5, 'details': {'warning': 'Insufficient data for generalization check'}}
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„
        performance_gaps = {}
        gap_scores = []
        
        common_metrics = set(train_metrics.keys()) & set(test_metrics.keys())
        
        for metric in common_metrics:
            train_val = train_metrics[metric]
            test_val = test_metrics[metric]
            
            # ì„±ëŠ¥ ì°¨ì´ (ê³¼ì í•© ì •ë„)
            gap = abs(train_val - test_val)
            gap_score = max(0, 1 - gap * 2)  # ì°¨ì´ê°€ í´ìˆ˜ë¡ ì ìˆ˜ ë‚®ìŒ
            
            performance_gaps[metric] = {
                'train': train_val,
                'test': test_val,
                'gap': gap,
                'score': gap_score
            }
            gap_scores.append(gap_score)
        
        overall_generalization = sum(gap_scores) / len(gap_scores) if gap_scores else 0.5
        
        return {
            'score': overall_generalization,
            'details': {
                'performance_gaps': performance_gaps,
                'generalization_score': overall_generalization
            }
        }
    
    def _check_bias(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """í¸í–¥ì„± ê²€ì‚¬"""
        
        predictions = data.get('predictions', [])
        true_labels = data.get('true_labels', [])
        
        if not predictions or not true_labels:
            return {'score': 0.8, 'details': {'note': 'Bias check skipped due to insufficient data'}}
        
        try:
            # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ê· í˜• í™•ì¸
            from collections import Counter
            
            # ì˜ˆì¸¡ ë¶„í¬
            pred_dist = Counter(predictions)
            true_dist = Counter(true_labels)
            
            # ë¶„í¬ ê· í˜• ì ìˆ˜ ê³„ì‚°
            total_predictions = len(predictions)
            balance_scores = []
            
            for label in true_dist.keys():
                true_ratio = true_dist[label] / len(true_labels)
                pred_ratio = pred_dist.get(label, 0) / total_predictions
                
                # ë¶„í¬ ì°¨ì´ê°€ í´ìˆ˜ë¡ í¸í–¥ì„± ë†’ìŒ
                ratio_diff = abs(true_ratio - pred_ratio)
                balance_score = max(0, 1 - ratio_diff * 2)
                balance_scores.append(balance_score)
            
            overall_balance = sum(balance_scores) / len(balance_scores) if balance_scores else 0.8
            
            return {
                'score': overall_balance,
                'details': {
                    'true_distribution': dict(true_dist),
                    'predicted_distribution': dict(pred_dist),
                    'balance_score': overall_balance
                }
            }
            
        except Exception as e:
            return {'score': 0.5, 'details': {'error': str(e)}}
    
    def _check_stability(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì•ˆì •ì„± ê²€ì‚¬ - ì˜ˆì¸¡ ì¼ê´€ì„±"""
        
        # ì—¬ëŸ¬ ì‹¤í–‰ ê²°ê³¼ ë˜ëŠ” êµì°¨ ê²€ì¦ ê²°ê³¼ í™•ì¸
        cv_scores = data.get('cross_validation_scores', [])
        
        if not cv_scores:
            return {'score': 0.7, 'details': {'note': 'Stability check skipped due to insufficient data'}}
        
        try:
            import numpy as np
            
            mean_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            
            # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì•ˆì •ì„± ë†’ìŒ
            stability_score = max(0, 1 - std_score * 5)
            
            return {
                'score': stability_score,
                'details': {
                    'cv_scores': cv_scores,
                    'mean_score': mean_score,
                    'std_score': std_score,
                    'stability_score': stability_score
                }
            }
            
        except Exception as e:
            return {'score': 0.5, 'details': {'error': str(e)}}

class QualityGateSystem:
    """í’ˆì§ˆ ê²Œì´íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.checkpoints: List[QualityCheckpoint] = []
        self.execution_history: List[Dict[str, Any]] = []
        self.global_thresholds = {
            CheckpointSeverity.BLOCKER: 0.0,     # ì°¨ë‹¨ ë ˆë²¨ì€ ì§„í–‰ ë¶ˆê°€
            CheckpointSeverity.CRITICAL: 0.6,    # ì¤‘ìš” ì´ìŠˆ ì„ê³„ê°’
            CheckpointSeverity.MAJOR: 0.8,       # ì£¼ìš” ì´ìŠˆ ì„ê³„ê°’
            CheckpointSeverity.MINOR: 0.9        # ê²½ë¯¸í•œ ì´ìŠˆ ì„ê³„ê°’
        }
    
    def add_checkpoint(self, checkpoint: QualityCheckpoint):
        """ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€"""
        self.checkpoints.append(checkpoint)
    
    def execute_all_checkpoints(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ì‹¤í–‰"""
        
        execution_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]
        checkpoint_results = []
        overall_passed = True
        blocker_count = 0
        critical_count = 0
        
        for checkpoint in self.checkpoints:
            result = checkpoint.evaluate(data)
            checkpoint_results.append(result)
            
            # ì‹¬ê°ë„ë³„ ì¹´ìš´íŠ¸
            if result.severity == CheckpointSeverity.BLOCKER:
                blocker_count += 1
                overall_passed = False
            elif result.severity == CheckpointSeverity.CRITICAL:
                critical_count += 1
                if not result.passed:
                    overall_passed = False
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        total_score = sum(result.score for result in checkpoint_results)
        average_score = total_score / len(checkpoint_results) if checkpoint_results else 0.0
        
        # ì§„í–‰ ê°€ëŠ¥ ì—¬ë¶€ ê²°ì •
        can_proceed = overall_passed and blocker_count == 0
        
        execution_summary = {
            'execution_id': execution_id,
            'timestamp': time.time(),
            'overall_passed': overall_passed,
            'can_proceed': can_proceed,
            'average_score': average_score,
            'checkpoint_results': checkpoint_results,
            'summary_stats': {
                'total_checkpoints': len(checkpoint_results),
                'passed_checkpoints': sum(1 for r in checkpoint_results if r.passed),
                'blocker_count': blocker_count,
                'critical_count': critical_count,
                'major_count': sum(1 for r in checkpoint_results if r.severity == CheckpointSeverity.MAJOR),
                'minor_count': sum(1 for r in checkpoint_results if r.severity == CheckpointSeverity.MINOR)
            },
            'recommendations': self._generate_global_recommendations(checkpoint_results)
        }
        
        # ì‹¤í–‰ ê¸°ë¡ ì €ì¥
        self.execution_history.append(execution_summary)
        
        return execution_summary
    
    def _generate_global_recommendations(self, results: List[CheckpointResult]) -> List[str]:
        """ì „ì²´ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì°¨ë‹¨ ì´ìŠˆê°€ ìˆëŠ” ê²½ìš°
        blockers = [r for r in results if r.severity == CheckpointSeverity.BLOCKER]
        if blockers:
            recommendations.append(f"ğŸš¨ {len(blockers)}ê°œì˜ ì°¨ë‹¨ ì´ìŠˆë¥¼ ì¦‰ì‹œ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤")
        
        # ì¤‘ìš” ì´ìŠˆê°€ ë§ì€ ê²½ìš°
        criticals = [r for r in results if r.severity == CheckpointSeverity.CRITICAL]
        if len(criticals) > 2:
            recommendations.append(f"âš ï¸ {len(criticals)}ê°œì˜ ì¤‘ìš” ì´ìŠˆê°€ ìˆìŠµë‹ˆë‹¤. ìš°ì„ ìˆœìœ„ë¥¼ ì •í•´ í•´ê²°í•˜ì„¸ìš”")
        
        # ì „ì²´ì ì¸ í’ˆì§ˆ ìˆ˜ì¤€ í‰ê°€
        avg_score = sum(r.score for r in results) / len(results) if results else 0
        if avg_score >= 0.9:
            recommendations.append("âœ… ì „ì²´ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í’ˆì§ˆ ìˆ˜ì¤€ì…ë‹ˆë‹¤")
        elif avg_score >= 0.7:
            recommendations.append("ğŸ‘ ì–‘í˜¸í•œ í’ˆì§ˆ ìˆ˜ì¤€ì´ì§€ë§Œ ì¼ë¶€ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤")
        else:
            recommendations.append("ğŸ“‹ ì „ì²´ì ì¸ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        return recommendations
    
    def get_quality_trends(self) -> Dict[str, Any]:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„"""
        
        if len(self.execution_history) < 2:
            return {'message': 'Insufficient data for trend analysis'}
        
        # ìµœê·¼ ì‹¤í–‰ë“¤ì˜ ì ìˆ˜ ì¶”ì´
        recent_scores = [exec['average_score'] for exec in self.execution_history[-10:]]
        
        # íŠ¸ë Œë“œ ê³„ì‚°
        if len(recent_scores) >= 2:
            trend = recent_scores[-1] - recent_scores[-2]
            if trend > 0.05:
                trend_status = 'improving'
            elif trend < -0.05:
                trend_status = 'declining'
            else:
                trend_status = 'stable'
        else:
            trend_status = 'insufficient_data'
        
        return {
            'recent_scores': recent_scores,
            'trend_status': trend_status,
            'average_score': sum(recent_scores) / len(recent_scores),
            'score_variance': sum((x - sum(recent_scores)/len(recent_scores))**2 for x in recent_scores) / len(recent_scores) if len(recent_scores) > 1 else 0
        }

# SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ í’ˆì§ˆ ê²Œì´íŠ¸ ì‹œì—°
print("\nğŸ¯ í’ˆì§ˆ ê´€ë¦¬ ì²´í¬í¬ì¸íŠ¸ ì‹œì—°")
print("=" * 50)

# í’ˆì§ˆ ê²Œì´íŠ¸ ì‹œìŠ¤í…œ ì„¤ì •
quality_system = QualityGateSystem()

# ë°ì´í„° í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
data_quality_checkpoint = DataQualityCheckpoint()
quality_system.add_checkpoint(data_quality_checkpoint)

# ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
algorithm_performance_checkpoint = AlgorithmPerformanceCheckpoint()
quality_system.add_checkpoint(algorithm_performance_checkpoint)

# ì‹œë®¬ë ˆì´ì…˜ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
import pandas as pd
import numpy as np

# SMS ë°ì´í„°ì…‹ ì‹œë®¬ë ˆì´ì…˜
sms_data = pd.DataFrame({
    'message': [
        'Hello, how are you today?',
        'FREE money! Call now!',
        'Meeting at 3pm tomorrow',
        'URGENT: Click this link immediately!',
        'Thanks for the great dinner last night'
    ],
    'label': ['ham', 'spam', 'ham', 'spam', 'ham']
})

# ì„±ëŠ¥ ì§€í‘œ ì‹œë®¬ë ˆì´ì…˜
performance_metrics = {
    'accuracy': 0.92,
    'precision': 0.88,
    'recall': 0.95,
    'f1_score': 0.91
}

# êµì°¨ ê²€ì¦ ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜
cv_scores = [0.89, 0.91, 0.90, 0.93, 0.88]

# í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ì„±
test_data = {
    'dataset': sms_data,
    'business_rules': {
        'valid_labels': ['spam', 'ham']
    },
    'expected_schema': {
        'required_columns': ['message', 'label'],
        'dtypes': {'message': 'object', 'label': 'object'}
    },
    'performance_metrics': performance_metrics,
    'train_metrics': performance_metrics,
    'test_metrics': {k: v * 0.95 for k, v in performance_metrics.items()},  # ì•½ê°„ ë‚®ì€ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
    'predictions': ['spam', 'ham', 'ham', 'spam', 'ham'],
    'true_labels': ['spam', 'ham', 'ham', 'spam', 'ham'],
    'cross_validation_scores': cv_scores
}

print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°:")
print(f"ë°ì´í„°ì…‹ í¬ê¸°: {len(sms_data)} ìƒ˜í”Œ")
print(f"ì»¬ëŸ¼: {list(sms_data.columns)}")
print(f"ì„±ëŠ¥ ì§€í‘œ: ì •í™•ë„ {performance_metrics['accuracy']:.1%}")

# í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰
print(f"\nğŸ” í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰:")
result = quality_system.execute_all_checkpoints(test_data)

print(f"ì‹¤í–‰ ID: {result['execution_id']}")
print(f"ì „ì²´ í†µê³¼ ì—¬ë¶€: {'âœ… í†µê³¼' if result['overall_passed'] else 'âŒ ì‹¤íŒ¨'}")
print(f"ì§„í–‰ ê°€ëŠ¥ ì—¬ë¶€: {'âœ… ê°€ëŠ¥' if result['can_proceed'] else 'ğŸš« ë¶ˆê°€'}")
print(f"í‰ê·  ì ìˆ˜: {result['average_score']:.3f}")

# ì²´í¬í¬ì¸íŠ¸ë³„ ê²°ê³¼
print(f"\nğŸ“‹ ì²´í¬í¬ì¸íŠ¸ë³„ ê²°ê³¼:")
for checkpoint_result in result['checkpoint_results']:
    status_icon = "âœ…" if checkpoint_result.passed else "âŒ"
    print(f"{status_icon} {checkpoint_result.checkpoint_id}")
    print(f"   ì ìˆ˜: {checkpoint_result.score:.3f}")
    print(f"   ì‹¬ê°ë„: {checkpoint_result.severity.value}")
    print(f"   ë©”ì‹œì§€: {checkpoint_result.message}")
    
    if checkpoint_result.recommendations:
        print(f"   ê¶Œì¥ì‚¬í•­:")
        for rec in checkpoint_result.recommendations[:2]:  # ìµœëŒ€ 2ê°œë§Œ í‘œì‹œ
            print(f"     â€¢ {rec}")

# ì „ì²´ ê¶Œì¥ì‚¬í•­
print(f"\nğŸ¯ ì „ì²´ ê¶Œì¥ì‚¬í•­:")
for rec in result['recommendations']:
    print(f"â€¢ {rec}")

# í†µê³„ ìš”ì•½
stats = result['summary_stats']
print(f"\nğŸ“ˆ ìš”ì•½ í†µê³„:")
print(f"ì´ ì²´í¬í¬ì¸íŠ¸: {stats['total_checkpoints']}")
print(f"í†µê³¼í•œ ì²´í¬í¬ì¸íŠ¸: {stats['passed_checkpoints']}")
print(f"ì°¨ë‹¨ ì´ìŠˆ: {stats['blocker_count']}")
print(f"ì¤‘ìš” ì´ìŠˆ: {stats['critical_count']}")
print(f"ì£¼ìš” ì´ìŠˆ: {stats['major_count']}")
print(f"ê²½ë¯¸í•œ ì´ìŠˆ: {stats['minor_count']}")
```

**ì½”ë“œ í•´ì„¤:**
- **ì²´ê³„ì  í’ˆì§ˆ ê´€ë¦¬**: ë°ì´í„° í’ˆì§ˆë¶€í„° ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ê¹Œì§€ í¬ê´„ì  í’ˆì§ˆ ê²€ì¦
- **ë‹¤ì°¨ì› í‰ê°€**: ì™„ì „ì„±, ì¼ê´€ì„±, ì •í™•ì„±, ìœ íš¨ì„± ë“± ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í’ˆì§ˆ ì¸¡ì •
- **ìë™í™”ëœ ì˜ì‚¬ê²°ì •**: ì ìˆ˜ì™€ ì‹¬ê°ë„ì— ë”°ë¥¸ ìë™ í†µê³¼/ì‹¤íŒ¨ íŒì •
- **ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ **: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆ ìë™ ìƒì„±

### 3.2 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì•ŒëŒ ì‹œìŠ¤í…œ

í’ˆì§ˆ ê´€ë¦¬ëŠ” ì¼íšŒì„±ì´ ì•„ë‹Œ ì§€ì†ì ì¸ í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ í†µí•´ í’ˆì§ˆ ì´ìƒ ìƒí™©ì„ ì¡°ê¸°ì— ê°ì§€í•˜ê³  ëŒ€ì‘í•´ì•¼ í•©ë‹ˆë‹¤.

```python
from datetime import datetime, timedelta
from typing import Optional, List
import threading
import queue
import time

class QualityAlert:
    """í’ˆì§ˆ ì•ŒëŒ"""
    
    def __init__(self, alert_id: str, severity: str, message: str, 
                 metric_name: str, current_value: float, threshold: float):
        self.alert_id = alert_id
        self.severity = severity
        self.message = message
        self.metric_name = metric_name
        self.current_value = current_value
        self.threshold = threshold
        self.timestamp = datetime.now()
        self.acknowledged = False
        self.resolved = False

class QualityMonitor:
    """ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.monitoring_active = False
        self.alert_queue = queue.Queue()
        self.monitoring_thread = None
        self.quality_metrics = {}
        self.alert_thresholds = {
            'data_completeness': 0.95,
            'model_accuracy': 0.85,
            'prediction_confidence': 0.80,
            'processing_speed': 2.0  # ì´ˆë‹¹ ì²˜ë¦¬ ê±´ìˆ˜
        }
        self.alert_handlers = []
    
    def add_alert_handler(self, handler_func):
        """ì•ŒëŒ í•¸ë“¤ëŸ¬ ì¶”ê°€"""
        self.alert_handlers.append(handler_func)
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitoring_thread = threading.Thread(target=self._monitor_loop)
            self.monitoring_thread.daemon = True
            self.monitoring_thread.start()
            print("ğŸ” í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
        print("â¹ï¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def update_metric(self, metric_name: str, value: float):
        """í’ˆì§ˆ ì§€í‘œ ì—…ë°ì´íŠ¸"""
        self.quality_metrics[metric_name] = {
            'value': value,
            'timestamp': datetime.now()
        }
    
    def _monitor_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            self._check_thresholds()
            time.sleep(1)  # 1ì´ˆë§ˆë‹¤ ì²´í¬
    
    def _check_thresholds(self):
        """ì„ê³„ê°’ í™•ì¸"""
        current_time = datetime.now()
        
        for metric_name, threshold in self.alert_thresholds.items():
            if metric_name in self.quality_metrics:
                metric_data = self.quality_metrics[metric_name]
                current_value = metric_data['value']
                metric_time = metric_data['timestamp']
                
                # ìµœê·¼ ë°ì´í„°ë§Œ í™•ì¸ (5ë¶„ ì´ë‚´)
                if current_time - metric_time < timedelta(minutes=5):
                    if self._is_threshold_violated(metric_name, current_value, threshold):
                        alert = self._create_alert(metric_name, current_value, threshold)
                        self.alert_queue.put(alert)
                        self._notify_handlers(alert)
    
    def _is_threshold_violated(self, metric_name: str, current_value: float, threshold: float) -> bool:
        """ì„ê³„ê°’ ìœ„ë°˜ í™•ì¸"""
        
        if metric_name in ['data_completeness', 'model_accuracy', 'prediction_confidence']:
            # ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ
            return current_value < threshold
        elif metric_name == 'processing_speed':
            # ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ì§€í‘œ (ì²˜ë¦¬ ì‹œê°„)
            return current_value > threshold
        
        return False
    
    def _create_alert(self, metric_name: str, current_value: float, threshold: float) -> QualityAlert:
        """ì•ŒëŒ ìƒì„±"""
        
        alert_id = f"{metric_name}_{int(time.time())}"
        
        # ì‹¬ê°ë„ ê²°ì •
        if metric_name in ['model_accuracy', 'data_completeness']:
            if current_value < threshold * 0.8:
                severity = 'critical'
            elif current_value < threshold * 0.9:
                severity = 'major'
            else:
                severity = 'minor'
        else:
            severity = 'minor'
        
        # ë©”ì‹œì§€ ìƒì„±
        if metric_name == 'data_completeness':
            message = f"ë°ì´í„° ì™„ì „ì„±ì´ ì„ê³„ê°’ ì´í•˜ì…ë‹ˆë‹¤ ({current_value:.1%} < {threshold:.1%})"
        elif metric_name == 'model_accuracy':
            message = f"ëª¨ë¸ ì •í™•ë„ê°€ ì„ê³„ê°’ ì´í•˜ì…ë‹ˆë‹¤ ({current_value:.1%} < {threshold:.1%})"
        elif metric_name == 'prediction_confidence':
            message = f"ì˜ˆì¸¡ ì‹ ë¢°ë„ê°€ ì„ê³„ê°’ ì´í•˜ì…ë‹ˆë‹¤ ({current_value:.1%} < {threshold:.1%})"
        elif metric_name == 'processing_speed':
            message = f"ì²˜ë¦¬ ì†ë„ê°€ ì„ê³„ê°’ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤ ({current_value:.1f}ì´ˆ > {threshold:.1f}ì´ˆ)"
        else:
            message = f"{metric_name} ì§€í‘œê°€ ì„ê³„ê°’ì„ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤"
        
        return QualityAlert(alert_id, severity, message, metric_name, current_value, threshold)
    
    def _notify_handlers(self, alert: QualityAlert):
        """ì•ŒëŒ í•¸ë“¤ëŸ¬ì— í†µì§€"""
        for handler in self.alert_handlers:
            try:
                handler(alert)
            except Exception as e:
                print(f"ì•ŒëŒ í•¸ë“¤ëŸ¬ ì˜¤ë¥˜: {e}")
    
    def get_active_alerts(self) -> List[QualityAlert]:
        """í™œì„± ì•ŒëŒ ëª©ë¡ ë°˜í™˜"""
        alerts = []
        while not self.alert_queue.empty():
            try:
                alert = self.alert_queue.get_nowait()
                if not alert.resolved:
                    alerts.append(alert)
            except queue.Empty:
                break
        return alerts

# ì•ŒëŒ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë“¤
def console_alert_handler(alert: QualityAlert):
    """ì½˜ì†” ì•ŒëŒ í•¸ë“¤ëŸ¬"""
    severity_icons = {
        'critical': 'ğŸš¨',
        'major': 'âš ï¸',
        'minor': 'ğŸ’¡'
    }
    
    icon = severity_icons.get(alert.severity, 'ğŸ“¢')
    print(f"{icon} [{alert.severity.upper()}] {alert.message}")
    print(f"   ì‹œê°„: {alert.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"   ì§€í‘œ: {alert.metric_name}")
    print(f"   í˜„ì¬ê°’: {alert.current_value:.3f}")
    print(f"   ì„ê³„ê°’: {alert.threshold:.3f}")

def email_alert_handler(alert: QualityAlert):
    """ì´ë©”ì¼ ì•ŒëŒ í•¸ë“¤ëŸ¬ (ì‹œë®¬ë ˆì´ì…˜)"""
    if alert.severity in ['critical', 'major']:
        print(f"ğŸ“§ ì´ë©”ì¼ ë°œì†¡: {alert.message}")

def slack_alert_handler(alert: QualityAlert):
    """Slack ì•ŒëŒ í•¸ë“¤ëŸ¬ (ì‹œë®¬ë ˆì´ì…˜)"""
    print(f"ğŸ’¬ Slack ë©”ì‹œì§€: {alert.message}")

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì—°
print("\nğŸ“Š ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì—°")
print("=" * 50)

# í’ˆì§ˆ ëª¨ë‹ˆí„° ì„¤ì •
monitor = QualityMonitor()
monitor.add_alert_handler(console_alert_handler)
monitor.add_alert_handler(email_alert_handler)
monitor.add_alert_handler(slack_alert_handler)

# ëª¨ë‹ˆí„°ë§ ì‹œì‘
monitor.start_monitoring()

# ì •ìƒì ì¸ ì§€í‘œë“¤ ì—…ë°ì´íŠ¸
print("âœ… ì •ìƒ ìƒíƒœ ì§€í‘œ ì—…ë°ì´íŠ¸:")
monitor.update_metric('data_completeness', 0.98)
monitor.update_metric('model_accuracy', 0.92)
monitor.update_metric('prediction_confidence', 0.87)
monitor.update_metric('processing_speed', 1.2)

time.sleep(2)  # ëª¨ë‹ˆí„°ë§ì´ ì‹¤í–‰ë  ì‹œê°„ ì œê³µ

# ë¬¸ì œ ìƒí™© ì‹œë®¬ë ˆì´ì…˜
print("\nğŸš¨ ë¬¸ì œ ìƒí™© ì‹œë®¬ë ˆì´ì…˜:")
print("ë°ì´í„° ì™„ì „ì„± ì €í•˜...")
monitor.update_metric('data_completeness', 0.85)  # ì„ê³„ê°’ ì´í•˜

time.sleep(1)

print("ëª¨ë¸ ì •í™•ë„ ê¸‰ë½...")
monitor.update_metric('model_accuracy', 0.75)  # ì„ê³„ê°’ ì´í•˜

time.sleep(1)

print("ì²˜ë¦¬ ì†ë„ ì €í•˜...")
monitor.update_metric('processing_speed', 3.5)  # ì„ê³„ê°’ ì´ˆê³¼

time.sleep(3)  # ì•ŒëŒì´ ë°œìƒí•  ì‹œê°„ ì œê³µ

# í™œì„± ì•ŒëŒ í™•ì¸
active_alerts = monitor.get_active_alerts()
print(f"\nğŸ“‹ í™œì„± ì•ŒëŒ ìˆ˜: {len(active_alerts)}")

# ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
monitor.stop_monitoring()
```

**ì½”ë“œ í•´ì„¤:**
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì§€ì†ì ìœ¼ë¡œ í’ˆì§ˆ ì§€í‘œ ê°ì‹œ
- **ë‹¤ë‹¨ê³„ ì•ŒëŒ**: ì‹¬ê°ë„ì— ë”°ë¥¸ ì°¨ë³„ì  ì•ŒëŒ ì²˜ë¦¬
- **ë‹¤ì¤‘ ì±„ë„ í†µì§€**: ì½˜ì†”, ì´ë©”ì¼, Slack ë“± ë‹¤ì–‘í•œ ì±„ë„ë¡œ ì•ŒëŒ ì „ì†¡
- **ì„ê³„ê°’ ê¸°ë°˜ ìë™ ê°ì§€**: ì„¤ì •ëœ ì„ê³„ê°’ ê¸°ì¤€ìœ¼ë¡œ ìë™ ì´ìƒ ìƒí™© íƒì§€

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "í’ˆì§ˆ ê´€ë¦¬ ëŒ€ì‹œë³´ë“œë¥¼ ë³´ì—¬ì£¼ëŠ” ëª¨ë‹ˆí„°ë§ í™”ë©´. ì—¬ëŸ¬ ê°œì˜ ê²Œì´ì§€ ì°¨íŠ¸ë¡œ ë°ì´í„° ì™„ì „ì„±, ëª¨ë¸ ì •í™•ë„, ì²˜ë¦¬ ì†ë„ ë“±ì˜ ì§€í‘œê°€ í‘œì‹œë˜ê³ , ìš°ì¸¡ì—ëŠ” ì‹¤ì‹œê°„ ì•ŒëŒ íŒ¨ë„ì´ ìˆëŠ” í˜„ëŒ€ì ì¸ ì¸í„°í˜ì´ìŠ¤"

## 4. ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: SMS ìŠ¤íŒ¸ íƒì§€ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•

ì´ì œ ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ì— ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

### 4.1 í”„ë¡œì íŠ¸ ì„¤ê³„

```python
from typing import Dict, List, Any, Optional
import time
from dataclasses import dataclass

@dataclass
class WorkflowConfig:
    """ì›Œí¬í”Œë¡œìš° ì„¤ì •"""
    automation_strategy: str = 'hybrid'  # 'full_auto', 'human_led', 'hybrid'
    quality_threshold: float = 0.85
    confidence_threshold: float = 0.80
    max_processing_time: int = 300  # ì´ˆ
    enable_monitoring: bool = True
    
class SMSSpamDetectionWorkflow:
    """SMS ìŠ¤íŒ¸ íƒì§€ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, config: WorkflowConfig):
        self.config = config
        self.workflow_components = {}
        self.quality_monitor = QualityMonitor() if config.enable_monitoring else None
        self.processing_stats = {
            'total_processed': 0,
            'auto_processed': 0,
            'human_reviewed': 0,
            'quality_issues': 0
        }
    
    def setup_workflow(self):
        """ì›Œí¬í”Œë¡œìš° êµ¬ì„± ìš”ì†Œ ì„¤ì •"""
        
        # 1. STAR í‰ê°€ê¸°
        self.workflow_components['star_analyzer'] = TaskStandardizationAnalyzer()
        
        # 2. AI ì—ì´ì „íŠ¸ë“¤
        self.workflow_components['ai_agents'] = [
            AIAgent('text_processor', ['text_preprocessing']),
            AIAgent('spam_classifier', ['spam_classification']),
            AIAgent('feature_extractor', ['feature_extraction'])
        ]
        
        # 3. ì¸ê°„ ê²€í† ìë“¤
        self.workflow_components['human_reviewers'] = [
            HumanAgent('junior_analyst', ['quality_review']),
            HumanAgent('senior_analyst', ['business_interpretation'])
        ]
        
        # 4. í’ˆì§ˆ ê²Œì´íŠ¸
        self.workflow_components['quality_gates'] = [
            DataQualityCheckpoint(),
            AlgorithmPerformanceCheckpoint()
        ]
        
        # 5. í˜‘ì—… ì—”ì§„
        self.workflow_components['collaboration_engine'] = ParallelCollaborationEngine()
        for agent in self.workflow_components['ai_agents']:
            self.workflow_components['collaboration_engine'].register_ai_agent(agent)
        for reviewer in self.workflow_components['human_reviewers']:
            self.workflow_components['collaboration_engine'].register_human_reviewer(reviewer)
        
        # 6. ëª¨ë‹ˆí„°ë§ ì„¤ì •
        if self.quality_monitor:
            self.quality_monitor.add_alert_handler(console_alert_handler)
            self.quality_monitor.start_monitoring()
        
        print("ğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì„¤ì • ì™„ë£Œ")
    
    async def process_sms_batch(self, sms_data: List[Dict[str, str]]) -> Dict[str, Any]:
        """SMS ë°°ì¹˜ ì²˜ë¦¬"""
        
        start_time = time.time()
        results = []
        
        print(f"ğŸ“¨ {len(sms_data)}ê°œ SMS ë©”ì‹œì§€ ì²˜ë¦¬ ì‹œì‘")
        
        for i, sms in enumerate(sms_data):
            print(f"\n--- ë©”ì‹œì§€ {i+1}/{len(sms_data)} ---")
            print(f"ë‚´ìš©: {sms['text'][:50]}{'...' if len(sms['text']) > 50 else ''}")
            
            # 1ë‹¨ê³„: ìë™í™” ì í•©ì„± í‰ê°€
            task_profile = {
                'input_types': ['text'],
                'steps_variability': 'low',
                'output_variance': 'medium',
                'rule_complexity': 'moderate'
            }
            
            star_result = self.workflow_components['star_analyzer'].evaluate_standardization(task_profile)
            automation_recommendation = star_result['automation_recommendation']
            
            print(f"ìë™í™” ê¶Œê³ : {automation_recommendation}")
            
            # 2ë‹¨ê³„: ì›Œí¬í”Œë¡œìš° ë¶„ê¸°
            if automation_recommendation in ['strongly_recommended', 'recommended']:
                # AI ìš°ì„  ì²˜ë¦¬
                message_result = await self._process_with_ai_first(sms)
            elif automation_recommendation == 'partial_automation':
                # ë³‘ë ¬ í˜‘ì—… ì²˜ë¦¬
                message_result = await self._process_with_collaboration(sms)
            else:
                # ì¸ê°„ ìš°ì„  ì²˜ë¦¬
                message_result = await self._process_with_human_first(sms)
            
            # 3ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦
            quality_result = self._validate_quality(message_result)
            
            # 4ë‹¨ê³„: ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸
            if self.quality_monitor:
                self._update_monitoring_metrics(message_result, quality_result)
            
            # ê²°ê³¼ ì €ì¥
            results.append({
                'message_id': i + 1,
                'input': sms,
                'automation_strategy': automation_recommendation,
                'processing_result': message_result,
                'quality_result': quality_result
            })
            
            self.processing_stats['total_processed'] += 1
        
        processing_time = time.time() - start_time
        
        # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        summary = self._generate_batch_summary(results, processing_time)
        
        return {
            'results': results,
            'summary': summary,
            'processing_stats': self.processing_stats
        }
    
    async def _process_with_ai_first(self, sms: Dict[str, str]) -> Dict[str, Any]:
        """AI ìš°ì„  ì²˜ë¦¬"""
        
        print("ğŸ¤– AI ìš°ì„  ì²˜ë¦¬ ëª¨ë“œ")
        
        # AI ì²˜ë¦¬
        ai_agent = self.workflow_components['ai_agents'][1]  # spam_classifier
        task = WorkflowTask(
            task_id=f"ai_first_{int(time.time())}",
            task_type='spam_classification',
            assigned_to='ai',
            input_data=sms
        )
        
        ai_result = ai_agent.process_task(task)
        
        # ì‹ ë¢°ë„ í™•ì¸
        confidence = ai_result.output_data.get('confidence', 0) if ai_result.output_data else 0
        
        if confidence >= self.config.confidence_threshold:
            # ìë™ ìŠ¹ì¸
            self.processing_stats['auto_processed'] += 1
            return {
                'final_decision': ai_result.output_data.get('prediction'),
                'confidence': confidence,
                'processing_path': 'ai_auto_approved',
                'details': ai_result.output_data
            }
        else:
            # ì¸ê°„ ê²€í†  í•„ìš”
            human_reviewer = self.workflow_components['human_reviewers'][0]
            review_result = human_reviewer.process_task(ai_result)
            
            self.processing_stats['human_reviewed'] += 1
            return {
                'final_decision': review_result.output_data.get('reviewed_prediction', 
                                                               ai_result.output_data.get('prediction')),
                'confidence': review_result.output_data.get('final_confidence', confidence),
                'processing_path': 'ai_with_human_review',
                'details': {
                    'ai_result': ai_result.output_data,
                    'human_review': review_result.output_data
                }
            }
    
    async def _process_with_collaboration(self, sms: Dict[str, str]) -> Dict[str, Any]:
        """ë³‘ë ¬ í˜‘ì—… ì²˜ë¦¬"""
        
        print("ğŸ¤ ë³‘ë ¬ í˜‘ì—… ì²˜ë¦¬ ëª¨ë“œ")
        
        collaboration_result = await self.workflow_components['collaboration_engine'].execute_parallel_workflow(
            task_data=sms,
            task_type='spam_classification',
            combination_strategy='confidence_based'
        )
        
        combined = collaboration_result.combined_results
        
        return {
            'final_decision': combined.get('final_prediction'),
            'confidence': combined.get('confidence', 0),
            'processing_path': 'parallel_collaboration',
            'details': {
                'ai_results': len(collaboration_result.ai_results),
                'human_results': len(collaboration_result.human_results),
                'consensus_score': collaboration_result.consensus_score,
                'combination_details': combined
            }
        }
    
    async def _process_with_human_first(self, sms: Dict[str, str]) -> Dict[str, Any]:
        """ì¸ê°„ ìš°ì„  ì²˜ë¦¬"""
        
        print("ğŸ‘¨â€ğŸ’¼ ì¸ê°„ ìš°ì„  ì²˜ë¦¬ ëª¨ë“œ")
        
        # ì¸ê°„ ë¶„ì„ê°€ê°€ ì£¼ë„ì ìœ¼ë¡œ ì²˜ë¦¬
        human_analyst = self.workflow_components['human_reviewers'][-1]  # senior_analyst
        
        task = WorkflowTask(
            task_id=f"human_first_{int(time.time())}",
            task_type='spam_classification',
            assigned_to='human',
            input_data=sms
        )
        
        human_result = human_analyst.process_task(task)
        self.processing_stats['human_reviewed'] += 1
        
        return {
            'final_decision': human_result.output_data.get('prediction', 'unknown'),
            'confidence': 0.9,  # ì¸ê°„ íŒë‹¨ì— ë†’ì€ ì‹ ë¢°ë„ ë¶€ì—¬
            'processing_path': 'human_led',
            'details': human_result.output_data
        }
    
    def _validate_quality(self, processing_result: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²€ì¦"""
        
        # ê°„ë‹¨í•œ í’ˆì§ˆ ê²€ì¦
        quality_score = 1.0
        issues = []
        
        # ì‹ ë¢°ë„ ê²€ì‚¬
        confidence = processing_result.get('confidence', 0)
        if confidence < 0.7:
            quality_score -= 0.3
            issues.append("ë‚®ì€ ì‹ ë¢°ë„")
        
        # ì²˜ë¦¬ ê²½ë¡œ ê²€ì‚¬
        processing_path = processing_result.get('processing_path', '')
        if 'auto' in processing_path and confidence < 0.9:
            quality_score -= 0.2
            issues.append("ìë™ ì²˜ë¦¬ì— ë¹„í•´ ë‚®ì€ ì‹ ë¢°ë„")
        
        if quality_score < self.config.quality_threshold:
            self.processing_stats['quality_issues'] += 1
        
        return {
            'quality_score': quality_score,
            'passed': quality_score >= self.config.quality_threshold,
            'issues': issues
        }
    
    def _update_monitoring_metrics(self, processing_result: Dict[str, Any], quality_result: Dict[str, Any]):
        """ëª¨ë‹ˆí„°ë§ ì§€í‘œ ì—…ë°ì´íŠ¸"""
        
        if self.quality_monitor:
            confidence = processing_result.get('confidence', 0)
            quality_score = quality_result.get('quality_score', 0)
            
            self.quality_monitor.update_metric('prediction_confidence', confidence)
            self.quality_monitor.update_metric('overall_quality', quality_score)
    
    def _generate_batch_summary(self, results: List[Dict[str, Any]], processing_time: float) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½ ìƒì„±"""
        
        total_messages = len(results)
        
        # ì²˜ë¦¬ ê²½ë¡œë³„ í†µê³„
        path_counts = {}
        for result in results:
            path = result['processing_result']['processing_path']
            path_counts[path] = path_counts.get(path, 0) + 1
        
        # í’ˆì§ˆ í†µê³„
        quality_passed = sum(1 for r in results if r['quality_result']['passed'])
        avg_confidence = sum(r['processing_result'].get('confidence', 0) for r in results) / total_messages
        
        # ì˜ˆì¸¡ ê²°ê³¼ í†µê³„
        predictions = [r['processing_result']['final_decision'] for r in results]
        from collections import Counter
        prediction_counts = Counter(predictions)
        
        return {
            'total_messages': total_messages,
            'processing_time': processing_time,
            'avg_time_per_message': processing_time / total_messages,
            'processing_paths': path_counts,
            'quality_stats': {
                'passed_count': quality_passed,
                'pass_rate': quality_passed / total_messages,
                'avg_confidence': avg_confidence
            },
            'prediction_stats': dict(prediction_counts),
            'efficiency_metrics': {
                'auto_processing_rate': self.processing_stats['auto_processed'] / total_messages,
                'human_review_rate': self.processing_stats['human_reviewed'] / total_messages,
                'quality_issue_rate': self.processing_stats['quality_issues'] / total_messages
            }
        }
    
    def cleanup(self):
        """ì›Œí¬í”Œë¡œìš° ì •ë¦¬"""
        if self.quality_monitor:
            self.quality_monitor.stop_monitoring()
        print("ğŸ§¹ ì›Œí¬í”Œë¡œìš° ì •ë¦¬ ì™„ë£Œ")

# SMS ìŠ¤íŒ¸ íƒì§€ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì‹œì—°
print("\nğŸš€ SMS ìŠ¤íŒ¸ íƒì§€ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì‹œì—°")
print("=" * 60)

# ì›Œí¬í”Œë¡œìš° ì„¤ì •
config = WorkflowConfig(
    automation_strategy='hybrid',
    quality_threshold=0.8,
    confidence_threshold=0.85,
    enable_monitoring=True
)

# ì›Œí¬í”Œë¡œìš° ìƒì„± ë° ì„¤ì •
workflow = SMSSpamDetectionWorkflow(config)
workflow.setup_workflow()

# í…ŒìŠ¤íŠ¸ SMS ë°ì´í„°
test_sms_data = [
    {'text': 'Hello, how are you doing today?'},  # ëª…í™•í•œ ì •ìƒ ë©”ì‹œì§€
    {'text': 'FREE money! Call 123-456-7890 NOW!'},  # ëª…í™•í•œ ìŠ¤íŒ¸
    {'text': 'Limited time offer for students only'},  # ì• ë§¤í•œ ê²½ìš°
    {'text': 'URGENT: Your account needs verification'},  # ì• ë§¤í•œ ê²½ìš°
    {'text': 'Thanks for the dinner yesterday!'}  # ëª…í™•í•œ ì •ìƒ ë©”ì‹œì§€
]

# ë¹„ë™ê¸° ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
import asyncio

async def run_workflow_demo():
    result = await workflow.process_sms_batch(test_sms_data)
    
    print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 40)
    
    summary = result['summary']
    print(f"ì´ ë©”ì‹œì§€ ìˆ˜: {summary['total_messages']}")
    print(f"ì²˜ë¦¬ ì‹œê°„: {summary['processing_time']:.2f}ì´ˆ")
    print(f"ë©”ì‹œì§€ë‹¹ í‰ê·  ì‹œê°„: {summary['avg_time_per_message']:.2f}ì´ˆ")
    
    print(f"\nğŸ“ˆ ì²˜ë¦¬ ê²½ë¡œë³„ í†µê³„:")
    for path, count in summary['processing_paths'].items():
        print(f"  {path}: {count}ê°œ ({count/summary['total_messages']:.1%})")
    
    print(f"\nâœ… í’ˆì§ˆ í†µê³„:")
    quality_stats = summary['quality_stats']
    print(f"  í’ˆì§ˆ í†µê³¼: {quality_stats['passed_count']}/{summary['total_messages']} ({quality_stats['pass_rate']:.1%})")
    print(f"  í‰ê·  ì‹ ë¢°ë„: {quality_stats['avg_confidence']:.3f}")
    
    print(f"\nğŸ¯ ì˜ˆì¸¡ ê²°ê³¼:")
    for prediction, count in summary['prediction_stats'].items():
        print(f"  {prediction}: {count}ê°œ")
    
    print(f"\nâš¡ íš¨ìœ¨ì„± ì§€í‘œ:")
    efficiency = summary['efficiency_metrics']
    print(f"  ìë™ ì²˜ë¦¬ìœ¨: {efficiency['auto_processing_rate']:.1%}")
    print(f"  ì¸ê°„ ê²€í† ìœ¨: {efficiency['human_review_rate']:.1%}")
    print(f"  í’ˆì§ˆ ì´ìŠˆìœ¨: {efficiency['quality_issue_rate']:.1%}")
    
    # ê°œë³„ ê²°ê³¼ ìƒì„¸ ë³´ê¸°
    print(f"\nğŸ“‹ ê°œë³„ ë©”ì‹œì§€ ê²°ê³¼:")
    for result_item in result['results'][:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
        msg_id = result_item['message_id']
        processing = result_item['processing_result']
        quality = result_item['quality_result']
        
        print(f"\n  ë©”ì‹œì§€ {msg_id}:")
        print(f"    ì˜ˆì¸¡: {processing['final_decision']}")
        print(f"    ì‹ ë¢°ë„: {processing['confidence']:.3f}")
        print(f"    ì²˜ë¦¬ ê²½ë¡œ: {processing['processing_path']}")
        print(f"    í’ˆì§ˆ ì ìˆ˜: {quality['quality_score']:.3f}")
        if quality['issues']:
            print(f"    í’ˆì§ˆ ì´ìŠˆ: {', '.join(quality['issues'])}")

# ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
asyncio.run(run_workflow_demo())

# ì •ë¦¬
workflow.cleanup()
```

**ì½”ë“œ í•´ì„¤:**
- **í†µí•©ì  ì›Œí¬í”Œë¡œìš°**: STAR í‰ê°€ë¶€í„° í’ˆì§ˆ ê´€ë¦¬ê¹Œì§€ ì „ì²´ ê³¼ì • í†µí•©
- **ì ì‘ì  ì²˜ë¦¬**: ë©”ì‹œì§€ íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  ì²˜ë¦¬ ê²½ë¡œ ìë™ ì„ íƒ
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì²˜ë¦¬ ê³¼ì • ì¤‘ í’ˆì§ˆ ì§€í‘œ ì‹¤ì‹œê°„ ì¶”ì 
- **ì¢…í•©ì  í‰ê°€**: íš¨ìœ¨ì„±, í’ˆì§ˆ, ì‚¬ìš©ì ë§Œì¡±ë„ ë“± ë‹¤ì°¨ì› ì„±ê³¼ ì¸¡ì •

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "SMS ìŠ¤íŒ¸ íƒì§€ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš°ì˜ ì „ì²´ ì•„í‚¤í…ì²˜ë¥¼ ë³´ì—¬ì£¼ëŠ” í”Œë¡œìš°ì°¨íŠ¸. ì…ë ¥ SMSì—ì„œ ì‹œì‘í•˜ì—¬ STAR í‰ê°€, AI/ì¸ê°„ í˜‘ì—…, í’ˆì§ˆ ê²Œì´íŠ¸, ëª¨ë‹ˆí„°ë§ì„ ê±°ì³ ìµœì¢… ê²°ê³¼ì— ì´ë¥´ëŠ” ì™„ì „í•œ í”„ë¡œì„¸ìŠ¤ê°€ í™”ì‚´í‘œì™€ ë¶„ê¸°ì ìœ¼ë¡œ í‘œí˜„ëœ ì‹œìŠ¤í…œ ë‹¤ì´ì–´ê·¸ë¨"

## ì§ì ‘ í•´ë³´ê¸° / ì—°ìŠµ ë¬¸ì œ

### **ì—°ìŠµ ë¬¸ì œ 1: ìë™í™” ì „ëµ ì„¤ê³„ (ì´ˆê¸‰)**

ë‹¤ìŒ ë°ì´í„° ë¶„ì„ ì‘ì—…ë“¤ì— ëŒ€í•´ STAR í”„ë ˆì„ì›Œí¬ë¥¼ ì ìš©í•˜ì—¬ ìµœì ì˜ ìë™í™” ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

**ì‘ì—… ì‹œë‚˜ë¦¬ì˜¤:**
1. **ê³ ê° ë§Œì¡±ë„ ì„¤ë¬¸ ë¶„ì„**: ë§¤ì›” 1,000ê°œì˜ ì„¤ë¬¸ ì‘ë‹µ ë¶„ì„
2. **ê¸ˆìœµ ì´ìƒ ê±°ë˜ íƒì§€**: ì‹¤ì‹œê°„ìœ¼ë¡œ ì´ˆë‹¹ 100ê±´ì˜ ê±°ë˜ ëª¨ë‹ˆí„°ë§
3. **ë§ˆì¼€íŒ… ìº í˜ì¸ íš¨ê³¼ ë¶„ì„**: ë¶„ê¸°ë³„ ë‹¤ì–‘í•œ ì±„ë„ì˜ ìº í˜ì¸ ì„±ê³¼ ì¢…í•© ë¶„ì„

**ìš”êµ¬ì‚¬í•­:**
- ê° ì‘ì—…ì— ëŒ€í•´ STAR(Standardization, Time sensitivity, Accuracy requirements, Resource requirements) ì ìˆ˜ ì‚°ì •
- ê¶Œì¥ ìë™í™” ìˆ˜ì¤€ê³¼ ê·¼ê±° ì œì‹œ
- ì¸ê°„-AI í˜‘ì—… ë°©ì‹ ì œì•ˆ

```python
# ì—¬ëŸ¬ë¶„ì˜ ë‹µì•ˆì„ ì‘ì„±í•˜ì„¸ìš”
def analyze_automation_strategy():
    # STAR í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•œ ë¶„ì„ ì½”ë“œ
    pass
```

### **ì—°ìŠµ ë¬¸ì œ 2: í˜‘ì—… íŒ¨í„´ êµ¬í˜„ (ì¤‘ê¸‰)**

ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì¸ê°„-AI í˜‘ì—… íŒ¨í„´ì„ êµ¬í˜„í•˜ì„¸ìš”.

**ì‹œë‚˜ë¦¬ì˜¤:** ì˜ë£Œ ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ
- AIê°€ ì˜ë£Œ ì´ë¯¸ì§€ì—ì„œ ì´ìƒ ì§•í›„ë¥¼ 1ì°¨ íƒì§€
- ì‹ ë¢°ë„ê°€ ë†’ì€ ê²½ìš°(>90%) ìë™ ë¶„ë¥˜
- ì‹ ë¢°ë„ê°€ ì¤‘ê°„ì¸ ê²½ìš°(70-90%) ì˜ì‚¬ê°€ ê²€í† 
- ì‹ ë¢°ë„ê°€ ë‚®ì€ ê²½ìš°(<70%) ì „ë¬¸ì˜ì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜

**êµ¬í˜„ ìš”êµ¬ì‚¬í•­:**
- ê³„ì¸µì  í˜‘ì—… íŒ¨í„´ ì‚¬ìš©
- ê° ë‹¨ê³„ë³„ í’ˆì§ˆ ê²Œì´íŠ¸ ì„¤ì •
- ì²˜ë¦¬ í†µê³„ ë° ì„±ê³¼ ì§€í‘œ ì¶”ì 

```python
# ì—¬ëŸ¬ë¶„ì˜ ì†”ë£¨ì…˜ì„ êµ¬í˜„í•˜ì„¸ìš”
class MedicalImageAnalysisWorkflow:
    def __init__(self):
        # ì´ˆê¸°í™” ì½”ë“œ
        pass
    
    def process_medical_image(self, image_data):
        # ì˜ë£Œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° êµ¬í˜„
        pass
```

### **ì—°ìŠµ ë¬¸ì œ 3: í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸ ì„¤ê³„ (ê³ ê¸‰)**

ì „ììƒê±°ë˜ ì¶”ì²œ ì‹œìŠ¤í…œì„ ìœ„í•œ ì¢…í•©ì ì¸ í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ì„¸ìš”.

**ìš”êµ¬ì‚¬í•­:**
1. **ë°ì´í„° í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸**: 
   - ì‚¬ìš©ì í–‰ë™ ë°ì´í„°ì˜ ì™„ì „ì„± ë° ì¼ê´€ì„± ê²€ì¦
   - ìƒí’ˆ ì •ë³´ì˜ ì •í™•ì„± í™•ì¸

2. **ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸**:
   - ì¶”ì²œ ì •í™•ë„ ë° ë‹¤ì–‘ì„± ì¸¡ì •
   - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

3. **ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²´í¬í¬ì¸íŠ¸**:
   - ì¶”ì²œ ê²°ê³¼ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¤€ìˆ˜ í™•ì¸
   - ì‚¬ìš©ì ê°œì¸ì •ë³´ ë³´í˜¸ ì •ì±… ì¤€ìˆ˜

**êµ¬í˜„ ëª©í‘œ:**
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
- ìë™ ì•ŒëŒ ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì‹œìŠ¤í…œ
- í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡

```python
# ì—¬ëŸ¬ë¶„ì˜ í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ì„¸ìš”
class EcommerceQualityManagementSystem:
    def __init__(self):
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        pass
    
    def setup_quality_gates(self):
        # í’ˆì§ˆ ê²Œì´íŠ¸ ì„¤ì •
        pass
    
    def monitor_recommendation_quality(self):
        # ì¶”ì²œ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
        pass
```

## ìš”ì•½ / í•µì‹¬ ì •ë¦¬

ì´ë²ˆ Partì—ì„œëŠ” **ìë™í™”ì™€ ìˆ˜ë™ ì‘ì—…ì˜ ê· í˜•**ì„ ì°¾ëŠ” ì²´ê³„ì ì¸ ë°©ë²•ë¡ ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´:

### ğŸ¯ **í•µì‹¬ ê°œë…**

1. **STAR í”„ë ˆì„ì›Œí¬**: ìë™í™” ì í•©ì„±ì„ 4ê°€ì§€ ê´€ì (í‘œì¤€í™”, ì‹œê°„ë¯¼ê°ì„±, ì •í™•ë„ ìš”êµ¬ì‚¬í•­, ìì› ìš”êµ¬ì‚¬í•­)ì—ì„œ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€

2. **ì¸ê°„-AI í˜‘ì—… ëª¨ë¸**: 
   - **ìˆœì°¨ì  í˜‘ì—…**: ë‹¨ê³„ë³„ ì •í™•ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°
   - **ë³‘ë ¬ì  í˜‘ì—…**: ë¹ ë¥¸ ì²˜ë¦¬ì™€ ë‹¤ì–‘í•œ ê´€ì ì´ í•„ìš”í•œ ê²½ìš°  
   - **ê³„ì¸µì  í˜‘ì—…**: í’ˆì§ˆ ë³´ì¥ì´ ìµœìš°ì„ ì¸ ê²½ìš°

3. **í’ˆì§ˆ ê´€ë¦¬ ì²´í¬í¬ì¸íŠ¸**: Critical Control Points(CCP)ë¥¼ ì‹ë³„í•˜ê³  ë‹¤ì°¨ì› í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•

### ğŸ’¡ **ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸**

- **ìë™í™” vs ì¸ê°„ ê°œì…ì˜ ê¸°ì¤€ì„ ëª…í™•íˆ ì„¤ì •**í•˜ì—¬ ì¼ê´€ëœ ì˜ì‚¬ê²°ì • ê°€ëŠ¥
- **ì‹ ë¢°ë„ ê¸°ë°˜ ì—ìŠ¤ì»¬ë ˆì´ì…˜**ìœ¼ë¡œ í’ˆì§ˆê³¼ íš¨ìœ¨ì„±ì˜ ê· í˜• ë‹¬ì„±
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**ì„ í†µí•œ í’ˆì§ˆ ì´ìƒ ìƒí™© ì¡°ê¸° ê°ì§€ ë° ëŒ€ì‘
- **ì²˜ë¦¬ í†µê³„ ì¶”ì **ìœ¼ë¡œ ì›Œí¬í”Œë¡œìš° ìµœì í™”ë¥¼ ìœ„í•œ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •

### ğŸš€ **ì£¼ìš” ì„±ê³¼**

- AIì˜ **ê³„ì‚° ëŠ¥ë ¥**ê³¼ ì¸ê°„ì˜ **ì°½ì˜ì„± ë° íŒë‹¨ë ¥**ì„ ì¡°í™”ë¡­ê²Œ ê²°í•©í•˜ëŠ” ë°©ë²• ìŠµë“
- í”„ë¡œì íŠ¸ íŠ¹ì„±ì— ë§ëŠ” **ìµœì  ìë™í™” ì „ëµ** ìˆ˜ë¦½ ëŠ¥ë ¥ ë°°ì–‘  
- **í’ˆì§ˆì„ ë³´ì¥í•˜ë©´ì„œë„ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”**í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ì—­ëŸ‰
- ì‹¤ë¬´ì—ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ **í…œí”Œë¦¿ê³¼ ê°€ì´ë“œë¼ì¸** í™•ë³´

### ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„ ì˜ˆê³ **

ë‹¤ìŒ Partì—ì„œëŠ” **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ë°ì´í„° ë¶„ì„**ì„ ë‹¤ë£¹ë‹ˆë‹¤. ChatGPT, Claudeì™€ ê°™ì€ ìµœì‹  LLMì„ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš°ì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

---

> ğŸ’¬ **í•™ìŠµ ì„±ì°°**
> 
> "ìë™í™”ì™€ ì¸ê°„ ì‘ì—…ì˜ ê· í˜•ì„ ì°¾ëŠ” ê²ƒì€ ë‹¨ìˆœíˆ íš¨ìœ¨ì„±ë§Œì˜ ë¬¸ì œê°€ ì•„ë‹ˆë¼, **í’ˆì§ˆê³¼ ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ë©´ì„œë„ í˜ì‹ ê³¼ ì°½ì˜ì„±ì„ ìƒì§€ ì•ŠëŠ”** ì§€í˜œë¡œìš´ ì„ íƒì˜ ë¬¸ì œì…ë‹ˆë‹¤. 
> 
> ì¤‘ìš”í•œ ê²ƒì€ ê¸°ìˆ ì´ ì¸ê°„ì„ ëŒ€ì²´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ì¸ê°„ì˜ ëŠ¥ë ¥ì„ ì¦ê°•í•˜ê³  ë” ê°€ì¹˜ ìˆëŠ” ì¼ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ”** ë„êµ¬ë¡œ í™œìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "ì¸ê°„ê³¼ AIê°€ í˜‘ë ¥í•˜ëŠ” ëª¨ìŠµì„ ìƒì§•í•˜ëŠ” ì¶”ìƒì  ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜. ì™¼ìª½ì—ëŠ” ì¸ê°„ì˜ ì°½ì˜ì  ì‚¬ê³ ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë‹¤ì±„ë¡œìš´ ì•„ì´ë””ì–´ êµ¬ë¦„ì´, ì˜¤ë¥¸ìª½ì—ëŠ” AIì˜ ì²´ê³„ì  ì²˜ë¦¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê¸°í•˜í•™ì  íŒ¨í„´ì´ ìˆê³ , ì¤‘ì•™ì—ì„œ ë‘ ìš”ì†Œê°€ ì¡°í™”ë¡­ê²Œ ê²°í•©ë˜ì–´ ìƒˆë¡œìš´ ê°€ì¹˜ë¥¼ ì°½ì¶œí•˜ëŠ” ëª¨ìŠµ"s else 0.8
            
            return {
                'score': overall_balance,
                'details': {
                    'true_distribution': dict(true_dist),
                    'predicted_distribution': dict(pred_dist),
                    'balance_score': overall_balance
                }
            }
            
        except Exception as e:
            return {'score': 0.5, 'details': {'error': str(e)}}
    
    def _check_stability(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ì•ˆì •ì„± ê²€ì‚¬ - ì˜ˆì¸¡ ì¼ê´€ì„±"""
        
        # ì—¬ëŸ¬ ì‹¤í–‰ ê²°ê³¼ ë˜ëŠ” êµì°¨ ê²€ì¦ ê²°ê³¼ í™•ì¸
        cv_scores = data.get('cross_validation_scores', [])
        
        if not cv_scores:
            return {'score': 0.7, 'details': {'note': 'Stability check skipped due to insufficient data'}}
        
        try:
            import numpy as np
            
            mean_score = np.mean(cv_scores)
            std_score = np.std(cv_scores)
            
            # í‘œì¤€í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì•ˆì •ì„± ë†’ìŒ
            stability_score = max(0, 1 - std_score * 5)
            
            return {
                'score': stability_score,
                'details': {
                    'cv_scores': cv_scores,
                    'mean_score': mean_score,
                    'std_score': std_score,
                    'stability_score': stability_score
                }
            }
            
        except Exception as e:
            return {'score': 0.5, 'details': {'error': str(e)}}

class QualityGateSystem:
    """í’ˆì§ˆ ê²Œì´íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.checkpoints: List[QualityCheckpoint] = []
        self.execution_history: List[Dict[str, Any]] = []
        self.global_thresholds = {
            CheckpointSeverity.BLOCKER: 0.0,     # ì°¨ë‹¨ ë ˆë²¨ì€ ì§„í–‰ ë¶ˆê°€
            CheckpointSeverity.CRITICAL: 0.6,    # ì¤‘ìš” ì´ìŠˆ ì„ê³„ê°’
            CheckpointSeverity.MAJOR: 0.8,       # ì£¼ìš” ì´ìŠˆ ì„ê³„ê°’
            CheckpointSeverity.MINOR: 0.9        # ê²½ë¯¸í•œ ì´ìŠˆ ì„ê³„ê°’
        }
    
    def add_checkpoint(self, checkpoint: QualityCheckpoint):
        """ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€"""
        self.checkpoints.append(checkpoint)
    
    def execute_all_checkpoints(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ì‹¤í–‰"""
        
        execution_id = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]
        checkpoint_results = []
        overall_passed = True
        blocker_count = 0
        critical_count = 0
        
        for checkpoint in self.checkpoints:
            result = checkpoint.evaluate(data)
            checkpoint_results.append(result)
            
            # ì‹¬ê°ë„ë³„ ì¹´ìš´íŠ¸
            if result.severity == CheckpointSeverity.BLOCKER:
                blocker_count += 1
                overall_passed = False
            elif result.severity == CheckpointSeverity.CRITICAL:
                critical_count += 1
                if not result.passed:
                    overall_passed = False
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
        total_score = sum(result.score for result in checkpoint_results)
        average_score = total_score / len(checkpoint_results) if checkpoint_results else 0.0
        
        # ì§„í–‰ ê°€ëŠ¥ ì—¬ë¶€ ê²°ì •
        can_proceed = overall_passed and blocker_count == 0
        
        execution_summary = {
            'execution_id': execution_id,
            'timestamp': time.time(),
            'overall_passed': overall_passed,
            'can_proceed': can_proceed,
            'average_score': average_score,
            'checkpoint_results': checkpoint_results,
            'summary_stats': {
                'total_checkpoints': len(checkpoint_results),
                'passed_checkpoints': sum(1 for r in checkpoint_results if r.passed),
                'blocker_count': blocker_count,
                'critical_count': critical_count,
                'major_count': sum(1 for r in checkpoint_results if r.severity == CheckpointSeverity.MAJOR),
                'minor_count': sum(1 for r in checkpoint_results if r.severity == CheckpointSeverity.MINOR)
            },
            'recommendations': self._generate_global_recommendations(checkpoint_results)
        }
        
        # ì‹¤í–‰ ê¸°ë¡ ì €ì¥
        self.execution_history.append(execution_summary)
        
        return execution_summary
    
    def _generate_global_recommendations(self, results: List[CheckpointResult]) -> List[str]:
        """ì „ì²´ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì°¨ë‹¨ ì´ìŠˆê°€ ìˆëŠ” ê²½ìš°
        blockers = [r for r in results if r.severity == CheckpointSeverity.BLOCKER]
        if blockers:
            recommendations.append(f"ğŸš¨ {len(blockers)}ê°œì˜ ì°¨ë‹¨ ì´ìŠˆë¥¼ ì¦‰ì‹œ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤")
        
        # ì¤‘ìš” ì´ìŠˆê°€ ë§ì€ ê²½ìš°
        criticals = [r for r in results if r.severity == CheckpointSeverity.CRITICAL]
        if len(criticals) > 2:
            recommendations.append(f"âš ï¸ {len(criticals)}ê°œì˜ ì¤‘ìš” ì´ìŠˆê°€ ìˆìŠµë‹ˆë‹¤. ìš°ì„ ìˆœìœ„ë¥¼ ì •í•´ í•´ê²°í•˜ì„¸ìš”")
        
        # ì „ì²´ì ì¸ í’ˆì§ˆ ìˆ˜ì¤€ í‰ê°€
        avg_score = sum(r.score for r in results) / len(results) if results else 0
        if avg_score >= 0.9:
            recommendations.append("âœ… ì „ì²´ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í’ˆì§ˆ ìˆ˜ì¤€ì…ë‹ˆë‹¤")
        elif avg_score >= 0.7:
            recommendations.append("ğŸ‘ ì–‘í˜¸í•œ í’ˆì§ˆ ìˆ˜ì¤€ì´ì§€ë§Œ ì¼ë¶€ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤")
        else:
            recommendations.append("ğŸ“‹ ì „ì²´ì ì¸ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        return recommendations
    
    def get_quality_trends(self) -> Dict[str, Any]:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„"""
        
        if len(self.execution_history) < 2:
            return {'message': 'Insufficient data for trend analysis'}
        
        # ìµœê·¼ ì‹¤í–‰ë“¤ì˜ ì ìˆ˜ ì¶”ì´
        recent_scores = [exec['average_score'] for exec in self.execution_history[-10:]]
        
        # íŠ¸ë Œë“œ ê³„ì‚°
        if len(recent_scores) >= 2:
            trend = recent_scores[-1] - recent_scores[-2]
            if trend > 0.05:
                trend_status = 'improving'
            elif trend < -0.05:
                trend_status = 'declining'
            else:
                trend_status = 'stable'
        else:
            trend_status = 'insufficient_data'
        
        return {
            'recent_scores': recent_scores,
            'trend_status': trend_status,
            'average_score': sum(recent_scores) / len(recent_scores),
            'score_variance': np.var(recent_scores) if len(recent_scores) > 1 else 0
        }

# SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ í’ˆì§ˆ ê²Œì´íŠ¸ ì‹œì—°
print("\nğŸ¯ í’ˆì§ˆ ê´€ë¦¬ ì²´í¬í¬ì¸íŠ¸ ì‹œì—°")
print("=" * 50)

# í’ˆì§ˆ ê²Œì´íŠ¸ ì‹œìŠ¤í…œ ì„¤ì •
quality_system = QualityGateSystem()

# ë°ì´í„° í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
data_quality_checkpoint = DataQualityCheckpoint()
algorithm_performance_checkpoint = AlgorithmPerformanceCheckpoint()

quality_system.add_checkpoint(data_quality_checkpoint)
quality_system.add_checkpoint(algorithm_performance_checkpoint)

# ì‹œë®¬ë ˆì´ì…˜ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
import pandas as pd
import numpy as np

# ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±
sample_data = pd.DataFrame({
    'message': [
        'Hello how are you',
        'FREE money call now',
        'Meeting at 3pm today',
        None,  # ê²°ì¸¡ì¹˜
        'Win $1000 prize!'
    ],
    'label': ['ham', 'spam', 'ham', 'ham', 'spam']
})

# ì„±ëŠ¥ ì§€í‘œ ì‹œë®¬ë ˆì´ì…˜
performance_metrics = {
    'accuracy': 0.87,
    'precision': 0.83,
    'recall': 0.89,
    'f1_score': 0.86
}

train_metrics = {
    'accuracy': 0.92,
    'precision': 0.88,
    'recall': 0.91,
    'f1_score': 0.89
}

test_metrics = {
    'accuracy': 0.87,
    'precision': 0.83,
    'recall': 0.89,
    'f1_score': 0.86
}

# í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ì„±
test_data = {
    'dataset': sample_data,
    'expected_schema': {
        'required_columns': ['message', 'label'],
        'dtypes': {'message': 'object', 'label': 'object'}
    },
    'business_rules': {
        'valid_labels': ['spam', 'ham']
    },
    'performance_metrics': performance_metrics,
    'train_metrics': train_metrics,
    'test_metrics': test_metrics,
    'cross_validation_scores': [0.85, 0.87, 0.86, 0.88, 0.84],
    'predictions': ['ham', 'spam', 'ham', 'ham', 'spam'],
    'true_labels': ['ham', 'spam', 'ham', 'ham', 'spam']
}

# í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰
print("ğŸ“‹ í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸ ì‹¤í–‰ ì¤‘...")
execution_result = quality_system.execute_all_checkpoints(test_data)

# ê²°ê³¼ ì¶œë ¥
print(f"\nğŸ¯ í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤í–‰ ê²°ê³¼:")
print(f"ì‹¤í–‰ ID: {execution_result['execution_id']}")
print(f"ì „ì²´ í†µê³¼: {'âœ… ì˜ˆ' if execution_result['overall_passed'] else 'âŒ ì•„ë‹ˆì˜¤'}")
print(f"ì§„í–‰ ê°€ëŠ¥: {'âœ… ì˜ˆ' if execution_result['can_proceed'] else 'âŒ ì•„ë‹ˆì˜¤'}")
print(f"í‰ê·  ì ìˆ˜: {execution_result['average_score']:.3f}")

# ì²´í¬í¬ì¸íŠ¸ë³„ ìƒì„¸ ê²°ê³¼
print(f"\nğŸ“Š ì²´í¬í¬ì¸íŠ¸ë³„ ê²°ê³¼:")
for result in execution_result['checkpoint_results']:
    status_emoji = "âœ…" if result.passed else "âŒ"
    severity_emoji = {
        CheckpointSeverity.BLOCKER: "ğŸš¨",
        CheckpointSeverity.CRITICAL: "âš ï¸", 
        CheckpointSeverity.MAJOR: "ğŸŸ¡",
        CheckpointSeverity.MINOR: "ğŸŸ¢"
    }
    
    print(f"{status_emoji} {result.checkpoint_id}")
    print(f"   ì ìˆ˜: {result.score:.3f} | ì‹¬ê°ë„: {severity_emoji[result.severity]} {result.severity.value}")
    print(f"   ë©”ì‹œì§€: {result.message}")
    
    if result.recommendations:
        print(f"   ê¶Œì¥ì‚¬í•­:")
        for rec in result.recommendations[:2]:  # ìƒìœ„ 2ê°œë§Œ í‘œì‹œ
            print(f"     â€¢ {rec}")

# ì „ì²´ ê¶Œì¥ì‚¬í•­
print(f"\nğŸ’¡ ì „ì²´ ê¶Œì¥ì‚¬í•­:")
for rec in execution_result['recommendations']:
    print(f"  {rec}")

# í†µê³„ ìš”ì•½
stats = execution_result['summary_stats']
print(f"\nğŸ“ˆ í†µê³„ ìš”ì•½:")
print(f"  ì´ ì²´í¬í¬ì¸íŠ¸: {stats['total_checkpoints']}")
print(f"  í†µê³¼í•œ ì²´í¬í¬ì¸íŠ¸: {stats['passed_checkpoints']}")
print(f"  ì°¨ë‹¨ ì´ìŠˆ: {stats['blocker_count']}")
print(f"  ì¤‘ìš” ì´ìŠˆ: {stats['critical_count']}")
print(f"  ì£¼ìš” ì´ìŠˆ: {stats['major_count']}")
print(f"  ê²½ë¯¸í•œ ì´ìŠˆ: {stats['minor_count']}")
```

**ì½”ë“œ í•´ì„¤:**
- **ê³„ì¸µì  í’ˆì§ˆ ê´€ë¦¬**: ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ë¥¼ í†µí•œ ë‹¤ì¸µ í’ˆì§ˆ ê²€ì¦ ì²´ê³„
- **ê°€ì¤‘ì¹˜ ê¸°ë°˜ í‰ê°€**: ê° ê²€ì¦ ê·œì¹™ì˜ ì¤‘ìš”ë„ì— ë”°ë¥¸ ì°¨ë³„ì  í‰ê°€
- **ì‹¬ê°ë„ ë¶„ë¥˜**: ë¬¸ì œì˜ ì‹¬ê°ì„±ì— ë”°ë¥¸ ëŒ€ì‘ ìš°ì„ ìˆœìœ„ ì„¤ì •
- **ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­**: êµ¬ì²´ì ì´ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆ ì œì‹œ

#### **ì›ì¹™ 2: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ì•ŒëŒ**
í’ˆì§ˆ ë¬¸ì œë¥¼ ì¡°ê¸°ì— ë°œê²¬í•˜ê³  ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

```python
import threading
import queue
import time
from typing import Callable, Dict, Any
from collections import deque
import json

class QualityAlert:
    """í’ˆì§ˆ ì•ŒëŒ"""
    
    def __init__(self, 
                 alert_id: str,
                 severity: CheckpointSeverity,
                 message: str,
                 checkpoint_id: str,
                 data: Dict[str, Any] = None):
        self.alert_id = alert_id
        self.severity = severity
        self.message = message
        self.checkpoint_id = checkpoint_id
        self.data = data or {}
        self.timestamp = time.time()
        self.acknowledged = False

class RealTimeQualityMonitor:
    """ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°"""
    
    def __init__(self, max_history_size: int = 1000):
        self.checkpoints: Dict[str, QualityCheckpoint] = {}
        self.alert_handlers: Dict[CheckpointSeverity, List[Callable]] = {
            CheckpointSeverity.BLOCKER: [],
            CheckpointSeverity.CRITICAL: [],
            CheckpointSeverity.MAJOR: [],
            CheckpointSeverity.MINOR: []
        }
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.is_monitoring = False
        self.monitoring_thread = None
        self.data_queue = queue.Queue()
        
        # í’ˆì§ˆ ê¸°ë¡
        self.quality_history = deque(maxlen=max_history_size)
        self.active_alerts: List[QualityAlert] = []
        
        # ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self.monitoring_interval = 1.0  # ì´ˆ
        self.alert_cooldown = {}  # ì•ŒëŒ ì¿¨ë‹¤ìš´ ê´€ë¦¬
    
    def register_checkpoint(self, checkpoint: QualityCheckpoint):
        """ì²´í¬í¬ì¸íŠ¸ ë“±ë¡"""
        self.checkpoints[checkpoint.checkpoint_id] = checkpoint
    
    def add_alert_handler(self, severity: CheckpointSeverity, handler: Callable):
        """ì•ŒëŒ í•¸ë“¤ëŸ¬ ì¶”ê°€"""
        self.alert_handlers[severity].append(handler)
    
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
        
        print("ğŸ“Š ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join()
        
        print("ğŸ“Š ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def submit_data_for_monitoring(self, data: Dict[str, Any]):
        """ëª¨ë‹ˆí„°ë§í•  ë°ì´í„° ì œì¶œ"""
        self.data_queue.put(data)
    
    def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        
        while self.is_monitoring:
            try:
                # ëŒ€ê¸° ì¤‘ì¸ ë°ì´í„° ì²˜ë¦¬
                while not self.data_queue.empty():
                    data = self.data_queue.get_nowait()
                    self._process_monitoring_data(data)
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                print(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(1.0)
    
    def _process_monitoring_data(self, data: Dict[str, Any]):
        """ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì²˜ë¦¬"""
        
        current_time = time.time()
        checkpoint_results = []
        
        # ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ ì‹¤í–‰
        for checkpoint_id, checkpoint in self.checkpoints.items():
            
            # ì¿¨ë‹¤ìš´ ì²´í¬
            if self._is_in_cooldown(checkpoint_id, current_time):
                continue
            
            try:
                result = checkpoint.evaluate(data)
                checkpoint_results.append(result)
                
                # ì•ŒëŒ ì¡°ê±´ í™•ì¸
                if self._should_trigger_alert(result):
                    alert = QualityAlert(
                        alert_id=f"{checkpoint_id}_{int(current_time)}",
                        severity=result.severity,
                        message=result.message,
                        checkpoint_id=checkpoint_id,
                        data={'score': result.score, 'details': result.details}
                    )
                    
                    self._trigger_alert(alert)
                    self._set_cooldown(checkpoint_id, current_time)
                
            except Exception as e:
                print(f"ì²´í¬í¬ì¸íŠ¸ {checkpoint_id} ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        # í’ˆì§ˆ ê¸°ë¡ ì €ì¥
        quality_record = {
            'timestamp': current_time,
            'checkpoint_results': checkpoint_results,
            'overall_score': sum(r.score for r in checkpoint_results) / len(checkpoint_results) if checkpoint_results else 0
        }
        
        self.quality_history.append(quality_record)
    
    def _should_trigger_alert(self, result: CheckpointResult) -> bool:
        """ì•ŒëŒ íŠ¸ë¦¬ê±° ì¡°ê±´ í™•ì¸"""
        
        # ì°¨ë‹¨ ë˜ëŠ” ì¤‘ìš” ì´ìŠˆëŠ” í•­ìƒ ì•ŒëŒ
        if result.severity in [CheckpointSeverity.BLOCKER, CheckpointSeverity.CRITICAL]:
            return True
        
        # ì ìˆ˜ê°€ ë‚®ì€ ê²½ìš°
        if result.score < 0.6:
            return True
        
        # í†µê³¼í•˜ì§€ ëª»í•œ ê²½ìš°
        if not result.passed:
            return True
        
        return False
    
    def _trigger_alert(self, alert: QualityAlert):
        """ì•ŒëŒ íŠ¸ë¦¬ê±°"""
        
        self.active_alerts.append(alert)
        
        # ì‹¬ê°ë„ë³„ í•¸ë“¤ëŸ¬ ì‹¤í–‰
        for handler in self.alert_handlers.get(alert.severity, []):
            try:
                handler(alert)
            except Exception as e:
                print(f"ì•ŒëŒ í•¸ë“¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    def _is_in_cooldown(self, checkpoint_id: str, current_time: float) -> bool:
        """ì¿¨ë‹¤ìš´ ìƒíƒœ í™•ì¸"""
        
        if checkpoint_id not in self.alert_cooldown:
            return False
        
        cooldown_period = 60  # 60ì´ˆ ì¿¨ë‹¤ìš´
        last_alert_time = self.alert_cooldown[checkpoint_id]
        
        return (current_time - last_alert_time) < cooldown_period
    
    def _set_cooldown(self, checkpoint_id: str, current_time: float):
        """ì¿¨ë‹¤ìš´ ì„¤ì •"""
        self.alert_cooldown[checkpoint_id] = current_time
    
    def acknowledge_alert(self, alert_id: str):
        """ì•ŒëŒ í™•ì¸ ì²˜ë¦¬"""
        
        for alert in self.active_alerts:
            if alert.alert_id == alert_id:
                alert.acknowledged = True
                break
    
    def get_active_alerts(self) -> List[QualityAlert]:
        """í™œì„± ì•ŒëŒ ì¡°íšŒ"""
        return [alert for alert in self.active_alerts if not alert.acknowledged]
    
    def get_quality_summary(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ìš”ì•½ ì •ë³´"""
        
        if not self.quality_history:
            return {'message': 'No quality data available'}
        
        recent_records = list(self.quality_history)[-10:]  # ìµœê·¼ 10ê°œ
        
        # í‰ê·  ì ìˆ˜ ì¶”ì´
        scores = [record['overall_score'] for record in recent_records]
        avg_score = sum(scores) / len(scores)
        
        # ì•ŒëŒ í†µê³„
        alert_counts = {}
        for alert in self.active_alerts:
            severity = alert.severity.value
            alert_counts[severity] = alert_counts.get(severity, 0) + 1
        
        return {
            'average_quality_score': avg_score,
            'recent_scores': scores,
            'total_active_alerts': len(self.get_active_alerts()),
            'alert_breakdown': alert_counts,
            'monitoring_status': 'active' if self.is_monitoring else 'stopped',
            'last_update': recent_records[-1]['timestamp'] if recent_records else None
        }

# ì•ŒëŒ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë“¤
def critical_alert_handler(alert: QualityAlert):
    """ì¤‘ìš” ì•ŒëŒ í•¸ë“¤ëŸ¬"""
    print(f"ğŸš¨ CRITICAL ALERT: {alert.message}")
    print(f"   ì²´í¬í¬ì¸íŠ¸: {alert.checkpoint_id}")
    print(f"   ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(alert.timestamp))}")
    
    # ì‹¤ì œë¡œëŠ” ì´ë©”ì¼, ìŠ¬ë™, SMS ë“±ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡
    # send_email_alert(alert)
    # send_slack_notification(alert)

def major_alert_handler(alert: QualityAlert):
    """ì£¼ìš” ì•ŒëŒ í•¸ë“¤ëŸ¬"""
    print(f"âš ï¸ MAJOR ALERT: {alert.message}")
    print(f"   ì²´í¬í¬ì¸íŠ¸: {alert.checkpoint_id}")

def minor_alert_handler(alert: QualityAlert):
    """ê²½ë¯¸í•œ ì•ŒëŒ í•¸ë“¤ëŸ¬"""
    print(f"â„¹ï¸ MINOR ALERT: {alert.message}")

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì—°
print("\nğŸ“Š ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œì—°")
print("=" * 50)

# ëª¨ë‹ˆí„° ì„¤ì •
monitor = RealTimeQualityMonitor()

# ì²´í¬í¬ì¸íŠ¸ ë“±ë¡
monitor.register_checkpoint(DataQualityCheckpoint())
monitor.register_checkpoint(AlgorithmPerformanceCheckpoint())

# ì•ŒëŒ í•¸ë“¤ëŸ¬ ë“±ë¡
monitor.add_alert_handler(CheckpointSeverity.CRITICAL, critical_alert_handler)
monitor.add_alert_handler(CheckpointSeverity.MAJOR, major_alert_handler)
monitor.add_alert_handler(CheckpointSeverity.MINOR, minor_alert_handler)

# ëª¨ë‹ˆí„°ë§ ì‹œì‘
monitor.start_monitoring()

# ì‹œë®¬ë ˆì´ì…˜ëœ ë°ì´í„° ìŠ¤íŠ¸ë¦¼
simulation_data = [
    {
        'scenario': 'ì •ìƒ ë°ì´í„°',
        'data': {
            'dataset': pd.DataFrame({
                'message': ['Hello world', 'Good morning', 'How are you'],
                'label': ['ham', 'ham', 'ham']
            }),
            'performance_metrics': {'accuracy': 0.95, 'precision': 0.92, 'recall': 0.96}
        }
    },
    {
        'scenario': 'í’ˆì§ˆ ì €í•˜ ë°ì´í„°',
        'data': {
            'dataset': pd.DataFrame({
                'message': ['Test', None, 'Bad data'],
                'label': ['ham', 'spam', None]
            }),
            'performance_metrics': {'accuracy': 0.65, 'precision': 0.60, 'recall': 0.58}
        }
    },
    {
        'scenario': 'ì‹¬ê°í•œ ë¬¸ì œ ë°ì´í„°',
        'data': {
            'dataset': pd.DataFrame({
                'message': [None, None, None],
                'label': [None, None, None]
            }),
            'performance_metrics': {'accuracy': 0.30, 'precision': 0.25, 'recall': 0.20}
        }
    }
]

# ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
print("\nğŸ”„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜:")
for i, sim_data in enumerate(simulation_data, 1):
    print(f"\n--- {i}. {sim_data['scenario']} ---")
    
    # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì œì¶œ
    monitor.submit_data_for_monitoring(sim_data['data'])
    
    # ì²˜ë¦¬ ëŒ€ê¸°
    time.sleep(0.5)
    
    # í™œì„± ì•ŒëŒ í™•ì¸
    active_alerts = monitor.get_active_alerts()
    if active_alerts:
        print(f"ğŸ“¢ ë°œìƒí•œ ì•ŒëŒ: {len(active_alerts)}ê°œ")
    else:
        print("âœ… ì•ŒëŒ ì—†ìŒ")

# ì ì‹œ ëŒ€ê¸° í›„ ìš”ì•½ ì •ë³´ ì¶œë ¥
time.sleep(1.0)

# í’ˆì§ˆ ìš”ì•½ ì •ë³´
summary = monitor.get_quality_summary()
print(f"\nğŸ“ˆ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìš”ì•½:")
print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {summary['average_quality_score']:.3f}")
print(f"í™œì„± ì•ŒëŒ ìˆ˜: {summary['total_active_alerts']}")
if summary['alert_breakdown']:
    print(f"ì•ŒëŒ ë¶„í¬: {summary['alert_breakdown']}")

# ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
monitor.stop_monitoring()
```

**ì½”ë“œ í•´ì„¤:**
- **ë¹„ë™ê¸° ëª¨ë‹ˆí„°ë§**: ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì§€ì†ì ì¸ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìˆ˜í–‰
- **ì•ŒëŒ ì‹œìŠ¤í…œ**: ì‹¬ê°ë„ë³„ ì°¨ë³„í™”ëœ ì•ŒëŒ ì²˜ë¦¬ ë° í†µì§€
- **ì¿¨ë‹¤ìš´ ë©”ì»¤ë‹ˆì¦˜**: ë™ì¼í•œ ë¬¸ì œì— ëŒ€í•œ ë°˜ë³µ ì•ŒëŒ ë°©ì§€
- **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**: í˜„ì¬ í’ˆì§ˆ ìƒíƒœì™€ íŠ¸ë Œë“œ ì‹¤ì‹œê°„ ì¶”ì 

> ğŸ’¡ **ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ëª¨ë²” ì‚¬ë¡€**
> 
> **ì„¤ì • ì›ì¹™:**
> - ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ê°€ í° ì§€ì ì— ìš°ì„  ì„¤ì •
> - ìë™í™” ê°€ëŠ¥í•œ ê²€ì¦ ê·œì¹™ ìµœëŒ€í•œ í™œìš©
> - ì¸ê°„ ê°œì…ì´ í•„ìš”í•œ ì§€ì  ëª…í™•íˆ êµ¬ë¶„
> 
> **ìš´ì˜ ì›ì¹™:**
> - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì¡°ê¸° ë°œê²¬
> - ì‹¬ê°ë„ë³„ ì°¨ë³„í™”ëœ ëŒ€ì‘ ì ˆì°¨
> - ì§€ì†ì ì¸ ì„ê³„ê°’ ì¡°ì •ê³¼ ê°œì„ 

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "í’ˆì§ˆ ê´€ë¦¬ ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œì„ ë³´ì—¬ì£¼ëŠ” ëŒ€ì‹œë³´ë“œ ìŠ¤íƒ€ì¼ ì´ë¯¸ì§€. ìƒë‹¨ì—ëŠ” ì‹¤ì‹œê°„ í’ˆì§ˆ ì ìˆ˜ ê²Œì´ì§€, ì¤‘ì•™ì—ëŠ” ë‹¤ì–‘í•œ ì²´í¬í¬ì¸íŠ¸ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì‹ í˜¸ë“± ì•„ì´ì½˜ë“¤, í•˜ë‹¨ì—ëŠ” ì•ŒëŒ ë° ê¶Œì¥ì‚¬í•­ì´ í‘œì‹œëœ ëª¨ë‹ˆí„°ë§ ì¸í„°í˜ì´ìŠ¤"





ğŸ“‹ ì£¼ìš” ë‚´ìš©
ğŸ¯ 1. STAR í”„ë ˆì„ì›Œí¬ ì™„ì „ ë§ˆìŠ¤í„°

Standardization(í‘œì¤€í™”), Time sensitivity(ì‹œê°„ë¯¼ê°ì„±), Accuracy requirements(ì •í™•ë„ ìš”êµ¬ì‚¬í•­), Resource requirements(ìì› ìš”êµ¬ì‚¬í•­) 4ì°¨ì› ë¶„ì„
ê° ì°¨ì›ë³„ ì •ëŸ‰ì  í‰ê°€ ì‹œìŠ¤í…œê³¼ ê°ê´€ì  ê¶Œê³ ì‚¬í•­ ìƒì„±
SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ì˜ ì‹¤ì „ ì ìš© ì‚¬ë¡€

ğŸ¤ 2. ì¸ê°„-AI í˜‘ì—… ëª¨ë¸ì˜ 3ê°€ì§€ íŒ¨í„´

ìˆœì°¨ì  í˜‘ì—…: ë‹¨ê³„ë³„ ì •í™•ì„±ì´ ì¤‘ìš”í•œ ì›Œí¬í”Œë¡œìš°
ë³‘ë ¬ì  í˜‘ì—…: ë¹ ë¥¸ ì²˜ë¦¬ì™€ ë‹¤ì–‘í•œ ê´€ì  ê²€ì¦
ê³„ì¸µì  í˜‘ì—…: í’ˆì§ˆ ê²Œì´íŠ¸ì™€ ì—ìŠ¤ì»¬ë ˆì´ì…˜ ì²´ê³„

ğŸ¯ 3. í’ˆì§ˆ ê´€ë¦¬ ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ

Critical Control Points(CCP) ì‹ë³„ê³¼ ì„¤ì •
ë°ì´í„° í’ˆì§ˆ, ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë“± ë‹¤ì°¨ì› ê²€ì¦
ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ìë™ ì•ŒëŒ ì‹œìŠ¤í…œ

ğŸš€ 4. ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•

STAR í‰ê°€ë¶€í„° í’ˆì§ˆ ê´€ë¦¬ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©
ë©”ì‹œì§€ íŠ¹ì„±ì— ë”°ë¥¸ ì ì‘ì  ì²˜ë¦¬ ë°©ì‹
ì‹¤ì‹œê°„ ì„±ê³¼ ì¶”ì ê³¼ ì§€ì†ì  ê°œì„ 

ğŸŒŸ í•µì‹¬ ì„±ê³¼
âœ… ìë™í™”ì™€ ìˆ˜ë™ ì‘ì—…ì˜ ìµœì  ê· í˜•ì ì„ ì°¾ëŠ” ì²´ê³„ì  ë°©ë²•ë¡  ì™„ì „ ë§ˆìŠ¤í„°
âœ… ì¸ê°„ì˜ ì°½ì˜ì„±ê³¼ AIì˜ íš¨ìœ¨ì„±ì„ ì¡°í™”ë¡­ê²Œ ê²°í•©í•˜ëŠ” í˜‘ì—… ëª¨ë¸ ì„¤ê³„ ëŠ¥ë ¥
âœ… ë°ì´í„° ë¶„ì„ ì „ ê³¼ì •ì— ê±¸ì¹œ í¬ê´„ì  í’ˆì§ˆ ê´€ë¦¬ ì²´ê³„ êµ¬ì¶• ì—­ëŸ‰
âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ì™€ ê¸°ìˆ ì  ì œì•½ì„ ê· í˜•ìˆê²Œ ê³ ë ¤í•˜ëŠ” ì‹¤ë¬´ì  íŒë‹¨ ëŠ¥ë ¥
âœ… AI ì‹œëŒ€ì— í•„ìš”í•œ ìƒˆë¡œìš´ ë°ì´í„° ë¶„ì„ê°€ ì—­ëŸ‰ê³¼ ë§ˆì¸ë“œì…‹ ì™„ì „ ì •ë¦½
ğŸ“ˆ ì‹¤ë¬´ ì ìš© ê°€ì¹˜

í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ë°”ë¡œ ì ìš© ê°€ëŠ¥í•œ í…œí”Œë¦¿ê³¼ ê°€ì´ë“œë¼ì¸ ì œê³µ
íš¨ìœ¨ì„±ê³¼ í’ˆì§ˆì„ ë™ì‹œì— ë³´ì¥í•˜ëŠ” ì‹¤ì „ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ê²½í—˜
ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ ìë™ ì•ŒëŒì„ í†µí•œ í’ˆì§ˆ ë³´ì¥ ì‹œìŠ¤í…œ êµ¬ì¶•
ì§€ì†ì  ê°œì„ ì„ ìœ„í•œ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì • ì²´ê³„ í™•ë¦½

ğŸ”œ ë‹¤ìŒ ë‹¨ê³„
ë‹¤ìŒì—ëŠ” 7ì¥ Part 4: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ í™œìš©í•œ ë°ì´í„° ë¶„ì„ì„ ì§„í–‰í•˜ì—¬, ChatGPT, Claudeì™€ ê°™ì€ ìµœì‹  LLMì„ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš°ì— íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤.

ğŸ’¡ í•µì‹¬ ë©”ì‹œì§€: "ìë™í™”ì™€ ì¸ê°„ ì‘ì—…ì˜ ê· í˜•ì€ ë‹¨ìˆœí•œ íš¨ìœ¨ì„±ì˜ ë¬¸ì œë¥¼ ë„˜ì–´ì„œ, í’ˆì§ˆê³¼ ì‹ ë¢°ì„±ì„ ë³´ì¥í•˜ë©´ì„œë„ í˜ì‹ ê³¼ ì°½ì˜ì„±ì„ ìƒì§€ ì•ŠëŠ” ì§€í˜œë¡œìš´ ì„ íƒì˜ ì˜ì—­ì…ë‹ˆë‹¤. ê¸°ìˆ ì´ ì¸ê°„ì„ ëŒ€ì²´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì¸ê°„ì˜ ëŠ¥ë ¥ì„ ì¦ê°•í•˜ê³  ë” ê°€ì¹˜ ìˆëŠ” ì¼ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ë„êµ¬ë¡œ í™œìš©í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤."