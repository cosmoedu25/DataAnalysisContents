# 1장: 데이터 분석 개요

데이터 분석은 오늘날 정보의 홍수 속에서 가치를 창출하는 핵심 기술입니다. AI 기술의 발전과 함께 데이터 분석은 단순한 숫자 계산을 넘어, 비즈니스와 일상에서 실질적인 의사결정을 돕는 강력한 도구로 자리 잡았습니다. 이 장에서는 데이터 분석의 역할, 프로세스, 그리고 파이썬 기반의 환경 설정 방법을 배우고, 간단한 데이터셋을 탐색하며 실습을 통해 데이터 분석을 이해합니다.

## 학습 목표
- AI 시대의 데이터 분석가 역할과 중요성을 이해할 수 있다.
- 데이터 분석 프로세스의 각 단계를 설명할 수 있다.
- 파이썬 기초 환경을 설정하고 간단한 코드를 실행할 수 있다.
- 간단한 데이터셋을 탐색하고 기본적인 인사이트를 도출할 수 있다.

## 1.1 AI 시대의 데이터 분석 역할과 방향

### 데이터 분석이란 무엇인가?

데이터 분석(Data Analysis)은 **원시 데이터(raw data)**를 조사·정제·변환(transform)하고 **모델링(modeling)**하여 의사결정에 활용할 수 있는 실행 가능한 인사이트(actionable insight)를 도출하는 과정입니다. 단순히 통계치를 계산하는 수준을 넘어, 데이터 속의 숨겨진 패턴(pattern), 추세(trend) 등을 찾아내고 이를 바탕으로 문제 해결의 실마리를 제공합니다. 예를 들어 기업에서는 판매 기록, 웹 로그, 고객 설문조사 데이터를 분석하여 매출 감소 원인을 파악하고 전략을 수립할 수 있습니다. 아마존(AWS)에 따르면 “데이터 분석은 원시 데이터를 실행 가능한 인사이트로 변환”하며, 이를 위해 각종 도구와 기술, 프로세스가 활용됩니다.

오늘날 우리 주변에는 온라인 쇼핑 클릭 기록, SNS 게시물, 위치 정보, 기상 데이터, 금융 시장 데이터 등 방대한 양의 **빅데이터(big data)**가 실시간으로 생성되고 있습니다. 이러한 데이터는 개별적으로는 의미가 작지만, 데이터 분석 기법을 통해 유의미한 정보로 바뀌어 새로운 가치를 만들어냅니다.

우리 주변에는 방대한 양의 데이터가 존재합니다:
- 여러분이 온라인에서 쇼핑할 때의 클릭 기록
- SNS에 게시하는 사진, 글, 댓글
- 이동 경로와 위치 정보
- 검색 기록과 웹 사이트 방문 기록
- 온도, 습도, 강수량과 같은 기상 데이터
- 주식 시장 가격 변동 데이터

이런 데이터는 그 자체로는 큰 가치가 없을 수 있지만, 데이터 분석을 통해 유용한 정보로 변환될 수 있습니다.

### AI 시대의 데이터 분석가 역할

인공지능(AI) 기술의 발전으로 데이터 분석가의 역할은 더욱 중요해지고 있습니다. 인공지능(AI, Artificial Intelligence)과 머신러닝(machine learning) 기술이 빠르게 발전하면서, 데이터 분석가의 역할은 더욱 중요해지고 있습니다. AI 모델이 높은 예측 성능을 내기 위해서는 **고품질의 데이터**와 이를 해석할 수 있는 **전문가의 통찰**이 필수적입니다. 데이터 분석가는 AI 도구가 자동화한 부분을 넘어, 결과를 비판적으로 평가하고 실제 문제 해결에 적용할 수 있도록 해야 합니다. 예를 들어, AI가 자동으로 데이터를 정리해줘도 분석가는 데이터의 **윤리성(ethics)**, **편향(bias)** 등을 검토하고, 결과의 타당성을 확인해야 합니다.

AI 시대에 데이터 분석가가 갖춰야 할 역량:

1. **기술적 역량**
   - 프로그래밍 언어(파이썬, R 등) 활용 능력
   - 통계적 분석 방법론 이해
   - 데이터 시각화 능력
   - 머신러닝/AI 알고리즘 이해

2. **비즈니스 통찰력**
   - 분석 결과를 비즈니스 가치로 연결하는 능력
   - 현업의 문제를 데이터 문제로 정의하는 능력
   - 결과를 명확하게 커뮤니케이션하는 능력

3. **비판적 사고**
   - AI 도구가 생성한 결과를 검증하고 평가하는 능력
   - 데이터의 품질과 분석 방법의 적절성을 판단하는 능력
   - 데이터 분석의 윤리적 측면 고려 능력

AI 도구의 등장으로 반복적이고 기계적인 데이터 처리 작업은 자동화되고 있지만, 데이터 분석가는 여전히 **창의적 문제 해결**과 **가치 있는 통찰 도출** 역할을 수행합니다. 예를 들어, AI가 피벗 테이블을 자동으로 생성하더라도 분석가는 그 결과를 기반으로 어떤 의미 있는 결론을 도출할지 판단해야 합니다. AI 시대에도 인간 분석가의 비판적 분석 능력과 커뮤니케이션 능력은 대체 불가능한 자산이 되는 셈입니다.

>>**데이터 분석 능력의 중요성**: 데이터 분석 능력은 미래 사회에서 핵심 역량입니다. 데이터 분석을 배우면 논리적 사고(logical reasoning)가 향상되고, 데이터 기반의 주장을 판단할 수 있는 디지털 리터러시(digital literacy)를 기를 수 있습니다. 또한 새로운 관점에서 창의적 문제 해결 능력을 발휘하고, AI와 효과적으로 협업하는 역량을 키울 수 있습니다.

### 고등학생이 데이터 분석을 배워야 하는 이유

데이터 분석 능력은 미래 사회에서 필수적인 역량이 될 것입니다:

1. **진로 경쟁력**: 데이터 분석 능력은 다양한 분야(경영, 과학, 의학, 공학 등)에서 요구되는 핵심 역량입니다.

2. **논리적 사고력 향상**: 데이터 분석은 문제를 구조화하고, 증거에 기반한 결론을 도출하는 논리적 사고력을 기릅니다.

3. **디지털 리터러시**: 데이터에 기반한 주장을 평가하고 가짜 정보를 식별할 수 있는 능력은 현대 사회의 시민으로서 필수적입니다.

4. **창의적 문제 해결**: 데이터 분석은 새로운 관점으로 문제를 바라보고 창의적인 해결책을 찾는 능력을 기릅니다.

5. **AI와의 협업 능력**: 미래 사회에서는 AI와 효과적으로 협업할 수 있는 능력이 중요해질 것입니다.

## 1.2 데이터 분석 프로세스 이해
데이터 분석은 일회성이 아닌 **반복적(iterative)**인 프로세스로 수행됩니다. 문제를 정의하고 데이터를 수집한 뒤 전처리하고 분석하고, 그 결과를 바탕으로 새로운 질문을 찾아 다시 분석에 활용하는 사이클을 거칩니다. 다음 그림 1-1은 데이터 과학(data science) 또는 데이터 분석 프로세스를 단계별로 시각화한 예시입니다. 데이터는 현실(reality)에서 수집(raw data collected)되어 처리(data is processed)·정제(clean dataset) 과정을 거쳐 모델링(models & algorithms) 및 결과 해석(communicate/visualize/report)을 통해 의사결정(make decisions)에 활용됩니다. 이 전체 과정은 필요에 따라 반복(iterate)되며, 최종적으로는 실제 의사결정(decision & action)을 지원합니다
**여기에 이미지 삽입**
그림 1-1: 데이터 분석 프로세스 개요 (Data Science Process). 데이터 수집부터 전처리, 모델링, 결과 해석을 거쳐 의사결정에 이르는 단계별 흐름을 보여준다. 이 과정은 반복적으로 실행되며 분석의 정확성과 효율성을 높인다.
### 데이터 분석의 주요 단계

1. **문제 정의 (Problem Definition)**
   - 해결하고자 하는 문제나 질문을 명확히 정의합니다.
   - 예: "고객 이탈율이 높은 이유는 무엇인가?", "어떤 요인이 학생들의 성적에 영향을 미치는가?"
   - 좋은 질문은 구체적이고, 측정 가능하며, 행동으로 옮길 수 있어야 합니다.

2. **데이터 수집 (Data Collection)**
   - 문제 해결에 필요한 데이터를 다양한 출처에서 수집합니다.
   - 데이터 출처: 데이터베이스, API, 웹 스크래핑, 설문조사, 센서 등
   - 데이터의 품질과 관련성이 분석 결과에 큰 영향을 미칩니다.

3. **데이터 전처리 (Data Preprocessing)**
   - 수집된 데이터를 분석에 적합한 형태로 정제합니다.
   - 결측치 처리, 이상치 식별, 데이터 타입 변환, 중복 제거 등의 작업이 포함됩니다.
   - 이 단계는 전체 프로세스에서 가장 많은 시간이 소요되는 경우가 많습니다.

4. **탐색적 데이터 분석 (Exploratory Data Analysis, EDA)**
   - 데이터의 특성, 패턴, 관계를 파악하기 위해 통계와 시각화 기법을 활용합니다.
   - 히스토그램, 산점도, 상관관계 분석 등 다양한 방법을 사용합니다.
   - 이 단계에서 초기 가설을 형성하고 데이터에 대한 직관을 얻습니다.

5. **모델링 (Modeling)**
   - 데이터의 관계를 설명하거나 예측하기 위한 통계적/수학적 모델을 개발합니다.
   - 회귀 분석, 분류, 군집화 등 다양한 머신러닝 기법이 사용될 수 있습니다.
   - 모델의 복잡성과 해석 가능성 사이의 균형을 고려해야 합니다.

6. **모델 평가 및 검증 (Model Evaluation)**
   - 개발된 모델의 성능과 유효성을 평가합니다.
   - 정확도, 정밀도, 재현율 등 다양한 평가 지표를 사용합니다.
   - 교차 검증 등의 기법으로 모델의 일반화 성능을 검증합니다.

7. **결과 해석 및 커뮤니케이션 (Interpretation & Communication)**
   - 분석 결과를 해석하고 현실적인 맥락에서 이해합니다.
   - 결과를 다른 사람들이 이해할 수 있도록 명확하게 설명합니다.
   - 데이터 시각화와 스토리텔링 기법을 활용합니다.

8. **의사결정 및 행동 (Decision Making & Action)**
   - 분석 결과를 바탕으로 실제 행동이나 전략을 수립합니다.
   - 결과가 비즈니스나 연구에 어떤 가치를 제공하는지 평가합니다.
   - 필요한 경우 프로세스를 반복하여 결과를 개선합니다.

### 데이터 분석의 반복적 특성

데이터 분석은 선형적 프로세스가 아닌 반복적(iterative) 프로세스입니다. 분석 과정에서 새로운 질문이 생기거나, 추가 데이터가 필요하다고 판단되면 이전 단계로 돌아가 프로세스를 반복할 수 있습니다. 이러한 반복을 통해 더 심층적인 인사이트와 정확한 결과를 얻을 수 있습니다.

### AI 시대의 데이터 분석 프로세스 변화

AI 기술의 발전으로 데이터 분석 프로세스도 변화하고 있습니다:

1. **자동화된 데이터 전처리**: AI 도구를 활용해 결측치 처리, 이상치 탐지 등을 자동화

2. **생성형 AI를 활용한 코드 작성**: ChatGPT 등을 통해 분석 코드 작성 시간 단축

3. **인터랙티브 시각화**: 고급 시각화 도구를 통한 직관적인 데이터 탐색

4. **자동화된 인사이트 도출**: AI가 데이터에서 자동으로 주요 패턴과 인사이트 추출

그러나 이러한 자동화 도구들에 과도하게 의존하면 분석의 깊이가 부족해질 수 있습니다. 효과적인 데이터 분석가는 AI 도구를 활용하면서도, 비판적 사고와 도메인 지식을 바탕으로 결과를 검증하고 해석할 수 있어야 합니다.

## 1.3 파이썬 기초와 데이터 분석 환경 설정

데이터 분석에는 다양한 프로그래밍 언어와 도구가 사용되지만, 파이썬(Python)은 다음과 같은 이유로 데이터 분석가들 사이에서 가장 인기 있는 언어 중 하나입니다:

1. **단순한 문법**: 파이썬은 배우기 쉽고 읽기 좋은 문법을 가지고 있어, 데이터 분석 초보자도 빠르게 시작할 수 있습니다.

2. **풍부한 라이브러리**: NumPy, pandas, Matplotlib 등 데이터 분석을 위한 강력한 라이브러리를 제공합니다.

3. **다양한 분야에서의 활용**: 데이터 전처리, 통계 분석, 머신러닝, 딥러닝 등 다양한 분야를 파이썬 하나로 작업할 수 있습니다.

4. **강력한 커뮤니티**: 활발한 커뮤니티와 풍부한 온라인 자료로 문제 해결이 용이합니다.

5. **무료 및 오픈소스**: 파이썬과 대부분의 라이브러리는 무료로 사용할 수 있습니다.

### 파이썬 기초 및 설치

파이썬을 사용하기 위해서는 먼저 컴퓨터에 파이썬을 설치해야 합니다. 파이썬을 설치하는 방법은 다음과 같습니다:

1. **파이썬 설치**: 공식 웹사이트(python.org)에서 최신 버전을 다운로드하여 설치합니다.

2. **Anaconda 사용**: 학습용으로는 Anaconda를 사용하는 것이 추천됩니다. Anaconda는 파이썬과 데이터 분석에 필요한 대부분의 라이브러리를 포함하고 있어 편리합니다.

3. **개발 환경 선택**: 코딩을 위한 환경으로는 Jupyter Notebook, JupyterLab, PyCharm, VS Code 등을 사용할 수 있습니다. 학습 용도로는 Jupyter Notebook이 가장 적합합니다.

### 주요 데이터 분석 라이브러리 소개

파이썬에서 데이터 분석을 위해 주로 사용되는 라이브러리는 다음과 같습니다:

1. **NumPy**: 고성능 수치 연산을 위한 라이브러리입니다. 다차원 배열(array) 객체를 제공하여 대규모 수치 데이터를 효율적으로 처리할 수 있습니다. 예를 들어 np.array([1,2,3,4,5])로 배열을 만들고, .mean()으로 평균, .sum()으로 합계를 구할 수 있습니다.
   ```python
   import numpy as np
   
   # 간단한 배열 생성
   arr = np.array([1, 2, 3, 4, 5])
   print("배열:", arr)
   print("평균:", arr.mean())
   print("합계:", arr.sum())
   ```
위 코드에서 np.array()는 리스트를 NumPy 배열로 변환합니다. arr.mean()은 배열 원소의 평균을, arr.sum()은 합계를 계산합니다. NumPy 배열은 벡터화(vectorized) 연산을 지원하여 빠른 계산이 가능합니다.

2. **pandas**: 표 형식(tabular)의 데이터를 다루기 위한 핵심 라이브러리입니다. DataFrame 구조를 사용해 여러 종류의 데이터를 열(column) 단위로 관리할 수 있습니다. 예를 들어 학생 성적 데이터를 사전(dict) 형태로 정의하고 DataFrame으로 변환할 수 있습니다.
   ```python
   import pandas as pd
   
   # 간단한 DataFrame 생성
   data = {'이름': ['김철수', '이영희', '박민수'],
           '나이': [17, 18, 16],
           '성적': [85, 92, 78]}
   df = pd.DataFrame(data)
   print(df)
   ```
   출력 결과:
   ```
         이름  나이  점수
      0  김철수  17   85
      1  이영희  18   92
      2  박민수  16   78
   ```
각 열은 서로 다른 데이터 유형(정수, 문자열 등)을 가질 수 있으며, 인덱스(index)를 통해 행에 접근할 수 있습니다. df.describe() 메서드를 사용하면 수치형 데이터의 요약 통계량(평균, 표준편차, 최소값, 사분위수 등)을 빠르게 확인할 수 있습니다. 또한 df['점수'].value_counts()와 같이 특정 열에서 값의 빈도를 계산할 수 있습니다. 데이터 필터링, 그룹화(groupby), 결합(join/merge) 등 다양한 기능을 지원하여 데이터 정제와 요약 작업을 편리하게 수행합니다.

3. **Matplotlib**: 파이썬의 기본 데이터 시각화 라이브러리입니다. 다양한 그래프와 차트를 생성할 수 있으며, 코드에서 세부 설정을 조정할 수 있습니다. 예를 들어 선 그래프를 그리는 간단한 코드는 다음과 같습니다:
   ```python
   import matplotlib.pyplot as plt
   
   # 간단한 선 그래프
   x = [1, 2, 3, 4]
   y = [10, 20, 25, 30]
   plt.plot(x, y, marker='o')    # 선 그래프 + 마커 표시
   plt.title('간단한 선 그래프')   # 그래프 제목
   plt.xlabel('X 축')            # X축 레이블
   plt.ylabel('Y 축')            # Y축 레이블
   plt.show()    
   ```
위 코드에서 plt.plot()은 선 그래프를 생성하고, marker='o'로 각 데이터 점에 동그라미 마커를 표시합니다. plt.title(), plt.xlabel(), plt.ylabel() 등으로 그래프의 제목과 축 이름을 지정합니다. plt.show()를 통해 화면에 그래프를 렌더링합니다. Matplotlib은 세부 설정이 자유롭지만 다소 복잡할 수 있습니다.

4. **Seaborn**: Matplotlib을 기반으로 한 고급 시각화 라이브러리로, 더 세련된 그래프를 쉽게 생성할 수 있습니다.
예를 들어 Seaborn 내장 데이터셋인 tips를 사용하여 요일별 평균 결제 금액을 막대그래프로 표현하는 코드는 다음과 같습니다:
   ```python
   import seaborn as sns
   
   # 간단한 막대 그래프
   sns.set_style('whitegrid')
   tips = sns.load_dataset('tips')
   sns.barplot(x='day', y='total_bill', data=tips)
   plt.title('요일별 평균 결제 금액')
   plt.show()
   ```
sns.barplot()은 주어진 x, y 데이터에 대해 평균을 보여주는 막대그래프를 그립니다. Seaborn은 set_style이나 set_palette로 일관된 테마를 적용할 수 있으며, 복잡한 분포도(swarmplot, violinplot 등)나 히트맵(heatmap)도 쉽게 그릴 수 있습니다.

### Jupyter Notebook 사용법

Jupyter Notebook은 코드, 텍스트, 시각화 등을 하나의 문서에 포함할 수 있는 대화형 개발 환경입니다. 데이터 분석 작업에 특히 적합하며, 다음과 같은 장점이 있습니다:

1. **대화형 실행**: 코드 블록(셀)을 개별적으로 실행하고 결과를 즉시 확인할 수 있습니다.
2. **문서화 용이**: 마크다운 셀을 통해 코드와 함께 설명을 추가할 수 있습니다.
3. **통합 시각화**: 그래프와 차트가 노트북 내에 직접 표시됩니다.

Jupyter Notebook 사용 방법:

1. **시작하기**: Anaconda Navigator에서 Jupyter Notebook을 실행하거나, 명령 프롬프트에서 다음 명령어를 입력합니다:
   ```
   jupyter notebook
   ```

2. **새 노트북 생성**: 'New' 버튼을 클릭하고 'Python 3'를 선택합니다.

3. **셀 유형**: 
   - Code 셀: Python 코드를 입력하고 실행합니다.
   - Markdown 셀: 텍스트, 설명, 수식 등을 작성합니다.

4. **셀 실행**: 셀을 선택하고 `Shift + Enter`를 누르거나 실행 버튼을 클릭합니다.

5. **단축키**:
   - `Esc + A`: 현재 셀 위에 새 셀 추가
   - `Esc + B`: 현재 셀 아래에 새 셀 추가
   - `Esc + M`: 셀을 Markdown으로 변경
   - `Esc + Y`: 셀을 Code로 변경

**예시 Jupyter Notebook 화면:**

![Jupyter Notebook 예시](../images/jupyter_example.png)
**그림 1-1**: Jupyter Notebook의 인터페이스와 코드/마크다운 셀

### 데이터 분석 환경 구축하기

데이터 분석을 위한 환경을 준비하는 단계별 가이드:

1. **Anaconda 설치**:
   - [Anaconda 공식 웹사이트](https://www.anaconda.com/products/distribution)에서 Python 3.x 버전의 Anaconda를 다운로드하여 설치합니다.
   - 설치 과정에서 "Add Anaconda to PATH" 옵션을 선택하면 편리합니다.

2. **필요한 라이브러리 설치**:
   Anaconda에는 대부분의 데이터 분석 라이브러리가 포함되어 있지만, 추가 라이브러리가 필요한 경우 다음 명령어로 설치할 수 있습니다:
   ```
   conda install 라이브러리_이름
   ```
   또는
   ```
   pip install 라이브러리_이름
   ```

3. **가상 환경 생성** (선택 사항):
   프로젝트별로 독립적인 환경을 구성하고 싶다면 가상 환경을 사용할 수 있습니다:
   ```
   conda create -n 환경_이름 python=3.9
   conda activate 환경_이름
   ```

4. **IDE 설정**:
   - Jupyter Notebook/Lab: 데이터 분석 및 시각화에 적합
   - VS Code: 복잡한 프로젝트와 확장성이 필요한 경우
   - PyCharm: 전문적인 기능이 필요한 경우

**팁 박스:**
> **팁**: 새로운 데이터 분석 프로젝트를 시작할 때마다 필요한 라이브러리의 목록과 버전을 `requirements.txt` 파일에 기록해두면, 나중에 다른 환경에서도 동일한 설정을 쉽게 재현할 수 있습니다.

## 1.4 프로젝트: 간단한 데이터 탐색하기

이제까지 데이터 분석의 개념과 기본 도구를 살펴보았습니다. 이번 절에서는 실제 데이터셋을 사용하여 간단한 탐색적 분석을 해보면서 앞의 이론을 적용해 봅니다. 데이터 분석의 전 과정을 경험하기 위해, 통계 및 머신러닝 분야에서 널리 사용되는 붓꽃(Iris) 데이터셋을 예제로 활용해보겠습니다. 붓꽃 데이터셋은 4개의 연속형 특성과 3개의 품종(target) 정보를 담고 있어, 데이터 프레임 처리와 시각화, 간단한 모델링 연습에 적합합니다.

### 1.4.1 데이터셋 선택 및 불러오기

우리가 사용할 첫 번째 데이터셋은 **Iris 데이터셋**입니다. 이 데이터셋은 통계학과 머신러닝 분야에서 가장 널리 사용되는 기초 데이터셋 중 하나입니다.

**Iris 데이터셋 소개:**
- 3가지 종류의 붓꽃(Iris Setosa, Iris Versicolor, Iris Virginica)에 대한 꽃잎과 꽃받침의 측정 데이터를 포함합니다.
- 각 종류별로 50개씩, 총 150개의 데이터 포인트가 있습니다.
- 4개의 특성(꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)을 포함합니다.
- 간단하지만 실제적인 패턴을 포함하고 있어 데이터 분석과 머신러닝 입문에 적합합니다.

**데이터 불러오기:**

붓꽃(Iris) 데이터셋은 사이킷런(scikit-learn) 라이브러리에 내장되어 있어 손쉽게 불러올 수 있습니다. 다음 코드는 붓꽃 데이터를 로드하고 특성(feature) 배열과 타깃(target) 배열을 확인합니다.

```python
# 방법 1: scikit-learn에서 불러오기
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data  # 특성 데이터
y = iris.target  # 타깃(꽃 종류)
feature_names = iris.feature_names
target_names = iris.target_names

print("데이터 크기:", X.shape)
print("특성 이름:", feature_names)
print("타깃 종류:", target_names)
```
위 코드에서 load_iris()는 Bunch 형태의 객체를 반환합니다. X는 150행 4열 크기의 넘파이 배열로 각각 꽃받침(sepal)과 꽃잎(petal)의 길이와 너비(cm) 정보를 담고 있습니다. y에는 0,1,2로 인코딩된 종(species) 정보가 들어 있습니다. 

실행 결과 예시는 다음과 같습니다:

출력 결과:
```
데이터 크기: (150, 4)
특성 이름: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
타깃 종류: ['setosa' 'versicolor' 'virginica']
```
즉, 붓꽃 데이터에는 150개의 샘플과 4개의 연속형 특성이 있으며, 3종(세토사, 버시컬러, 버지니카)이 존재합니다.


### 1.4.2 기본 데이터 속성 확인하기

데이터를 불러온 후, 가장 먼저 해야 할 일은 데이터의 기본적인 속성과 구조를 파악하는 것입니다. 이는 데이터의 크기, 형태, 결측치 여부, 기술 통계량 등을 확인하는 과정을 포함합니다.

pandas DataFrame으로 변환하여 사용하면 더 편리하게 데이터를 다룰 수 있습니다:

**1. 데이터 형태 변환:**
```python
# pandas DataFrame으로 변환
import pandas as pd

iris_df = pd.DataFrame(X, columns=feature_names)
iris_df['species'] = [target_names[i] for i in y]  # 꽃 종류 추가

# 데이터 확인
print(iris_df.head())
```

출력 결과:
```
   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm) species
0               5.1               3.5                1.4               0.2  setosa
1               4.9               3.0                1.4               0.2  setosa
2               4.7               3.2                1.3               0.2  setosa
3               4.6               3.1                1.5               0.2  setosa
4               5.0               3.6                1.4               0.2  setosa
```
위 출력에서 알 수 있듯이 iris_df는 5열(4개 특성 + 종)로 구성된 표 형태입니다.

**2. 데이터 형태와 크기 확인:**

```python
# 데이터 크기와 컬럼 확인
print("데이터 크기:", iris_df.shape)
print("컬럼 목록:", iris_df.columns.tolist())

# 데이터 타입 확인
print("\n데이터 타입:")
print(iris_df.dtypes)
```

출력 결과:
```
데이터 크기: (150, 5)
컬럼 목록: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)', 'species']

데이터 타입:
sepal length (cm)    float64
sepal width (cm)     float64
petal length (cm)    float64
petal width (cm)     float64
species               object
dtype: object
```
이 결과에서 첫 줄은 행과 열의 수((150, 5)를) 보여줍니다. dtypes를 보면 네 개의 측정 값은 부동소수(float64)이고, species는 문자열(object)임을 알 수 있습니다.

**3. 기술 통계량 확인:**

```python
# 수치형 데이터의 통계량
print("기술 통계량:")
print(iris_df.describe())

# 각 종별 데이터 수
print("\n붓꽃 종류별 개수:")
print(iris_df['species'].value_counts())
```

출력 결과:
```
기술 통계량:
       sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)
count         150.000000        150.000000         150.000000        150.000000
mean            5.843333          3.057333           3.758000          1.199333
std             0.828066          0.435866           1.765298          0.762238
min             4.300000          2.000000           1.000000          0.100000
25%             5.100000          2.800000           1.600000          0.300000
50%             5.800000          3.000000           4.350000          1.300000
75%             6.400000          3.300000           5.100000          1.800000
max             7.900000          4.400000           6.900000          2.500000

붓꽃 종류별 개수:
setosa        50
versicolor    50
virginica     50
Name: species, dtype: int64
```

describe()는 각 수치형 열의 평균(mean), 표준편차(std), 최솟값(min), 1사분위(25%), 중간값(50%), 3사분위(75%), 최댓값(max) 등을 요약하여 보여줍니다. 이를 통해 각 특성이 어느 범위에 있는지 감을 잡을 수 있습니다. 예를 들어 petal length (cm)의 평균은 약 3.76cm이고, 분산이 크다는 것을 확인할 수 있습니다. 또한 종별 데이터 분포를 확인하기 위해 종(species)별 개수를 세어봅니다:

각 품종이 50개씩 균등하게 들어있음을 알 수 있습니다. 이처럼 데이터프레임을 활용하면 데이터의 형태, 크기, 결측치 여부, 분포 등을 쉽고 빠르게 확인할 수 있습니다.

**4. 결측치 확인:**

```python
# 결측치 확인
print("결측치 개수:")
print(iris_df.isnull().sum())
```

출력 결과:
```
결측치 개수:
sepal length (cm)    0
sepal width (cm)     0
petal length (cm)    0
petal width (cm)     0
species              0
dtype: int64
```

Iris 데이터셋은 결측치가 없는 깨끗한 데이터셋입니다. 이는 교육용 데이터셋이기 때문인데, 실제 데이터에서는 종종 결측치가 발생합니다.

### 1.4.3 초기 인사이트 도출하기

데이터의 기본 속성을 파악한 후에는 데이터를 시각화하고 분석하여 인사이트를 도출하는 단계로 넘어갑니다. 데이터 시각화는 데이터의 패턴과 관계를 직관적으로 이해하는 데 도움이 됩니다.

**1. 히스토그램으로 특성 분포 확인:**

```python
import matplotlib.pyplot as plt
import seaborn as sns

# 시각화 설정
plt.figure(figsize=(12, 8))
sns.set_style('whitegrid')

# 히스토그램
for i, feature in enumerate(feature_names):
    plt.subplot(2, 2, i+1)
    sns.histplot(data=iris_df, x=feature, hue='species', kde=True, bins=20)
    plt.title(f'{feature} 분포')

plt.tight_layout()
plt.show()
```

**그림 1-2**: 붓꽃 특성별 히스토그램

히스토그램을 통해 다음과 같은 인사이트를 얻을 수 있습니다:
- Setosa 종은 꽃잎 길이와 너비가 다른 종에 비해 뚜렷하게 작습니다.
- Versicolor와 Virginica는 꽃받침 크기에서는 상당히 겹치지만, 꽃잎 크기에서는 어느 정도 구분됩니다.
- 꽃잎 특성이 꽃받침 특성보다 종 구분에 더 유용해 보입니다.

**2. 산점도로 특성 간 관계 확인:**

```python
# 산점도 행렬
plt.figure(figsize=(10, 8))
sns.pairplot(iris_df, hue='species', markers=["o", "s", "D"])
plt.suptitle('Iris 데이터셋 산점도 행렬', y=1.02, fontsize=16)
plt.show()
```

**그림 1-3**: Iris 데이터셋 산점도 행렬

산점도 행렬에서 확인할 수 있는 인사이트:
- 꽃잎 길이와 꽃잎 너비는 강한 양의 상관관계를 보입니다.
- Setosa 종은 다른 두 종과 특성 공간에서 명확히 구분됩니다.
- Versicolor와 Virginica는 일부 영역에서 겹치지만, 꽃잎 특성을 기준으로 어느 정도 구분이 가능합니다.

**3. 상관관계 분석:**

특성 간의 상관관계를 수치적으로 확인해보겠습니다.

```python
# 상관관계 분석
plt.figure(figsize=(8, 6))
correlation = iris_df.iloc[:, :4].corr()  # 수치형 변수만 선택
sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Iris 데이터셋 특성 간 상관관계')
plt.show()
```

**그림 1-4**: Iris 데이터셋 특성 간 상관관계 히트맵

상관관계 분석 결과:
- 꽃잎 길이와 꽃잎 너비 사이에 0.96의 매우 높은 상관관계가 있습니다.
- 꽃잎 길이와 꽃받침 길이 사이에도 0.87의 강한 상관관계가 있습니다.
- 꽃받침 너비는 다른 특성들과 상대적으로 낮은 상관관계를 보입니다.

**4. 종별 특성 평균 비교:**

각 종별로 특성의 평균값을 비교해보겠습니다.

```python
# 종별 특성 평균
plt.figure(figsize=(10, 6))
species_means = iris_df.groupby('species').mean()
species_means.plot(kind='bar', yerr=iris_df.groupby('species').std())
plt.title('붓꽃 종별 특성 평균')
plt.ylabel('cm')
plt.xticks(rotation=0)
plt.legend(loc='upper left')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()
```

**그림 1-5**: 붓꽃 종별 특성 평균 비교

종별 특성 평균 비교 결과:
- Setosa: 꽃받침이 짧고 넓으며, 꽃잎이 매우 작습니다.
- Versicolor: 중간 크기의 꽃잎과 꽃받침을 가집니다.
- Virginica: 가장 긴 꽃잎과 꽃받침을 가지며, 꽃잎이 넓습니다.

### 1.4.4 결과 정리 및 발표

지금까지의 분석 결과를 종합하여 주요 인사이트를 정리해 보겠습니다:

**1. 주요 발견:**
- Iris 데이터셋의 세 종류는 꽃잎 크기(길이와 너비)에 따라 뚜렷하게 구분됩니다.
- Setosa 종은 다른 두 종과 확연히 구분되는 특성을 보입니다.
- Versicolor와 Virginica는 일부 특성에서 겹치지만, 꽃잎 크기로 대체로 구분 가능합니다.
- 꽃잎의 길이와 너비는 매우 높은 상관관계를 보입니다.

**2. 데이터 기반 의사결정 예시:**
- 붓꽃 자동 분류 시스템을 개발한다면, 꽃잎 특성을 중심으로 설계하는 것이 효과적일 것입니다.
- Setosa 종은 간단한 규칙(예: 꽃잎 길이 < 2cm)만으로도 높은 정확도로 분류할 수 있을 것입니다.
- Versicolor와 Virginica 구분을 위해서는 더 복잡한 모델이 필요할 수 있습니다.

**3. 추가 분석 방향:**
- 더 정교한 머신러닝 모델을 적용하여 종 분류 정확도를 측정해볼 수 있습니다.
- 다른 환경에서 자란 붓꽃 데이터를 추가로 수집하여 환경 요인의 영향을 분석할 수 있습니다.
- 다변량 분석 기법을 적용하여 특성들의 조합이 종 구분에 미치는 영향을 더 자세히 분석할 수 있습니다.

**데이터 분석 발표 시 고려할 점:**
- 청중의 배경 지식 수준에 맞춰 내용의 깊이를 조절합니다.
- 시각화 자료는 직관적이고 이해하기 쉽게 제작합니다.
- 기술적 용어를 사용할 때는 필요한 경우 간략한 설명을 덧붙입니다.
- 분석의 한계점과 추가 연구 방향도 함께 제시합니다.

## 직접 해보기 / 연습 문제

### 문제 1: 데이터 불러오기 및 기본 탐색
Iris 데이터셋을 불러와서 다음 질문에 답하세요:
1. 데이터셋에는 몇 개의 행과 열이 있나요?
2. 각 특성(꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비)의 최댓값, 최솟값, 평균은 얼마인가요?
3. 세 종류의 붓꽃 중 꽃잎 길이의 표준편차가 가장 큰 종은 무엇인가요?

### 문제 2: 데이터 시각화
Iris 데이터셋을 사용하여 다음 시각화를 수행하세요:
1. 꽃잎 길이와 꽃잎 너비를 x축과 y축으로 하는 산점도를 그리고, 종별로 다른 색상을 사용하세요.
2. 종별로 꽃받침 길이의 분포를 보여주는 박스플롯을 그리세요.
3. 위 시각화 결과를 바탕으로 어떤 인사이트를 얻을 수 있는지 설명하세요.

### 문제 3: 데이터 분석 확장
1. sepal_ratio = sepal length / sepal width와 petal_ratio = petal length / petal width 두 개의 파생 변수를 생성하세요.
2. 이 두 비율 변수가 종 구분에 유용한지 시각화를 통해 확인하세요.
3. 원래 특성과 비교했을 때, 이 비율 변수들이 주는 추가적인 정보나 장점이 있는지 분석하세요.

## 요약 / 핵심 정리

이번 장에서 배운 주요 내용을 정리하면 다음과 같습니다:

- **데이터 분석가의 역할**은 AI 시대에 더욱 중요해지고 있으며, 기술적 역량과 비즈니스 통찰력, 비판적 사고 능력이 필요합니다.
- **데이터 분석 프로세스**는 문제 정의, 데이터 수집, 전처리, 탐색적 분석, 모델링, 평가, 결과 해석, 의사결정 단계로 구성됩니다.
- **파이썬 데이터 분석 환경** 구축을 위해 Anaconda와 Jupyter Notebook을 사용하며, NumPy, pandas, Matplotlib, Seaborn 등의 라이브러리가 활용됩니다.
- **탐색적 데이터 분석(EDA)**은 데이터의 기본 속성 확인, 시각화, 통계 분석을 통해 인사이트를 도출하는 과정입니다.
- **데이터 시각화**는 히스토그램, 산점도, 히트맵 등 다양한 방법으로 데이터의 패턴과 관계를 직관적으로 이해하는 데 도움을 줍니다.
- **Iris 데이터셋** 분석을 통해 특성 간 관계와 종별 차이점을 탐색하여 의미 있는 인사이트를 도출할 수 있었습니다.

## 생각해보기 / 다음 장 예고

### 생각해보기
- 데이터 분석에서 AI 도구(예: ChatGPT)를 활용할 때의 장단점은 무엇일까요?
- 잘못된 데이터 분석이나 해석이 가져올 수 있는 위험에는 어떤 것들이 있을까요?
- 여러분의 일상이나 학교생활에서 데이터 분석을 적용할 수 있는 문제나 상황은 무엇이 있을까요?

### 다음 장 예고
다음 장에서는 '탐색적 데이터 분석(EDA)'에 대해 더 자세히 알아보겠습니다. 실제 데이터셋(Titanic)을 사용하여 결측치와 이상치 처리 방법, 다양한 시각화 기법, 데이터 요약 방법 등을 배우고, 이를 통해 더 심층적인 인사이트를 도출하는 방법을 학습할 것입니다.
