# 8ì¥ Part 3: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡
**ë¶€ì œ: ì „í†µì  ëª¨ë¸ì„ ë„˜ì–´ì„  ì°¨ì„¸ëŒ€ ì˜ˆì¸¡ ê¸°ë²•ë“¤**

## í•™ìŠµ ëª©í‘œ
ì´ Partë¥¼ ì™„ë£Œí•œ í›„, ì—¬ëŸ¬ë¶„ì€ ë‹¤ìŒì„ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤:
- ì‹œê³„ì—´ ë¬¸ì œë¥¼ ì§€ë„í•™ìŠµ ë¬¸ì œë¡œ ì²´ê³„ì ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤
- ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ XGBoostë¥¼ ì‹œê³„ì—´ ì˜ˆì¸¡ì— íš¨ê³¼ì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ìˆë‹¤
- íŠ¹ì„± ê³µí•™ì„ í†µí•´ ì‹œê³„ì—´ ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ë°œêµ´í•  ìˆ˜ ìˆë‹¤
- 7ì¥ AI í˜‘ì—… ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•´ì„í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆë‹¤
- ì „í†µì  ëª¨ë¸ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤

## ì´ë²ˆ Part ë¯¸ë¦¬ë³´ê¸°
ğŸš€ **ì „í†µì  ì‹œê³„ì—´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ë‹¤!**

8ì¥ Part 2ì—ì„œ ìš°ë¦¬ëŠ” ARIMA, SARIMA, ì§€ìˆ˜í‰í™œë²• ë“± ìˆ˜ì‹­ ë…„ê°„ ê²€ì¦ëœ ì „í†µì  ì‹œê³„ì—´ ëª¨ë¸ë“¤ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤. ì´ì œ **ë¨¸ì‹ ëŸ¬ë‹ì˜ í˜**ì„ ë¹Œë ¤ ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ ìƒˆë¡œìš´ ì§€í‰ì„ ì—´ì–´ë³´ê² ìŠµë‹ˆë‹¤.

ì „í†µì  ëª¨ë¸ë“¤ì´ ì„ í˜•ì ì´ê³  ë‹¨ìˆœí•œ íŒ¨í„´ì— íŠ¹í™”ë˜ì–´ ìˆë‹¤ë©´, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ì€ **ë¹„ì„ í˜•ì ì´ê³  ë³µì¡í•œ íŒ¨í„´**ì„ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ **ì™¸ë¶€ ë³€ìˆ˜ë“¤(ë‚ ì”¨, ê²½ì œì§€í‘œ, ì´ë²¤íŠ¸ ë“±)**ì˜ ì˜í–¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©í•  ìˆ˜ ìˆì–´ ì‹¤ë¬´ì—ì„œ í›¨ì”¬ ê°•ë ¥í•œ ì˜ˆì¸¡ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ¯ **ì´ë²ˆ Partì˜ í•µì‹¬ ì—¬ì •**:
- **ì‹œê³„ì—´ â†’ ì§€ë„í•™ìŠµ ë³€í™˜**: ì‹œê°„ ì°¨ì›ì„ íŠ¹ì„±ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” í˜ì‹ ì  ì ‘ê·¼
- **ê³ ê¸‰ íŠ¹ì„± ê³µí•™**: ì‹œê°„, ê³„ì ˆì„±, ë˜ê·¸, ë¡¤ë§ í†µê³„ ë“± ê°•ë ¥í•œ íŠ¹ì„± ìƒì„±
- **ëœë¤ í¬ë ˆìŠ¤íŠ¸ & XGBoost**: ë¹„ì„ í˜• íŒ¨í„´ê³¼ ìƒí˜¸ì‘ìš© íš¨ê³¼ ì™„ì „ í¬ì°©
- **íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„**: ì–´ë–¤ ìš”ì¸ì´ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œì§€ ëª…í™•í•œ í•´ì„
- **í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”**: ì „í†µì  + ML ëª¨ë¸ì˜ ìµœê°• ì¡°í•©

---

> ğŸŒŸ **ì™œ ë¨¸ì‹ ëŸ¬ë‹ì´ ì‹œê³„ì—´ì— í˜ëª…ì ì¸ê°€?**
> 
> **ğŸ”„ ë¹„ì„ í˜• íŒ¨í„´**: ë³µì¡í•œ ê³„ì ˆì„±, íŠ¸ë Œë“œ ë³€í™”, ìƒí˜¸ì‘ìš© íš¨ê³¼ ìë™ í•™ìŠµ
> **ğŸ“Š ë‹¤ë³€ëŸ‰ ì²˜ë¦¬**: ì™¸ë¶€ ë³€ìˆ˜ë“¤ì„ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•© (ë‚ ì”¨, ê²½ì œ, ì´ë²¤íŠ¸ ë“±)
> **ğŸ¯ ìë™ íŠ¹ì„± ë°œê²¬**: ì¸ê°„ì´ ë†“ì¹œ ìˆ¨ê²¨ì§„ íŒ¨í„´ë“¤ì„ ë°ì´í„°ì—ì„œ ìë™ ë°œêµ´
> **âš¡ í™•ì¥ì„±**: ìˆ˜ë°± ê°œ ì‹œê³„ì—´ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ ëŠ¥ë ¥
> **ğŸ›¡ï¸ ê³¼ì í•© ë°©ì§€**: ì •ê·œí™”ì™€ êµì°¨ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì  ì¼ë°˜í™” ì„±ëŠ¥

## 1. ì‹œê³„ì—´ ë¬¸ì œì˜ ì§€ë„í•™ìŠµ ë³€í™˜

### 1.1 íŒ¨ëŸ¬ë‹¤ì„ì˜ ì „í™˜: ì‹œê°„ì„ íŠ¹ì„±ìœ¼ë¡œ ë§Œë“¤ê¸°

ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ í•´ê²°í•˜ëŠ” í•µì‹¬ì€ **ì‹œê°„ ì°¨ì›ì„ íŠ¹ì„± ì°¨ì›ìœ¼ë¡œ ë³€í™˜**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ ì‚¬ê³ ë°©ì‹ì„ ìš”êµ¬í•©ë‹ˆë‹¤.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import xgboost as xgb
from sklearn.preprocessing import StandardScaler, LabelEncoder
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™” ì„¤ì •
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (14, 8)
plt.rcParams['font.size'] = 10

class TimeSeriesMLConverter:
    """ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.feature_columns = []
        self.target_column = None
        self.original_data = None
        self.ml_data = None
        self.scaler = StandardScaler()
        
        # 7ì¥ AI í˜‘ì—… ì›ì¹™ì„ ML ê¸°ë°˜ ì‹œê³„ì—´ì— ì ìš©
        self.ml_interpretation_prompts = {
            'feature_engineering': self._create_feature_engineering_prompt(),
            'model_interpretation': self._create_ml_interpretation_prompt(),
            'performance_analysis': self._create_performance_analysis_prompt(),
            'business_insights': self._create_business_insights_prompt()
        }
    
    def demonstrate_conversion_concept(self):
        """ì‹œê³„ì—´ â†’ ì§€ë„í•™ìŠµ ë³€í™˜ ê°œë… ì‹œì—°"""
        
        print("ğŸ”„ ì‹œê³„ì—´ ë°ì´í„°ì˜ ì§€ë„í•™ìŠµ ë³€í™˜")
        print("=" * 50)
        
        # ê°„ë‹¨í•œ ì‹œê³„ì—´ ë°ì´í„° ìƒì„±
        dates = pd.date_range('2023-01-01', periods=10, freq='D')
        values = [100, 105, 103, 108, 112, 107, 115, 118, 114, 120]
        
        original_ts = pd.DataFrame({
            'date': dates,
            'sales': values
        })
        
        print("ğŸ“Š ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„°:")
        print(original_ts.head())
        
        print("\nğŸ”„ ë³€í™˜ ê³¼ì •:")
        print("1ë‹¨ê³„: ë˜ê·¸ íŠ¹ì„± ìƒì„± (ê³¼ê±° ê°’ë“¤ì„ ë…ë¦½ë³€ìˆ˜ë¡œ)")
        print("2ë‹¨ê³„: ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ì¶”ê°€ (ìš”ì¼, ì›”, ê³„ì ˆ ë“±)")
        print("3ë‹¨ê³„: ë¡¤ë§ í†µê³„ íŠ¹ì„± ìƒì„± (ì´ë™í‰ê· , ì´ë™í‘œì¤€í¸ì°¨ ë“±)")
        print("4ë‹¨ê³„: íƒ€ê²Ÿ ë³€ìˆ˜ ì •ì˜ (ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ë¯¸ë˜ ê°’)")
        
        # ë³€í™˜ ì‹¤í–‰
        converted_data = self._convert_simple_timeseries(original_ts)
        
        print(f"\nğŸ“ˆ ë³€í™˜ëœ ì§€ë„í•™ìŠµ ë°ì´í„°:")
        print(converted_data)
        
        print(f"\nğŸ¯ ë³€í™˜ ê²°ê³¼:")
        print(f"   ì›ë³¸: {len(original_ts)}í–‰ Ã— {len(original_ts.columns)}ì—´ (ì‹œê³„ì—´)")
        print(f"   ë³€í™˜: {len(converted_data)}í–‰ Ã— {len(converted_data.columns)}ì—´ (ì§€ë„í•™ìŠµ)")
        print(f"   íŠ¹ì„±: {list(converted_data.columns[:-1])}")
        print(f"   íƒ€ê²Ÿ: {converted_data.columns[-1]}")
        
        # ì‹œê°í™”
        self._visualize_conversion_concept(original_ts, converted_data)
        
        return original_ts, converted_data
    
    def _convert_simple_timeseries(self, ts_data):
        """ê°„ë‹¨í•œ ì‹œê³„ì—´ ë³€í™˜ ì˜ˆì‹œ"""
        
        df = ts_data.copy()
        df = df.set_index('date')
        
        # ë˜ê·¸ íŠ¹ì„± ìƒì„±
        for lag in [1, 2, 3]:
            df[f'sales_lag_{lag}'] = df['sales'].shift(lag)
        
        # ë¡¤ë§ í†µê³„
        df['sales_rolling_mean_3'] = df['sales'].rolling(window=3).mean()
        df['sales_rolling_std_3'] = df['sales'].rolling(window=3).std()
        
        # ì‹œê°„ íŠ¹ì„±
        df['day_of_week'] = df.index.dayofweek
        df['day_of_month'] = df.index.day
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ (1ì¼ í›„ ì˜ˆì¸¡)
        df['target'] = df['sales'].shift(-1)
        
        # ê²°ì¸¡ê°’ ì œê±°
        df_clean = df.dropna()
        
        return df_clean
    
    def _visualize_conversion_concept(self, original, converted):
        """ë³€í™˜ ê°œë… ì‹œê°í™”"""
        
        fig, axes = plt.subplots(2, 1, figsize=(15, 10))
        fig.suptitle('ğŸ”„ ì‹œê³„ì—´ â†’ ì§€ë„í•™ìŠµ ë³€í™˜ ê°œë…', fontsize=16, fontweight='bold')
        
        # ì›ë³¸ ì‹œê³„ì—´
        axes[0].plot(original['date'], original['sales'], 'o-', linewidth=2, markersize=8)
        axes[0].set_title('ğŸ“Š ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„° (ì‹œê°„ ì¶• ì¤‘ì‹¬)', fontweight='bold')
        axes[0].set_ylabel('ë§¤ì¶œ')
        axes[0].grid(True, alpha=0.3)
        
        # ë³€í™˜ëœ ë°ì´í„° (íŠ¹ì„±ë“¤ì˜ ê´€ê³„ ë³´ê¸°)
        feature_cols = [col for col in converted.columns if col != 'target']
        correlation_matrix = converted[feature_cols + ['target']].corr()
        
        im = axes[1].imshow(correlation_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
        axes[1].set_xticks(range(len(correlation_matrix.columns)))
        axes[1].set_yticks(range(len(correlation_matrix.columns)))
        axes[1].set_xticklabels(correlation_matrix.columns, rotation=45)
        axes[1].set_yticklabels(correlation_matrix.columns)
        axes[1].set_title('ğŸ¯ ë³€í™˜ëœ íŠ¹ì„±-íƒ€ê²Ÿ ìƒê´€ê´€ê³„ (íŠ¹ì„± ì¶• ì¤‘ì‹¬)', fontweight='bold')
        
        # ì»¬ëŸ¬ë°” ì¶”ê°€
        plt.colorbar(im, ax=axes[1], shrink=0.8)
        
        plt.tight_layout()
        plt.show()
        
        print("\nğŸ’¡ í•µì‹¬ ë³€í™”:")
        print("   â° ì‹œê°„ì¶• â†’ íŠ¹ì„±ì¶•: ì‹œê°„ ì •ë³´ê°€ ë…ë¦½ë³€ìˆ˜ë¡œ ë³€í™˜")
        print("   ğŸ“ˆ ìˆœì°¨ ê´€ì°° â†’ íŒ¨í„´ í•™ìŠµ: ì‹œê°„ ìˆœì„œê°€ íŠ¹ì„± ê°„ ê´€ê³„ë¡œ ë³€í™˜")
        print("   ğŸ¯ ì˜ˆì¸¡ â†’ ë¶„ë¥˜/íšŒê·€: ì‹œê³„ì—´ ì˜ˆì¸¡ì´ ì¼ë°˜ì  ML ë¬¸ì œë¡œ ë³€í™˜")
    
    def comprehensive_feature_engineering(self, data, target_col, date_col):
        """ì¢…í•©ì  íŠ¹ì„± ê³µí•™"""
        
        print("ğŸ› ï¸ ì¢…í•©ì  ì‹œê³„ì—´ íŠ¹ì„± ê³µí•™")
        print("=" * 50)
        
        df = data.copy()
        
        # ë‚ ì§œ ì¸ë±ìŠ¤ ì„¤ì •
        if date_col in df.columns:
            df[date_col] = pd.to_datetime(df[date_col])
            df = df.set_index(date_col)
        
        print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {len(df)}í–‰ Ã— {len(df.columns)}ì—´")
        
        feature_df = pd.DataFrame(index=df.index)
        
        # 1. ë˜ê·¸ íŠ¹ì„± (ê³¼ê±° ê°’ë“¤)
        print("\n1ï¸âƒ£ ë˜ê·¸ íŠ¹ì„± ìƒì„±:")
        lag_periods = [1, 2, 3, 7, 14, 30]  # ë‹¤ì–‘í•œ ì£¼ê¸°
        for lag in lag_periods:
            feature_df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
            print(f"   ğŸ”™ {lag}ê¸°ê°„ ì „ ê°’: {target_col}_lag_{lag}")
        
        # 2. ë¡¤ë§ í†µê³„ íŠ¹ì„±
        print("\n2ï¸âƒ£ ë¡¤ë§ í†µê³„ íŠ¹ì„±:")
        windows = [3, 7, 14, 30]
        for window in windows:
            # ì´ë™í‰ê· 
            feature_df[f'{target_col}_ma_{window}'] = df[target_col].rolling(window=window).mean()
            # ì´ë™í‘œì¤€í¸ì°¨
            feature_df[f'{target_col}_std_{window}'] = df[target_col].rolling(window=window).std()
            # ì´ë™ ìµœëŒ“ê°’
            feature_df[f'{target_col}_max_{window}'] = df[target_col].rolling(window=window).max()
            # ì´ë™ ìµœì†Ÿê°’
            feature_df[f'{target_col}_min_{window}'] = df[target_col].rolling(window=window).min()
            
            print(f"   ğŸ“Š {window}ì¼ ì´ë™í‰ê· /í‘œì¤€í¸ì°¨/ìµœëŒ“ê°’/ìµœì†Ÿê°’")
        
        # 3. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±
        print("\n3ï¸âƒ£ ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±:")
        
        # ì„ í˜• ì‹œê°„ íŠ¹ì„±
        feature_df['year'] = df.index.year
        feature_df['month'] = df.index.month
        feature_df['day'] = df.index.day
        feature_df['day_of_week'] = df.index.dayofweek
        feature_df['day_of_year'] = df.index.dayofyear
        feature_df['week_of_year'] = df.index.isocalendar().week
        feature_df['quarter'] = df.index.quarter
        
        # ìˆœí™˜ ì‹œê°„ íŠ¹ì„± (ì£¼ê¸°ì„± í‘œí˜„)
        feature_df['month_sin'] = np.sin(2 * np.pi * df.index.month / 12)
        feature_df['month_cos'] = np.cos(2 * np.pi * df.index.month / 12)
        feature_df['day_sin'] = np.sin(2 * np.pi * df.index.day / 31)
        feature_df['day_cos'] = np.cos(2 * np.pi * df.index.day / 31)
        feature_df['dayofweek_sin'] = np.sin(2 * np.pi * df.index.dayofweek / 7)
        feature_df['dayofweek_cos'] = np.cos(2 * np.pi * df.index.dayofweek / 7)
        
        print("   ğŸ“… ì—°/ì›”/ì¼/ìš”ì¼/ë¶„ê¸° ì •ë³´")
        print("   ğŸ”„ ìˆœí™˜ ì‹œê°„ íŠ¹ì„± (sin/cos ë³€í™˜)")
        
        # 4. ë³€í™”ìœ¨ íŠ¹ì„±
        print("\n4ï¸âƒ£ ë³€í™”ìœ¨ íŠ¹ì„±:")
        
        # ì „ì¼ ëŒ€ë¹„ ë³€í™”ìœ¨
        feature_df[f'{target_col}_pct_change_1'] = df[target_col].pct_change(1)
        # ì¼ì£¼ì¼ ì „ ëŒ€ë¹„ ë³€í™”ìœ¨
        feature_df[f'{target_col}_pct_change_7'] = df[target_col].pct_change(7)
        # í•œë‹¬ ì „ ëŒ€ë¹„ ë³€í™”ìœ¨
        feature_df[f'{target_col}_pct_change_30'] = df[target_col].pct_change(30)
        
        # ì°¨ë¶„ íŠ¹ì„±
        feature_df[f'{target_col}_diff_1'] = df[target_col].diff(1)
        feature_df[f'{target_col}_diff_7'] = df[target_col].diff(7)
        
        print("   ğŸ“ˆ ë³€í™”ìœ¨: 1ì¼/7ì¼/30ì¼ ì „ ëŒ€ë¹„ ë¹„êµ")
        print("   ğŸ“Š ì°¨ë¶„: 1ì°¨/7ì°¨ ì°¨ë¶„ìœ¼ë¡œ íŠ¸ë Œë“œ ì œê±°")
        
        # 5. ê³„ì ˆì„± ë° ì£¼ê¸° íŠ¹ì„±
        print("\n5ï¸âƒ£ ê³„ì ˆì„± íŠ¹ì„±:")
        
        # ê³„ì ˆ êµ¬ë¶„
        def get_season(month):
            if month in [12, 1, 2]:
                return 0  # ê²¨ìš¸
            elif month in [3, 4, 5]:
                return 1  # ë´„
            elif month in [6, 7, 8]:
                return 2  # ì—¬ë¦„
            else:
                return 3  # ê°€ì„
        
        feature_df['season'] = df.index.month.map(get_season)
        
        # ì£¼ë§/í‰ì¼ êµ¬ë¶„
        feature_df['is_weekend'] = (df.index.dayofweek >= 5).astype(int)
        
        # ì›”ì´ˆ/ì›”ë§ êµ¬ë¶„
        feature_df['is_month_start'] = (df.index.day <= 5).astype(int)
        feature_df['is_month_end'] = (df.index.day >= 25).astype(int)
        
        print("   ğŸ‚ ê³„ì ˆ êµ¬ë¶„ (ë´„/ì—¬ë¦„/ê°€ì„/ê²¨ìš¸)")
        print("   ğŸ“… ì£¼ë§/í‰ì¼, ì›”ì´ˆ/ì›”ë§ êµ¬ë¶„")
        
        # 6. ì™¸ë¶€ ë³€ìˆ˜ (ê°€ìƒì˜ ì˜ˆì‹œ)
        print("\n6ï¸âƒ£ ì™¸ë¶€ ë³€ìˆ˜ íŠ¹ì„±:")
        
        # ê°€ìƒì˜ ì™¸ë¶€ ë³€ìˆ˜ë“¤
        np.random.seed(42)
        feature_df['temperature'] = 20 + 10 * np.sin(2 * np.pi * df.index.dayofyear / 365) + np.random.normal(0, 2, len(df))
        feature_df['promotion'] = np.random.choice([0, 1], len(df), p=[0.8, 0.2])  # 20% í”„ë¡œëª¨ì…˜
        feature_df['economic_index'] = 100 + np.cumsum(np.random.normal(0, 0.5, len(df)))
        
        print("   ğŸŒ¡ï¸ ì˜¨ë„: ê³„ì ˆì„± íŒ¨í„´ + ë…¸ì´ì¦ˆ")
        print("   ğŸ¯ í”„ë¡œëª¨ì…˜: ì´ì§„ ë³€ìˆ˜ (20% í™•ë¥ )")
        print("   ğŸ’¹ ê²½ì œì§€í‘œ: ëœë¤ì›Œí¬ íŒ¨í„´")
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ì¶”ê°€
        feature_df['target'] = df[target_col].shift(-1)  # 1ì¼ í›„ ì˜ˆì¸¡
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ í†µê³„
        before_dropna = len(feature_df)
        feature_df_clean = feature_df.dropna()
        after_dropna = len(feature_df_clean)
        
        print(f"\nğŸ“Š íŠ¹ì„± ê³µí•™ ê²°ê³¼:")
        print(f"   ìƒì„±ëœ íŠ¹ì„± ìˆ˜: {len(feature_df_clean.columns) - 1}ê°œ")
        print(f"   ìœ íš¨ ë°ì´í„°: {after_dropna}í–‰ (ì›ë³¸ ëŒ€ë¹„ {after_dropna/len(df):.1%})")
        print(f"   ê²°ì¸¡ê°’ ì œê±°: {before_dropna - after_dropna}í–‰")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¯¸ë¦¬ë³´ê¸° (ê°„ë‹¨í•œ ìƒê´€ê´€ê³„)
        correlation_with_target = feature_df_clean.corr()['target'].abs().sort_values(ascending=False)
        top_features = correlation_with_target.head(8).drop('target')
        
        print(f"\nğŸ¯ ìƒê´€ê´€ê³„ ê¸°ë°˜ ì£¼ìš” íŠ¹ì„± TOP 7:")
        for i, (feature, corr) in enumerate(top_features.items(), 1):
            print(f"   {i}. {feature}: {corr:.3f}")
        
        self.original_data = df
        self.ml_data = feature_df_clean
        self.feature_columns = [col for col in feature_df_clean.columns if col != 'target']
        self.target_column = 'target'
        
        # íŠ¹ì„± ê·¸ë£¹ë³„ ì‹œê°í™”
        self._visualize_feature_groups(feature_df_clean)
        
        return feature_df_clean
    
    def _visualize_feature_groups(self, feature_df):
        """íŠ¹ì„± ê·¸ë£¹ë³„ ì‹œê°í™”"""
        
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('ğŸ› ï¸ íŠ¹ì„± ê³µí•™ ê²°ê³¼ ì‹œê°í™”', fontsize=16, fontweight='bold')
        
        # 1. ë˜ê·¸ íŠ¹ì„±ë“¤ì˜ ìƒê´€ê´€ê³„
        lag_features = [col for col in feature_df.columns if 'lag' in col]
        if len(lag_features) > 0:
            lag_corr = feature_df[lag_features + ['target']].corr()['target'].drop('target')
            lag_corr.plot(kind='bar', ax=axes[0, 0], color='skyblue')
            axes[0, 0].set_title('ğŸ“ˆ ë˜ê·¸ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ìƒê´€ê´€ê³„', fontweight='bold')
            axes[0, 0].set_ylabel('ìƒê´€ê³„ìˆ˜')
            axes[0, 0].tick_params(axis='x', rotation=45)
            axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ë¡¤ë§ í†µê³„ íŠ¹ì„±ë“¤
        rolling_features = [col for col in feature_df.columns if any(x in col for x in ['ma_', 'std_', 'max_', 'min_'])]
        if len(rolling_features) > 0:
            rolling_corr = feature_df[rolling_features[:8] + ['target']].corr()['target'].drop('target')
            rolling_corr.plot(kind='bar', ax=axes[0, 1], color='lightgreen')
            axes[0, 1].set_title('ğŸ“Š ë¡¤ë§ í†µê³„ì™€ íƒ€ê²Ÿ ìƒê´€ê´€ê³„', fontweight='bold')
            axes[0, 1].set_ylabel('ìƒê´€ê³„ìˆ˜')
            axes[0, 1].tick_params(axis='x', rotation=45)
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ë“¤
        time_features = ['month', 'day_of_week', 'quarter', 'season', 'is_weekend']
        available_time_features = [f for f in time_features if f in feature_df.columns]
        if len(available_time_features) > 0:
            time_data = feature_df[available_time_features].mean()
            time_data.plot(kind='bar', ax=axes[1, 0], color='orange')
            axes[1, 0].set_title('ğŸ“… ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± í‰ê· ê°’', fontweight='bold')
            axes[1, 0].set_ylabel('í‰ê· ê°’')
            axes[1, 0].tick_params(axis='x', rotation=45)
            axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ë³€í™”ìœ¨ íŠ¹ì„±ë“¤
        change_features = [col for col in feature_df.columns if 'pct_change' in col or 'diff' in col]
        if len(change_features) > 0:
            change_corr = feature_df[change_features + ['target']].corr()['target'].drop('target')
            change_corr.plot(kind='bar', ax=axes[1, 1], color='lightcoral')
            axes[1, 1].set_title('ğŸ“ˆ ë³€í™”ìœ¨ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ìƒê´€ê´€ê³„', fontweight='bold')
            axes[1, 1].set_ylabel('ìƒê´€ê³„ìˆ˜')
            axes[1, 1].tick_params(axis='x', rotation=45)
            axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def _create_feature_engineering_prompt(self):
        """íŠ¹ì„± ê³µí•™ AI í”„ë¡¬í”„íŠ¸"""
        return """
ì‹œê³„ì—´ íŠ¹ì„± ê³µí•™ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ íŠ¹ì„±ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**Context**: ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì í•©í•˜ë„ë¡ ë³€í™˜
**Length**: ê° íŠ¹ì„± ê·¸ë£¹ë³„ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ í•´ì„
**Examples**: 
- ë˜ê·¸ íŠ¹ì„±: "lag_7ì´ ë†’ì€ ìƒê´€ê´€ê³„ â†’ ì£¼ê°„ íŒ¨í„´ ê°•í•¨"
- ë¡¤ë§ í†µê³„: "ma_30ì´ ì¤‘ìš” â†’ ì¥ê¸° íŠ¸ë Œë“œ ì˜í–¥"
**Actionable**: íŠ¹ì„± ì„ íƒ ë° ì¶”ê°€ íŠ¹ì„± ì œì•ˆ
**Role**: ì‹¤ë¬´ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸

**íŠ¹ì„± ë¶„ì„ ëŒ€ìƒ**:
ë˜ê·¸ íŠ¹ì„±: {lag_features}
ë¡¤ë§ í†µê³„: {rolling_features}
ì‹œê°„ íŠ¹ì„±: {time_features}
ë³€í™”ìœ¨ íŠ¹ì„±: {change_features}
ìƒê´€ê´€ê³„: {correlations}

íŠ¹ì„±ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  ì˜ë¯¸ì™€ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
        """

# ì‹œê³„ì—´ ML ë³€í™˜ ì‹œìŠ¤í…œ ì‹¤í–‰
ts_ml_converter = TimeSeriesMLConverter()

print("ğŸ”„ ì‹œê³„ì—´ ë¨¸ì‹ ëŸ¬ë‹ ë³€í™˜ ì‹œìŠ¤í…œ ì‹œì‘")
print("=" * 60)

# 1. ë³€í™˜ ê°œë… ì‹œì—°
original_ts, converted_ml = ts_ml_converter.demonstrate_conversion_concept()

print(f"\n" + "="*60)

# 2. ì‹¤ì œ ë°ì´í„°ë¡œ ì¢…í•©ì  íŠ¹ì„± ê³µí•™ ì‹œì—°
# 8ì¥ Part 2ì—ì„œ ì‚¬ìš©í•œ Store Sales ë°ì´í„° ì¬í™œìš©
print("ğŸª Store Sales ë°ì´í„° ë¨¸ì‹ ëŸ¬ë‹ ë³€í™˜")

# ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± (Part 2ì™€ ë™ì¼í•œ íŒ¨í„´)
np.random.seed(42)
dates = pd.date_range('2020-01-01', '2023-12-31', freq='D')
n_days = len(dates)

# ë³µì¡í•œ íŒ¨í„´ì˜ ë§¤ì¶œ ë°ì´í„° ìƒì„±
trend = np.linspace(1000, 1500, n_days)
annual_seasonal = 100 * np.sin(2 * np.pi * np.arange(n_days) / 365.25)
weekly_seasonal = 50 * np.sin(2 * np.pi * np.arange(n_days) / 7)
monthly_seasonal = 30 * np.sin(2 * np.pi * np.arange(n_days) / 30.44)

# íŠ¹ë³„ ì´ë²¤íŠ¸ íš¨ê³¼
special_events = np.zeros(n_days)
for year in range(2020, 2024):
    # ì—°ë§ì—°ì‹œ (12ì›” 20ì¼-1ì›” 5ì¼)
    christmas_start = pd.to_datetime(f'{year}-12-20').dayofyear - 1
    christmas_end = min(pd.to_datetime(f'{year+1}-01-05').dayofyear + 365, n_days)
    if christmas_start < n_days:
        special_events[christmas_start:min(christmas_end, n_days)] += 200
    
    # ì—¬ë¦„ íœ´ê°€ì²  (7ì›”-8ì›”)
    summer_start = pd.to_datetime(f'{year}-07-01').dayofyear - 1
    summer_end = pd.to_datetime(f'{year}-08-31').dayofyear
    if summer_start < n_days and summer_end < n_days:
        special_events[summer_start:summer_end] -= 50  # íœ´ê°€ì² ì—ëŠ” ë§¤ì¶œ ê°ì†Œ

# ëœë¤ ë…¸ì´ì¦ˆ
noise = np.random.normal(0, 50, n_days)

# ìµœì¢… ë§¤ì¶œ ë°ì´í„° ìƒì„±
sales = trend + annual_seasonal + weekly_seasonal + monthly_seasonal + special_events + noise

# ìŒìˆ˜ ê°’ ë°©ì§€
sales = np.maximum(sales, 100)

# ë°ì´í„°í”„ë ˆì„ ìƒì„±
store_sales = pd.DataFrame({
    'date': dates,
    'sales': sales
})

store_sales.set_index('date', inplace=True)

print(f"ğŸ“Š Store Sales ë°ì´í„° ìƒì„± ì™„ë£Œ!")
print(f"   ê¸°ê°„: {store_sales.index.min()} ~ {store_sales.index.max()}")
print(f"   ì¼ìˆ˜: {len(store_sales)}ì¼")
print(f"   í‰ê·  ë§¤ì¶œ: ${store_sales['sales'].mean():.0f}")
print(f"   ë§¤ì¶œ ë²”ìœ„: ${store_sales['sales'].min():.0f} ~ ${store_sales['sales'].max():.0f}")

# íŠ¹ì„± ê³µí•™ ìˆ˜í–‰
feature_df = ts_ml_converter.comprehensive_feature_engineering(
    store_sales, 
    target_col='sales', 
    date_col=None  # ì´ë¯¸ ì¸ë±ìŠ¤ê°€ ë‚ ì§œ
)

## 2. ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ XGBoostì˜ ì‹œê³„ì—´ ì ìš©

### 2.1 ì‹œê³„ì—´ ë°ì´í„°ì— ë¨¸ì‹ ëŸ¬ë‹ì„ ì ìš©í•˜ëŠ” í˜ëª…ì  ì ‘ê·¼

ì „í†µì  ì‹œê³„ì—´ ëª¨ë¸ì´ **ì„ í˜•ì ì´ê³  ë‹¨ì¼ íŒ¨í„´**ì— ì§‘ì¤‘í•œë‹¤ë©´, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ì€ **ë¹„ì„ í˜• ê´€ê³„ì™€ ë³µì¡í•œ ìƒí˜¸ì‘ìš©**ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error
import xgboost as xgb
from sklearn.preprocessing import StandardScaler

class TimeSeriesMLModeler:
    """ì‹œê³„ì—´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.models = {}
        self.feature_importance = {}
        self.predictions = {}
        self.scaler = StandardScaler()
        
    def demonstrate_ml_advantages(self, feature_df):
        """ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì‹œê³„ì—´ ì ìš© ì¥ì  ì‹œì—°"""
        
        print("ğŸš€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ í˜ì‹ ì  ì¥ì ")
        print("=" * 60)
        
        print("ğŸ”¥ 1. ë¹„ì„ í˜• íŒ¨í„´ ìë™ í¬ì°©")
        print("   ì „í†µì  ëª¨ë¸: y = Î± + Î²â‚Ã—lagâ‚ + Î²â‚‚Ã—trend (ì„ í˜•)")
        print("   ë¨¸ì‹ ëŸ¬ë‹: ë³µì¡í•œ if-else ê·œì¹™ìœ¼ë¡œ ë¹„ì„ í˜• ê´€ê³„ í•™ìŠµ")
        print("   ğŸ’¡ ì˜ˆ: 'ì›”ìš”ì¼ + ë¹„ì˜¤ëŠ”ë‚  + í”„ë¡œëª¨ì…˜' â†’ íŠ¹ë³„í•œ ë§¤ì¶œ íŒ¨í„´")
        
        print("\nğŸ“Š 2. ë‹¤ë³€ëŸ‰ ì •ë³´ ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©")
        print("   ì™¸ë¶€ ë³€ìˆ˜ë“¤(ë‚ ì”¨, ê²½ì œì§€í‘œ, ì´ë²¤íŠ¸)ì„ ë‹¨ìˆœíˆ íŠ¹ì„±ìœ¼ë¡œ ì¶”ê°€")
        print("   ARIMAë¡œëŠ” ì–´ë ¤ìš´ ì™¸ë¶€ ìš”ì¸ ë°˜ì˜ì´ MLì—ì„œëŠ” ìë™í™”")
        
        print("\nğŸ¯ 3. íŠ¹ì„± ì¤‘ìš”ë„ ìë™ ë°œê²¬")
        print("   ì–´ë–¤ ìš”ì¸ì´ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œì§€ ì •ëŸ‰ì  ì¸¡ì •")
        print("   ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ì™€ ì˜ì‚¬ê²°ì • ê·¼ê±° ì œê³µ")
        
        print("\nâš¡ 4. í™•ì¥ì„±ê³¼ ì†ë„")
        print("   ìˆ˜ë°± ê°œ ì‹œê³„ì—´ì„ ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥")
        print("   ì‹¤ì‹œê°„ ì˜ˆì¸¡ê³¼ ìë™ ì¬í•™ìŠµ ìš©ì´")
        
        # ê°„ë‹¨í•œ ì„±ëŠ¥ ë¹„êµ ì‹œì—°
        X = feature_df.drop('target', axis=1)
        y = feature_df['target']
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        X = X.fillna(X.mean())
        
        # ì‹œê³„ì—´ ë¶„í• 
        tscv = TimeSeriesSplit(n_splits=3)
        
        # ë‹¨ìˆœ ëª¨ë¸ (ë§ˆì§€ë§‰ ê°’ ì‚¬ìš©)
        naive_scores = []
        rf_scores = []
        
        for train_idx, test_idx in tscv.split(X):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]
            
            # Naive ì˜ˆì¸¡ (ë§ˆì§€ë§‰ ê°’ ì‚¬ìš©)
            naive_pred = [y_train.iloc[-1]] * len(y_test)
            naive_score = mean_squared_error(y_test, naive_pred, squared=False)  # RMSE
            naive_scores.append(naive_score)
            
            # RandomForest
            rf = RandomForestRegressor(n_estimators=50, random_state=42)
            rf.fit(X_train, y_train)
            rf_pred = rf.predict(X_test)
            rf_score = mean_squared_error(y_test, rf_pred, squared=False)  # RMSE
            rf_scores.append(rf_score)
        
        naive_avg = np.mean(naive_scores)
        rf_avg = np.mean(rf_scores)
        improvement = ((naive_avg - rf_avg) / naive_avg) * 100
        
        print(f"\nğŸ“ˆ ê°„ë‹¨í•œ ì„±ëŠ¥ ë¹„êµ (RMSE):")
        print(f"   Naive (ë§ˆì§€ë§‰ ê°’): {naive_avg:.1f}")
        print(f"   RandomForest: {rf_avg:.1f}")
        print(f"   ğŸ‰ ê°œì„ ë„: {improvement:.1f}%")
        
        return X, y
    
    def build_random_forest_model(self, X, y):
        """ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì‹œê³„ì—´ ëª¨ë¸ êµ¬ì¶•"""
        
        print("\nğŸŒ² RandomForest ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸")
        print("=" * 50)
        
        print("ğŸ” RandomForestê°€ ì‹œê³„ì—´ì— íŠ¹íˆ íš¨ê³¼ì ì¸ ì´ìœ :")
        print("   1ï¸âƒ£ ê²°í•© ì•™ìƒë¸”: ì—¬ëŸ¬ íŠ¸ë¦¬ì˜ ì§‘ë‹¨ ì§€í˜œë¡œ ì•ˆì •ì  ì˜ˆì¸¡")
        print("   2ï¸âƒ£ íŠ¹ì„± ë¬´ì‘ìœ„ì„±: ê° íŠ¸ë¦¬ë§ˆë‹¤ ë‹¤ë¥¸ íŠ¹ì„± ì¡°í•©ìœ¼ë¡œ ë‹¤ì–‘ì„± í™•ë³´")
        print("   3ï¸âƒ£ ê³¼ì í•© ë°©ì§€: ê°œë³„ íŠ¸ë¦¬ëŠ” ê³¼ì í•©ë˜ì–´ë„ ì•™ìƒë¸”ë¡œ ì¼ë°˜í™”")
        print("   4ï¸âƒ£ ìë™ íŠ¹ì„± ì„ íƒ: ì¤‘ìš”í•œ ì‹œê³„ì—´ íŒ¨í„´ ìë™ ë°œê²¬")
        print("   5ï¸âƒ£ ëˆ„ë½ê°’ ë‚´ì„±: ì‹œê³„ì—´ ë°ì´í„°ì˜ ë¶ˆì™„ì „ì„±ì— ê²¬ê³ í•¨")
        
        # ì‹œê³„ì—´ êµì°¨ ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=5)
        
        # ê¸°ë³¸ RandomForest
        rf_basic = RandomForestRegressor(
            n_estimators=100,
            random_state=42,
            n_jobs=-1
        )
        
        # ìµœì í™”ëœ RandomForest
        rf_optimized = RandomForestRegressor(
            n_estimators=200,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=4,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )
        
        # ì„±ëŠ¥ ë¹„êµ
        basic_scores = cross_val_score(rf_basic, X, y, cv=tscv, 
                                     scoring='neg_mean_squared_error', n_jobs=-1)
        optimized_scores = cross_val_score(rf_optimized, X, y, cv=tscv, 
                                         scoring='neg_mean_squared_error', n_jobs=-1)
        
        basic_rmse = np.sqrt(-basic_scores)
        optimized_rmse = np.sqrt(-optimized_scores)
        
        print(f"\nğŸ“Š RandomForest ì„±ëŠ¥ ë¹„êµ:")
        print(f"   ê¸°ë³¸ ì„¤ì • RMSE: {basic_rmse.mean():.2f} Â± {basic_rmse.std():.2f}")
        print(f"   ìµœì í™” RMSE: {optimized_rmse.mean():.2f} Â± {optimized_rmse.std():.2f}")
        print(f"   ê°œì„ ë„: {((basic_rmse.mean() - optimized_rmse.mean()) / basic_rmse.mean() * 100):.1f}%")
        
        # ìµœì¢… ëª¨ë¸ í•™ìŠµ
        rf_optimized.fit(X, y)
        self.models['RandomForest'] = rf_optimized
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': rf_optimized.feature_importances_
        }).sort_values('importance', ascending=False)
        
        self.feature_importance['RandomForest'] = feature_importance
        
        print(f"\nğŸ¯ RandomForest ì£¼ìš” íŠ¹ì„± TOP 10:")
        for i, row in feature_importance.head(10).iterrows():
            print(f"   {row.name + 1:2d}. {row['feature']:<25} {row['importance']:.4f}")
        
        return rf_optimized, feature_importance
    
    def build_xgboost_model(self, X, y):
        """XGBoost ì‹œê³„ì—´ ëª¨ë¸ êµ¬ì¶•"""
        
        print("\nğŸš€ XGBoost ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸")
        print("=" * 50)
        
        print("âš¡ XGBoostê°€ ì‹œê³„ì—´ì— í˜ëª…ì ì¸ ì´ìœ :")
        print("   1ï¸âƒ£ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…: ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë‹¤ìŒ ëª¨ë¸ì´ í•™ìŠµ")
        print("   2ï¸âƒ£ ì •ê·œí™” ë‚´ì¥: L1/L2 ì •ê·œí™”ë¡œ ê³¼ì í•© ìë™ ë°©ì§€")
        print("   3ï¸âƒ£ ê²°ì¸¡ê°’ ì²˜ë¦¬: ì‹œê³„ì—´ íŠ¹ì„±ìƒ í”í•œ ê²°ì¸¡ê°’ì„ ìë™ ì²˜ë¦¬")
        print("   4ï¸âƒ£ ì´ˆê³ ì† í•™ìŠµ: ë³‘ë ¬ ì²˜ë¦¬ì™€ ë©”ëª¨ë¦¬ ìµœì í™”")
        print("   5ï¸âƒ£ ì¡°ê¸° ì¤‘ë‹¨: ê³¼ì í•© ê°ì§€ ì‹œ ìë™ìœ¼ë¡œ í•™ìŠµ ì¤‘ë‹¨")
        print("   6ï¸âƒ£ íŠ¹ì„± ìƒí˜¸ì‘ìš©: ë³µì¡í•œ ì‹œê³„ì—´ íŒ¨í„´ì˜ ìƒí˜¸ì‘ìš© ìë™ í•™ìŠµ")
        
        # ì‹œê³„ì—´ ë¶„í• ë¡œ í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ ë‚˜ëˆ„ê¸°
        split_point = int(len(X) * 0.8)
        X_train, X_val = X.iloc[:split_point], X.iloc[split_point:]
        y_train, y_val = y.iloc[:split_point], y.iloc[split_point:]
        
        # ê¸°ë³¸ XGBoost
        xgb_basic = xgb.XGBRegressor(
            n_estimators=100,
            random_state=42,
            n_jobs=-1
        )
        
        # ìµœì í™”ëœ XGBoost
        xgb_optimized = xgb.XGBRegressor(
            n_estimators=500,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.1,
            reg_lambda=1.0,
            random_state=42,
            n_jobs=-1
        )
        
        # ì¡°ê¸° ì¤‘ë‹¨ì„ í¬í•¨í•œ í›ˆë ¨
        xgb_optimized.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=50,
            verbose=False
        )
        
        # ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥
        xgb_basic.fit(X_train, y_train)
        basic_pred = xgb_basic.predict(X_val)
        basic_rmse = mean_squared_error(y_val, basic_pred, squared=False)
        
        # ìµœì í™” ëª¨ë¸ ì„±ëŠ¥
        optimized_pred = xgb_optimized.predict(X_val)
        optimized_rmse = mean_squared_error(y_val, optimized_pred, squared=False)
        
        print(f"\nğŸ“Š XGBoost ì„±ëŠ¥ ë¹„êµ:")
        print(f"   ê¸°ë³¸ ì„¤ì • RMSE: {basic_rmse:.2f}")
        print(f"   ìµœì í™” RMSE: {optimized_rmse:.2f}")
        print(f"   ê°œì„ ë„: {((basic_rmse - optimized_rmse) / basic_rmse * 100):.1f}%")
        print(f"   ìµœì  ë°˜ë³µ ìˆ˜: {xgb_optimized.best_iteration}")
        
        # ëª¨ë¸ ì €ì¥
        self.models['XGBoost'] = xgb_optimized
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': xgb_optimized.feature_importances_
        }).sort_values('importance', ascending=False)
        
        self.feature_importance['XGBoost'] = feature_importance
        
        print(f"\nğŸ¯ XGBoost ì£¼ìš” íŠ¹ì„± TOP 10:")
        for i, row in feature_importance.head(10).iterrows():
            print(f"   {row.name + 1:2d}. {row['feature']:<25} {row['importance']:.4f}")
        
        return xgb_optimized, feature_importance
    
    def compare_models_comprehensive(self, X, y):
        """í¬ê´„ì  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        
        print("\nğŸ† ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ ë¹„êµ")
        print("=" * 60)
        
        # ì‹œê³„ì—´ êµì°¨ ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=5)
        
        models = {
            'RandomForest': RandomForestRegressor(
                n_estimators=200, max_depth=15, min_samples_split=10,
                min_samples_leaf=4, random_state=42, n_jobs=-1
            ),
            'XGBoost': xgb.XGBRegressor(
                n_estimators=300, max_depth=6, learning_rate=0.05,
                subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
            )
        }
        
        results = {}
        
        for name, model in models.items():
            scores = cross_val_score(model, X, y, cv=tscv, 
                                   scoring='neg_mean_squared_error', n_jobs=-1)
            rmse_scores = np.sqrt(-scores)
            
            results[name] = {
                'RMSE_mean': rmse_scores.mean(),
                'RMSE_std': rmse_scores.std(),
                'scores': rmse_scores
            }
        
        # ê²°ê³¼ ì¶œë ¥
        print("ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½:")
        for name, metrics in results.items():
            print(f"   {name:12} RMSE: {metrics['RMSE_mean']:.2f} Â± {metrics['RMSE_std']:.2f}")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì •
        best_model_name = min(results.keys(), key=lambda x: results[x]['RMSE_mean'])
        print(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
        
        # ì„±ëŠ¥ ì°¨ì´ ë¶„ì„
        rf_rmse = results['RandomForest']['RMSE_mean']
        xgb_rmse = results['XGBoost']['RMSE_mean']
        
        if rf_rmse < xgb_rmse:
            winner, loser = 'RandomForest', 'XGBoost'
            winner_score, loser_score = rf_rmse, xgb_rmse
        else:
            winner, loser = 'XGBoost', 'RandomForest'
            winner_score, loser_score = xgb_rmse, rf_rmse
        
        improvement = ((loser_score - winner_score) / loser_score) * 100
        print(f"   ì„±ëŠ¥ ìš°ìœ„: {improvement:.1f}%")
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì •
        from scipy import stats
        if len(results['RandomForest']['scores']) > 1:
            t_stat, p_value = stats.ttest_rel(results['RandomForest']['scores'], 
                                            results['XGBoost']['scores'])
            print(f"   í†µê³„ì  ì°¨ì´: {'ìœ ì˜í•¨' if p_value < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'} (p={p_value:.3f})")
        
        # ì‹œê°í™”
        self._visualize_model_comparison(results)
        
        return results, best_model_name

    def _visualize_model_comparison(self, results):
        """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        fig.suptitle('ğŸ† ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')
        
        # 1. RMSE ë¹„êµ ë°” ì°¨íŠ¸
        models = list(results.keys())
        rmse_means = [results[model]['RMSE_mean'] for model in models]
        rmse_stds = [results[model]['RMSE_std'] for model in models]
        
        colors = ['skyblue', 'lightgreen']
        bars = axes[0].bar(models, rmse_means, yerr=rmse_stds, 
                          capsize=5, color=colors, alpha=0.7, edgecolor='black')
        axes[0].set_title('ğŸ“Š RMSE ì„±ëŠ¥ ë¹„êµ', fontweight='bold')
        axes[0].set_ylabel('RMSE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')
        axes[0].grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for bar, mean, std in zip(bars, rmse_means, rmse_stds):
            axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.5,
                        f'{mean:.1f}Â±{std:.1f}', ha='center', va='bottom', fontweight='bold')
        
        # 2. CV ì ìˆ˜ ë¶„í¬ ë°•ìŠ¤í”Œë¡¯
        cv_data = [results[model]['scores'] for model in models]
        box_plot = axes[1].boxplot(cv_data, labels=models, patch_artist=True)
        
        for patch, color in zip(box_plot['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        axes[1].set_title('ğŸ“ˆ êµì°¨ê²€ì¦ ì ìˆ˜ ë¶„í¬', fontweight='bold')
        axes[1].set_ylabel('RMSE')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

# ì‹œê³„ì—´ ML ëª¨ë¸ëŸ¬ ì‹¤í–‰
ts_ml_modeler = TimeSeriesMLModeler()

print("ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œì‘")
print("=" * 60)

# 1. ML ì¥ì  ì‹œì—°
X, y = ts_ml_modeler.demonstrate_ml_advantages(feature_df)

# 2. RandomForest ëª¨ë¸ êµ¬ì¶•
rf_model, rf_importance = ts_ml_modeler.build_random_forest_model(X, y)

# 3. XGBoost ëª¨ë¸ êµ¬ì¶•  
xgb_model, xgb_importance = ts_ml_modeler.build_xgboost_model(X, y)

# 4. ì¢…í•© ì„±ëŠ¥ ë¹„êµ
model_results, best_model = ts_ml_modeler.compare_models_comprehensive(X, y)

## 3. íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ê³¼ AI í˜‘ì—… ëª¨ë¸ í•´ì„

### 3.1 ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ "ë¸”ë™ë°•ìŠ¤" í•´ì„í•˜ê¸°

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ê°€ì¥ í° ì¥ì  ì¤‘ í•˜ë‚˜ëŠ” **ì–´ë–¤ ìš”ì¸ì´ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œì§€ ì •ëŸ‰ì ìœ¼ë¡œ ì•Œë ¤ì¤€ë‹¤**ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œê³¼ ì „ëµ ìˆ˜ë¦½ì— ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

```python
class TimeSeriesMLInterpreter:
    """ì‹œê³„ì—´ ML ëª¨ë¸ í•´ì„ ë° AI í˜‘ì—… í´ë˜ìŠ¤"""
    
    def __init__(self, ts_ml_converter):
        self.converter = ts_ml_converter
        self.interpretation_results = {}
        
        # 7ì¥ AI í˜‘ì—… í”„ë¡¬í”„íŠ¸ë¥¼ ëª¨ë¸ í•´ì„ì— íŠ¹í™”
        self.ml_interpretation_prompts = {
            'feature_importance': self._create_feature_importance_prompt(),
            'business_insights': self._create_business_insights_prompt(),
            'model_comparison': self._create_model_comparison_prompt(),
            'actionable_strategies': self._create_actionable_strategies_prompt()
        }
    
    def comprehensive_feature_analysis(self, ts_ml_modeler, X, y):
        """ì¢…í•©ì  íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        
        print("ğŸ” ì‹œê³„ì—´ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° AI í˜‘ì—… í•´ì„")
        print("=" * 60)
        
        # RandomForestì™€ XGBoost íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ
        rf_importance = ts_ml_modeler.feature_importance['RandomForest']
        xgb_importance = ts_ml_modeler.feature_importance['XGBoost']
        
        # íŠ¹ì„± ì¤‘ìš”ë„ í†µí•© ë¶„ì„
        importance_comparison = self._compare_feature_importance(rf_importance, xgb_importance)
        
        # íŠ¹ì„± ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ë¶„ì„
        grouped_importance = self._analyze_feature_groups(importance_comparison)
        
        # AI í˜‘ì—…ì„ í†µí•œ íŠ¹ì„± í•´ì„
        ai_interpretation = self._ai_assisted_feature_interpretation(
            grouped_importance, importance_comparison
        )
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
        business_insights = self._extract_business_insights(
            grouped_importance, ai_interpretation
        )
        
        # ì‹œê°í™”
        self._visualize_comprehensive_importance(
            importance_comparison, grouped_importance
        )
        
        return {
            'importance_comparison': importance_comparison,
            'grouped_importance': grouped_importance,
            'ai_interpretation': ai_interpretation,
            'business_insights': business_insights
        }
    
    def _compare_feature_importance(self, rf_importance, xgb_importance):
        """RandomForestì™€ XGBoost íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ"""
        
        print("\nğŸ”„ RandomForest vs XGBoost íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ")
        print("-" * 50)
        
        # ë‘ ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ë³‘í•©
        comparison = rf_importance.merge(
            xgb_importance, on='feature', suffixes=('_rf', '_xgb')
        )
        
        # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
        comparison['importance_avg'] = (comparison['importance_rf'] + comparison['importance_xgb']) / 2
        comparison['importance_diff'] = abs(comparison['importance_rf'] - comparison['importance_xgb'])
        
        # ì¼ì¹˜ë„ ë¶„ì„
        comparison = comparison.sort_values('importance_avg', ascending=False)
        
        print("ğŸ“Š TOP 15 ì¤‘ìš” íŠ¹ì„± (ë‘ ëª¨ë¸ í‰ê· ):")
        for i, row in comparison.head(15).iterrows():
            rf_rank = rf_importance[rf_importance['feature'] == row['feature']].index[0] + 1
            xgb_rank = xgb_importance[xgb_importance['feature'] == row['feature']].index[0] + 1
            
            print(f"   {i+1:2d}. {row['feature']:<30} "
                  f"í‰ê· : {row['importance_avg']:.4f} "
                  f"(RF: {rf_rank:2d}ìœ„, XGB: {xgb_rank:2d}ìœ„)")
        
        # ëª¨ë¸ ê°„ ì¼ì¹˜ë„ ë¶„ì„
        top_10_rf = set(rf_importance.head(10)['feature'])
        top_10_xgb = set(xgb_importance.head(10)['feature'])
        common_features = top_10_rf.intersection(top_10_xgb)
        
        print(f"\nğŸ¯ TOP 10 íŠ¹ì„± ì¼ì¹˜ë„: {len(common_features)}/10 = {len(common_features)/10*100:.0f}%")
        print(f"   ê³µí†µ ì¤‘ìš” íŠ¹ì„±: {', '.join(list(common_features)[:5])}")
        
        return comparison
    
    def _analyze_feature_groups(self, importance_comparison):
        """íŠ¹ì„± ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ë¶„ì„"""
        
        print("\nğŸ“Š íŠ¹ì„± ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ë¶„ì„")
        print("-" * 50)
        
        # íŠ¹ì„±ì„ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜
        def categorize_feature(feature_name):
            if 'lag_' in feature_name:
                return 'ê³¼ê±°ê°’(Lag)'
            elif any(x in feature_name for x in ['ma_', 'std_', 'max_', 'min_']):
                return 'ë¡¤ë§í†µê³„'
            elif any(x in feature_name for x in ['year', 'month', 'day', 'quarter', 'season']):
                return 'ì‹œê°„ê¸°ë°˜'
            elif any(x in feature_name for x in ['sin', 'cos']):
                return 'ìˆœí™˜ì‹œê°„'
            elif any(x in feature_name for x in ['pct_change', 'diff']):
                return 'ë³€í™”ìœ¨'
            elif any(x in feature_name for x in ['weekend', 'month_start', 'month_end']):
                return 'ë²”ì£¼í˜•ì‹œê°„'
            elif any(x in feature_name for x in ['temperature', 'promotion', 'economic']):
                return 'ì™¸ë¶€ë³€ìˆ˜'
            else:
                return 'ê¸°íƒ€'
        
        importance_comparison['feature_group'] = importance_comparison['feature'].apply(categorize_feature)
        
        # ê·¸ë£¹ë³„ ì¤‘ìš”ë„ ì§‘ê³„
        group_analysis = importance_comparison.groupby('feature_group').agg({
            'importance_avg': ['count', 'sum', 'mean', 'std'],
            'importance_diff': 'mean'
        }).round(4)
        
        group_analysis.columns = ['íŠ¹ì„±ìˆ˜', 'ì´ì¤‘ìš”ë„', 'í‰ê· ì¤‘ìš”ë„', 'ì¤‘ìš”ë„í‘œì¤€í¸ì°¨', 'ëª¨ë¸ê°„ì°¨ì´']
        group_analysis = group_analysis.sort_values('ì´ì¤‘ìš”ë„', ascending=False)
        
        print("ğŸ” íŠ¹ì„± ê·¸ë£¹ë³„ ì¢…í•© ë¶„ì„:")
        for group, row in group_analysis.iterrows():
            print(f"   {group:<12} | íŠ¹ì„±: {row['íŠ¹ì„±ìˆ˜']:2.0f}ê°œ | "
                  f"ì´í•©: {row['ì´ì¤‘ìš”ë„']:.3f} | í‰ê· : {row['í‰ê· ì¤‘ìš”ë„']:.4f} | "
                  f"ì¼ì¹˜ë„: {1-row['ëª¨ë¸ê°„ì°¨ì´']:.2f}")
        
        # ê° ê·¸ë£¹ì˜ ëŒ€í‘œ íŠ¹ì„± ì°¾ê¸°
        print(f"\nğŸ¯ ê·¸ë£¹ë³„ ìµœê³  ì¤‘ìš”ë„ íŠ¹ì„±:")
        for group in group_analysis.index:
            group_features = importance_comparison[importance_comparison['feature_group'] == group]
            best_feature = group_features.loc[group_features['importance_avg'].idxmax()]
            print(f"   {group:<12}: {best_feature['feature']:<25} ({best_feature['importance_avg']:.4f})")
        
        return group_analysis
    
    def _ai_assisted_feature_interpretation(self, grouped_importance, importance_comparison):
        """AI í˜‘ì—…ì„ í†µí•œ íŠ¹ì„± í•´ì„"""
        
        print("\nğŸ¤– AI í˜‘ì—… íŠ¹ì„± í•´ì„ ì‹œìŠ¤í…œ")
        print("-" * 50)
        
        # 7ì¥ì—ì„œ í•™ìŠµí•œ CLEAR ì›ì¹™ ì ìš©
        interpretation_prompt = f"""
**Context**: ì‹œê³„ì—´ ë§¤ì¶œ ì˜ˆì¸¡ ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ê²°ê³¼ í•´ì„
**Length**: ê° ì¸ì‚¬ì´íŠ¸ëŠ” 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
**Examples**: 
- "lag_1ì´ ë†’ì€ ì¤‘ìš”ë„ â†’ ì „ì¼ ë§¤ì¶œì´ ë‹¤ìŒë‚  ë§¤ì¶œì˜ ê°•ë ¥í•œ ì˜ˆì¸¡ ì§€í‘œ"
- "season íŠ¹ì„± ì¤‘ìš” â†’ ê³„ì ˆì  ì†Œë¹„ íŒ¨í„´ì´ ëª…í™•íˆ ì¡´ì¬"
**Actionable**: ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— í™œìš© ê°€ëŠ¥í•œ êµ¬ì²´ì  ì¸ì‚¬ì´íŠ¸
**Role**: ì‹œê³„ì—´ ë¶„ì„ ì „ë¬¸ê°€

**ë¶„ì„ ëŒ€ìƒ ë°ì´í„°**:
TOP 5 íŠ¹ì„±: {', '.join(importance_comparison.head(5)['feature'].tolist())}
ê·¸ë£¹ë³„ ì¤‘ìš”ë„: {dict(grouped_importance['ì´ì¤‘ìš”ë„'].head(3))}

ê° íŠ¹ì„±ê³¼ ê·¸ë£¹ì´ ì‹œê³„ì—´ ë§¤ì¶œ ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì˜ë¯¸ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œì‚¬ì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.
        """
        
        print("ğŸ’­ AI ë¶„ì„ í”„ë¡¬í”„íŠ¸ (7ì¥ CLEAR ì›ì¹™ ì ìš©):")
        print(f"   Context: ì‹œê³„ì—´ ë§¤ì¶œ ì˜ˆì¸¡ íŠ¹ì„± ë¶„ì„")
        print(f"   Length: ì¸ì‚¬ì´íŠ¸ë³„ 2-3ë¬¸ì¥")
        print(f"   Examples: êµ¬ì²´ì  í•´ì„ ì˜ˆì‹œ ì œê³µ")
        print(f"   Actionable: ì‹¤í–‰ ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸")
        print(f"   Role: ì‹œê³„ì—´ ë¶„ì„ ì „ë¬¸ê°€")
        
        # AI ì‹œë®¬ë ˆì´ì…˜ ì‘ë‹µ (ì‹¤ì œë¡œëŠ” LLM API í˜¸ì¶œ)
        ai_insights = {
            'lag_features': "ê³¼ê±°ê°’ íŠ¹ì„±(lag)ì˜ ë†’ì€ ì¤‘ìš”ë„ëŠ” ë§¤ì¶œì˜ ê°•í•œ ìê¸°ìƒê´€ì„±ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. "
                           "íŠ¹íˆ 1-7ì¼ ì „ ë§¤ì¶œì´ ë¯¸ë˜ ì˜ˆì¸¡ì— í•µì‹¬ì ì´ë©°, ì´ëŠ” ê³ ê°ì˜ êµ¬ë§¤ íŒ¨í„´ì´ "
                           "ë‹¨ê¸°ì ìœ¼ë¡œ ì§€ì†ë˜ëŠ” ê´€ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
            
            'rolling_stats': "ë¡¤ë§ í†µê³„ íŠ¹ì„±ì˜ ì¤‘ìš”ì„±ì€ ë§¤ì¶œì˜ ì¤‘ê¸° íŠ¸ë Œë“œê°€ ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. "
                            "30ì¼ ì´ë™í‰ê· ê³¼ í‘œì¤€í¸ì°¨ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì€ ì›”ë³„ ì„±ê³¼ íŒ¨í„´ì´ "
                            "í–¥í›„ ë§¤ì¶œ ì•ˆì •ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” ì§€í‘œì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.",
            
            'seasonal_patterns': "ê³„ì ˆì„± ë° ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±ì˜ ì¤‘ìš”ë„ëŠ” ê³ ê°ì˜ ì†Œë¹„ íŒ¨í„´ì´ "
                               "ìº˜ë¦°ë” ì´ë²¤íŠ¸ì™€ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤. "
                               "ì›”ë³„, ìš”ì¼ë³„ íŒ¨í„´ í™œìš©ìœ¼ë¡œ ê³„ì ˆ ë§ˆì¼€íŒ… ì „ëµ ìµœì í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
            
            'external_factors': "ì™¸ë¶€ ë³€ìˆ˜(ì˜¨ë„, í”„ë¡œëª¨ì…˜, ê²½ì œì§€í‘œ)ì˜ ì˜í–¥ë ¥ì€ "
                              "ë‚´ë¶€ ìš´ì˜ë¿ë§Œ ì•„ë‹ˆë¼ í™˜ê²½ì  ìš”ì¸ì´ ë§¤ì¶œì— ë¯¸ì¹˜ëŠ” "
                              "ë³µí•©ì  ì˜í–¥ì„ ë³´ì—¬ì£¼ë©°, í†µí•©ì  ì˜ˆì¸¡ ì „ëµì´ í•„ìš”í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤."
        }
        
        print("\nğŸ¯ AI ìƒì„± ì¸ì‚¬ì´íŠ¸:")
        for category, insight in ai_insights.items():
            print(f"   ğŸ“Œ {category}: {insight}")
        
        return ai_insights
    
    def _extract_business_insights(self, grouped_importance, ai_interpretation):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"""
        
        print("\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì•¡ì…˜ ì¸ì‚¬ì´íŠ¸")
        print("-" * 50)
        
        business_insights = [
            {
                'category': 'ë‹¨ê¸° ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ',
                'insight': 'ê³¼ê±° 1-7ì¼ ë§¤ì¶œ ë°ì´í„°ì˜ ë†’ì€ ì¤‘ìš”ë„ë¥¼ í™œìš©í•˜ì—¬ ì£¼ê°„ ì¬ê³  ê´€ë¦¬ ìµœì í™”',
                'action': 'ì¼ì¼ ë§¤ì¶œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ë° ì£¼ê°„ ì¬ê³  ìë™ ì¡°ì ˆ ì‹œìŠ¤í…œ',
                'priority': 'High',
                'impact': 'ì¬ê³  ê³¼ë¶€ì¡± 20% ê°ì†Œ ì˜ˆìƒ'
            },
            {
                'category': 'ê³„ì ˆ ë§ˆì¼€íŒ… ì „ëµ',
                'insight': 'ì›”ë³„/ê³„ì ˆë³„ íŒ¨í„´ì´ ê°•í•˜ë¯€ë¡œ ê³„ì ˆ ë§ì¶¤í˜• í”„ë¡œëª¨ì…˜ ì „ëµ íš¨ê³¼ì ',
                'action': 'ê³„ì ˆë³„ ìƒí’ˆ ê¸°íš ë° í”„ë¡œëª¨ì…˜ ì¼ì • 3ê°œì›” ì „ ë¯¸ë¦¬ ìˆ˜ë¦½',
                'priority': 'Medium',
                'impact': 'ê³„ì ˆ ìƒí’ˆ ë§¤ì¶œ 15% ì¦ê°€ ì˜ˆìƒ'
            },
            {
                'category': 'ì™¸ë¶€ ìš”ì¸ í™œìš©',
                'insight': 'ë‚ ì”¨ì™€ ê²½ì œì§€í‘œê°€ ë§¤ì¶œì— ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ ì˜ˆì¸¡ ëª¨ë¸ì— í†µí•© í•„ìš”',
                'action': 'ê¸°ìƒì²­ APIì™€ ê²½ì œì§€í‘œ ë°ì´í„° ì‹¤ì‹œê°„ ì—°ë™ ì‹œìŠ¤í…œ êµ¬ì¶•',
                'priority': 'Medium',
                'impact': 'ì˜ˆì¸¡ ì •í™•ë„ 8-12% í–¥ìƒ'
            },
            {
                'category': 'ë¡¤ë§ íŠ¸ë Œë“œ ëª¨ë‹ˆí„°ë§',
                'insight': '30ì¼ ì´ë™í‰ê· ì˜ ì¤‘ìš”ì„±ìœ¼ë¡œ ì¤‘ê¸° íŠ¸ë Œë“œ ë³€í™” ì¡°ê¸° ê°ì§€ ê°€ëŠ¥',
                'action': 'ì›”ë³„ ì„±ê³¼ íŠ¸ë Œë“œ ì•Œë¦¼ ì‹œìŠ¤í…œ ë° ì´ìƒ íŒ¨í„´ ìë™ ê°ì§€',
                'priority': 'High',
                'impact': 'íŠ¸ë Œë“œ ë³€í™” ëŒ€ì‘ ì‹œê°„ 50% ë‹¨ì¶•'
            }
        ]
        
        print("ğŸš€ ìš°ì„ ìˆœìœ„ë³„ ì‹¤í–‰ ê³„íš:")
        for i, insight in enumerate(business_insights, 1):
            print(f"\n   {i}. {insight['category']} [{insight['priority']}]")
            print(f"      ğŸ’¡ ì¸ì‚¬ì´íŠ¸: {insight['insight']}")
            print(f"      ğŸ¯ ì‹¤í–‰ë°©ì•ˆ: {insight['action']}")
            print(f"      ğŸ“ˆ ê¸°ëŒ€íš¨ê³¼: {insight['impact']}")
        
        return business_insights
    
    def _visualize_comprehensive_importance(self, importance_comparison, grouped_importance):
        """ì¢…í•©ì  íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”"""
        
        fig, axes = plt.subplots(2, 2, figsize=(18, 12))
        fig.suptitle('ğŸ” ì‹œê³„ì—´ ML ëª¨ë¸ íŠ¹ì„± ì¤‘ìš”ë„ ì¢…í•© ë¶„ì„', fontsize=16, fontweight='bold')
        
        # 1. ìƒìœ„ íŠ¹ì„± ë¹„êµ (RF vs XGB)
        top_features = importance_comparison.head(12)
        x_pos = np.arange(len(top_features))
        
        axes[0, 0].barh(x_pos - 0.2, top_features['importance_rf'], 0.4, 
                       label='RandomForest', color='skyblue', alpha=0.8)
        axes[0, 0].barh(x_pos + 0.2, top_features['importance_xgb'], 0.4, 
                       label='XGBoost', color='lightgreen', alpha=0.8)
        
        axes[0, 0].set_yticks(x_pos)
        axes[0, 0].set_yticklabels(top_features['feature'], fontsize=9)
        axes[0, 0].set_xlabel('Feature Importance')
        axes[0, 0].set_title('ğŸ“Š TOP 12 íŠ¹ì„± ì¤‘ìš”ë„ ë¹„êµ', fontweight='bold')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ê·¸ë£¹ë³„ ì¤‘ìš”ë„
        group_importance = grouped_importance['ì´ì¤‘ìš”ë„'].sort_values(ascending=True)
        colors = plt.cm.Set3(np.linspace(0, 1, len(group_importance)))
        
        bars = axes[0, 1].barh(range(len(group_importance)), group_importance.values, color=colors)
        axes[0, 1].set_yticks(range(len(group_importance)))
        axes[0, 1].set_yticklabels(group_importance.index)
        axes[0, 1].set_xlabel('ì´ ì¤‘ìš”ë„')
        axes[0, 1].set_title('ğŸ·ï¸ íŠ¹ì„± ê·¸ë£¹ë³„ ì´ ì¤‘ìš”ë„', fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for i, bar in enumerate(bars):
            width = bar.get_width()
            axes[0, 1].text(width + 0.001, bar.get_y() + bar.get_height()/2, 
                           f'{width:.3f}', ha='left', va='center', fontweight='bold')
        
        # 3. ëª¨ë¸ë³„ ìƒê´€ê´€ê³„
        rf_importance_vals = importance_comparison['importance_rf']
        xgb_importance_vals = importance_comparison['importance_xgb']
        
        axes[1, 0].scatter(rf_importance_vals, xgb_importance_vals, alpha=0.6, s=50)
        axes[1, 0].plot([0, max(rf_importance_vals.max(), xgb_importance_vals.max())], 
                       [0, max(rf_importance_vals.max(), xgb_importance_vals.max())], 
                       'r--', alpha=0.8)
        axes[1, 0].set_xlabel('RandomForest Importance')
        axes[1, 0].set_ylabel('XGBoost Importance')
        axes[1, 0].set_title('ğŸ”„ ëª¨ë¸ ê°„ íŠ¹ì„± ì¤‘ìš”ë„ ìƒê´€ê´€ê³„', fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3)
        
        # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        correlation = np.corrcoef(rf_importance_vals, xgb_importance_vals)[0, 1]
        axes[1, 0].text(0.05, 0.95, f'ìƒê´€ê³„ìˆ˜: {correlation:.3f}', 
                       transform=axes[1, 0].transAxes, fontweight='bold',
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # 4. ê·¸ë£¹ë³„ í‰ê·  ì¤‘ìš”ë„ì™€ íŠ¹ì„± ìˆ˜
        group_stats = grouped_importance[['íŠ¹ì„±ìˆ˜', 'í‰ê· ì¤‘ìš”ë„']]
        
        # ì´ì¤‘ yì¶•
        ax4_twin = axes[1, 1].twinx()
        
        bars1 = axes[1, 1].bar(range(len(group_stats)), group_stats['íŠ¹ì„±ìˆ˜'], 
                              alpha=0.7, color='lightblue', label='íŠ¹ì„± ìˆ˜')
        line1 = ax4_twin.plot(range(len(group_stats)), group_stats['í‰ê· ì¤‘ìš”ë„'], 
                             'ro-', linewidth=2, markersize=6, label='í‰ê·  ì¤‘ìš”ë„')
        
        axes[1, 1].set_xticks(range(len(group_stats)))
        axes[1, 1].set_xticklabels(group_stats.index, rotation=45)
        axes[1, 1].set_ylabel('íŠ¹ì„± ìˆ˜', color='blue')
        ax4_twin.set_ylabel('í‰ê·  ì¤‘ìš”ë„', color='red')
        axes[1, 1].set_title('ğŸ“ˆ ê·¸ë£¹ë³„ íŠ¹ì„± ìˆ˜ vs í‰ê·  ì¤‘ìš”ë„', fontweight='bold')
        
        # ë²”ë¡€
        lines1, labels1 = axes[1, 1].get_legend_handles_labels()
        lines2, labels2 = ax4_twin.get_legend_handles_labels()
        axes[1, 1].legend(lines1 + lines2, labels1 + labels2, loc='upper right')
        
        plt.tight_layout()
        plt.show()
    
    def _create_feature_importance_prompt(self):
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ìš© í”„ë¡¬í”„íŠ¸"""
        return """
ì‹œê³„ì—´ íŠ¹ì„± ì¤‘ìš”ë„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**Context**: ì‹œê³„ì—´ ë§¤ì¶œ ì˜ˆì¸¡ ëª¨ë¸ì˜ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
**Length**: íŠ¹ì„±ë³„ë¡œ 2-3ë¬¸ì¥ìœ¼ë¡œ í•´ì„
**Examples**: 
- "lag_7ì´ ë†’ì€ ì¤‘ìš”ë„ â†’ ì£¼ê°„ ì£¼ê¸°ì„±ì´ ê°•í•¨"
- "temperature ì¤‘ìš” â†’ ë‚ ì”¨ê°€ ì†Œë¹„íŒ¨í„´ì— ì˜í–¥"
**Actionable**: ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµì— í™œìš© ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸
**Role**: ì‹œê³„ì—´ ë¶„ì„ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸

**ë¶„ì„ ëŒ€ìƒ**:
íŠ¹ì„± ì¤‘ìš”ë„: {feature_importance}
ê·¸ë£¹ë³„ ì¤‘ìš”ë„: {group_importance}

ê° íŠ¹ì„±ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  ì˜ë¯¸ì™€ í™œìš© ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.
        """

# ì‹œê³„ì—´ ML í•´ì„ê¸° ì‹¤í–‰
ts_ml_interpreter = TimeSeriesMLInterpreter(ts_ml_converter)

print("ğŸ” ì‹œê³„ì—´ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•´ì„ ë° AI í˜‘ì—…")
print("=" * 60)

# íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ìˆ˜í–‰
interpretation_results = ts_ml_interpreter.comprehensive_feature_analysis(ts_ml_modeler, X, y)

## 4. í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”: ì „í†µì  + ML ëª¨ë¸ì˜ ìµœê°• ì¡°í•©

### 4.1 ì™œ í•˜ì´ë¸Œë¦¬ë“œê°€ í˜ì‹ ì ì¸ê°€?

ì „í†µì  ì‹œê³„ì—´ ëª¨ë¸(ARIMA, SARIMA)ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸(RandomForest, XGBoost)ì€ ê°ê° ê³ ìœ í•œ ì¥ì ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. **í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”**ì€ ì´ëŸ¬í•œ ì„œë¡œ ë‹¤ë¥¸ ì ‘ê·¼ë²•ì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ **ìƒí˜¸ ë³´ì™„ì  ì˜ˆì¸¡ë ¥**ì„ ë°œíœ˜í•©ë‹ˆë‹¤.

```python
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import LinearRegression

class HybridTimeSeriesEnsemble:
    """ì „í†µì  + ML í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ì•™ìƒë¸”"""
    
    def __init__(self):
        self.traditional_models = {}
        self.ml_models = {}
        self.hybrid_ensemble = None
        self.performance_comparison = {}
        
    def build_traditional_models(self, timeseries_data):
        """ì „í†µì  ì‹œê³„ì—´ ëª¨ë¸ êµ¬ì¶•"""
        
        print("ğŸ“ˆ ì „í†µì  ì‹œê³„ì—´ ëª¨ë¸ êµ¬ì¶• (8ì¥ Part 2 ë³µìŠµ)")
        print("-" * 50)
        
        ts = timeseries_data['sales']
        
        # 1. ARIMA ëª¨ë¸
        try:
            arima_model = ARIMA(ts, order=(2, 1, 2))
            arima_fitted = arima_model.fit()
            self.traditional_models['ARIMA'] = arima_fitted
            print("âœ… ARIMA(2,1,2) ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ARIMA ëª¨ë¸ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        
        # 2. Holt-Winters ì§€ìˆ˜í‰í™œë²•
        try:
            hw_model = ExponentialSmoothing(
                ts, 
                trend='add', 
                seasonal='add', 
                seasonal_periods=7  # ì£¼ê°„ ê³„ì ˆì„±
            )
            hw_fitted = hw_model.fit()
            self.traditional_models['HoltWinters'] = hw_fitted
            print("âœ… Holt-Winters ëª¨ë¸ êµ¬ì¶• ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Holt-Winters ëª¨ë¸ êµ¬ì¶• ì‹¤íŒ¨: {e}")
        
        # 3. Naive ëª¨ë¸ (ê¸°ì¤€ì„ )
        self.traditional_models['Naive'] = None  # ë‹¨ìˆœ êµ¬í˜„
        print("âœ… Naive ê¸°ì¤€ ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
        
        return self.traditional_models
    
    def prepare_ml_models(self, ts_ml_modeler):
        """ML ëª¨ë¸ ì¤€ë¹„"""
        
        print("\nğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì¤€ë¹„")
        print("-" * 30)
        
        self.ml_models = {
            'RandomForest': ts_ml_modeler.models['RandomForest'],
            'XGBoost': ts_ml_modeler.models['XGBoost']
        }
        
        print("âœ… RandomForest ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print("âœ… XGBoost ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        return self.ml_models
    
    def create_hybrid_ensemble(self, X, y, timeseries_data):
        """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ìƒì„±"""
        
        print("\nğŸ”— í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” êµ¬ì¶•")
        print("-" * 40)
        
        print("ğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”ì˜ 3ê°€ì§€ í•µì‹¬ ì „ëµ:")
        print("   1ï¸âƒ£ ëª¨ë¸ ë‹¤ì–‘ì„±: ì„ í˜•(ARIMA) + ë¹„ì„ í˜•(ML) ê²°í•©")
        print("   2ï¸âƒ£ ì˜¤ë¥˜ íŒ¨í„´ ë³´ì™„: ê° ëª¨ë¸ì˜ ì•½ì ì„ ë‹¤ë¥¸ ëª¨ë¸ì´ ë³´ì™„")
        print("   3ï¸âƒ£ ê°€ì¤‘ íˆ¬í‘œ: ì„±ëŠ¥ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ ìë™ ì¡°ì •")
        
        # ì‹œê³„ì—´ ë¶„í• ë¡œ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ì¤€ë¹„
        split_point = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]
        y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]
        
        ts_train = timeseries_data.iloc[:split_point]
        ts_test = timeseries_data.iloc[split_point:]
        
        # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€
        model_predictions = {}
        model_scores = {}
        
        print("\nğŸ“Š ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:")
        
        # 1. ì „í†µì  ëª¨ë¸ í‰ê°€
        for name, model in self.traditional_models.items():
            if name == 'Naive':
                pred = [ts_train['sales'].iloc[-1]] * len(ts_test)
            elif name == 'ARIMA' and model is not None:
                pred = model.forecast(steps=len(ts_test))
            elif name == 'HoltWinters' and model is not None:
                pred = model.forecast(steps=len(ts_test))
            else:
                continue
                
            rmse = mean_squared_error(y_test, pred, squared=False)
            model_predictions[name] = pred
            model_scores[name] = rmse
            print(f"   {name:<15} RMSE: {rmse:.2f}")
        
        # 2. ML ëª¨ë¸ í‰ê°€
        for name, model in self.ml_models.items():
            pred = model.predict(X_test)
            rmse = mean_squared_error(y_test, pred, squared=False)
            model_predictions[name] = pred
            model_scores[name] = rmse
            print(f"   {name:<15} RMSE: {rmse:.2f}")
        
        # 3. ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        # ë‚®ì€ RMSEì¼ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
        total_inverse_rmse = sum(1/score for score in model_scores.values())
        model_weights = {name: (1/score)/total_inverse_rmse for name, score in model_scores.items()}
        
        print(f"\nğŸ¯ ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜:")
        for name, weight in model_weights.items():
            print(f"   {name:<15} ê°€ì¤‘ì¹˜: {weight:.3f} (ì„±ëŠ¥: {model_scores[name]:.2f})")
        
        # 4. ê°€ì¤‘ ì•™ìƒë¸” ì˜ˆì¸¡
        ensemble_pred = np.zeros(len(y_test))
        for name, pred in model_predictions.items():
            ensemble_pred += np.array(pred) * model_weights[name]
        
        ensemble_rmse = mean_squared_error(y_test, ensemble_pred, squared=False)
        
        # 5. ìµœê³  ê°œë³„ ëª¨ë¸ê³¼ ë¹„êµ
        best_individual_rmse = min(model_scores.values())
        improvement = ((best_individual_rmse - ensemble_rmse) / best_individual_rmse) * 100
        
        print(f"\nğŸ† í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì„±ê³¼:")
        print(f"   ì•™ìƒë¸” RMSE: {ensemble_rmse:.2f}")
        print(f"   ìµœê³  ê°œë³„ ëª¨ë¸: {best_individual_rmse:.2f}")
        print(f"   ğŸ‰ ì„±ëŠ¥ í–¥ìƒ: {improvement:.1f}%")
        
        # ê²°ê³¼ ì €ì¥
        self.performance_comparison = {
            'individual_scores': model_scores,
            'individual_predictions': model_predictions,
            'ensemble_prediction': ensemble_pred,
            'ensemble_score': ensemble_rmse,
            'weights': model_weights,
            'improvement': improvement,
            'test_actual': y_test.values
        }
        
        # ì‹œê°í™”
        self._visualize_hybrid_ensemble_performance(X_test.index)
        
        return self.performance_comparison
    
    def _visualize_hybrid_ensemble_performance(self, test_index):
        """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì„±ëŠ¥ ì‹œê°í™”"""
        
        fig, axes = plt.subplots(2, 2, figsize=(18, 12))
        fig.suptitle('ğŸ”— í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì„±ëŠ¥ ë¶„ì„', fontsize=16, fontweight='bold')
        
        # 1. ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ
        actual = self.performance_comparison['test_actual']
        ensemble_pred = self.performance_comparison['ensemble_prediction']
        
        axes[0, 0].plot(test_index, actual, 'k-', linewidth=2, label='ì‹¤ì œê°’', alpha=0.8)
        axes[0, 0].plot(test_index, ensemble_pred, 'r-', linewidth=2, label='í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”', alpha=0.8)
        
        # ê°œë³„ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë„ í‘œì‹œ
        best_model = min(self.performance_comparison['individual_scores'], 
                        key=self.performance_comparison['individual_scores'].get)
        best_pred = self.performance_comparison['individual_predictions'][best_model]
        
        axes[0, 0].plot(test_index, best_pred, '--', linewidth=1.5, 
                       label=f'ìµœê³  ê°œë³„ ëª¨ë¸ ({best_model})', alpha=0.7)
        
        axes[0, 0].set_title('ğŸ“ˆ ì˜ˆì¸¡ ì„±ëŠ¥ ë¹„êµ', fontweight='bold')
        axes[0, 0].set_ylabel('ë§¤ì¶œ')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. ëª¨ë¸ë³„ RMSE ë¹„êµ
        models = list(self.performance_comparison['individual_scores'].keys()) + ['í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”']
        rmse_scores = list(self.performance_comparison['individual_scores'].values()) + [self.performance_comparison['ensemble_score']]
        
        colors = ['lightblue', 'lightgreen', 'orange', 'lightcoral', 'red']
        bars = axes[0, 1].bar(models, rmse_scores, color=colors[:len(models)], alpha=0.7)
        
        # ì•™ìƒë¸” ë°” ê°•ì¡°
        bars[-1].set_color('darkred')
        bars[-1].set_alpha(1.0)
        
        axes[0, 1].set_title('ğŸ“Š ëª¨ë¸ë³„ RMSE ì„±ëŠ¥', fontweight='bold')
        axes[0, 1].set_ylabel('RMSE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        axes[0, 1].grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for bar, score in zip(bars, rmse_scores):
            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                           f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
        
        # 3. ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì´ì°¨íŠ¸
        weights = self.performance_comparison['weights']
        axes[1, 0].pie(weights.values(), labels=weights.keys(), autopct='%1.1f%%',
                      colors=colors[:len(weights)], startangle=90)
        axes[1, 0].set_title('ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ê°€ì¤‘ì¹˜', fontweight='bold')
        
        # 4. ì˜¤ì°¨ ë¶„ì„
        ensemble_errors = actual - ensemble_pred
        best_errors = actual - np.array(best_pred)
        
        axes[1, 1].hist(ensemble_errors, bins=20, alpha=0.7, label='í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”', color='red')
        axes[1, 1].hist(best_errors, bins=20, alpha=0.5, label=f'ìµœê³  ê°œë³„ ëª¨ë¸ ({best_model})', color='blue')
        
        axes[1, 1].set_title('ğŸ“ˆ ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„í¬ ë¹„êµ', fontweight='bold')
        axes[1, 1].set_xlabel('ì˜ˆì¸¡ ì˜¤ì°¨')
        axes[1, 1].set_ylabel('ë¹ˆë„')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
        print(f"\nğŸ“‹ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì„±ëŠ¥ ìš”ì•½:")
        print(f"   ğŸ¯ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ: {self.performance_comparison['improvement']:.1f}%")
        print(f"   ğŸ“Š ì•™ìƒë¸” RMSE: {self.performance_comparison['ensemble_score']:.2f}")
        print(f"   ğŸ† ìµœê³  ê°œë³„ ëª¨ë¸: {best_model} (RMSE: {min(self.performance_comparison['individual_scores'].values()):.2f})")
        print(f"   ğŸ”— í™œìš© ëª¨ë¸ ìˆ˜: {len(weights)}ê°œ")


# í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ì‹¤í–‰
hybrid_ensemble = HybridTimeSeriesEnsemble()

print("ğŸ”— í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ì•™ìƒë¸” ì‹œìŠ¤í…œ")
print("=" * 60)

# 1. ì „í†µì  ëª¨ë¸ êµ¬ì¶•
traditional_models = hybrid_ensemble.build_traditional_models(store_sales)

# 2. ML ëª¨ë¸ ì¤€ë¹„
ml_models = hybrid_ensemble.prepare_ml_models(ts_ml_modeler)

# 3. í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ìƒì„±
ensemble_results = hybrid_ensemble.create_hybrid_ensemble(X, y, store_sales)

## 5. ì‹¤ì „ í”„ë¡œì íŠ¸: Store Sales ì‹œê³„ì—´ ML ì˜ˆì¸¡ ì‹œìŠ¤í…œ

### 5.1 ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ ì •ì˜ì™€ ì‹œìŠ¤í…œ ì„¤ê³„

```python
class StoreTimeSeriesMLSystem:
    """ì™„ì „í•œ Store Sales ì‹œê³„ì—´ ML ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.data_pipeline = None
        self.model_pipeline = None
        self.monitoring_system = None
        self.business_dashboard = None
        
    def create_production_system(self, store_sales_data, feature_df, best_models):
        """í”„ë¡œë•ì…˜ ë ˆë²¨ ì‹œê³„ì—´ ML ì‹œìŠ¤í…œ êµ¬ì¶•"""
        
        print("ğŸ­ Store Sales ì‹œê³„ì—´ ML í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ êµ¬ì¶•")
        print("=" * 60)
        
        print("ğŸ¯ ì‹œìŠ¤í…œ ëª©í‘œ:")
        print("   1ï¸âƒ£ ì‹¤ì‹œê°„ ë§¤ì¶œ ì˜ˆì¸¡ (ì¼ë³„/ì£¼ë³„/ì›”ë³„)")
        print("   2ï¸âƒ£ ìë™ ì¬í•™ìŠµ ë° ëª¨ë¸ ì—…ë°ì´íŠ¸")
        print("   3ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ì•Œë¦¼ ë° ì˜ì‚¬ê²°ì • ì§€ì›")
        print("   4ï¸âƒ£ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í’ˆì§ˆ ê´€ë¦¬")
        
        # 1. ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„¤ê³„
        data_pipeline = self._design_data_pipeline(store_sales_data, feature_df)
        
        # 2. ëª¨ë¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
        model_pipeline = self._build_model_pipeline(best_models)
        
        # 3. ì˜ˆì¸¡ ë° í‰ê°€ ì‹œìŠ¤í…œ
        prediction_system = self._create_prediction_system()
        
        # 4. ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ 
        business_dashboard = self._create_business_dashboard(store_sales_data, feature_df)
        
        # 5. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ
        monitoring_system = self._setup_monitoring_system()
        
        print(f"\nğŸ‰ í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
        print(f"   ğŸ“Š ë°ì´í„° íŒŒì´í”„ë¼ì¸: âœ…")
        print(f"   ğŸ¤– ëª¨ë¸ íŒŒì´í”„ë¼ì¸: âœ…") 
        print(f"   ğŸ“ˆ ì˜ˆì¸¡ ì‹œìŠ¤í…œ: âœ…")
        print(f"   ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ: âœ…")
        print(f"   ğŸ”” ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ: âœ…")
        
        return {
            'data_pipeline': data_pipeline,
            'model_pipeline': model_pipeline,
            'prediction_system': prediction_system,
            'business_dashboard': business_dashboard,
            'monitoring_system': monitoring_system
        }
    
    def _design_data_pipeline(self, store_sales_data, feature_df):
        """ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„¤ê³„"""
        
        print("\nğŸ“Š ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„¤ê³„")
        print("-" * 40)
        
        pipeline_config = {
            'data_source': 'ë§¤ì¥ POS ì‹œìŠ¤í…œ, ì™¸ë¶€ API (ë‚ ì”¨, ê²½ì œì§€í‘œ)',
            'update_frequency': 'ì¼ 1íšŒ (ìì • 30ë¶„)',
            'feature_engineering': 'TimeSeriesMLConverter í´ë˜ìŠ¤ ìë™ ì‹¤í–‰',
            'data_validation': 'í’ˆì§ˆ ì²´í¬, ì´ìƒê°’ íƒì§€, ì™„ì •ì„± ê²€ì¦',
            'storage': 'PostgreSQL ì‹œê³„ì—´ DB + Redis ìºì‹œ',
            'backup': 'ì¼ë³„ ë°±ì—…, 1ë…„ ë³´ê´€'
        }
        
        print("ğŸ”„ ë°ì´í„° í”Œë¡œìš°:")
        print("   1. POS ì‹œìŠ¤í…œ â†’ ETL â†’ ì›ë³¸ ë°ì´í„° ìˆ˜ì§‘")
        print("   2. ì™¸ë¶€ API â†’ ë‚ ì”¨/ê²½ì œ ë°ì´í„° ìˆ˜ì§‘")
        print("   3. íŠ¹ì„± ê³µí•™ â†’ 60+ íŒŒìƒ íŠ¹ì„± ìë™ ìƒì„±")
        print("   4. ë°ì´í„° ê²€ì¦ â†’ í’ˆì§ˆ ì²´í¬ ë° ì´ìƒê°’ ì²˜ë¦¬")
        print("   5. ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ â†’ DB ì €ì¥ ë° ìºì‹œ ê°±ì‹ ")
        
        return pipeline_config
    
    def _build_model_pipeline(self, best_models):
        """ëª¨ë¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"""
        
        print("\nğŸ¤– ëª¨ë¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•")
        print("-" * 40)
        
        pipeline_config = {
            'model_types': ['RandomForest', 'XGBoost', 'ARIMA', 'HoltWinters'],
            'ensemble_strategy': 'ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ íˆ¬í‘œ',
            'retrain_frequency': 'ì£¼ 1íšŒ (ì¼ìš”ì¼ ìƒˆë²½ 2ì‹œ)',
            'model_validation': 'TimeSeriesSplit 5-fold êµì°¨ê²€ì¦',
            'performance_threshold': 'RMSE ê¸°ì¤€ì„  ëŒ€ë¹„ 95% ì´ìƒ',
            'deployment_strategy': 'Blue-Green ë°°í¬'
        }
        
        print("ğŸš€ ëª¨ë¸ ë°°í¬ ì „ëµ:")
        print("   1. ì£¼ê°„ ì¬í•™ìŠµ â†’ ìµœì‹  ë°ì´í„°ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸")
        print("   2. A/B í…ŒìŠ¤íŠ¸ â†’ ì‹ ê·œ ëª¨ë¸ vs ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        print("   3. ìë™ ë°°í¬ â†’ ì„±ëŠ¥ ê°œì„  ì‹œ ìë™ ë°°í¬")
        print("   4. ë¡¤ë°± ì¤€ë¹„ â†’ ì„±ëŠ¥ ì €í•˜ ì‹œ ì´ì „ ëª¨ë¸ë¡œ ì¦‰ì‹œ ë³µêµ¬")
        
        return pipeline_config
    
    def _create_prediction_system(self):
        """ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•"""
        
        print("\nğŸ“ˆ ì˜ˆì¸¡ ì‹œìŠ¤í…œ êµ¬ì¶•")
        print("-" * 40)
        
        prediction_config = {
            'prediction_horizons': {
                'ë‹¨ê¸° (1-7ì¼)': 'ì¬ê³  ê´€ë¦¬, ì¸ë ¥ ë°°ì¹˜',
                'ì¤‘ê¸° (1-4ì£¼)': 'êµ¬ë§¤ ê³„íš, í”„ë¡œëª¨ì…˜ ì¼ì •',
                'ì¥ê¸° (1-3ê°œì›”)': 'ì˜ˆì‚° ê³„íš, í™•ì¥ ì „ëµ'
            },
            'confidence_intervals': '95% ì‹ ë¢°êµ¬ê°„ ì œê³µ',
            'scenario_analysis': 'ë‚™ê´€/ê¸°ì¤€/ë¹„ê´€ ì‹œë‚˜ë¦¬ì˜¤',
            'real_time_updates': 'ë§¤ì‹œê°„ ì˜ˆì¸¡ ì—…ë°ì´íŠ¸',
            'api_endpoint': '/api/v1/forecast'
        }
        
        print("ğŸ¯ ì˜ˆì¸¡ ì„œë¹„ìŠ¤:")
        print("   ğŸ“… ì¼ë³„ ì˜ˆì¸¡: í–¥í›„ 30ì¼ ë§¤ì¶œ ì˜ˆì¸¡")
        print("   ğŸ“Š ì£¼ë³„ ì˜ˆì¸¡: í–¥í›„ 12ì£¼ íŠ¸ë Œë“œ ë¶„ì„")
        print("   ğŸ“ˆ ì›”ë³„ ì˜ˆì¸¡: í–¥í›„ 6ê°œì›” ê³„íš ìˆ˜ë¦½")
        print("   ğŸ”® ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„: ë‹¤ì–‘í•œ ìƒí™©ë³„ ì˜ˆì¸¡")
        
        return prediction_config
    
    def _create_business_dashboard(self, store_sales_data, feature_df):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•"""
        
        print("\nğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•")
        print("-" * 40)
        
        # ì‹¤ì œ ëŒ€ì‹œë³´ë“œ ì‹œë®¬ë ˆì´ì…˜
        current_date = store_sales_data.index[-1]
        recent_sales = store_sales_data['sales'].iloc[-30:].mean()
        
        dashboard_metrics = {
            'í˜„ì¬ ì¼í‰ê·  ë§¤ì¶œ': f'${recent_sales:,.0f}',
            'ì „ì›” ëŒ€ë¹„ ì„±ì¥ë¥ ': f'{np.random.uniform(-5, 15):.1f}%',
            'ì˜ˆì¸¡ ì •í™•ë„ (RMSE)': f'{ensemble_results["ensemble_score"]:.0f}',
            'ë‹¤ìŒì£¼ ì˜ˆìƒ ë§¤ì¶œ': f'${recent_sales * (1 + np.random.uniform(-0.1, 0.1)) * 7:,.0f}',
            'ì¬ê³  ìµœì í™” íš¨ê³¼': f'{np.random.uniform(15, 25):.0f}% ê°œì„ ',
            'ë§¤ì¶œ ì˜ˆì¸¡ ì‹ ë¢°ë„': '91.2%'
        }
        
        print("ğŸ“Š ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ KPI:")
        for metric, value in dashboard_metrics.items():
            print(f"   ğŸ“ˆ {metric}: {value}")
        
        # ì‹œê°í™” ì‹œë®¬ë ˆì´ì…˜
        self._simulate_business_dashboard(store_sales_data, dashboard_metrics)
        
        return dashboard_metrics
    
    def _simulate_business_dashboard(self, store_sales_data, metrics):
        """ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ ì‹œê°í™” ì‹œë®¬ë ˆì´ì…˜"""
        
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('ğŸ’¼ Store Sales ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ', fontsize=18, fontweight='bold')
        
        # 1. ë§¤ì¶œ íŠ¸ë Œë“œ
        recent_data = store_sales_data['sales'].iloc[-60:]
        axes[0, 0].plot(recent_data.index, recent_data.values, linewidth=2, color='darkblue')
        axes[0, 0].fill_between(recent_data.index, recent_data.values, alpha=0.3, color='lightblue')
        axes[0, 0].set_title('ğŸ“ˆ ìµœê·¼ 60ì¼ ë§¤ì¶œ íŠ¸ë Œë“œ', fontweight='bold', fontsize=14)
        axes[0, 0].set_ylabel('ë§¤ì¶œ ($)')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. ì£¼ìš” KPI í‘œì‹œ
        kpi_names = ['í˜„ì¬ ì¼í‰ê·  ë§¤ì¶œ', 'ì „ì›” ëŒ€ë¹„ ì„±ì¥ë¥ ', 'ì˜ˆì¸¡ ì •í™•ë„ (RMSE)']
        kpi_values = [metrics[name] for name in kpi_names]
        colors = ['green', 'orange', 'purple']
        
        for i, (name, value, color) in enumerate(zip(kpi_names, kpi_values, colors)):
            axes[0, 1].text(0.1, 0.8 - i*0.25, name, fontsize=12, fontweight='bold')
            axes[0, 1].text(0.1, 0.7 - i*0.25, value, fontsize=16, color=color, fontweight='bold')
        
        axes[0, 1].set_xlim(0, 1)
        axes[0, 1].set_ylim(0, 1)
        axes[0, 1].axis('off')
        axes[0, 1].set_title('ğŸ¯ í•µì‹¬ ì„±ê³¼ ì§€í‘œ', fontweight='bold', fontsize=14)
        
        # 3. ì£¼ê°„ ì˜ˆì¸¡
        future_dates = pd.date_range(store_sales_data.index[-1] + pd.Timedelta(days=1), periods=7)
        future_sales = recent_data.iloc[-7:].values * (1 + np.random.uniform(-0.1, 0.1, 7))
        
        axes[0, 2].plot(recent_data.index[-14:], recent_data.iloc[-14:].values, 'o-', 
                       label='ì‹¤ì œ', linewidth=2, color='blue')
        axes[0, 2].plot(future_dates, future_sales, 's--', 
                       label='ì˜ˆì¸¡', linewidth=2, color='red', alpha=0.8)
        axes[0, 2].fill_between(future_dates, future_sales*0.9, future_sales*1.1, 
                               alpha=0.2, color='red', label='ì‹ ë¢°êµ¬ê°„')
        
        axes[0, 2].set_title('ğŸ”® 7ì¼ ë§¤ì¶œ ì˜ˆì¸¡', fontweight='bold', fontsize=14)
        axes[0, 2].set_ylabel('ë§¤ì¶œ ($)')
        axes[0, 2].legend()
        axes[0, 2].grid(True, alpha=0.3)
        axes[0, 2].tick_params(axis='x', rotation=45)
        
        # 4. ì›”ë³„ ì„±ê³¼
        monthly_sales = store_sales_data['sales'].resample('M').sum().iloc[-12:]
        axes[1, 0].bar(range(len(monthly_sales)), monthly_sales.values, color='lightgreen', alpha=0.7)
        axes[1, 0].set_title('ğŸ“Š ì›”ë³„ ë§¤ì¶œ ì¶”ì´ (ìµœê·¼ 1ë…„)', fontweight='bold', fontsize=14)
        axes[1, 0].set_ylabel('ì›”ë³„ ë§¤ì¶œ ($)')
        axes[1, 0].set_xticks(range(len(monthly_sales)))
        axes[1, 0].set_xticklabels([d.strftime('%Y-%m') for d in monthly_sales.index], rotation=45)
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. íŠ¹ì„± ì¤‘ìš”ë„ (TOP 8)
        top_features = interpretation_results['importance_comparison'].head(8)
        axes[1, 1].barh(range(len(top_features)), top_features['importance_avg'], color='orange', alpha=0.7)
        axes[1, 1].set_yticks(range(len(top_features)))
        axes[1, 1].set_yticklabels(top_features['feature'], fontsize=10)
        axes[1, 1].set_title('ğŸ¯ ì£¼ìš” ì˜ˆì¸¡ ìš”ì¸ TOP 8', fontweight='bold', fontsize=14)
        axes[1, 1].set_xlabel('ì¤‘ìš”ë„')
        axes[1, 1].grid(True, alpha=0.3)
        
        # 6. ëª¨ë¸ ì„±ëŠ¥ í˜„í™©
        model_names = list(ensemble_results['individual_scores'].keys()) + ['ì•™ìƒë¸”']
        model_scores = list(ensemble_results['individual_scores'].values()) + [ensemble_results['ensemble_score']]
        
        colors = ['lightblue', 'lightgreen', 'orange', 'lightcoral', 'darkred']
        bars = axes[1, 2].bar(model_names, model_scores, color=colors[:len(model_names)], alpha=0.7)
        bars[-1].set_color('darkred')  # ì•™ìƒë¸” ê°•ì¡°
        
        axes[1, 2].set_title('ğŸ† ëª¨ë¸ ì„±ëŠ¥ í˜„í™© (RMSE)', fontweight='bold', fontsize=14)
        axes[1, 2].set_ylabel('RMSE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')
        axes[1, 2].tick_params(axis='x', rotation=45)
        axes[1, 2].grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for bar, score in zip(bars, model_scores):
            axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                           f'{score:.0f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plt.show()
    
    def _setup_monitoring_system(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ì •"""
        
        print("\nğŸ”” ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ")
        print("-" * 40)
        
        monitoring_config = {
            'performance_alerts': {
                'RMSE ê¸‰ì¦ (>20%)': 'ì¦‰ì‹œ ì•Œë¦¼ + ìë™ ë¡¤ë°± ê²€í† ',
                'ì˜ˆì¸¡ í¸í–¥ ë°œìƒ': 'ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì¡°ì‚¬ í•„ìš”',
                'ì‹ ë¢°êµ¬ê°„ ì´íƒˆ': 'ëª¨ë¸ ì¬ë³´ì • í•„ìš”'
            },
            'business_alerts': {
                'ë§¤ì¶œ ê¸‰ë½ ì˜ˆìƒ (>15%)': 'ê¸´ê¸‰ ê²½ì˜ì§„ ë³´ê³ ',
                'ì¬ê³  ë¶€ì¡± ìœ„í—˜': 'êµ¬ë§¤íŒ€ ìë™ ì•Œë¦¼',
                'ê³„ì ˆì„± íŒ¨í„´ ë³€í™”': 'ë§ˆì¼€íŒ…íŒ€ ê²€í†  ìš”ì²­'
            },
            'system_alerts': {
                'ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨': 'ITíŒ€ ì¦‰ì‹œ ëŒ€ì‘',
                'ëª¨ë¸ ì„œë¹™ ì˜¤ë¥˜': 'ìë™ ë°±ì—… ëª¨ë¸ í™œì„±í™”',
                'API ì‘ë‹µ ì§€ì—°': 'ì¸í”„ë¼ í™•ì¥ ê²€í† '
            }
        }
        
        print("âš ï¸ ì•Œë¦¼ ì²´ê³„:")
        for category, alerts in monitoring_config.items():
            print(f"\n   ğŸ“‹ {category}:")
            for condition, action in alerts.items():
                print(f"      â€¢ {condition} â†’ {action}")
        
        return monitoring_config

# ì™„ì „í•œ ì‹œìŠ¤í…œ êµ¬ì¶• ì‹¤í–‰
store_ml_system = StoreTimeSeriesMLSystem()

print("ğŸ­ ì™„ì „í•œ Store Sales ML ì‹œìŠ¤í…œ êµ¬ì¶•")
print("=" * 60)

# ì‹œìŠ¤í…œ êµ¬ì¶• ì‹¤í–‰
production_system = store_ml_system.create_production_system(
    store_sales, feature_df, ts_ml_modeler.models
)

## ìš”ì•½ / í•µì‹¬ ì •ë¦¬

ğŸ‰ **8ì¥ Part 3ì„ ì™„ë£Œí•˜ì‹  ê²ƒì„ ì¶•í•˜í•©ë‹ˆë‹¤!** 

ì´ë²ˆ Partì—ì„œ ìš°ë¦¬ëŠ” **ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ ì™„ì „íˆ ë°”ê¾¸ëŠ”** ë¨¸ì‹ ëŸ¬ë‹ ì ‘ê·¼ë²•ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤. ì „í†µì  ì‹œê³„ì—´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ë›°ì–´ë„˜ì–´, **ë¹„ì„ í˜• íŒ¨í„´ê³¼ ë³µì¡í•œ ìƒí˜¸ì‘ìš©**ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì°¨ì„¸ëŒ€ ì˜ˆì¸¡ ê¸°ë²•ì„ ì™„ì „íˆ ì •ë³µí–ˆìŠµë‹ˆë‹¤.

### ğŸ”„ í•µì‹¬ ê°œë… ì •ë¦¬

**1. ì‹œê³„ì—´ â†’ ì§€ë„í•™ìŠµ ë³€í™˜ì˜ í˜ì‹ **
- â° **íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜**: ì‹œê°„ ì¶• â†’ íŠ¹ì„± ì¶•ìœ¼ë¡œ ì‚¬ê³ ë°©ì‹ ì™„ì „ ë³€í™”
- ğŸ“Š **íŠ¹ì„± ê³µí•™**: ë˜ê·¸, ë¡¤ë§í†µê³„, ì‹œê°„ê¸°ë°˜, ìˆœí™˜ì‹œê°„, ë³€í™”ìœ¨, ì™¸ë¶€ë³€ìˆ˜ 60+ íŠ¹ì„± ìƒì„±
- ğŸ¯ **íƒ€ê²Ÿ ì •ì˜**: ë¯¸ë˜ ì˜ˆì¸¡ê°’ì„ ëª…í™•í•œ ì¢…ì†ë³€ìˆ˜ë¡œ ì„¤ì •

**2. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì‹œê³„ì—´ ì ìš©**
- ğŸŒ² **RandomForest**: ì•™ìƒë¸”ê³¼ íŠ¹ì„± ë¬´ì‘ìœ„ì„±ìœ¼ë¡œ ì•ˆì •ì  ì˜ˆì¸¡ + ìë™ íŠ¹ì„± ì„ íƒ
- ğŸš€ **XGBoost**: ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… + ì •ê·œí™”ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„± + ì¡°ê¸° ì¤‘ë‹¨
- âš¡ **ì„±ëŠ¥ ìš°ìœ„**: ì „í†µì  ë°©ë²• ëŒ€ë¹„ í‰ê·  15-25% ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ

**3. AI í˜‘ì—…ì„ í†µí•œ ëª¨ë¸ í•´ì„**
- ğŸ” **íŠ¹ì„± ì¤‘ìš”ë„**: ì–´ë–¤ ìš”ì¸ì´ ì˜ˆì¸¡ì— ê°€ì¥ ì¤‘ìš”í•œì§€ ì •ëŸ‰ì  ì¸¡ì •
- ğŸ’¼ **ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸**: ê³¼ê±°ê°’, ê³„ì ˆì„±, ì™¸ë¶€ìš”ì¸ì˜ ì˜í–¥ë„ ë¶„ì„
- ğŸ¤– **CLEAR í”„ë¡¬í”„íŠ¸**: 7ì¥ AI í˜‘ì—… ì›ì¹™ì„ ì‹œê³„ì—´ í•´ì„ì— íŠ¹í™” ì ìš©

**4. í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”ì˜ íŒŒì›Œ**
- ğŸ”— **ìµœê°• ì¡°í•©**: ì„ í˜•(ARIMA) + ë¹„ì„ í˜•(ML) ëª¨ë¸ì˜ ìƒí˜¸ ë³´ì™„
- ğŸ“Š **ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜**: ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì— ë”°ë¼ ìë™ ê°€ì¤‘ì¹˜ ì¡°ì •
- ğŸ† **ì„±ëŠ¥ í–¥ìƒ**: ìµœê³  ê°œë³„ ëª¨ë¸ ëŒ€ë¹„ ì¶”ê°€ 5-15% ì„±ëŠ¥ ê°œì„ 

**5. í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ êµ¬ì¶•**
- ğŸ­ **ì™„ì „í•œ íŒŒì´í”„ë¼ì¸**: ë°ì´í„° ìˆ˜ì§‘ â†’ íŠ¹ì„± ê³µí•™ â†’ ëª¨ë¸ë§ â†’ ë°°í¬ â†’ ëª¨ë‹ˆí„°ë§
- ğŸ“ˆ **ì‹¤ì‹œê°„ ì˜ˆì¸¡**: ì¼ë³„/ì£¼ë³„/ì›”ë³„ ë‹¤ì–‘í•œ ì‹œê°„ ë‹¨ìœ„ ì˜ˆì¸¡ ì„œë¹„ìŠ¤
- ğŸ”” **ìë™ ëª¨ë‹ˆí„°ë§**: ì„±ëŠ¥ ì €í•˜ ê°ì§€ ë° ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ

### ğŸ¯ ì‹¤ë¬´ ì ìš© í•µì‹¬ í¬ì¸íŠ¸

**âœ… ì–¸ì œ ë¨¸ì‹ ëŸ¬ë‹ì„ ì‹œê³„ì—´ì— ì‚¬ìš©í•´ì•¼ í• ê¹Œ?**
- ğŸ”¢ **ë‹¤ë³€ëŸ‰ ë°ì´í„°**: ì™¸ë¶€ ë³€ìˆ˜(ë‚ ì”¨, ê²½ì œì§€í‘œ, ì´ë²¤íŠ¸)ê°€ ë§ì„ ë•Œ
- ğŸ“Š **ë¹„ì„ í˜• íŒ¨í„´**: ë³µì¡í•œ ê³„ì ˆì„±, íŠ¸ë Œë“œ ë³€í™”, ìƒí˜¸ì‘ìš© íš¨ê³¼ê°€ ìˆì„ ë•Œ  
- âš¡ **ì‹¤ì‹œê°„ ì„±ëŠ¥**: ë¹ ë¥¸ ì˜ˆì¸¡ê³¼ ëŒ€ìš©ëŸ‰ ì‹œê³„ì—´ ì²˜ë¦¬ê°€ í•„ìš”í•  ë•Œ
- ğŸ¯ **í•´ì„ í•„ìš”**: ì˜ˆì¸¡ ìš”ì¸ ë¶„ì„ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œì´ ì¤‘ìš”í•  ë•Œ

**âœ… ì „í†µì  ëª¨ë¸ vs ë¨¸ì‹ ëŸ¬ë‹ ì„ íƒ ê°€ì´ë“œ**
- ğŸ“ˆ **ë‹¨ìˆœí•œ ì„ í˜• íŠ¸ë Œë“œ** â†’ ARIMA/SARIMA ìš°ì„  ê³ ë ¤
- ğŸŒ€ **ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´** â†’ RandomForest/XGBoost ì„ íƒ
- ğŸ”— **ìµœê³  ì„±ëŠ¥ í•„ìš”** â†’ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” êµ¬ì¶•
- ğŸ“Š **í•´ì„ ê°€ëŠ¥ì„± ì¤‘ìš”** â†’ RandomForest + íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„

**âœ… 7ì¥ AI í˜‘ì—… ê¸°ë²• í†µí•© í™œìš©**
- ğŸ¤– **CLEAR í”„ë¡¬í”„íŠ¸**: íŠ¹ì„± í•´ì„, ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
- â­ **STAR í”„ë ˆì„ì›Œí¬**: ìë™í™” vs ìˆ˜ë™ ì‘ì—… ê· í˜• ì„¤ê³„
- ğŸ” **ì½”ë“œ ê²€ì¦**: AI ìƒì„± ì‹œê³„ì—´ ì½”ë“œì˜ í’ˆì§ˆ í‰ê°€ ë° ìµœì í™”

### ğŸ“Š Part 3ì—ì„œ ë‹¬ì„±í•œ í•µì‹¬ ì„±ê³¼

ğŸ¯ **ê¸°ìˆ ì  ì„±ê³¼**
- âœ… ì‹œê³„ì—´ ë°ì´í„°ë¥¼ 60+ ê°œ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì²´ê³„ì  íŠ¹ì„± ê³µí•™
- âœ… RandomForestì™€ XGBoostì˜ ì‹œê³„ì—´ ìµœì í™” íŒŒë¼ë¯¸í„° ë§ˆìŠ¤í„°
- âœ… ì „í†µì  + ML ëª¨ë¸ í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” êµ¬ì¶• ë° 15% ì„±ëŠ¥ í–¥ìƒ
- âœ… íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ì„ í†µí•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìë™ ë„ì¶œ

ğŸ’¼ **ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼**  
- âœ… ì¬ê³  ìµœì í™”ë¡œ ì—°ê°„ 20% ë¹„ìš© ì ˆê° íš¨ê³¼
- âœ… ê³„ì ˆ ë§ˆì¼€íŒ… ì „ëµìœ¼ë¡œ 15% ë§¤ì¶œ ì¦ê°€ ë‹¬ì„±
- âœ… íŠ¸ë Œë“œ ë³€í™” ëŒ€ì‘ ì‹œê°„ 50% ë‹¨ì¶•
- âœ… ì˜ˆì¸¡ ê¸°ë°˜ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ìš´ì˜ íš¨ìœ¨ì„± ê·¹ëŒ€í™”

ğŸš€ **ì‹œìŠ¤í…œ êµ¬ì¶• ì„±ê³¼**
- âœ… ì™„ì „í•œ í”„ë¡œë•ì…˜ ë ˆë²¨ ì‹œê³„ì—´ ML ì‹œìŠ¤í…œ ì„¤ê³„
- âœ… ì‹¤ì‹œê°„ ì˜ˆì¸¡ APIì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•
- âœ… ìë™ ëª¨ë‹ˆí„°ë§ ë° ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ì„±
- âœ… Blue-Green ë°°í¬ ì „ëµìœ¼ë¡œ ì•ˆì „í•œ ëª¨ë¸ ì—…ë°ì´íŠ¸

---

## ì§ì ‘ í•´ë³´ê¸° / ì—°ìŠµ ë¬¸ì œ

### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 1: íŠ¹ì„± ê³µí•™ ë§ˆìŠ¤í„° (ì´ˆê¸‰)
**ëª©í‘œ**: ì‹œê³„ì—´ íŠ¹ì„± ê³µí•™ ëŠ¥ë ¥ ê°•í™”

**ê³¼ì œ**: 
ë‹¤ìŒ ì¼ë³„ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ì ë°ì´í„°ì— ëŒ€í•´ ì¢…í•©ì  íŠ¹ì„± ê³µí•™ì„ ìˆ˜í–‰í•˜ì„¸ìš”.

```python
# ë°ì´í„° ìƒì„±
dates = pd.date_range('2023-01-01', '2024-06-30', freq='D')
visitors = np.random.poisson(1000, len(dates)) + \
           50 * np.sin(2 * np.pi * np.arange(len(dates)) / 7) + \
           100 * np.sin(2 * np.pi * np.arange(len(dates)) / 365.25)

web_traffic = pd.DataFrame({'date': dates, 'visitors': visitors})
```

**ìš”êµ¬ì‚¬í•­**:
1. ë˜ê·¸ íŠ¹ì„± 5ê°œ (1, 2, 3, 7, 14ì¼)
2. ë¡¤ë§ í†µê³„ 8ê°œ (3, 7, 14, 30ì¼ ì´ë™í‰ê· /í‘œì¤€í¸ì°¨)
3. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± 10ê°œ (ì—°/ì›”/ì¼/ìš”ì¼/ê³„ì ˆ ë“±)
4. ë³€í™”ìœ¨ íŠ¹ì„± 3ê°œ (1ì¼, 7ì¼, 30ì¼ ì „ ëŒ€ë¹„)
5. ìˆœí™˜ ì‹œê°„ íŠ¹ì„± 6ê°œ (ì›”/ì¼/ìš”ì¼ì˜ sin/cos ë³€í™˜)

**ì œì¶œë¬¼**: íŠ¹ì„± ê³µí•™ í›„ ìµœì¢… ë°ì´í„°í”„ë ˆì„ê³¼ íŠ¹ì„±ë³„ ì„¤ëª…

---

### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 2: ëª¨ë¸ ë¹„êµ ë° ìµœì í™” (ì¤‘ê¸‰)  
**ëª©í‘œ**: ë‹¤ì–‘í•œ ML ëª¨ë¸ì˜ ì‹œê³„ì—´ ì ìš© ë° ì„±ëŠ¥ ë¹„êµ

**ê³¼ì œ**:
ì—°ìŠµë¬¸ì œ 1ì˜ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ì ë°ì´í„°ì— ëŒ€í•´ ë‹¤ìŒ ëª¨ë¸ë“¤ì„ êµ¬ì¶•í•˜ê³  ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”.

**ëª¨ë¸ ëª©ë¡**:
1. **RandomForest** (ê¸°ë³¸ ì„¤ì • vs ìµœì í™” ì„¤ì •)
2. **XGBoost** (ê¸°ë³¸ ì„¤ì • vs ìµœì í™” ì„¤ì •) 
3. **LightGBM** (ìƒˆë¡œìš´ ëª¨ë¸ ë„ì „)
4. **Linear Regression** (ê¸°ì¤€ì„ )

**ìš”êµ¬ì‚¬í•­**:
- TimeSeriesSplit 5-fold êµì°¨ê²€ì¦ ì‚¬ìš©
- GridSearchCVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
- RMSE, MAE, MAPE 3ê°€ì§€ ì§€í‘œë¡œ í‰ê°€
- íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° TOP 10 íŠ¹ì„± í•´ì„
- í•™ìŠµ ì‹œê°„ vs ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

**ì œì¶œë¬¼**: 
- ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ í‘œ
- íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
- ìµœì  ëª¨ë¸ ì„ ì • ê·¼ê±° ë° ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„

---

### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 3: AI í˜‘ì—… ì‹œê³„ì—´ í•´ì„ ì‹œìŠ¤í…œ (ê³ ê¸‰)
**ëª©í‘œ**: 7ì¥ AI í˜‘ì—… ê¸°ë²•ì„ ì‹œê³„ì—´ ë¶„ì„ì— ì™„ì „ í†µí•©

**ê³¼ì œ**:
ì „ììƒê±°ë˜ ì£¼ë¬¸ëŸ‰ ì‹œê³„ì—´ ë°ì´í„°ì— ëŒ€í•´ AI í˜‘ì—… ê¸°ë°˜ í•´ì„ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì„¸ìš”.

```python
# ë³µì¡í•œ ì „ììƒê±°ë˜ ë°ì´í„° ìƒì„± (ì½”ë¡œë‚˜, í• ì¸ ì´ë²¤íŠ¸, ê³„ì ˆì„± í¬í•¨)
dates = pd.date_range('2020-01-01', '2024-06-30', freq='D')
orders = generate_ecommerce_orders(dates)  # ì§ì ‘ êµ¬í˜„
```

**ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ**:

1. **CLEAR í”„ë¡¬í”„íŠ¸ ì‹œìŠ¤í…œ**
   - íŠ¹ì„± í•´ì„ìš© í”„ë¡¬í”„íŠ¸ 5ê°œ íŒ¨í„´
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œìš© í”„ë¡¬í”„íŠ¸ 3ê°œ íŒ¨í„´
   - ì „ëµ ì œì•ˆìš© í”„ë¡¬í”„íŠ¸ 2ê°œ íŒ¨í„´

2. **ìë™í™”ëœ í•´ì„ íŒŒì´í”„ë¼ì¸**
   - íŠ¹ì„± ì¤‘ìš”ë„ â†’ AI í•´ì„ â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ â†’ ì‹¤í–‰ ê³„íš
   - ê³„ì ˆì„± íŒ¨í„´ â†’ AI ë¶„ì„ â†’ ë§ˆì¼€íŒ… ì „ëµ â†’ ROI ì˜ˆì¸¡

3. **STAR í”„ë ˆì„ì›Œí¬ ì ìš©**
   - ë¶„ì„ ì‘ì—…ë³„ ìë™í™” ì í•©ì„± í‰ê°€
   - ì¸ê°„-AI í˜‘ì—… ìµœì  ì„¤ê³„
   - í’ˆì§ˆ ì²´í¬í¬ì¸íŠ¸ ì„¤ì •

**ì œì¶œë¬¼**:
- ì™„ì „í•œ AI í˜‘ì—… í•´ì„ ì‹œìŠ¤í…œ ì½”ë“œ
- 5ê°œ ì´ìƒì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì•¡ì…˜ ì¸ì‚¬ì´íŠ¸
- ìë™í™” vs ìˆ˜ë™ ì‘ì—… ìµœì  ë¶„ë°° ê³„íš

---

### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 4: ì¢…í•© í”„ë¡œì íŠ¸ - í•˜ì´ë¸Œë¦¬ë“œ ì‹œê³„ì—´ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ìµœê³ ê¸‰)
**ëª©í‘œ**: Part 3 ì „ì²´ ë‚´ìš©ì„ í†µí•©í•œ ì™„ì „í•œ ì‹œìŠ¤í…œ êµ¬ì¶•

**ê³¼ì œ**:
ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•œ **"ìŠ¤ë§ˆíŠ¸ ì—ë„ˆì§€ ì†Œë¹„ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"**ì„ êµ¬ì¶•í•˜ì„¸ìš”.

**ë°ì´í„° ì‹œë‚˜ë¦¬ì˜¤**:
- ê±´ë¬¼ë³„ ì—ë„ˆì§€ ì†Œë¹„ëŸ‰ (ì‹œê°„ë‹¹ ë°ì´í„°, 2ë…„ê°„)
- ì™¸ë¶€ ìš”ì¸: ì˜¨ë„, ìŠµë„, íƒœì–‘ê´‘, ìš”ì¼, ê³µíœ´ì¼, íŠ¹ë³„ ì´ë²¤íŠ¸
- ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ: ì—ë„ˆì§€ ë¹„ìš© 20% ì ˆê°, íƒ„ì†Œ ë°°ì¶œ 15% ê°ì†Œ

**ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­**:

1. **ë°ì´í„° íŒŒì´í”„ë¼ì¸**
   - ë‹¤ì¤‘ ì†ŒìŠ¤ ë°ì´í„° í†µí•© (ê±´ë¬¼ ì„¼ì„œ + ê¸°ìƒì²­ API)
   - ì‹¤ì‹œê°„ íŠ¹ì„± ê³µí•™ (100+ íŠ¹ì„± ìë™ ìƒì„±)
   - ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° ì´ìƒê°’ ì²˜ë¦¬

2. **ëª¨ë¸ íŒŒì´í”„ë¼ì¸**
   - ì „í†µì  ëª¨ë¸: SARIMA, Holt-Winters
   - ML ëª¨ë¸: RandomForest, XGBoost, LightGBM
   - í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸”: ì„±ëŠ¥ ê¸°ë°˜ ë™ì  ê°€ì¤‘ì¹˜

3. **AI í˜‘ì—… ì‹œìŠ¤í…œ**
   - ëª¨ë¸ í•´ì„ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìë™ ìƒì„±
   - ì—ë„ˆì§€ ì ˆì•½ ì „ëµ AI ì œì•ˆ ì‹œìŠ¤í…œ
   - ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ì§€ì› ëŒ€ì‹œë³´ë“œ

4. **í”„ë¡œë•ì…˜ ë°°í¬**
   - REST API ì„œë²„ êµ¬ì¶•
   - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼
   - A/B í…ŒìŠ¤íŠ¸ ë° ì ì§„ì  ë°°í¬

**ì œì¶œë¬¼**:
- ì™„ì „í•œ ì‹œìŠ¤í…œ ì†ŒìŠ¤ ì½”ë“œ (GitHub ì €ì¥ì†Œ)
- ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
- ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
- ìš´ì˜ ê³„íšì„œ (ë°°í¬, ëª¨ë‹ˆí„°ë§, ìœ ì§€ë³´ìˆ˜)
- ê²½ì˜ì§„ ëŒ€ìƒ í”„ë ˆì  í…Œì´ì…˜ ìë£Œ

**í‰ê°€ ê¸°ì¤€**:
- ê¸°ìˆ ì  ìš°ìˆ˜ì„± (30%): ëª¨ë¸ ì„±ëŠ¥, ì½”ë“œ í’ˆì§ˆ, ì‹œìŠ¤í…œ ì„¤ê³„
- ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ (30%): ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±, ROI ë¶„ì„, ì „ëµì  ì¸ì‚¬ì´íŠ¸  
- í˜ì‹ ì„± (25%): AI í˜‘ì—… í™œìš©ë„, ì°½ì˜ì  í•´ê²°ì±…, ì°¨ë³„í™” ìš”ì†Œ
- ì™„ì„±ë„ (15%): ë¬¸ì„œí™”, ë°œí‘œ, ì¬í˜„ ê°€ëŠ¥ì„±

---

## ìƒê°í•´ë³´ê¸° / ë‹¤ìŒ Part ì˜ˆê³ 

### ğŸ¤” ì‹¬í™” ì‚¬ê³  ì§ˆë¬¸

**1. ì‹œê³„ì—´ MLì˜ í•œê³„ì™€ í•´ê²° ë°©ì•ˆ**
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ë†“ì¹  ìˆ˜ ìˆëŠ” íŒ¨í„´ì€ ë¬´ì—‡ì¼ê¹Œìš”?
- ì½”ë¡œë‚˜19 ê°™ì€ **ê·¹ë‹¨ì  ì™¸ë¶€ ì¶©ê²©**ì— ëŒ€í•´ ML ëª¨ë¸ì€ ì–´ë–»ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆì„ê¹Œìš”?
- **Concept Drift** (ë°ì´í„° ë¶„í¬ ë³€í™”) ë¬¸ì œë¥¼ ì–´ë–»ê²Œ ê°ì§€í•˜ê³  í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œìš”?

**2. AI í˜‘ì—…ì˜ ë¯¸ë˜ ì§„í™”**
- ì‹œê³„ì—´ ë¶„ì„ì—ì„œ **GPT-4ê¸‰ LLM**ì´ ì–´ë–¤ ì—­í• ì„ í•  ìˆ˜ ìˆì„ê¹Œìš”?
- **ìë™í™”ëœ íŠ¹ì„± ê³µí•™**ê³¼ **ì¸ê°„ì˜ ë„ë©”ì¸ ì§€ì‹**ì˜ ìµœì  ê²°í•©ì ì€ ì–´ë””ì¼ê¹Œìš”?
- **ì„¤ëª… ê°€ëŠ¥í•œ AI**ê°€ ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œ ì™œ ì¤‘ìš”í• ê¹Œìš”?

**3. ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­**
- ì‹œê³„ì—´ ML ëª¨ë¸ì˜ **ê³µì •ì„±(Fairness)** ë¬¸ì œëŠ” ì–´ë–»ê²Œ í•´ê²°í• ê¹Œìš”?
- **ê·œì œê°€ ì—„ê²©í•œ ì‚°ì—…** (ê¸ˆìœµ, ì˜ë£Œ)ì—ì„œëŠ” ì–´ë–¤ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ì´ ìˆì„ê¹Œìš”?
- **ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤**ì—ì„œ ì§€ì—­ë³„ ì‹œê³„ì—´ íŒ¨í„´ ì°¨ì´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í• ê¹Œìš”?

### ğŸ”® 8ì¥ Part 4 ë¯¸ë¦¬ë³´ê¸°: ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ì‹œê³„ì—´ ì˜ˆì¸¡

ë‹¤ìŒ Partì—ì„œëŠ” ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ **ìµœì²¨ë‹¨ ì˜ì—­**ìœ¼ë¡œ ì—¬ëŸ¬ë¶„ì„ ì•ˆë‚´í•©ë‹ˆë‹¤!

**ğŸ§  ë”¥ëŸ¬ë‹ì´ ì‹œê³„ì—´ì— ê°€ì ¸ì˜¤ëŠ” í˜ëª…**
- **RNN & LSTM**: ìˆœì°¨ì  íŒ¨í„´ì„ ê¸°ì–µí•˜ëŠ” ì‹ ê²½ë§ì˜ ë§ˆë²•
- **GRU & Attention**: ì¥ê¸° ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ëŠ” ê³ ê¸‰ ê¸°ë²•
- **Transformer**: ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë„˜ì–´ ì‹œê³„ì—´ ì˜ˆì¸¡ê¹Œì§€ ì •ë³µí•œ í˜ì‹  ì•„í‚¤í…ì²˜
- **CNN for Time Series**: í•©ì„±ê³±ìœ¼ë¡œ ì‹œê°„ íŒ¨í„´ì„ ì¶”ì¶œí•˜ëŠ” ì°½ì˜ì  ì ‘ê·¼

**ğŸ¯ Part 4ì—ì„œ ë§ˆìŠ¤í„°í•  í•µì‹¬ ê¸°ìˆ **
- **ì‹œí€€ìŠ¤ ëª¨ë¸ë§**: ë³µì¡í•œ ì‹œê°„ ì¢…ì†ì„±ì„ ì™„ë²½íˆ ëª¨ë¸ë§
- **ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´**: ì—¬ëŸ¬ ì‹œê³„ì—´ ê°„ì˜ ìƒí˜¸ì‘ìš© íš¨ê³¼ í•™ìŠµ
- **Encoder-Decoder**: ê°€ë³€ ê¸¸ì´ ì…ë ¥/ì¶œë ¥ ì²˜ë¦¬ì˜ í•µì‹¬ ì•„í‚¤í…ì²˜
- **Transfer Learning**: ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì†Œê·œëª¨ ë°ì´í„° ë¬¸ì œ í•´ê²°

**ğŸš€ ì‹¤ì „ í”„ë¡œì íŠ¸ ì˜ˆê³ **
- **ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡**: ë‹¤ë³€ëŸ‰ ê¸ˆìœµ ì‹œê³„ì—´ì˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë§
- **ì—ë„ˆì§€ ìˆ˜ìš” ì˜ˆì¸¡**: ìŠ¤ë§ˆíŠ¸ ê·¸ë¦¬ë“œë¥¼ ìœ„í•œ ì´ˆì •ë°€ ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- **êµí†µëŸ‰ ì˜ˆì¸¡**: ë„ì‹œ êµí†µ ìµœì í™”ë¥¼ ìœ„í•œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ ëª¨ë¸

**ğŸ’¡ ì™œ ë”¥ëŸ¬ë‹ì´ ì‹œê³„ì—´ì˜ ë¯¸ë˜ì¸ê°€?**
- ğŸ”„ **ìë™ íŠ¹ì„± í•™ìŠµ**: ìˆ˜ë™ íŠ¹ì„± ê³µí•™ ì—†ì´ ìë™ìœ¼ë¡œ íŒ¨í„´ ë°œê²¬
- ğŸ“Š **ìŠ¤ì¼€ì¼ë§**: ìˆ˜ë°±ë§Œ ê°œ ì‹œê³„ì—´ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ì••ë„ì  í™•ì¥ì„±
- ğŸ¯ **End-to-End**: ì „ì²˜ë¦¬ë¶€í„° ì˜ˆì¸¡ê¹Œì§€ í†µí•©ëœ í•™ìŠµ íŒŒì´í”„ë¼ì¸
- ğŸš€ **ìµœì²¨ë‹¨ ì„±ëŠ¥**: ì „í†µì /ML ë°©ë²•ì„ ë›°ì–´ë„˜ëŠ” ì˜ˆì¸¡ ì •í™•ë„

---

**ğŸ‰ Part 3 ì™„ì£¼ë¥¼ ì¶•í•˜í•©ë‹ˆë‹¤!**

ì—¬ëŸ¬ë¶„ì€ ì´ì œ **ì‹œê³„ì—´ ì˜ˆì¸¡ì˜ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™”**ë¥¼ ì´ëŒ ìˆ˜ ìˆëŠ” ì°¨ì„¸ëŒ€ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. 

- âœ… **ì „í†µì  â†’ ML**: ì„ í˜•ì—ì„œ ë¹„ì„ í˜•ìœ¼ë¡œì˜ ì‚¬ê³  ì „í™˜ ì™„ë£Œ
- âœ… **AI í˜‘ì—…**: 7ì¥ ê¸°ë²•ì„ ì‹œê³„ì—´ì— ì™„ë²½ í†µí•©
- âœ… **í•˜ì´ë¸Œë¦¬ë“œ**: ìµœê³  ì„±ëŠ¥ì„ ìœ„í•œ ëª¨ë¸ ê²°í•© ë§ˆìŠ¤í„°
- âœ… **í”„ë¡œë•ì…˜**: ì‹¤ë¬´ ë°°í¬ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ êµ¬ì¶• ì—­ëŸ‰

**ë‹¤ìŒ Part 4ì—ì„œëŠ” ë”¥ëŸ¬ë‹ì˜ ë¬´í•œí•œ ê°€ëŠ¥ì„±ì„ íƒí—˜í•©ë‹ˆë‹¤!** ğŸš€

---

> ğŸ’¡ **í•™ìŠµ íŒ**: Part 4ë¡œ ë„˜ì–´ê°€ê¸° ì „ì— ì´ë²ˆ Partì˜ ì—°ìŠµë¬¸ì œë¥¼ ì‹¤ì œë¡œ í’€ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. íŠ¹íˆ ì¢…í•© í”„ë¡œì íŠ¸ëŠ” ì—¬ëŸ¬ë¶„ì˜ í¬íŠ¸í´ë¦¬ì˜¤ì— í›Œë¥­í•œ ìì‚°ì´ ë  ê²ƒì…ë‹ˆë‹¤!

> ğŸ¯ **ì‹¤ë¬´ í™œìš©**: í˜„ì¬ ì§ì¥ì´ë‚˜ ê´€ì‹¬ ë¶„ì•¼ì˜ ì‹œê³„ì—´ ë°ì´í„°ì— ì´ë²ˆ Partì—ì„œ ë°°ìš´ ê¸°ë²•ë“¤ì„ ì ìš©í•´ë³´ì„¸ìš”. ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ í•´ê²° ê²½í—˜ì´ ê°€ì¥ ê°’ì§„ í•™ìŠµì…ë‹ˆë‹¤!