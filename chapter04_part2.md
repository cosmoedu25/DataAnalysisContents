# 4ì¥ Part 2: ë°ì´í„° ë³€í™˜ê³¼ ì •ê·œí™”
## ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° í‘œì¤€í™”ì™€ ì¸ì½”ë”© ì „ë¬¸ ê¸°ë²•

---

## ğŸ“š í•™ìŠµ ëª©í‘œ

ì´ë²ˆ Partì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤:

âœ… **ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ë§ ê¸°ë²•ì˜ ì›ë¦¬ì™€ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì´í•´í•  ìˆ˜ ìˆë‹¤**
âœ… **ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¸ì½”ë”© ê¸°ë²•ì„ ë§ˆìŠ¤í„°í•  ìˆ˜ ìˆë‹¤**  
âœ… **ë¶„í¬ ë³€í™˜ì„ í†µí•´ ë°ì´í„°ì˜ ì™œë„ë¥¼ ì œê±°í•˜ê³  ì •ê·œì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤**
âœ… **scikit-learn Pipelineì„ í™œìš©í•œ ì²´ê³„ì ì¸ ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤**
âœ… **House Prices ë°ì´í„°ì…‹ì— ì í•©í•œ ë§ì¶¤í˜• ë³€í™˜ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•  ìˆ˜ ìˆë‹¤**

---

## ğŸ¯ ì´ë²ˆ Part ë¯¸ë¦¬ë³´ê¸°

ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•œ ê¹¨ë—í•œ ë°ì´í„°ê°€ ìˆë‹¤ê³  í•´ì„œ ë°”ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì…ë ¥í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ë°ì´í„°ì˜ **í¬ê¸°(Scale)**ì™€ **í˜•íƒœ(Format)**ë¥¼ ì•Œê³ ë¦¬ì¦˜ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, ì£¼íƒ ë©´ì ì€ ìˆ˜ë°±~ìˆ˜ì²œ ì œê³±í”¼íŠ¸ ë‹¨ìœ„ì¸ ë°˜ë©´, ë°©ì˜ ê°œìˆ˜ëŠ” 1~10 ì •ë„ì˜ ì‘ì€ ìˆ«ìì…ë‹ˆë‹¤. ì´ëŸ° **í¬ê¸° ì°¨ì´**ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ í° ê°’ì„ ê°€ì§„ ë³€ìˆ˜ì—ë§Œ ì§‘ì¤‘í•˜ê²Œ ë§Œë“¤ì–´ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚µë‹ˆë‹¤. ë§ˆì¹˜ ì‹œë„ëŸ¬ìš´ ì†Œë¦¬ì— ë¬»í˜€ ì‘ì€ ì†Œë¦¬ë¥¼ ë“£ì§€ ëª»í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

ë˜í•œ "Excellent", "Good", "Fair" ê°™ì€ **ë²”ì£¼í˜• ë°ì´í„°**ëŠ” ì»´í“¨í„°ê°€ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìˆ«ìë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¨ìˆœíˆ 1, 2, 3ìœ¼ë¡œ ë°”ê¾¸ë©´ "Excellentê°€ Fairì˜ 3ë°° ì¢‹ë‹¤"ëŠ” ì˜ëª»ëœ ì˜ë¯¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë²ˆ Partì—ì„œëŠ” ì´ëŸ° ë¬¸ì œë“¤ì„ í•´ê²°í•˜ëŠ” **ì „ë¬¸ì ì¸ ë³€í™˜ ê¸°ë²•**ë“¤ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤. íŠ¹íˆ House Prices ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ **ë§ì¶¤í˜• ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**ì„ êµ¬ì¶•í•˜ì—¬ ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ ê¸°ë¥´ê² ìŠµë‹ˆë‹¤.

> **ğŸ’¡ Part 2ì˜ í•µì‹¬ í¬ì¸íŠ¸**  
> "ë°ì´í„° ë³€í™˜ì€ ë‹¨ìˆœí•œ í˜•ì‹ ë³€ê²½ì´ ì•„ë‹ˆë¼ ì•Œê³ ë¦¬ì¦˜ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜¬ë°”ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” 'ë²ˆì—­' ê³¼ì •ì…ë‹ˆë‹¤."

---

## ğŸ“– 4.2.1 ìŠ¤ì¼€ì¼ë§ ê¸°ë²• (Scaling Techniques)

### ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ì´ìœ 

ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ë³€ìˆ˜ ê°„ì˜ **í¬ê¸° ì°¨ì´**ì— ë¯¼ê°í•©ë‹ˆë‹¤. í¬ê¸°ê°€ í° ë³€ìˆ˜ê°€ ëª¨ë¸ì˜ í•™ìŠµì„ ì§€ë°°í•˜ê²Œ ë˜ì–´ ì‘ì€ ê°’ì„ ê°€ì§„ ì¤‘ìš”í•œ ë³€ìˆ˜ë“¤ì˜ ì˜í–¥ì„ ë¬´ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **ìŠ¤ì¼€ì¼ë§(Scaling)**: ì„œë¡œ ë‹¤ë¥¸ ë²”ìœ„ì˜ ë°ì´í„°ë¥¼ ë™ì¼í•œ ë²”ìœ„ë¡œ ì¡°ì •í•˜ëŠ” ê³¼ì •
> - **í‘œì¤€í™”(Standardization)**: í‰ê·  0, í‘œì¤€í¸ì°¨ 1ì´ ë˜ë„ë¡ ë³€í™˜
> - **ì •ê·œí™”(Normalization)**: ìµœì†Ÿê°’ 0, ìµœëŒ“ê°’ 1ì´ ë˜ë„ë¡ ë³€í™˜
> - **ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§**: ì´ìƒì¹˜ì— ì˜í–¥ì„ ëœ ë°›ëŠ” ì¤‘ì•™ê°’ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§

### ì‹¤ì œ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± í™•ì¸

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# House Prices ë°ì´í„° ë¡œë“œ (Part 1ì—ì„œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì‚¬ìš© ê°€ì •)
try:
    train_data = pd.read_csv('datasets/house_prices/train.csv')
    print("âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
except FileNotFoundError:
    print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± ì‹œê°í™”
def demonstrate_scaling_need(df):
    """
    ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í•œ ì´ìœ ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
    """
    # ëŒ€í‘œì ì¸ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ì„ íƒ
    key_columns = ['LotArea', 'GrLivArea', 'TotalBsmtSF', 'BedroomAbvGr', 'FullBath']
    available_columns = [col for col in key_columns if col in df.columns]
    
    if len(available_columns) < 3:
        print("âš ï¸ ì¶©ë¶„í•œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ë³¸ í†µê³„ í™•ì¸
    print("ğŸ“Š ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ í¬ê¸° ì°¨ì´:")
    stats_df = df[available_columns].describe()
    print(stats_df)
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ğŸ” ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± ë¶„ì„', fontsize=16, fontweight='bold')
    
    # 1. ì›ë³¸ ë°ì´í„° ë¶„í¬
    df[available_columns].hist(bins=30, ax=axes[0,0])
    axes[0,0].set_title('ì›ë³¸ ë°ì´í„° ë¶„í¬\n(ê° ë³€ìˆ˜ì˜ í¬ê¸°ê°€ í¬ê²Œ ë‹¤ë¦„)')
    
    # 2. ë°•ìŠ¤í”Œë¡¯ìœ¼ë¡œ ë²”ìœ„ ë¹„êµ
    df[available_columns].plot(kind='box', ax=axes[0,1])
    axes[0,1].set_title('ë³€ìˆ˜ë³„ ê°’ì˜ ë²”ìœ„ ë¹„êµ\n(ì„¸ë¡œì¶• ìŠ¤ì¼€ì¼ ì£¼ëª©)')
    axes[0,1].tick_params(axis='x', rotation=45)
    
    # 3. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    corr_matrix = df[available_columns].corr()
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', 
                center=0, ax=axes[1,0], fmt='.2f')
    axes[1,0].set_title('ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„')
    
    # 4. í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ë¹„êµ
    means = df[available_columns].mean()
    stds = df[available_columns].std()
    
    x_pos = range(len(available_columns))
    width = 0.35
    
    axes[1,1].bar([x - width/2 for x in x_pos], means, width, 
                  label='í‰ê· ', alpha=0.7, color='skyblue')
    axes[1,1].bar([x + width/2 for x in x_pos], stds, width, 
                  label='í‘œì¤€í¸ì°¨', alpha=0.7, color='lightcoral')
    
    axes[1,1].set_title('í‰ê·  vs í‘œì¤€í¸ì°¨ ë¹„êµ\n(í¬ê¸° ì°¨ì´ê°€ í¬ë©´ ìŠ¤ì¼€ì¼ë§ í•„ìš”)')
    axes[1,1].set_xticks(x_pos)
    axes[1,1].set_xticklabels(available_columns, rotation=45)
    axes[1,1].legend()
    axes[1,1].set_yscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ í‘œì‹œ
    
    plt.tight_layout()
    plt.show()
    
    # ìˆ˜ì¹˜ì  ë¶„ì„
    print(f"\nğŸ”¢ í¬ê¸° ì°¨ì´ ë¶„ì„:")
    max_mean = means.max()
    min_mean = means.min()
    print(f"   ìµœëŒ€ í‰ê· ê°’: {max_mean:,.0f} ({means.idxmax()})")
    print(f"   ìµœì†Œ í‰ê· ê°’: {min_mean:,.0f} ({means.idxmin()})")
    print(f"   í‰ê· ê°’ ì°¨ì´: {max_mean/min_mean:.0f}ë°°")
    print(f"   ğŸ’¡ ì´ëŸ° ì°¨ì´ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

# ìŠ¤ì¼€ì¼ë§ í•„ìš”ì„± ì‹œì—°
demonstrate_scaling_need(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `describe()`: ê¸°ìˆ í†µê³„ëŸ‰(í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ìœ„ìˆ˜ ë“±)ì„ í•œ ë²ˆì— ê³„ì‚°
- `hist(bins=30)`: íˆìŠ¤í† ê·¸ë¨ìœ¼ë¡œ ë°ì´í„° ë¶„í¬ ì‹œê°í™”
- `set_yscale('log')`: yì¶•ì„ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ì„¤ì •í•˜ì—¬ í° ì°¨ì´ë¥¼ ì‰½ê²Œ ë¹„êµ

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a 2x2 dashboard showing the need for data scaling in machine learning. Include: 1) Histograms of different numerical variables (LotArea, GrLivArea, BedroomAbvGr) showing vastly different scales and distributions, 2) Box plots comparing the ranges of these variables side by side, 3) A correlation heatmap between the variables, 4) A bar chart comparing means vs standard deviations of variables using log scale. Use professional styling with clear labels showing the dramatic scale differences that would confuse ML algorithms."

### ë°©ë²• 1: í‘œì¤€í™” (Standardization) - StandardScaler

í‘œì¤€í™”ëŠ” ë°ì´í„°ë¥¼ **í‰ê·  0, í‘œì¤€í¸ì°¨ 1**ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë°ì´í„°ì— íŠ¹íˆ íš¨ê³¼ì ì…ë‹ˆë‹¤.

**ê³µì‹**: z = (x - Î¼) / Ïƒ

```python
# í‘œì¤€í™” êµ¬í˜„ ë° ë¶„ì„
def apply_standardization(df, columns=None):
    """
    í‘œì¤€í™”ë¥¼ ì ìš©í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìë™ ì„ íƒ
        columns = df.select_dtypes(include=[np.number]).columns.tolist()
        if 'SalePrice' in columns:
            columns.remove('SalePrice')  # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸
        columns = columns[:5]  # ì²˜ìŒ 5ê°œë§Œ ì‚¬ìš©
    
    print("ğŸ“Š í‘œì¤€í™”(StandardScaler) ì ìš©:")
    
    # ì›ë³¸ ë°ì´í„° í†µê³„
    original_stats = df[columns].describe()
    print(f"\nğŸ”¸ ì›ë³¸ ë°ì´í„° í†µê³„:")
    print(original_stats.loc[['mean', 'std']].round(2))
    
    # í‘œì¤€í™” ì ìš©
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df[columns])
    scaled_df = pd.DataFrame(scaled_data, columns=columns, index=df.index)
    
    # ë³€í™˜ í›„ í†µê³„
    scaled_stats = scaled_df.describe()
    print(f"\nğŸ”¸ í‘œì¤€í™” í›„ í†µê³„:")
    print(scaled_stats.loc[['mean', 'std']].round(6))
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ğŸ“Š í‘œì¤€í™”(StandardScaler) íš¨ê³¼ ë¶„ì„', fontsize=16, fontweight='bold')
    
    # 1. ì›ë³¸ vs ë³€í™˜ í›„ ë¶„í¬ ë¹„êµ (ì²« ë²ˆì§¸ ë³€ìˆ˜)
    first_col = columns[0]
    axes[0,0].hist(df[first_col].dropna(), bins=30, alpha=0.7, 
                   label='ì›ë³¸', color='skyblue', density=True)
    axes[0,0].hist(scaled_df[first_col], bins=30, alpha=0.7, 
                   label='í‘œì¤€í™” í›„', color='lightcoral', density=True)
    axes[0,0].set_title(f'{first_col} ë¶„í¬ ë³€í™”')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. ëª¨ë“  ë³€ìˆ˜ì˜ í‘œì¤€í™” í›„ ë¶„í¬
    scaled_df.hist(bins=30, ax=axes[0,1])
    axes[0,1].set_title('í‘œì¤€í™” í›„ ëª¨ë“  ë³€ìˆ˜ ë¶„í¬\n(í‰ê·  0, í‘œì¤€í¸ì°¨ 1 ê·¼ì²˜)')
    
    # 3. ì›ë³¸ ë°ì´í„° ë°•ìŠ¤í”Œë¡¯
    df[columns].plot(kind='box', ax=axes[1,0])
    axes[1,0].set_title('ì›ë³¸ ë°ì´í„° (í¬ê¸° ì°¨ì´ í¼)')
    axes[1,0].tick_params(axis='x', rotation=45)
    
    # 4. í‘œì¤€í™” í›„ ë°•ìŠ¤í”Œë¡¯
    scaled_df.plot(kind='box', ax=axes[1,1])
    axes[1,1].set_title('í‘œì¤€í™” í›„ (ë™ì¼í•œ ìŠ¤ì¼€ì¼)')
    axes[1,1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nâœ… í‘œì¤€í™” ì™„ë£Œ!")
    print(f"   í‰ê· : ëª¨ë‘ 0ì— ê°€ê¹Œì›€ (ëª©í‘œ: 0)")
    print(f"   í‘œì¤€í¸ì°¨: ëª¨ë‘ 1ì— ê°€ê¹Œì›€ (ëª©í‘œ: 1)")
    print(f"   ğŸ’¡ ì´ì œ ëª¨ë“  ë³€ìˆ˜ê°€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ì„ ê°€ì§‘ë‹ˆë‹¤!")
    
    return scaled_df, scaler

# í‘œì¤€í™” ì ìš©
standardized_data, standard_scaler = apply_standardization(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `StandardScaler()`: í‘œì¤€í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” scikit-learn í´ë˜ìŠ¤
- `fit_transform()`: ë³€í™˜ ê·œì¹™ì„ í•™ìŠµí•˜ê³  ë™ì‹œì— ì ìš©
- `density=True`: íˆìŠ¤í† ê·¸ë¨ì„ í™•ë¥ ë°€ë„ë¡œ í‘œì‹œí•˜ì—¬ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì˜ ë¶„í¬ë¥¼ ë¹„êµ ê°€ëŠ¥

### ë°©ë²• 2: ì •ê·œí™” (Normalization) - MinMaxScaler

ì •ê·œí™”ëŠ” ë°ì´í„°ë¥¼ **0ê³¼ 1 ì‚¬ì´**ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ìµœì†Ÿê°’ê³¼ ìµœëŒ“ê°’ì´ ëª…í™•í•œ ë°ì´í„°ì— ì í•©í•©ë‹ˆë‹¤.

**ê³µì‹**: x_scaled = (x - min) / (max - min)

```python
# ì •ê·œí™” êµ¬í˜„ ë° ë¶„ì„
def apply_normalization(df, columns=None):
    """
    ì •ê·œí™”ë¥¼ ì ìš©í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns.tolist()
        if 'SalePrice' in columns:
            columns.remove('SalePrice')
        columns = columns[:5]
    
    print("ğŸ“Š ì •ê·œí™”(MinMaxScaler) ì ìš©:")
    
    # ì›ë³¸ ë°ì´í„° ë²”ìœ„
    print(f"\nğŸ”¸ ì›ë³¸ ë°ì´í„° ë²”ìœ„:")
    for col in columns:
        if col in df.columns:
            min_val = df[col].min()
            max_val = df[col].max()
            range_val = max_val - min_val
            print(f"   {col}: {min_val:.1f} ~ {max_val:.1f} (ë²”ìœ„: {range_val:.1f})")
    
    # ì •ê·œí™” ì ìš©
    scaler = MinMaxScaler()
    normalized_data = scaler.fit_transform(df[columns])
    normalized_df = pd.DataFrame(normalized_data, columns=columns, index=df.index)
    
    # ë³€í™˜ í›„ ë²”ìœ„
    print(f"\nğŸ”¸ ì •ê·œí™” í›„ ë²”ìœ„:")
    for col in columns:
        min_val = normalized_df[col].min()
        max_val = normalized_df[col].max()
        print(f"   {col}: {min_val:.3f} ~ {max_val:.3f}")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('ğŸ“Š ì •ê·œí™”(MinMaxScaler) íš¨ê³¼ ë¶„ì„', fontsize=16, fontweight='bold')
    
    # 1. ì›ë³¸ ë°ì´í„° ë²”ìœ„
    ranges_original = [df[col].max() - df[col].min() for col in columns]
    axes[0].bar(columns, ranges_original, color='skyblue', alpha=0.7)
    axes[0].set_title('ì›ë³¸ ë°ì´í„° ë²”ìœ„')
    axes[0].set_ylabel('ë²”ìœ„ í¬ê¸°')
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].set_yscale('log')
    
    # 2. ì •ê·œí™” í›„ ë²”ìœ„ (ëª¨ë‘ 1)
    ranges_normalized = [1.0] * len(columns)  # ëª¨ë‘ 0~1 ë²”ìœ„
    axes[1].bar(columns, ranges_normalized, color='lightcoral', alpha=0.7)
    axes[1].set_title('ì •ê·œí™” í›„ ë²”ìœ„ (ëª¨ë‘ 0~1)')
    axes[1].set_ylabel('ë²”ìœ„ í¬ê¸°')
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].set_ylim(0, 1.2)
    
    # 3. ì •ê·œí™” í›„ ë°ì´í„° ë¶„í¬
    normalized_df.plot(kind='box', ax=axes[2])
    axes[2].set_title('ì •ê·œí™” í›„ ë¶„í¬ (0~1 ë²”ìœ„)')
    axes[2].tick_params(axis='x', rotation=45)
    axes[2].set_ylim(-0.1, 1.1)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nâœ… ì •ê·œí™” ì™„ë£Œ!")
    print(f"   ëª¨ë“  ë³€ìˆ˜ê°€ 0~1 ë²”ìœ„ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"   ğŸ’¡ ìµœì†Ÿê°’ 0, ìµœëŒ“ê°’ 1ì´ ëª…í™•í•œ ì˜ë¯¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤!")
    
    return normalized_df, scaler

# ì •ê·œí™” ì ìš©
normalized_data, minmax_scaler = apply_normalization(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `MinMaxScaler()`: 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ëŠ” í´ë˜ìŠ¤
- `set_yscale('log')`: ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ yì¶•ì„ ì„¤ì •í•˜ì—¬ í° ì°¨ì´ë¥¼ ëª…í™•íˆ í‘œì‹œ
- `set_ylim()`: yì¶• ë²”ìœ„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •

### ë°©ë²• 3: ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ (Robust Scaling) - RobustScaler

ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ì€ **ì¤‘ì•™ê°’ê³¼ IQR**ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

**ê³µì‹**: x_scaled = (x - median) / IQR

```python
# ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„ ë° ë¶„ì„
def apply_robust_scaling(df, columns=None):
    """
    ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ì„ ì ìš©í•˜ê³  ì´ìƒì¹˜ ì²˜ë¦¬ íš¨ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns.tolist()
        if 'SalePrice' in columns:
            columns.remove('SalePrice')
        columns = columns[:5]
    
    print("ğŸ“Š ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§(RobustScaler) ì ìš©:")
    
    # ì´ìƒì¹˜ê°€ í¬í•¨ëœ ë°ì´í„° ìƒì„± (ì‹œì—° ëª©ì )
    df_with_outliers = df.copy()
    
    # ì²« ë²ˆì§¸ ë³€ìˆ˜ì— ì¸ìœ„ì ìœ¼ë¡œ ê·¹ë‹¨ê°’ ì¶”ê°€
    if len(columns) > 0:
        first_col = columns[0]
        if first_col in df.columns:
            # ìƒìœ„ 1%ì— ë§¤ìš° í° ê°’ ì¶”ê°€
            extreme_value = df[first_col].quantile(0.99) * 5
            outlier_indices = df.sample(n=5).index  # 5ê°œ ë°ì´í„°í¬ì¸íŠ¸ì— ê·¹ë‹¨ê°’ ì¶”ê°€
            df_with_outliers.loc[outlier_indices, first_col] = extreme_value
            
            print(f"\nğŸ”¸ {first_col}ì— ê·¹ë‹¨ê°’ ì¶”ê°€ (ì‹œì—°ìš©):")
            print(f"   ê·¹ë‹¨ê°’: {extreme_value:.0f}")
            print(f"   ì˜í–¥ë°›ëŠ” ë°ì´í„°: {len(outlier_indices)}ê°œ")
    
    # 3ê°€ì§€ ìŠ¤ì¼€ì¼ë§ ë°©ë²• ë¹„êµ
    scalers = {
        'Standard': StandardScaler(),
        'MinMax': MinMaxScaler(), 
        'Robust': RobustScaler()
    }
    
    scaled_results = {}
    
    for name, scaler in scalers.items():
        scaled_data = scaler.fit_transform(df_with_outliers[columns])
        scaled_results[name] = pd.DataFrame(scaled_data, columns=columns)
    
    # ì²« ë²ˆì§¸ ë³€ìˆ˜ë¡œ ë¹„êµ ë¶„ì„
    first_col = columns[0]
    
    print(f"\nğŸ”¸ {first_col} ë³€ìˆ˜ì˜ ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë³„ ë¹„êµ:")
    for name, scaled_df in scaled_results.items():
        q1, q3 = scaled_df[first_col].quantile([0.25, 0.75])
        iqr = q3 - q1
        lower_fence = q1 - 1.5 * iqr
        upper_fence = q3 + 1.5 * iqr
        outliers = scaled_df[first_col][(scaled_df[first_col] < lower_fence) | 
                                        (scaled_df[first_col] > upper_fence)]
        
        print(f"   {name:8}: ì´ìƒì¹˜ {len(outliers)}ê°œ, ë²”ìœ„ [{scaled_df[first_col].min():.2f}, {scaled_df[first_col].max():.2f}]")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ğŸ“Š ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ vs ë‹¤ë¥¸ ë°©ë²•ë“¤', fontsize=16, fontweight='bold')
    
    # 1. ì›ë³¸ ë°ì´í„° (ì´ìƒì¹˜ í¬í•¨)
    axes[0,0].hist(df_with_outliers[first_col].dropna(), bins=50, 
                   alpha=0.7, color='gray', label='ì´ìƒì¹˜ í¬í•¨')
    axes[0,0].hist(df[first_col].dropna(), bins=50, 
                   alpha=0.7, color='skyblue', label='ì›ë³¸')
    axes[0,0].set_title(f'{first_col} ì›ë³¸ vs ì´ìƒì¹˜ í¬í•¨')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. í‘œì¤€í™” ê²°ê³¼
    axes[0,1].hist(scaled_results['Standard'][first_col], bins=50, 
                   alpha=0.7, color='lightcoral')
    axes[0,1].set_title('í‘œì¤€í™” ê²°ê³¼\n(ì´ìƒì¹˜ì— ë¯¼ê°)')
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. MinMax ì •ê·œí™” ê²°ê³¼
    axes[1,0].hist(scaled_results['MinMax'][first_col], bins=50, 
                   alpha=0.7, color='lightgreen')
    axes[1,0].set_title('MinMax ì •ê·œí™” ê²°ê³¼\n(ì´ìƒì¹˜ì— ë§¤ìš° ë¯¼ê°)')
    axes[1,0].grid(True, alpha=0.3)
    
    # 4. ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼
    axes[1,1].hist(scaled_results['Robust'][first_col], bins=50, 
                   alpha=0.7, color='orange')
    axes[1,1].set_title('ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ ê²°ê³¼\n(ì´ìƒì¹˜ì— ëœ ë¯¼ê°)')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nâœ… ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ì˜ ì¥ì :")
    print(f"   ğŸ”¹ ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ìµœì†Œí™”")
    print(f"   ğŸ”¹ ì¤‘ì•™ê°’ê³¼ IQR ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •ì ")
    print(f"   ğŸ”¹ ì´ìƒì¹˜ê°€ ë§ì€ ì‹¤ì œ ë°ì´í„°ì— ì í•©")
    
    return scaled_results['Robust'], RobustScaler().fit(df[columns])

# ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§ ì ìš©
robust_scaled_data, robust_scaler = apply_robust_scaling(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `RobustScaler()`: ì¤‘ì•™ê°’ê³¼ IQRì„ ì‚¬ìš©í•˜ëŠ” ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§
- `quantile([0.25, 0.75])`: ì œ1ì‚¬ë¶„ìœ„ìˆ˜ì™€ ì œ3ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ ë™ì‹œì— ê³„ì‚°
- `sample(n=5)`: ë¬´ì‘ìœ„ë¡œ 5ê°œ í–‰ì„ ì„ íƒí•˜ì—¬ ê·¹ë‹¨ê°’ ì¶”ê°€

### ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì„ íƒ ê°€ì´ë“œ

```python
# ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì„ íƒ ê°€ì´ë“œ
def scaling_method_guide():
    """
    ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ ìŠ¤ì¼€ì¼ë§ ë°©ë²•ì„ ì‚¬ìš©í• ì§€ ê°€ì´ë“œ ì œê³µ
    """
    print("ğŸ¯ ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì„ íƒ ê°€ì´ë“œ:")
    print()
    
    guide_data = {
        'ë°©ë²•': ['StandardScaler', 'MinMaxScaler', 'RobustScaler'],
        'ì ìš© ì‹œë‚˜ë¦¬ì˜¤': [
            'ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´ ë°ì´í„°\nì‹ ê²½ë§, SVM ë“±',
            '0~1 ë²”ìœ„ê°€ ì˜ë¯¸ìˆëŠ” ê²½ìš°\nì´ë¯¸ì§€ ë°ì´í„°, í™•ë¥ ê°’',
            'ì´ìƒì¹˜ê°€ ë§ì€ ë°ì´í„°\nì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë°ì´í„°'
        ],
        'ì¥ì ': [
            'â€¢ ì •ê·œë¶„í¬ ê°€ì • ì•Œê³ ë¦¬ì¦˜ì— ìµœì \nâ€¢ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ í•´ì„ ìš©ì´',
            'â€¢ ëª…í™•í•œ 0~1 ë²”ìœ„\nâ€¢ ì´í•´í•˜ê¸° ì‰¬ì›€',
            'â€¢ ì´ìƒì¹˜ì— ê°•ê±´\nâ€¢ ì‹¤ì œ ë°ì´í„°ì— ì•ˆì •ì '
        ],
        'ë‹¨ì ': [
            'â€¢ ì´ìƒì¹˜ì— ë¯¼ê°\nâ€¢ ì •ê·œë¶„í¬ ì•„ë‹ ë•Œ íš¨ê³¼ ì œí•œ',
            'â€¢ ì´ìƒì¹˜ì— ë§¤ìš° ë¯¼ê°\nâ€¢ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë²”ìœ„ ë²—ì–´ë‚  ìˆ˜ ìˆìŒ',
            'â€¢ í•´ì„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì–´ë ¤ì›€\nâ€¢ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ë¶€ì í•©'
        ]
    }
    
    guide_df = pd.DataFrame(guide_data)
    
    for i, row in guide_df.iterrows():
        print(f"ğŸ“Š {row['ë°©ë²•']}")
        print(f"   ğŸ¯ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤: {row['ì ìš© ì‹œë‚˜ë¦¬ì˜¤']}")
        print(f"   âœ… ì¥ì : {row['ì¥ì ']}")
        print(f"   âš ï¸ ë‹¨ì : {row['ë‹¨ì ']}")
        print()
    
    print("ğŸ’¡ ì‹¤ë¬´ íŒ:")
    print("   1ï¸âƒ£ ë¨¼ì € RobustScalerë¡œ ì‹œì‘í•´ë³´ì„¸ìš” (ì´ìƒì¹˜ ë§ì€ ì‹¤ì œ ë°ì´í„°)")
    print("   2ï¸âƒ£ ì‹ ê²½ë§ ì‚¬ìš©ì‹œ StandardScaler ê³ ë ¤")
    print("   3ï¸âƒ£ í•´ì„ì´ ì¤‘ìš”í•˜ë©´ MinMaxScaler ì‚¬ìš©")
    print("   4ï¸âƒ£ ì—¬ëŸ¬ ë°©ë²•ì„ ì‹¤í—˜í•´ë³´ê³  ì„±ëŠ¥ ë¹„êµ!")

# ê°€ì´ë“œ ì¶œë ¥
scaling_method_guide()
```

ì´ì œ ì‹¤ì œ House Prices ë°ì´í„°ì— ì í•©í•œ ìŠ¤ì¼€ì¼ë§ ë°©ë²•ì„ ì„ íƒí•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# House Prices ë°ì´í„° íŠ¹ì„± ë¶„ì„ ë° ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì¶”ì²œ
def recommend_scaling_for_house_data(df):
    """
    House Prices ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ìµœì  ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì¶”ì²œ
    """
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'SalePrice' in numeric_cols:
        numeric_cols.remove('SalePrice')
    
    print("ğŸ¡ House Prices ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì¶”ì²œ:")
    print()
    
    recommendations = {}
    
    for col in numeric_cols[:8]:  # ì£¼ìš” 8ê°œ ë³€ìˆ˜ ë¶„ì„
        if col in df.columns:
            data = df[col].dropna()
            
            # ê¸°ë³¸ í†µê³„
            mean_val = data.mean()
            median_val = data.median()
            std_val = data.std()
            q1, q3 = data.quantile([0.25, 0.75])
            iqr = q3 - q1
            
            # ì´ìƒì¹˜ ë¹„ìœ¨
            lower_fence = q1 - 1.5 * iqr
            upper_fence = q3 + 1.5 * iqr
            outlier_ratio = len(data[(data < lower_fence) | (data > upper_fence)]) / len(data)
            
            # ì™œë„ ê³„ì‚°
            from scipy.stats import skew
            skewness = skew(data)
            
            # ì¶”ì²œ ë¡œì§
            if outlier_ratio > 0.05:  # ì´ìƒì¹˜ 5% ì´ìƒ
                if abs(skewness) > 1:  # ì‹¬í•œ ì™œë„
                    recommendation = "RobustScaler + ë¡œê·¸ë³€í™˜"
                else:
                    recommendation = "RobustScaler"
            elif abs(skewness) > 1:
                recommendation = "StandardScaler + ë¡œê·¸ë³€í™˜"
            elif (data >= 0).all() and data.max() > 1000:  # í° ì–‘ìˆ˜ê°’
                recommendation = "MinMaxScaler ë˜ëŠ” StandardScaler"
            else:
                recommendation = "StandardScaler"
            
            recommendations[col] = {
                'method': recommendation,
                'outlier_ratio': outlier_ratio,
                'skewness': skewness,
                'mean': mean_val,
                'median': median_val
            }
            
            print(f"ğŸ“Š {col}:")
            print(f"   ì´ìƒì¹˜ ë¹„ìœ¨: {outlier_ratio:.2%}")
            print(f"   ì™œë„: {skewness:.2f}")
            print(f"   ì¶”ì²œ ë°©ë²•: {recommendation}")
            print()
    
    return recommendations

# House Prices ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì¶”ì²œ
scaling_recommendations = recommend_scaling_for_house_data(train_data)

---

## ğŸ“– 4.2.2 ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (Categorical Variable Encoding)

### ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì´í•´

**ë²”ì£¼í˜• ë³€ìˆ˜**ëŠ” ì¹´í…Œê³ ë¦¬ë‚˜ ê·¸ë£¹ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„°ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì´ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¨ìˆœíˆ ìˆ«ìë¥¼ í• ë‹¹í•˜ë©´ ì˜ëª»ëœ ìˆœì„œë‚˜ ê±°ë¦¬ ê´€ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **ëª…ëª©í˜•(Nominal)**: ìˆœì„œê°€ ì—†ëŠ” ë²”ì£¼ (ìƒ‰ê¹”, ì§€ì—­ ë“±)
> - **ìˆœì„œí˜•(Ordinal)**: ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼ (ë“±ê¸‰, í¬ê¸° ë“±)
> - **ì›-í•« ì¸ì½”ë”©**: ê° ë²”ì£¼ë¥¼ ë³„ë„ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜
> - **ë ˆì´ë¸” ì¸ì½”ë”©**: ë²”ì£¼ë¥¼ ì—°ì†ëœ ì •ìˆ˜ë¡œ ë³€í™˜
> - **íƒ€ê²Ÿ ì¸ì½”ë”©**: íƒ€ê²Ÿ ë³€ìˆ˜ì˜ í‰ê· ê°’ìœ¼ë¡œ ë²”ì£¼ë¥¼ ì¸ì½”ë”©

### House Prices ë°ì´í„°ì˜ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„

```python
# ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„ í•¨ìˆ˜
def analyze_categorical_variables(df):
    """
    ë°ì´í„°ì…‹ì˜ ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ë¶„ì„í•˜ê³  ì¸ì½”ë”© ì „ëµ ìˆ˜ë¦½
    """
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    
    print(f"ğŸ·ï¸ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„ ({len(categorical_cols)}ê°œ ë³€ìˆ˜):")
    print()
    
    # ë³€ìˆ˜ë³„ ìƒì„¸ ë¶„ì„
    for col in categorical_cols[:10]:  # ì²˜ìŒ 10ê°œ ë³€ìˆ˜ë§Œ ë¶„ì„
        if col in df.columns:
            unique_count = df[col].nunique()
            missing_count = df[col].isnull().sum()
            missing_ratio = missing_count / len(df)
            
            print(f"ğŸ“Š {col}:")
            print(f"   ê³ ìœ ê°’ ìˆ˜: {unique_count}ê°œ")
            print(f"   ê²°ì¸¡ì¹˜: {missing_count}ê°œ ({missing_ratio:.1%})")
            
            # ìƒìœ„ ë¹ˆë„ê°’ í‘œì‹œ
            value_counts = df[col].value_counts().head(5)
            print(f"   ìƒìœ„ 5ê°œ ê°’:")
            for value, count in value_counts.items():
                percentage = count / len(df) * 100
                print(f"      {value}: {count}ê°œ ({percentage:.1f}%)")
            
            # ì¸ì½”ë”© ë°©ë²• ì¶”ì²œ
            if unique_count <= 5:
                encoding_method = "ì›-í•« ì¸ì½”ë”©"
            elif unique_count <= 20:
                encoding_method = "ì›-í•« ì¸ì½”ë”© ë˜ëŠ” íƒ€ê²Ÿ ì¸ì½”ë”©"
            else:
                encoding_method = "íƒ€ê²Ÿ ì¸ì½”ë”© ë˜ëŠ” ë¹ˆë„ ì¸ì½”ë”©"
            
            print(f"   ğŸ’¡ ì¶”ì²œ ì¸ì½”ë”©: {encoding_method}")
            print()
    
    # ì¹´ë””ë„ë¦¬í‹°ë³„ ë¶„ë¥˜
    low_cardinality = [col for col in categorical_cols if df[col].nunique() <= 5]
    medium_cardinality = [col for col in categorical_cols if 5 < df[col].nunique() <= 20]
    high_cardinality = [col for col in categorical_cols if df[col].nunique() > 20]
    
    print(f"ğŸ“ˆ ì¹´ë””ë„ë¦¬í‹°ë³„ ë¶„ë¥˜:")
    print(f"   ğŸŸ¢ ë‚®ìŒ (â‰¤5ê°œ): {len(low_cardinality)}ê°œ ë³€ìˆ˜")
    print(f"   ğŸŸ¡ ì¤‘ê°„ (6-20ê°œ): {len(medium_cardinality)}ê°œ ë³€ìˆ˜") 
    print(f"   ğŸ”´ ë†’ìŒ (>20ê°œ): {len(high_cardinality)}ê°œ ë³€ìˆ˜")
    
    return {
        'low_cardinality': low_cardinality,
        'medium_cardinality': medium_cardinality,
        'high_cardinality': high_cardinality
    }

# ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„ ì‹¤í–‰
categorical_analysis = analyze_categorical_variables(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `nunique()`: ê²°ì¸¡ì¹˜ë¥¼ ì œì™¸í•œ ê³ ìœ ê°’ì˜ ê°œìˆ˜ë¥¼ ê³„ì‚°
- `value_counts()`: ê° ê°’ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•˜ì—¬ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
- ì¹´ë””ë„ë¦¬í‹°ì— ë”°ë¼ ë‹¤ë¥¸ ì¸ì½”ë”© ì „ëµì„ ì¶”ì²œ

### ë°©ë²• 1: ì›-í•« ì¸ì½”ë”© (One-Hot Encoding)

ê° ë²”ì£¼ë¥¼ ë³„ë„ì˜ ì´ì§„(0/1) ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ìˆœì„œê°€ ì—†ëŠ” ëª…ëª©í˜• ë³€ìˆ˜ì— ì í•©í•©ë‹ˆë‹¤.

```python
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

# ì›-í•« ì¸ì½”ë”© êµ¬í˜„
def apply_onehot_encoding(df, columns=None, max_categories=10):
    """
    ì›-í•« ì¸ì½”ë”©ì„ ì ìš©í•˜ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        # ë‚®ì€ ì¹´ë””ë„ë¦¬í‹° ë³€ìˆ˜ ì„ íƒ
        categorical_cols = df.select_dtypes(include=['object']).columns
        columns = [col for col in categorical_cols if df[col].nunique() <= max_categories][:3]
    
    print(f"ğŸ¯ ì›-í•« ì¸ì½”ë”© ì ìš© ({len(columns)}ê°œ ë³€ìˆ˜):")
    
    original_shape = df.shape
    encoded_dfs = []
    column_info = {}
    
    for col in columns:
        if col in df.columns:
            print(f"\nğŸ“Š {col} ë³€ìˆ˜:")
            
            # ì›ë³¸ ì •ë³´
            unique_values = df[col].dropna().unique()
            print(f"   ì›ë³¸ ë²”ì£¼: {list(unique_values)}")
            print(f"   ë²”ì£¼ ìˆ˜: {len(unique_values)}ê°œ")
            
            # ì›-í•« ì¸ì½”ë”© ì ìš©
            encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì„ì‹œë¡œ 'Unknown'ìœ¼ë¡œ ëŒ€ì²´)
            col_data = df[col].fillna('Unknown').values.reshape(-1, 1)
            encoded_array = encoder.fit_transform(col_data)
            
            # ì»¬ëŸ¼ëª… ìƒì„±
            feature_names = [f"{col}_{category}" for category in encoder.categories_[0]]
            encoded_df = pd.DataFrame(encoded_array, columns=feature_names, index=df.index)
            
            encoded_dfs.append(encoded_df)
            column_info[col] = {
                'original_categories': list(unique_values),
                'encoded_columns': feature_names,
                'encoder': encoder
            }
            
            print(f"   ìƒì„±ëœ ì»¬ëŸ¼: {len(feature_names)}ê°œ")
            print(f"   ì»¬ëŸ¼ëª…: {feature_names}")
    
    # ê²°ê³¼ ê²°í•©
    if encoded_dfs:
        final_encoded_df = pd.concat(encoded_dfs, axis=1)
        
        print(f"\nğŸ“ˆ ì¸ì½”ë”© ê²°ê³¼ ìš”ì•½:")
        print(f"   ì›ë³¸ ë²”ì£¼í˜• ë³€ìˆ˜: {len(columns)}ê°œ")
        print(f"   ìƒì„±ëœ ì´ì§„ ë³€ìˆ˜: {final_encoded_df.shape[1]}ê°œ")
        print(f"   ë°ì´í„° í¬ê¸° ë³€í™”: {original_shape} â†’ {(df.shape[0], df.shape[1] - len(columns) + final_encoded_df.shape[1])}")
        
        # ìƒ˜í”Œ ë°ì´í„° ì‹œê°í™”
        print(f"\nğŸ” ì²« ë²ˆì§¸ ë³€ìˆ˜ ì¸ì½”ë”© ì˜ˆì‹œ:")
        first_col = columns[0]
        print(f"   ì›ë³¸ {first_col} ê°’ë“¤:")
        sample_indices = df.sample(5).index
        for idx in sample_indices:
            original_value = df.loc[idx, first_col]
            encoded_values = final_encoded_df.loc[idx, [col for col in final_encoded_df.columns if col.startswith(first_col)]]
            active_column = encoded_values[encoded_values == 1]
            if len(active_column) > 0:
                print(f"      '{original_value}' â†’ {active_column.index[0]} = 1")
            else:
                print(f"      '{original_value}' â†’ ëª¨ë“  ì»¬ëŸ¼ = 0 (ê²°ì¸¡ì¹˜)")
        
        return final_encoded_df, column_info
    else:
        print("âš ï¸ ì¸ì½”ë”©í•  ë²”ì£¼í˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

# ì›-í•« ì¸ì½”ë”© ì ìš©
onehot_encoded_data, onehot_info = apply_onehot_encoding(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `OneHotEncoder(sparse_output=False)`: í¬ì†Œí–‰ë ¬ ëŒ€ì‹  ì¼ë°˜ ë°°ì—´ë¡œ ì¶œë ¥
- `handle_unknown='ignore'`: ìƒˆë¡œìš´ ë²”ì£¼ê°€ ë‚˜íƒ€ë‚˜ë©´ ëª¨ë“  ê°’ì„ 0ìœ¼ë¡œ ì„¤ì •
- `reshape(-1, 1)`: 1ì°¨ì› ë°°ì—´ì„ 2ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (scikit-learn ìš”êµ¬ì‚¬í•­)

### ë°©ë²• 2: ìˆœì„œí˜• ì¸ì½”ë”© (Ordinal Encoding)

ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆœì„œë¥¼ ìœ ì§€í•˜ë©´ì„œ ìˆ˜ì¹˜ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
from sklearn.preprocessing import OrdinalEncoder

# ìˆœì„œí˜• ì¸ì½”ë”© êµ¬í˜„
def apply_ordinal_encoding(df):
    """
    House Prices ë°ì´í„°ì˜ í’ˆì§ˆ ê´€ë ¨ ë³€ìˆ˜ë“¤ì— ìˆœì„œí˜• ì¸ì½”ë”© ì ìš©
    """
    print("ğŸ“Š ìˆœì„œí˜• ì¸ì½”ë”© ì ìš©:")
    
    # House Prices ë°ì´í„°ì˜ í’ˆì§ˆ ê´€ë ¨ ë³€ìˆ˜ë“¤ (ìˆœì„œê°€ ìˆìŒ)
    quality_columns = {
        'ExterQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],  # Poor â†’ Excellent
        'ExterCond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
        'BsmtQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
        'BsmtCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],
        'HeatingQC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
        'KitchenQual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],
        'FireplaceQu': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex']
    }
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_quality_cols = {col: order for col, order in quality_columns.items() 
                             if col in df.columns}
    
    encoded_df = df.copy()
    encoding_info = {}
    
    for col, order_list in available_quality_cols.items():
        print(f"\nğŸ·ï¸ {col} ì¸ì½”ë”©:")
        
        # ì›ë³¸ ë°ì´í„° ë¶„í¬ í™•ì¸
        original_counts = df[col].value_counts()
        print(f"   ì›ë³¸ ë¶„í¬: {dict(original_counts)}")
        
        # ê²°ì¸¡ì¹˜ë¥¼ 'None'ìœ¼ë¡œ ëŒ€ì²´
        col_data = df[col].fillna('None')
        
        # ìˆœì„œí˜• ì¸ì½”ë”© ì ìš©
        encoder = OrdinalEncoder(categories=[order_list])
        
        try:
            encoded_values = encoder.fit_transform(col_data.values.reshape(-1, 1)).flatten()
            encoded_df[col + '_Encoded'] = encoded_values
            
            # ì¸ì½”ë”© ê²°ê³¼ í™•ì¸
            encoding_map = {category: i for i, category in enumerate(order_list)}
            print(f"   ì¸ì½”ë”© ë§µí•‘: {encoding_map}")
            
            # í†µê³„ ì •ë³´
            print(f"   ì¸ì½”ë”©ëœ ê°’ ë²”ìœ„: {encoded_values.min():.0f} ~ {encoded_values.max():.0f}")
            print(f"   í‰ê·  í’ˆì§ˆ ì ìˆ˜: {encoded_values.mean():.2f}")
            
            encoding_info[col] = {
                'order': order_list,
                'mapping': encoding_map,
                'encoder': encoder
            }
            
        except ValueError as e:
            print(f"   âš ï¸ ì¸ì½”ë”© ì‹¤íŒ¨: {e}")
            print(f"   ë°ì´í„°ì— ì •ì˜ë˜ì§€ ì•Šì€ ë²”ì£¼ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # í’ˆì§ˆ ì ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
    encoded_quality_cols = [col + '_Encoded' for col in available_quality_cols.keys()]
    existing_encoded_cols = [col for col in encoded_quality_cols if col in encoded_df.columns]
    
    if len(existing_encoded_cols) > 1:
        correlation_matrix = encoded_df[existing_encoded_cols].corr()
        
        print(f"\nğŸ“Š í’ˆì§ˆ ë³€ìˆ˜ë“¤ ê°„ ìƒê´€ê´€ê³„:")
        print(correlation_matrix.round(3))
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', 
                   center=0, square=True, fmt='.3f')
        plt.title('í’ˆì§ˆ ë³€ìˆ˜ë“¤ì˜ ìˆœì„œí˜• ì¸ì½”ë”© í›„ ìƒê´€ê´€ê³„')
        plt.tight_layout()
        plt.show()
    
    print(f"\nâœ… ìˆœì„œí˜• ì¸ì½”ë”© ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ ë³€ìˆ˜: {len(encoding_info)}ê°œ")
    print(f"   ğŸ’¡ ìˆœì„œê°€ ìˆëŠ” í’ˆì§ˆ ì •ë³´ê°€ ìˆ˜ì¹˜ë¡œ ë³€í™˜ë˜ì–´ ëª¨ë¸ì´ íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì‰¬ì›Œì¡ŒìŠµë‹ˆë‹¤!")
    
    return encoded_df, encoding_info

# ìˆœì„œí˜• ì¸ì½”ë”© ì ìš©
ordinal_encoded_data, ordinal_info = apply_ordinal_encoding(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `OrdinalEncoder(categories=[order_list])`: ë¯¸ë¦¬ ì •ì˜ëœ ìˆœì„œë¡œ ì¸ì½”ë”©
- `flatten()`: 2ì°¨ì› ë°°ì—´ì„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
- í’ˆì§ˆ ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„ìœ¼ë¡œ ì¸ì½”ë”© ê²°ê³¼ ê²€ì¦

### ë°©ë²• 3: íƒ€ê²Ÿ ì¸ì½”ë”© (Target Encoding)

ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ê° ë²”ì£¼ë¥¼ í•´ë‹¹ ë²”ì£¼ì—ì„œì˜ íƒ€ê²Ÿ ë³€ìˆ˜ í‰ê· ê°’ìœ¼ë¡œ ì¸ì½”ë”©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ë³€ìˆ˜ì— íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.

```python
# íƒ€ê²Ÿ ì¸ì½”ë”© êµ¬í˜„
def apply_target_encoding(df, target_col='SalePrice', categorical_cols=None, smoothing=1.0):
    """
    íƒ€ê²Ÿ ì¸ì½”ë”©ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜ (ê³¼ì í•© ë°©ì§€ ê¸°ë²• í¬í•¨)
    
    Parameters:
    smoothing (float): ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„° (ê³¼ì í•© ë°©ì§€)
    """
    if target_col not in df.columns:
        print(f"âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ '{target_col}'ì´ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        return df, {}
    
    if categorical_cols is None:
        # ì¤‘ê°„~ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ë³€ìˆ˜ ì„ íƒ
        categorical_all = df.select_dtypes(include=['object']).columns
        categorical_cols = [col for col in categorical_all 
                           if 5 < df[col].nunique() <= 50][:3]  # ì˜ˆì‹œë¡œ 3ê°œë§Œ
    
    print(f"ğŸ¯ íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš© ({len(categorical_cols)}ê°œ ë³€ìˆ˜):")
    print(f"   íƒ€ê²Ÿ ë³€ìˆ˜: {target_col}")
    print(f"   ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„°: {smoothing}")
    
    encoded_df = df.copy()
    encoding_info = {}
    
    # ì „ì²´ íƒ€ê²Ÿ í‰ê·  (ìŠ¤ë¬´ë”©ì— ì‚¬ìš©)
    global_mean = df[target_col].mean()
    
    for col in categorical_cols:
        if col in df.columns:
            print(f"\nğŸ“Š {col} íƒ€ê²Ÿ ì¸ì½”ë”©:")
            
            # ë²”ì£¼ë³„ íƒ€ê²Ÿ í†µê³„ ê³„ì‚°
            category_stats = df.groupby(col)[target_col].agg(['count', 'mean']).reset_index()
            category_stats.columns = [col, 'count', 'target_mean']
            
            # ìŠ¤ë¬´ë”© ì ìš© (ê³¼ì í•© ë°©ì§€)
            # ê³µì‹: (count * category_mean + smoothing * global_mean) / (count + smoothing)
            category_stats['smoothed_mean'] = (
                (category_stats['count'] * category_stats['target_mean'] + 
                 smoothing * global_mean) / 
                (category_stats['count'] + smoothing)
            )
            
            print(f"   ë²”ì£¼ë³„ íƒ€ê²Ÿ ì¸ì½”ë”© ê²°ê³¼:")
            for _, row in category_stats.head().iterrows():
                print(f"      {row[col]}: {row['target_mean']:.0f} â†’ {row['smoothed_mean']:.0f} (ìƒ˜í”Œ {row['count']}ê°œ)")
            
            # ì¸ì½”ë”© ì ìš©
            encoding_map = dict(zip(category_stats[col], category_stats['smoothed_mean']))
            encoded_df[col + '_TargetEnc'] = df[col].map(encoding_map)
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì „ì²´ í‰ê· ìœ¼ë¡œ)
            encoded_df[col + '_TargetEnc'] = encoded_df[col + '_TargetEnc'].fillna(global_mean)
            
            # ì •ë³´ ì €ì¥
            encoding_info[col] = {
                'encoding_map': encoding_map,
                'global_mean': global_mean,
                'category_stats': category_stats
            }
            
            # íš¨ê³¼ ë¶„ì„
            original_correlation = df[col].astype('category').cat.codes.corr(df[target_col])
            encoded_correlation = encoded_df[col + '_TargetEnc'].corr(df[target_col])
            
            print(f"   ìƒê´€ê´€ê³„ ë³€í™”: {original_correlation:.3f} â†’ {encoded_correlation:.3f}")
            
            # ì‹œê°í™” (ì²« ë²ˆì§¸ ë³€ìˆ˜ë§Œ)
            if col == categorical_cols[0]:
                fig, axes = plt.subplots(1, 2, figsize=(15, 5))
                
                # ì›ë³¸ ë²”ì£¼ë³„ íƒ€ê²Ÿ ë¶„í¬
                df.boxplot(column=target_col, by=col, ax=axes[0])
                axes[0].set_title(f'{col}ë³„ {target_col} ë¶„í¬ (ì›ë³¸)')
                axes[0].tick_params(axis='x', rotation=45)
                
                # íƒ€ê²Ÿ ì¸ì½”ë”© ê²°ê³¼ vs ì‹¤ì œ íƒ€ê²Ÿ
                axes[1].scatter(encoded_df[col + '_TargetEnc'], df[target_col], alpha=0.6)
                axes[1].set_xlabel(f'{col} íƒ€ê²Ÿ ì¸ì½”ë”© ê°’')
                axes[1].set_ylabel(f'ì‹¤ì œ {target_col}')
                axes[1].set_title(f'íƒ€ê²Ÿ ì¸ì½”ë”© íš¨ê³¼ (ìƒê´€ê³„ìˆ˜: {encoded_correlation:.3f})')
                
                plt.tight_layout()
                plt.show()
    
    print(f"\nâœ… íƒ€ê²Ÿ ì¸ì½”ë”© ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ ë³€ìˆ˜: {len(encoding_info)}ê°œ")
    print(f"   ğŸ’¡ ë²”ì£¼ë³„ íƒ€ê²Ÿ í‰ê· ì´ ì§ì ‘ì ì¸ ì˜ˆì¸¡ ì •ë³´ë¡œ í™œìš©ë©ë‹ˆë‹¤!")
    
    return encoded_df, encoding_info

# íƒ€ê²Ÿ ì¸ì½”ë”© ì ìš©
target_encoded_data, target_info = apply_target_encoding(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- ìŠ¤ë¬´ë”© ê¸°ë²•ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€: ìƒ˜í”Œì´ ì ì€ ë²”ì£¼ëŠ” ì „ì²´ í‰ê· ì— ê°€ê¹Œì›Œì§
- `groupby().agg(['count', 'mean'])`: ë²”ì£¼ë³„ë¡œ ê°œìˆ˜ì™€ í‰ê· ì„ ë™ì‹œì— ê³„ì‚°
- `astype('category').cat.codes`: ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì„ì‹œë¡œ ìˆ«ì ì½”ë“œë¡œ ë³€í™˜í•˜ì—¬ ìƒê´€ê³„ìˆ˜ ê³„ì‚°

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive comparison visualization of categorical encoding methods showing: 1) A bar chart comparing the number of features created by One-Hot vs Ordinal vs Target encoding, 2) A correlation heatmap showing relationships between ordinal-encoded quality variables (ExterQual, BsmtQual, KitchenQual etc.), 3) A scatter plot showing target encoding effectiveness with actual SalePrice vs encoded values, 4) A box plot showing how different categories relate to target variable before encoding. Use professional styling with clear legends and annotations."

### ì¸ì½”ë”© ë°©ë²• ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

```python
# ì¸ì½”ë”© ë°©ë²• ì¢…í•© ë¹„êµ
def compare_encoding_methods(df, target_col='SalePrice'):
    """
    ë‹¤ì–‘í•œ ì¸ì½”ë”© ë°©ë²•ì˜ íš¨ê³¼ë¥¼ ë¹„êµ ë¶„ì„
    """
    print("ğŸ“Š ì¸ì½”ë”© ë°©ë²• ì¢…í•© ë¹„êµ:")
    
    comparison_results = {}
    
    # 1. ë°ì´í„° í¬ê¸° ë¹„êµ
    original_shape = df.shape
    
    # ì›-í•« ì¸ì½”ë”© ì‹œë®¬ë ˆì´ì…˜
    categorical_cols = df.select_dtypes(include=['object']).columns
    low_card_cols = [col for col in categorical_cols if df[col].nunique() <= 10]
    
    if low_card_cols:
        total_onehot_features = sum(df[col].nunique() for col in low_card_cols)
        onehot_shape = (original_shape[0], original_shape[1] - len(low_card_cols) + total_onehot_features)
    else:
        onehot_shape = original_shape
    
    # ìˆœì„œí˜•/íƒ€ê²Ÿ ì¸ì½”ë”©ì€ í¬ê¸° ë³€í™” ì—†ìŒ
    ordinal_shape = original_shape
    target_shape = original_shape
    
    print(f"\nğŸ“ˆ ë°ì´í„° í¬ê¸° ë³€í™”:")
    print(f"   ì›ë³¸: {original_shape}")
    print(f"   ì›-í•« ì¸ì½”ë”© í›„: {onehot_shape} (+{onehot_shape[1] - original_shape[1]}ê°œ ì»¬ëŸ¼)")
    print(f"   ìˆœì„œí˜• ì¸ì½”ë”© í›„: {ordinal_shape} (ë³€í™” ì—†ìŒ)")
    print(f"   íƒ€ê²Ÿ ì¸ì½”ë”© í›„: {target_shape} (ë³€í™” ì—†ìŒ)")
    
    # 2. ì¥ë‹¨ì  ë¹„êµí‘œ
    methods_comparison = {
        'ë°©ë²•': ['ì›-í•« ì¸ì½”ë”©', 'ìˆœì„œí˜• ì¸ì½”ë”©', 'íƒ€ê²Ÿ ì¸ì½”ë”©'],
        'ì ìš© ëŒ€ìƒ': [
            'ëª…ëª©í˜• ë³€ìˆ˜\n(ë‚®ì€ ì¹´ë””ë„ë¦¬í‹°)',
            'ìˆœì„œí˜• ë³€ìˆ˜\n(í’ˆì§ˆ, ë“±ê¸‰)',
            'ëª¨ë“  ë²”ì£¼í˜• ë³€ìˆ˜\n(ë†’ì€ ì¹´ë””ë„ë¦¬í‹°)'
        ],
        'ì¥ì ': [
            'â€¢ ìˆœì„œ ê´€ê³„ ì—†ìŒ\nâ€¢ í•´ì„ ëª…í™•\nâ€¢ ì•ˆì •ì ',
            'â€¢ ìˆœì„œ ì •ë³´ ë³´ì¡´\nâ€¢ íš¨ìœ¨ì \nâ€¢ ì§ê´€ì ',
            'â€¢ ì˜ˆì¸¡ë ¥ ë†’ìŒ\nâ€¢ ì°¨ì› ì¦ê°€ ì—†ìŒ\nâ€¢ ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ì²˜ë¦¬'
        ],
        'ë‹¨ì ': [
            'â€¢ ì°¨ì› í­ë°œ\nâ€¢ í¬ì†Œì„± ë¬¸ì œ\nâ€¢ ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ë¶€ì í•©',
            'â€¢ ìˆœì„œ ì •ë³´ í•„ìš”\nâ€¢ ëª…ëª©í˜•ì— ë¶€ì í•©\nâ€¢ ì„ì˜ì  ìˆœì„œ ìœ„í—˜',
            'â€¢ ê³¼ì í•© ìœ„í—˜\nâ€¢ ë°ì´í„° ëˆ„ìˆ˜ ê°€ëŠ¥\nâ€¢ êµì°¨ê²€ì¦ ë³µì¡'
        ],
        'ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤': [
            'Neighborhood, GarageType\në“± ì§€ì—­/ìœ í˜• ë³€ìˆ˜',
            'ExterQual, BsmtQual\në“± í’ˆì§ˆ ë³€ìˆ˜',
            'Neighborhood (ë†’ì€ ì¹´ë””ë„ë¦¬í‹°)\nìƒˆë¡œìš´ ë²”ì£¼ ë§ì€ ê²½ìš°'
        ]
    }
    
    comparison_df = pd.DataFrame(methods_comparison)
    
    print(f"\nğŸ“‹ ì¸ì½”ë”© ë°©ë²•ë³„ ìƒì„¸ ë¹„êµ:")
    for i, row in comparison_df.iterrows():
        print(f"\nğŸ”¹ {row['ë°©ë²•']}")
        print(f"   ì ìš© ëŒ€ìƒ: {row['ì ìš© ëŒ€ìƒ']}")
        print(f"   âœ… ì¥ì : {row['ì¥ì ']}")
        print(f"   âš ï¸ ë‹¨ì : {row['ë‹¨ì ']}")
        print(f"   ğŸ¯ ì‚¬ìš© ì˜ˆ: {row['ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤']}")
    
    # 3. ì‹¤ë¬´ ì„ íƒ ê°€ì´ë“œ
    print(f"\nğŸ’¡ ì‹¤ë¬´ ì¸ì½”ë”© ì„ íƒ ê°€ì´ë“œ:")
    print(f"   1ï¸âƒ£ ë²”ì£¼ ìˆ˜ â‰¤ 10ê°œ + ëª…ëª©í˜• â†’ ì›-í•« ì¸ì½”ë”©")
    print(f"   2ï¸âƒ£ ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼ â†’ ìˆœì„œí˜• ì¸ì½”ë”©") 
    print(f"   3ï¸âƒ£ ë²”ì£¼ ìˆ˜ > 10ê°œ â†’ íƒ€ê²Ÿ ì¸ì½”ë”© (ê³¼ì í•© ì£¼ì˜)")
    print(f"   4ï¸âƒ£ ë¶ˆí™•ì‹¤í•˜ë©´ â†’ ì—¬ëŸ¬ ë°©ë²• ì‹¤í—˜ í›„ ì„±ëŠ¥ ë¹„êµ")
    print(f"   5ï¸âƒ£ ìƒˆë¡œìš´ ë²”ì£¼ ë§ìœ¼ë©´ â†’ íƒ€ê²Ÿ ì¸ì½”ë”© + ìŠ¤ë¬´ë”©")

# ì¸ì½”ë”© ë°©ë²• ë¹„êµ ì‹¤í–‰
compare_encoding_methods(train_data)

---

## ğŸ“– 4.2.3 ë¹„ì„ í˜• ë³€í™˜ê³¼ ë¶„í¬ ì •ê·œí™”

### ì™œë„ì™€ ë¶„í¬ì˜ ì¤‘ìš”ì„±

ë§ì€ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ê°€ **ì •ê·œë¶„í¬**ì— ê°€ê¹ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ëŠ” ì¢…ì¢… **ì™œë„(Skewness)**ë¥¼ ê°€ì§€ë©°, ì´ëŠ” ëª¨ë¸ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **ì™œë„(Skewness)**: ë¶„í¬ì˜ ë¹„ëŒ€ì¹­ ì •ë„ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëŒ€ì¹­ì )
> - **ì²¨ë„(Kurtosis)**: ë¶„í¬ì˜ ë¾°ì¡±í•œ ì •ë„
> - **ë¡œê·¸ ë³€í™˜**: ì–‘ì˜ ì™œë„ë¥¼ ì¤„ì´ëŠ” ëŒ€í‘œì ì¸ ë³€í™˜ ë°©ë²•
> - **Box-Cox ë³€í™˜**: ìµœì ì˜ ë³€í™˜ íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ì°¾ëŠ” ë°©ë²•

### ë¶„í¬ ë¶„ì„ ë° ë³€í™˜ í•„ìš”ì„± í™•ì¸

```python
from scipy import stats
from scipy.stats import boxcox, yeojohnson
import numpy as np

# ë¶„í¬ ë¶„ì„ í•¨ìˆ˜
def analyze_distribution(df, columns=None):
    """
    ë³€ìˆ˜ë“¤ì˜ ë¶„í¬ë¥¼ ë¶„ì„í•˜ê³  ë³€í™˜ í•„ìš”ì„±ì„ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if 'SalePrice' in numeric_cols:
            numeric_cols.remove('SalePrice')
        columns = numeric_cols[:6]  # ì²˜ìŒ 6ê°œ ë³€ìˆ˜
    
    print("ğŸ“Š ë¶„í¬ ë¶„ì„ ë° ë³€í™˜ í•„ìš”ì„± í‰ê°€:")
    print()
    
    distribution_info = {}
    
    for col in columns:
        if col in df.columns:
            data = df[col].dropna()
            
            # ê¸°ë³¸ í†µê³„
            skewness = stats.skew(data)
            kurtosis = stats.kurtosis(data)
            
            # Shapiro-Wilk ì •ê·œì„± ê²€ì • (ìƒ˜í”Œ í¬ê¸° ì œí•œ)
            if len(data) <= 5000:
                shapiro_stat, shapiro_p = stats.shapiro(data.sample(min(len(data), 1000)))
            else:
                shapiro_stat, shapiro_p = stats.normaltest(data.sample(1000))
            
            # ë³€í™˜ í•„ìš”ì„± íŒë‹¨
            needs_transform = abs(skewness) > 1.0  # ì™œë„ ì ˆëŒ“ê°’ì´ 1 ì´ìƒ
            
            distribution_info[col] = {
                'skewness': skewness,
                'kurtosis': kurtosis,
                'shapiro_p': shapiro_p,
                'needs_transform': needs_transform
            }
            
            print(f"ğŸ“ˆ {col}:")
            print(f"   ì™œë„: {skewness:.3f} {'(ë³€í™˜ í•„ìš”)' if abs(skewness) > 1 else '(ì–‘í˜¸)'}")
            print(f"   ì²¨ë„: {kurtosis:.3f}")
            print(f"   ì •ê·œì„± ê²€ì • p-ê°’: {shapiro_p:.3e}")
            print(f"   ğŸ’¡ ë³€í™˜ ê¶Œì¥: {'ì˜ˆ' if needs_transform else 'ì•„ë‹ˆì˜¤'}")
            print()
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('ğŸ“Š ë³€ìˆ˜ë³„ ë¶„í¬ ë¶„ì„', fontsize=16, fontweight='bold')
    
    for i, col in enumerate(columns[:6]):
        if i >= 6:
            break
            
        row = i // 3
        col_idx = i % 3
        
        if col in df.columns:
            data = df[col].dropna()
            
            # íˆìŠ¤í† ê·¸ë¨ + ì •ê·œë¶„í¬ ê³¡ì„ 
            axes[row, col_idx].hist(data, bins=50, density=True, alpha=0.7, color='skyblue')
            
            # ì •ê·œë¶„í¬ ê³¡ì„  ì¶”ê°€
            mu, sigma = data.mean(), data.std()
            x = np.linspace(data.min(), data.max(), 100)
            normal_curve = stats.norm.pdf(x, mu, sigma)
            axes[row, col_idx].plot(x, normal_curve, 'r-', linewidth=2, label='ì •ê·œë¶„í¬')
            
            # ì™œë„ ì •ë³´ ì¶”ê°€
            skew_val = distribution_info[col]['skewness']
            axes[row, col_idx].set_title(f'{col}\nì™œë„: {skew_val:.2f}')
            axes[row, col_idx].legend()
            axes[row, col_idx].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return distribution_info

# ë¶„í¬ ë¶„ì„ ì‹¤í–‰
distribution_analysis = analyze_distribution(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `stats.skew()`: ì™œë„ ê³„ì‚° (ì–‘ìˆ˜ë©´ ì˜¤ë¥¸ìª½ ê¼¬ë¦¬, ìŒìˆ˜ë©´ ì™¼ìª½ ê¼¬ë¦¬)
- `stats.shapiro()`: ì†Œí‘œë³¸ìš© ì •ê·œì„± ê²€ì • (p > 0.05ì´ë©´ ì •ê·œë¶„í¬)
- `stats.norm.pdf()`: ì •ê·œë¶„í¬ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¡œ ì´ë¡ ì  ê³¡ì„  ê·¸ë¦¬ê¸°

### ë°©ë²• 1: ë¡œê·¸ ë³€í™˜ (Log Transformation)

ê°€ì¥ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë³€í™˜ ë°©ë²•ìœ¼ë¡œ, ì–‘ì˜ ì™œë„ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ë¡œê·¸ ë³€í™˜ êµ¬í˜„
def apply_log_transformation(df, columns=None):
    """
    ë¡œê·¸ ë³€í™˜ì„ ì ìš©í•˜ê³  íš¨ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        # ì–‘ì˜ ì™œë„ê°€ í° ë³€ìˆ˜ë“¤ ìë™ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if 'SalePrice' in numeric_cols:
            numeric_cols.remove('SalePrice')
        
        columns = []
        for col in numeric_cols:
            if col in df.columns:
                data = df[col].dropna()
                if (data > 0).all() and stats.skew(data) > 1:  # ì–‘ìˆ˜ì´ë©´ì„œ ì™œë„ > 1
                    columns.append(col)
        
        columns = columns[:4]  # ìµœëŒ€ 4ê°œ
    
    if not columns:
        print("âš ï¸ ë¡œê·¸ ë³€í™˜ì´ ì í•©í•œ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df, {}
    
    print(f"ğŸ“Š ë¡œê·¸ ë³€í™˜ ì ìš© ({len(columns)}ê°œ ë³€ìˆ˜):")
    
    transformed_df = df.copy()
    transformation_info = {}
    
    # ë³€í™˜ ì „í›„ ë¹„êµ
    fig, axes = plt.subplots(len(columns), 3, figsize=(15, 4*len(columns)))
    if len(columns) == 1:
        axes = axes.reshape(1, -1)
    
    for i, col in enumerate(columns):
        if col in df.columns:
            original_data = df[col].dropna()
            
            # 0ì´ë‚˜ ìŒìˆ˜ ì²˜ë¦¬
            if (original_data <= 0).any():
                # log1p ì‚¬ìš© (log(1+x))
                transformed_data = np.log1p(original_data)
                transform_type = "log1p"
            else:
                # ìì—°ë¡œê·¸ ì‚¬ìš©
                transformed_data = np.log(original_data)
                transform_type = "log"
            
            # ë³€í™˜ ì ìš©
            transformed_df[col + '_log'] = np.nan
            valid_indices = original_data.index
            if transform_type == "log1p":
                transformed_df.loc[valid_indices, col + '_log'] = np.log1p(df.loc[valid_indices, col])
            else:
                transformed_df.loc[valid_indices, col + '_log'] = np.log(df.loc[valid_indices, col])
            
            # í†µê³„ ê³„ì‚°
            original_skew = stats.skew(original_data)
            transformed_skew = stats.skew(transformed_data)
            
            transformation_info[col] = {
                'transform_type': transform_type,
                'original_skew': original_skew,
                'transformed_skew': transformed_skew,
                'improvement': original_skew - transformed_skew
            }
            
            print(f"\nğŸ“ˆ {col}:")
            print(f"   ë³€í™˜ ë°©ë²•: {transform_type}")
            print(f"   ì™œë„ ë³€í™”: {original_skew:.3f} â†’ {transformed_skew:.3f}")
            print(f"   ê°œì„ ë„: {original_skew - transformed_skew:.3f}")
            
            # ì‹œê°í™”
            # ì›ë³¸ ë¶„í¬
            axes[i, 0].hist(original_data, bins=50, alpha=0.7, color='skyblue')
            axes[i, 0].set_title(f'{col} ì›ë³¸\nì™œë„: {original_skew:.2f}')
            axes[i, 0].grid(True, alpha=0.3)
            
            # ë³€í™˜ í›„ ë¶„í¬
            axes[i, 1].hist(transformed_data, bins=50, alpha=0.7, color='lightcoral')
            axes[i, 1].set_title(f'{col} {transform_type} ë³€í™˜\nì™œë„: {transformed_skew:.2f}')
            axes[i, 1].grid(True, alpha=0.3)
            
            # Q-Q í”Œë¡¯ (ì •ê·œì„± í™•ì¸)
            stats.probplot(transformed_data, dist="norm", plot=axes[i, 2])
            axes[i, 2].set_title(f'{col} Q-Q í”Œë¡¯\n(ì§ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •ê·œë¶„í¬)')
            axes[i, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nâœ… ë¡œê·¸ ë³€í™˜ ì™„ë£Œ!")
    print(f"   ë³€í™˜ëœ ë³€ìˆ˜: {len(transformation_info)}ê°œ")
    print(f"   ğŸ’¡ ì™œë„ê°€ í¬ê²Œ ê°œì„ ë˜ì–´ ì •ê·œë¶„í¬ì— ê°€ê¹Œì›Œì¡ŒìŠµë‹ˆë‹¤!")
    
    return transformed_df, transformation_info

# ë¡œê·¸ ë³€í™˜ ì ìš©
log_transformed_data, log_info = apply_log_transformation(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `np.log1p()`: log(1+x) ë³€í™˜ìœ¼ë¡œ 0ê°’ í¬í•¨ ë°ì´í„°ì— ì•ˆì „í•˜ê²Œ ì ìš©
- `stats.probplot()`: Q-Q í”Œë¡¯ìœ¼ë¡œ ì •ê·œì„± ì‹œê°ì  í™•ì¸
- ë³€í™˜ ì „í›„ ì™œë„ ë¹„êµë¡œ ê°œì„  íš¨ê³¼ ì •ëŸ‰í™”

### ë°©ë²• 2: Box-Cox ë³€í™˜

ìµœì ì˜ ë³€í™˜ íŒŒë¼ë¯¸í„°(Î»)ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ê³ ê¸‰ ë³€í™˜ ë°©ë²•ì…ë‹ˆë‹¤.

```python
# Box-Cox ë³€í™˜ êµ¬í˜„
def apply_boxcox_transformation(df, columns=None):
    """
    Box-Cox ë³€í™˜ì„ ì ìš©í•˜ê³  ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        # ì–‘ìˆ˜ì¸ ë³€ìˆ˜ë“¤ ì¤‘ ì™œë„ê°€ í° ê²ƒ ì„ íƒ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if 'SalePrice' in numeric_cols:
            numeric_cols.remove('SalePrice')
        
        columns = []
        for col in numeric_cols[:6]:  # ìµœëŒ€ 6ê°œ
            if col in df.columns:
                data = df[col].dropna()
                if (data > 0).all() and abs(stats.skew(data)) > 0.5:  # ì–‘ìˆ˜ì´ë©´ì„œ ì™œë„ > 0.5
                    columns.append(col)
    
    if not columns:
        print("âš ï¸ Box-Cox ë³€í™˜ì´ ì í•©í•œ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return df, {}
    
    print(f"ğŸ“Š Box-Cox ë³€í™˜ ì ìš© ({len(columns)}ê°œ ë³€ìˆ˜):")
    
    transformed_df = df.copy()
    transformation_info = {}
    
    for col in columns:
        if col in df.columns:
            data = df[col].dropna()
            
            try:
                # Box-Cox ë³€í™˜ ì ìš© (ìµœì  Î» ìë™ ê³„ì‚°)
                transformed_data, optimal_lambda = boxcox(data)
                
                # ë³€í™˜ ì ìš©
                transformed_df[col + '_boxcox'] = np.nan
                valid_indices = data.index
                transformed_values, _ = boxcox(df.loc[valid_indices, col])
                transformed_df.loc[valid_indices, col + '_boxcox'] = transformed_values
                
                # í†µê³„ ê³„ì‚°
                original_skew = stats.skew(data)
                transformed_skew = stats.skew(transformed_data)
                
                transformation_info[col] = {
                    'lambda': optimal_lambda,
                    'original_skew': original_skew,
                    'transformed_skew': transformed_skew,
                    'improvement': abs(original_skew) - abs(transformed_skew)
                }
                
                print(f"\nğŸ“ˆ {col}:")
                print(f"   ìµœì  Î» (ëŒë‹¤): {optimal_lambda:.3f}")
                print(f"   ì™œë„ ë³€í™”: {original_skew:.3f} â†’ {transformed_skew:.3f}")
                print(f"   ê°œì„ ë„: {abs(original_skew) - abs(transformed_skew):.3f}")
                
                # ëŒë‹¤ ê°’ í•´ì„
                if abs(optimal_lambda) < 0.1:
                    interpretation = "ë¡œê·¸ ë³€í™˜ê³¼ ìœ ì‚¬"
                elif abs(optimal_lambda - 0.5) < 0.1:
                    interpretation = "ì œê³±ê·¼ ë³€í™˜ê³¼ ìœ ì‚¬"
                elif abs(optimal_lambda - 1.0) < 0.1:
                    interpretation = "ë³€í™˜ ë¶ˆí•„ìš”"
                elif abs(optimal_lambda - 2.0) < 0.1:
                    interpretation = "ì œê³± ë³€í™˜ê³¼ ìœ ì‚¬"
                else:
                    interpretation = f"Î»={optimal_lambda:.2f} ê±°ë“­ì œê³± ë³€í™˜"
                
                print(f"   í•´ì„: {interpretation}")
                
            except Exception as e:
                print(f"   âš ï¸ {col} ë³€í™˜ ì‹¤íŒ¨: {e}")
    
    # ë³€í™˜ íš¨ê³¼ ì‹œê°í™”
    if transformation_info:
        # ìµœì  Î» ë¶„í¬
        lambdas = [info['lambda'] for info in transformation_info.values()]
        improvements = [info['improvement'] for info in transformation_info.values()]
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # Î» ê°’ ë¶„í¬
        axes[0].bar(range(len(lambdas)), lambdas, color='skyblue', alpha=0.7)
        axes[0].set_title('Box-Cox ìµœì  Î» (ëŒë‹¤) ê°’')
        axes[0].set_xlabel('ë³€ìˆ˜')
        axes[0].set_ylabel('Î» ê°’')
        axes[0].set_xticks(range(len(columns)))
        axes[0].set_xticklabels(columns, rotation=45)
        axes[0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Î»=0 (ë¡œê·¸ë³€í™˜)')
        axes[0].axhline(y=1, color='green', linestyle='--', alpha=0.5, label='Î»=1 (ë³€í™˜ì—†ìŒ)')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # ê°œì„  íš¨ê³¼
        axes[1].bar(range(len(improvements)), improvements, color='lightcoral', alpha=0.7)
        axes[1].set_title('ì™œë„ ê°œì„  íš¨ê³¼')
        axes[1].set_xlabel('ë³€ìˆ˜')
        axes[1].set_ylabel('ì™œë„ ê°œì„  ì •ë„')
        axes[1].set_xticks(range(len(columns)))
        axes[1].set_xticklabels(columns, rotation=45)
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    print(f"\nâœ… Box-Cox ë³€í™˜ ì™„ë£Œ!")
    print(f"   ë³€í™˜ëœ ë³€ìˆ˜: {len(transformation_info)}ê°œ")
    print(f"   ğŸ’¡ ê° ë³€ìˆ˜ì— ìµœì í™”ëœ ë³€í™˜ìœ¼ë¡œ ìµœëŒ€ ì™œë„ ê°œì„  íš¨ê³¼!")
    
    return transformed_df, transformation_info

# Box-Cox ë³€í™˜ ì ìš©
boxcox_transformed_data, boxcox_info = apply_boxcox_transformation(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `boxcox()`: ìµœì ì˜ Î»(ëŒë‹¤) íŒŒë¼ë¯¸í„°ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ë³€í™˜ ìˆ˜í–‰
- Î»=0ì´ë©´ ë¡œê·¸ë³€í™˜, Î»=1ì´ë©´ ë³€í™˜ ë¶ˆí•„ìš”, Î»=0.5ë©´ ì œê³±ê·¼ ë³€í™˜ê³¼ ìœ ì‚¬
- ê° ë³€ìˆ˜ë³„ë¡œ ìµœì í™”ëœ ë³€í™˜ìœ¼ë¡œ ë” ë‚˜ì€ ê²°ê³¼ ê¸°ëŒ€

---

## ğŸ“– 4.2.4 scikit-learn Pipelineì„ í™œìš©í•œ í†µí•© ì „ì²˜ë¦¬

ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ê¸°ë²•ë“¤ì„ **ì²´ê³„ì ìœ¼ë¡œ ê²°í•©**í•˜ì—¬ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
def create_preprocessing_pipeline(df):
    """
    House Prices ë°ì´í„°ì— íŠ¹í™”ëœ ì¢…í•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    """
    print("ğŸ”§ House Prices ë§ì¶¤í˜• ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•:")
    
    # 1. ë³€ìˆ˜ ìœ í˜•ë³„ ë¶„ë¥˜
    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'SalePrice' in numeric_features:
        numeric_features.remove('SalePrice')  # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸
    
    categorical_features = df.select_dtypes(include=['object']).columns.tolist()
    
    # ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì¹´ë””ë„ë¦¬í‹°ì— ë”°ë¼ ì„¸ë¶„í™”
    low_cardinality_features = [col for col in categorical_features 
                               if df[col].nunique() <= 10]
    high_cardinality_features = [col for col in categorical_features 
                                if df[col].nunique() > 10]
    
    # ìˆœì„œí˜• ë³€ìˆ˜ (í’ˆì§ˆ ê´€ë ¨)
    ordinal_features = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',
                       'HeatingQC', 'KitchenQual', 'FireplaceQu']
    ordinal_features = [col for col in ordinal_features if col in df.columns]
    
    # ìˆœì„œí˜• ë³€ìˆ˜ë¥¼ ë‹¤ë¥¸ ë²”ì£¼í˜•ì—ì„œ ì œê±°
    low_cardinality_features = [col for col in low_cardinality_features 
                               if col not in ordinal_features]
    high_cardinality_features = [col for col in high_cardinality_features 
                                if col not in ordinal_features]
    
    print(f"   ğŸ“Š ë³€ìˆ˜ ë¶„ë¥˜:")
    print(f"      ìˆ˜ì¹˜í˜•: {len(numeric_features)}ê°œ")
    print(f"      ë‚®ì€ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜•: {len(low_cardinality_features)}ê°œ")
    print(f"      ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜•: {len(high_cardinality_features)}ê°œ")
    print(f"      ìˆœì„œí˜•: {len(ordinal_features)}ê°œ")
    
    # 2. ê° ë³€ìˆ˜ ìœ í˜•ë³„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
    
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ íŒŒì´í”„ë¼ì¸
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),  # ê²°ì¸¡ì¹˜ ì¤‘ì•™ê°’ ëŒ€ì²´
        ('scaler', RobustScaler())  # ì´ìƒì¹˜ì— ê°•ê±´í•œ ìŠ¤ì¼€ì¼ë§
    ])
    
    # ë‚®ì€ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜• ë³€ìˆ˜ íŒŒì´í”„ë¼ì¸
    low_cardinality_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
    ])
    
    # ë†’ì€ ì¹´ë””ë„ë¦¬í‹° ë²”ì£¼í˜• ë³€ìˆ˜ íŒŒì´í”„ë¼ì¸ (ë‹¨ìˆœí™”)
    high_cardinality_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('label', LabelEncoder())  # ì‹¤ì œë¡œëŠ” íƒ€ê²Ÿ ì¸ì½”ë”©ì´ ë” ì¢‹ì§€ë§Œ ë‹¨ìˆœí™”
    ])
    
    # ìˆœì„œí˜• ë³€ìˆ˜ íŒŒì´í”„ë¼ì¸
    ordinal_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
        ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))
    ])
    
    # 3. ColumnTransformerë¡œ í†µí•©
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('low_cat', low_cardinality_transformer, low_cardinality_features),
            ('high_cat', high_cardinality_transformer, high_cardinality_features),
            ('ord', ordinal_transformer, ordinal_features)
        ]
    )
    
    print(f"\nğŸ”§ íŒŒì´í”„ë¼ì¸ êµ¬ì„±:")
    print(f"   1ï¸âƒ£ ìˆ˜ì¹˜í˜•: ì¤‘ì•™ê°’ ëŒ€ì²´ â†’ ë¡œë²„ìŠ¤íŠ¸ ìŠ¤ì¼€ì¼ë§")
    print(f"   2ï¸âƒ£ ë‚®ì€ ì¹´ë””ë„ë¦¬í‹°: Unknown ëŒ€ì²´ â†’ ì›-í•« ì¸ì½”ë”©")
    print(f"   3ï¸âƒ£ ë†’ì€ ì¹´ë””ë„ë¦¬í‹°: ìµœë¹ˆê°’ ëŒ€ì²´ â†’ ë ˆì´ë¸” ì¸ì½”ë”©")
    print(f"   4ï¸âƒ£ ìˆœì„œí˜•: Unknown ëŒ€ì²´ â†’ ìˆœì„œí˜• ì¸ì½”ë”©")
    
    return preprocessor, {
        'numeric_features': numeric_features,
        'low_cardinality_features': low_cardinality_features,
        'high_cardinality_features': high_cardinality_features,
        'ordinal_features': ordinal_features
    }

# íŒŒì´í”„ë¼ì¸ ìƒì„±
preprocessor, feature_info = create_preprocessing_pipeline(train_data)

# íŒŒì´í”„ë¼ì¸ ì ìš© ë° ê²°ê³¼ í™•ì¸
def apply_and_evaluate_pipeline(df, preprocessor, feature_info):
    """
    íŒŒì´í”„ë¼ì¸ì„ ì ìš©í•˜ê³  ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” í•¨ìˆ˜
    """
    print("\nğŸš€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©:")
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ë¦¬
    if 'SalePrice' in df.columns:
        X = df.drop('SalePrice', axis=1)
        y = df['SalePrice']
    else:
        X = df
        y = None
    
    original_shape = X.shape
    
    # íŒŒì´í”„ë¼ì¸ ì ìš©
    try:
        X_transformed = preprocessor.fit_transform(X)
        
        print(f"   âœ… ë³€í™˜ ì„±ê³µ!")
        print(f"   ğŸ“Š ë°ì´í„° í¬ê¸° ë³€í™”: {original_shape} â†’ {X_transformed.shape}")
        print(f"   ğŸ“ˆ íŠ¹ì„± ê°œìˆ˜ ë³€í™”: {original_shape[1]} â†’ {X_transformed.shape[1]}")
        
        # ê²°ì¸¡ì¹˜ í™•ì¸
        if hasattr(X_transformed, 'isnan'):
            missing_after = np.isnan(X_transformed).sum()
        else:
            missing_after = pd.DataFrame(X_transformed).isnull().sum().sum()
        
        print(f"   ğŸ” ë³€í™˜ í›„ ê²°ì¸¡ì¹˜: {missing_after}ê°œ")
        
        # ë³€í™˜ëœ ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„
        X_transformed_df = pd.DataFrame(X_transformed)
        print(f"\nğŸ“Š ë³€í™˜ëœ ë°ì´í„° í†µê³„:")
        print(f"   í‰ê·  ë²”ìœ„: [{X_transformed_df.mean().min():.3f}, {X_transformed_df.mean().max():.3f}]")
        print(f"   í‘œì¤€í¸ì°¨ ë²”ìœ„: [{X_transformed_df.std().min():.3f}, {X_transformed_df.std().max():.3f}]")
        
        return X_transformed, y
        
    except Exception as e:
        print(f"   âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None, y

# íŒŒì´í”„ë¼ì¸ ì ìš©
X_processed, y = apply_and_evaluate_pipeline(train_data, preprocessor, feature_info)

print(f"\nâœ… í†µí•© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ì„±!")
print(f"ğŸ’¡ ì´ì œ ì´ íŒŒì´í”„ë¼ì¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ì—ë„ ë™ì¼í•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `ColumnTransformer`: ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ ìœ í˜•ì— ë‹¤ë¥¸ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì ìš©
- `Pipeline`: ì—¬ëŸ¬ ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°
- `fit_transform()`: íŒŒì´í”„ë¼ì¸ í•™ìŠµê³¼ ì ìš©ì„ ë™ì‹œì— ìˆ˜í–‰

---

## ğŸ¯ ì§ì ‘ í•´ë³´ê¸° - ì—°ìŠµ ë¬¸ì œ

### ì—°ìŠµ ë¬¸ì œ 1: ìŠ¤ì¼€ì¼ë§ ë°©ë²• ë¹„êµ â­â­
ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ìŠ¤ì¼€ì¼ë§ ë°©ë²•ë“¤ì˜ íš¨ê³¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”.

```python
# ì—°ìŠµ ë¬¸ì œ 1: ìŠ¤ì¼€ì¼ë§ ë°©ë²• ë¹„êµ
def exercise_compare_scaling(df):
    """
    ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ë§ ë°©ë²•ì„ ë¹„êµí•˜ê³  ìµœì  ë°©ë²•ì„ ì°¾ëŠ” í•¨ìˆ˜
    """
    from sklearn.metrics import mean_squared_error
    from sklearn.linear_model import LinearRegression
    from sklearn.model_selection import train_test_split
    
    # TODO: ë‹¤ìŒ ë‹¨ê³„ë¥¼ êµ¬í˜„í•˜ì„¸ìš”
    # 1. GrLivArea ë³€ìˆ˜ì— 3ê°€ì§€ ìŠ¤ì¼€ì¼ë§ ë°©ë²• ì ìš©
    # 2. ê°ê°ìœ¼ë¡œ SalePrice ì˜ˆì¸¡í•˜ëŠ” ê°„ë‹¨í•œ ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
    # 3. ì„±ëŠ¥(RMSE) ë¹„êµí•˜ì—¬ ìµœì  ë°©ë²• ì„ íƒ
    
    scalers = {
        'Standard': StandardScaler(),
        'MinMax': MinMaxScaler(),
        'Robust': RobustScaler()
    }
    
    results = {}
    
    # ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
    
    return results

# íŒíŠ¸: fit_transform(), train_test_split(), mean_squared_error() í™œìš©
```

### ì—°ìŠµ ë¬¸ì œ 2: ë§ì¶¤í˜• ì¸ì½”ë”© ì „ëµ â­â­â­
ë¶€ë™ì‚° ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì¸ì½”ë”© ì „ëµì„ ìˆ˜ë¦½í•´ë³´ì„¸ìš”.

```python
# ì—°ìŠµ ë¬¸ì œ 2: ë§ì¶¤í˜• ì¸ì½”ë”© ì „ëµ
def exercise_custom_encoding(df):
    """
    ë³€ìˆ˜ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ì¸ì½”ë”© ì „ëµ ìˆ˜ë¦½
    """
    # TODO: ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ì¸ì½”ë”© ë°©ë²•ì„ ê²°ì •í•˜ê³  ì ìš©í•˜ì„¸ìš”
    # 1. Neighborhood: ì¹´ë””ë„ë¦¬í‹°ê°€ ë†’ìœ¼ë¯€ë¡œ íƒ€ê²Ÿ ì¸ì½”ë”©
    # 2. MSSubClass: ëª…ëª©í˜•ì´ì§€ë§Œ ìˆ«ìë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì›-í•« ì¸ì½”ë”©
    # 3. GarageType: ì¹´ë””ë„ë¦¬í‹°ê°€ ë‚®ìœ¼ë¯€ë¡œ ì›-í•« ì¸ì½”ë”©
    
    encoding_strategy = {}
    
    # ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
    
    return encoding_strategy
```

### ì—°ìŠµ ë¬¸ì œ 3: ë¶„í¬ ë³€í™˜ íš¨ê³¼ ë¶„ì„ â­â­â­â­
SalePriceì˜ ë¶„í¬ë¥¼ ê°œì„ í•˜ëŠ” ìµœì  ë³€í™˜ ë°©ë²•ì„ ì°¾ì•„ë³´ì„¸ìš”.

```python
# ì—°ìŠµ ë¬¸ì œ 3: ë¶„í¬ ë³€í™˜ íš¨ê³¼ ë¶„ì„
def exercise_transform_target(df):
    """
    íƒ€ê²Ÿ ë³€ìˆ˜(SalePrice)ì˜ ë¶„í¬ë¥¼ ê°œì„ í•˜ëŠ” ìµœì  ë³€í™˜ ì°¾ê¸°
    """
    # TODO: ë‹¤ìŒ ë³€í™˜ë“¤ì„ ì‹œë„í•˜ê³  íš¨ê³¼ë¥¼ ë¹„êµí•˜ì„¸ìš”
    # 1. ë¡œê·¸ ë³€í™˜: log(SalePrice)
    # 2. ì œê³±ê·¼ ë³€í™˜: sqrt(SalePrice)
    # 3. Box-Cox ë³€í™˜: ìµœì  Î» ì°¾ê¸°
    # 4. ë³€í™˜ ì „í›„ ì™œë„, ì •ê·œì„± ê²€ì • ê²°ê³¼ ë¹„êµ
    
    transformations = {}
    
    # ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
    
    return transformations
```

---

## ğŸ“š í•µì‹¬ ì •ë¦¬

ì´ë²ˆ Partì—ì„œ ë°°ìš´ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### âœ… ìŠ¤ì¼€ì¼ë§ ê¸°ë²• í•µì‹¬ í¬ì¸íŠ¸

1. **StandardScaler**: í‰ê·  0, í‘œì¤€í¸ì°¨ 1 â†’ ì •ê·œë¶„í¬ ê°€ì • ì•Œê³ ë¦¬ì¦˜ì— ìµœì 
2. **MinMaxScaler**: 0~1 ë²”ìœ„ â†’ ëª…í™•í•œ ìµœì†Ÿê°’/ìµœëŒ“ê°’ì´ ì˜ë¯¸ìˆëŠ” ê²½ìš°
3. **RobustScaler**: ì¤‘ì•™ê°’ê³¼ IQR ì‚¬ìš© â†’ ì´ìƒì¹˜ê°€ ë§ì€ ì‹¤ì œ ë°ì´í„°ì— ì•ˆì •ì 

### âœ… ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© í•µì‹¬ í¬ì¸íŠ¸

1. **ì›-í•« ì¸ì½”ë”©**: ëª…ëª©í˜• + ë‚®ì€ ì¹´ë””ë„ë¦¬í‹° â†’ ì•ˆì „í•˜ê³  í•´ì„í•˜ê¸° ì‰¬ì›€
2. **ìˆœì„œí˜• ì¸ì½”ë”©**: ìˆœì„œê°€ ìˆëŠ” ë³€ìˆ˜ â†’ ìˆœì„œ ì •ë³´ ë³´ì¡´í•˜ë©´ì„œ íš¨ìœ¨ì 
3. **íƒ€ê²Ÿ ì¸ì½”ë”©**: ë†’ì€ ì¹´ë””ë„ë¦¬í‹° â†’ ì˜ˆì¸¡ë ¥ ë†’ì§€ë§Œ ê³¼ì í•© ì£¼ì˜

### âœ… ë¶„í¬ ë³€í™˜ í•µì‹¬ í¬ì¸íŠ¸

1. **ë¡œê·¸ ë³€í™˜**: ì–‘ì˜ ì™œë„ ê°œì„  â†’ ê°„ë‹¨í•˜ê³  í•´ì„í•˜ê¸° ì‰¬ì›€
2. **Box-Cox ë³€í™˜**: ìµœì  íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰ â†’ ë” ì •êµí•œ ë³€í™˜
3. **ì •ê·œì„± ê°œì„ **: ë§ì€ ML ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ í–¥ìƒ

### ğŸ’¡ ì‹¤ë¬´ ì ìš© íŒ

- **íŒŒì´í”„ë¼ì¸ í™œìš©**: ì²´ê³„ì ì´ê³  ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•
- **ë³€í™˜ ìˆœì„œ**: ê²°ì¸¡ì¹˜ ì²˜ë¦¬ â†’ ì¸ì½”ë”© â†’ ìŠ¤ì¼€ì¼ë§ â†’ ë¶„í¬ ë³€í™˜
- **ê²€ì¦**: ë³€í™˜ ì „í›„ ì„±ëŠ¥ ë¹„êµë¡œ íš¨ê³¼ í™•ì¸
- **ë¬¸ì„œí™”**: ì„ íƒí•œ ë°©ë²•ê³¼ ì´ìœ ë¥¼ ëª…í™•íˆ ê¸°ë¡

---

## ğŸ¤” ìƒê°í•´ë³´ê¸°

1. **ìŠ¤ì¼€ì¼ë§ì˜ í•„ìš”ì„±**: ì™œ ë‚˜ì´(20-80)ì™€ ì—°ë´‰(2000-8000ë§Œì›) ê°™ì€ ë³€ìˆ˜ë¥¼ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”í• ê¹Œìš”? êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™©ì„ ìƒê°í•´ë³´ì„¸ìš”.

2. **ì¸ì½”ë”© ë°©ë²•ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„**: ì›-í•« ì¸ì½”ë”©ì€ ì•ˆì „í•˜ì§€ë§Œ ì°¨ì›ì´ í­ë°œí•  ìˆ˜ ìˆê³ , íƒ€ê²Ÿ ì¸ì½”ë”©ì€ íš¨ê³¼ì ì´ì§€ë§Œ ê³¼ì í•© ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒí•´ì•¼ í• ê¹Œìš”?

3. **ë³€í™˜ì˜ í•´ì„ì„±**: ë¡œê·¸ ë³€í™˜ëœ ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ í•´ì„í•  ìˆ˜ ìˆì„ê¹Œìš”? ë¹„ì¦ˆë‹ˆìŠ¤ ë‹´ë‹¹ìì—ê²Œ ì–´ë–»ê²Œ ì„¤ëª…í•˜ì‹œê² ìŠµë‹ˆê¹Œ?

---

## ğŸ”œ ë‹¤ìŒ Part ì˜ˆê³ : íŠ¹ì„± ê³µí•™(Feature Engineering)

ë‹¤ìŒ Partì—ì„œëŠ” ì›ë³¸ ë°ì´í„°ë¡œë¶€í„° **ìƒˆë¡œìš´ ì˜ë¯¸ìˆëŠ” íŠ¹ì„±**ì„ ìƒì„±í•˜ëŠ” ê³ ê¸‰ ê¸°ë²•ë“¤ì„ ë°°ì›ë‹ˆë‹¤:

- **ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± ìƒì„±**: ì£¼íƒ ë°ì´í„°ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ í™œìš©í•œ íŒŒìƒ ë³€ìˆ˜
- **ìˆ˜í•™ì  íŠ¹ì„± ì¡°í•©**: ë¹„ìœ¨, ì°¨ì´, ìƒí˜¸ì‘ìš© ë“± ìˆ˜ì¹˜ì  íŠ¹ì„± ê²°í•©
- **ì‹œê°„ ê¸°ë°˜ íŠ¹ì„±**: ì—°ë„, ê³„ì ˆì„± ë“± ì‹œê°„ ì •ë³´ í™œìš©
- **íŠ¹ì„± ì„ íƒ**: ì¤‘ìš”í•œ íŠ¹ì„±ë§Œ ê³¨ë¼ë‚´ëŠ” ì²´ê³„ì  ë°©ë²•
- **ìë™ íŠ¹ì„± ê³µí•™**: AI ë„êµ¬ë¥¼ í™œìš©í•œ íŠ¹ì„± ìƒì„±ê³¼ ê²€ì¦

ë°ì´í„°ì—ì„œ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  ëª¨ë¸ì˜ ì˜ˆì¸¡ë ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì°½ì˜ì ì¸ íŠ¹ì„± ê³µí•™ ê¸°ë²•ë“¤ì„ ë§ˆìŠ¤í„°í•´ë³´ê² ìŠµë‹ˆë‹¤!

---

*"ë°ì´í„° ë³€í™˜ì€ ì•Œê³ ë¦¬ì¦˜ê³¼ ë°ì´í„° ì‚¬ì´ì˜ 'ë²ˆì—­' ì‘ì—…ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ë³€í™˜ì€ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ë“œëŸ¬ë‚´ê³  ëª¨ë¸ì˜ í•™ìŠµì„ ë„ì™€ì¤ë‹ˆë‹¤."*
```
```

