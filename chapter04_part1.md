# 4ì¥ Part 1: ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ ì²˜ë¦¬
## ì‹¤ì œ ë°ì´í„°ì˜ ë¶ˆì™„ì „ì„±ì„ í•´ê²°í•˜ëŠ” ì „ë¬¸ê°€ ê¸°ë²•

---

## ğŸ“š í•™ìŠµ ëª©í‘œ

ì´ë²ˆ Partì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤:

âœ… **ê²°ì¸¡ì¹˜ì˜ ì¢…ë¥˜ì™€ ë°œìƒ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  ì ì ˆí•œ ì²˜ë¦¬ ë°©ë²•ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤**
âœ… **ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì—ì„œ ì ì ˆíˆ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤**  
âœ… **House Prices ë°ì´í„°ì…‹ì„ í™œìš©í•œ ì‹¤ì „ ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ ì²˜ë¦¬ ê²½í—˜ì„ ìŒ“ëŠ”ë‹¤**
âœ… **Pandasì™€ scikit-learnì„ í™œìš©í•œ ê³ ê¸‰ ì²˜ë¦¬ ê¸°ë²•ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤**
âœ… **ì²˜ë¦¬ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ì„ ë¶„ì„í•  ìˆ˜ ìˆë‹¤**

---

## ğŸ¯ ì´ë²ˆ Part ë¯¸ë¦¬ë³´ê¸°

ì‹¤ì œ ë°ì´í„° ë¶„ì„ í”„ë¡œì íŠ¸ì—ì„œ ê°€ì¥ ë¨¼ì € ë§ˆì£¼í•˜ê²Œ ë˜ëŠ” ë„ì „ì€ ë°”ë¡œ **ë¶ˆì™„ì „í•œ ë°ì´í„°**ì…ë‹ˆë‹¤. ì„¤ë¬¸ì¡°ì‚¬ì—ì„œ ì‘ë‹µí•˜ì§€ ì•Šì€ ë¬¸í•­, ì„¼ì„œ ì˜¤ë¥˜ë¡œ ëˆ„ë½ëœ ì¸¡ì •ê°’, ì…ë ¥ ì‹¤ìˆ˜ë¡œ ë°œìƒí•œ ê·¹ë‹¨ê°’ë“¤... ì´ëŸ° ë¬¸ì œë“¤ì€ ë°ì´í„° ë¶„ì„ê°€ë¼ë©´ ë°˜ë“œì‹œ í•´ê²°í•´ì•¼ í•  í˜„ì‹¤ì ì¸ ê³¼ì œì…ë‹ˆë‹¤.

ì´ë²ˆ Partì—ì„œëŠ” Kaggleì˜ **House Prices Dataset**ì„ í™œìš©í•˜ì—¬ ì‹¤ì œ ë¶€ë™ì‚° ë°ì´í„°ì—ì„œ ë°œìƒí•˜ëŠ” ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ê²°ì¸¡ì¹˜ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ í‰ê· ê°’ìœ¼ë¡œ ì±„ìš°ëŠ” ìˆ˜ì¤€ì„ ë„˜ì–´ì„œ, **ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì„ ê³ ë ¤í•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ì²˜ë¦¬ ê¸°ë²•**ì„ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤.

íŠ¹íˆ ì£¼íƒ ë°ì´í„°ì˜ íŠ¹ì„±ìƒ "ì§€í•˜ì‹¤ì´ ì—†ìœ¼ë©´ ì§€í•˜ì‹¤ ë©´ì ì´ ê²°ì¸¡"ë˜ëŠ” ê²ƒì²˜ëŸ¼ **êµ¬ì¡°ì ì¸ ê²°ì¸¡ì¹˜ì˜ ì˜ë¯¸**ë¥¼ íŒŒì•…í•˜ê³ , ì£¼íƒ ê°€ê²©ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” **ì´ìƒì¹˜ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  í•´ì„**ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ìµíˆê²Œ ë©ë‹ˆë‹¤.

> **ğŸ’¡ Part 1ì˜ í•µì‹¬ í¬ì¸íŠ¸**  
> "ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ëŠ” ë‹¨ìˆœí•œ 'ë¬¸ì œ'ê°€ ì•„ë‹ˆë¼ ë°ì´í„°ê°€ ì „í•˜ëŠ” 'ì •ë³´'ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ í•´ì„í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ë°ì´í„° ê³¼í•™ìì˜ í•µì‹¬ ì—­ëŸ‰ì…ë‹ˆë‹¤."

---

## ğŸ“– 4.1.1 ê²°ì¸¡ì¹˜ì˜ ì´í•´ì™€ ë¶„ë¥˜

### ê²°ì¸¡ì¹˜ë€ ë¬´ì—‡ì¸ê°€?

**ê²°ì¸¡ì¹˜(Missing Value)**ëŠ” ë°ì´í„°ì…‹ì—ì„œ ê°’ì´ ëˆ„ë½ëœ ê´€ì¸¡ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Pythonì˜ pandasì—ì„œëŠ” ì£¼ë¡œ `NaN`(Not a Number), `None`, ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **NaN (Not a Number)**: ìˆ«ìí˜• ë°ì´í„°ì—ì„œ ì •ì˜ë˜ì§€ ì•Šì€ ê°’
> - **None**: Pythonì˜ null ê°ì²´ë¡œ, ê°’ì´ ì—†ìŒì„ ë‚˜íƒ€ëƒ„
> - **ê²°ì¸¡ë¥ (Missing Rate)**: ì „ì²´ ë°ì´í„° ì¤‘ ê²°ì¸¡ì¹˜ê°€ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨

ê²°ì¸¡ì¹˜ê°€ ë°œìƒí•˜ëŠ” ì´ìœ ëŠ” ë‹¤ì–‘í•©ë‹ˆë‹¤:

1. **ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì˜ ì˜¤ë¥˜**: ì„¼ì„œ ê³ ì¥, ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë“±
2. **ì‘ë‹µìì˜ ì˜ë„ì /ë¹„ì˜ë„ì  ëˆ„ë½**: ì„¤ë¬¸ì¡°ì‚¬ì—ì„œ ë¯¼ê°í•œ ì§ˆë¬¸ íšŒí”¼
3. **ì‹œìŠ¤í…œ ì„¤ê³„ìƒì˜ í•œê³„**: íŠ¹ì • ì¡°ê±´ì—ì„œë§Œ ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„°
4. **ë°ì´í„° ì „ì†¡/ì €ì¥ ê³¼ì •ì˜ ì†ì‹¤**: íŒŒì¼ ì „ì†¡ ì˜¤ë¥˜, ì €ì¥ ê³µê°„ ë¶€ì¡±

### ê²°ì¸¡ì¹˜ì˜ 3ê°€ì§€ ìœ í˜• (Missing Data Mechanisms)

í†µê³„í•™ì—ì„œëŠ” ê²°ì¸¡ì¹˜ë¥¼ ë°œìƒ ë©”ì»¤ë‹ˆì¦˜ì— ë”°ë¼ 3ê°€ì§€ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤:

#### 1. MCAR (Missing Completely At Random)
- **ì™„ì „ ë¬´ì‘ìœ„ ê²°ì¸¡**: ê²°ì¸¡ ì—¬ë¶€ê°€ ë‹¤ë¥¸ ë³€ìˆ˜ì™€ ì „í˜€ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°
- **ì˜ˆì‹œ**: ì„¤ë¬¸ ì¡°ì‚¬ ì¤‘ ì»´í“¨í„° ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ëœë¤í•˜ê²Œ ì¼ë¶€ ì‘ë‹µì´ ëˆ„ë½
- **ì²˜ë¦¬**: ë‹¨ìˆœ ì‚­ì œë‚˜ í‰ê· ê°’ ëŒ€ì²´ê°€ ë¹„êµì  ì•ˆì „

#### 2. MAR (Missing At Random)  
- **ë¬´ì‘ìœ„ ê²°ì¸¡**: ê²°ì¸¡ ì—¬ë¶€ê°€ ê´€ì¸¡ëœ ë‹¤ë¥¸ ë³€ìˆ˜ì™€ëŠ” ê´€ë ¨ì´ ìˆì§€ë§Œ, í•´ë‹¹ ë³€ìˆ˜ ìì²´ì˜ ê°’ê³¼ëŠ” ë¬´ê´€
- **ì˜ˆì‹œ**: ë‚˜ì´ê°€ ë§ì„ìˆ˜ë¡ ì†Œë“ ì •ë³´ ì œê³µì„ êº¼ë ¤í•˜ëŠ” ê²½ìš° (ë‚˜ì´ëŠ” ê´€ì¸¡ë¨)
- **ì²˜ë¦¬**: ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ í™œìš©í•œ ì˜ˆì¸¡ ê¸°ë°˜ ëŒ€ì²´ ë°©ë²• í™œìš©

#### 3. MNAR (Missing Not At Random)
- **ë¹„ë¬´ì‘ìœ„ ê²°ì¸¡**: ê²°ì¸¡ ì—¬ë¶€ê°€ í•´ë‹¹ ë³€ìˆ˜ ìì²´ì˜ ê°’ê³¼ ê´€ë ¨ì´ ìˆëŠ” ê²½ìš°  
- **ì˜ˆì‹œ**: ì†Œë“ì´ ë§¤ìš° ë†’ê±°ë‚˜ ë‚®ì€ ì‚¬ëŒë“¤ì´ ì†Œë“ ì •ë³´ ì œê³µì„ ê±°ë¶€
- **ì²˜ë¦¬**: ê°€ì¥ ë³µì¡í•˜ë©°, ë„ë©”ì¸ ì§€ì‹ê³¼ ê³ ê¸‰ í†µê³„ ê¸°ë²• í•„ìš”

### House Prices ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ íƒìƒ‰

ì´ì œ ì‹¤ì œ ë°ì´í„°ë¥¼ í†µí•´ ê²°ì¸¡ì¹˜ì˜ íŒ¨í„´ì„ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì • (ì´ë¯¸ì§€ ìƒì„± ì‹œ í•œê¸€ í‘œì‹œë¥¼ ìœ„í•¨)
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# House Prices ë°ì´í„° ë¡œë“œ
# ì£¼ì˜: ë°ì´í„° íŒŒì¼ ê²½ë¡œëŠ” ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ì¡°ì •í•˜ì„¸ìš”
try:
    train_data = pd.read_csv('datasets/house_prices/train.csv')
    print("âœ… ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
except FileNotFoundError:
    print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    print("ğŸ’¡ Kaggleì—ì„œ House Prices Datasetì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ datasets/house_prices/ í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")

# ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸
print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {train_data.shape}")
print(f"ğŸ  ì´ ì£¼íƒ ìˆ˜: {train_data.shape[0]:,}ê°œ")
print(f"ğŸ“ˆ ì´ ë³€ìˆ˜ ìˆ˜: {train_data.shape[1]:,}ê°œ")

# ë°ì´í„° íƒ€ì…ë³„ ë³€ìˆ˜ í˜„í™©
print("\nğŸ“‹ ë°ì´í„° íƒ€ì…ë³„ ë³€ìˆ˜ í˜„í™©:")
print(train_data.dtypes.value_counts())
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `pd.read_csv()`: CSV íŒŒì¼ì„ pandas DataFrameìœ¼ë¡œ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜
- `train_data.shape`: ë°ì´í„°ì˜ (í–‰ ìˆ˜, ì—´ ìˆ˜)ë¥¼ ë°˜í™˜í•˜ëŠ” ì†ì„±  
- `dtypes.value_counts()`: ê° ë°ì´í„° íƒ€ì…ë³„ ë³€ìˆ˜ ê°œìˆ˜ë¥¼ ê³„ì‚°

### ê²°ì¸¡ì¹˜ í˜„í™© ì¢…í•© ë¶„ì„

```python
# ê²°ì¸¡ì¹˜ í˜„í™© ë¶„ì„ í•¨ìˆ˜
def analyze_missing_data(df):
    """
    ë°ì´í„°í”„ë ˆì„ì˜ ê²°ì¸¡ì¹˜ í˜„í™©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
    
    Returns:
    pd.DataFrame: ê²°ì¸¡ì¹˜ ë¶„ì„ ê²°ê³¼
    """
    # ê²°ì¸¡ì¹˜ ê°œìˆ˜ì™€ ë¹„ìœ¨ ê³„ì‚°
    missing_count = df.isnull().sum()
    missing_percent = (missing_count / len(df)) * 100
    
    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
    missing_data = pd.DataFrame({
        'Column': missing_count.index,
        'Missing_Count': missing_count.values,
        'Missing_Percent': missing_percent.values,
        'Data_Type': df.dtypes.values
    })
    
    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    missing_data = missing_data[missing_data['Missing_Count'] > 0]
    
    # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    missing_data = missing_data.sort_values('Missing_Percent', ascending=False)
    
    return missing_data

# ê²°ì¸¡ì¹˜ ë¶„ì„ ì‹¤í–‰
missing_analysis = analyze_missing_data(train_data)
print("ğŸ” ê²°ì¸¡ì¹˜ í˜„í™© ë¶„ì„:")
print(missing_analysis.head(10))

# ê²°ì¸¡ì¹˜ ë¹„ìœ¨ë³„ ë¶„ë¥˜
high_missing = missing_analysis[missing_analysis['Missing_Percent'] > 50]
medium_missing = missing_analysis[(missing_analysis['Missing_Percent'] > 15) & 
                                 (missing_analysis['Missing_Percent'] <= 50)]
low_missing = missing_analysis[missing_analysis['Missing_Percent'] <= 15]

print(f"\nğŸ“Š ê²°ì¸¡ì¹˜ ìˆ˜ì¤€ë³„ ë¶„ë¥˜:")
print(f"ğŸ”´ ë†’ì€ ê²°ì¸¡ì¹˜ (50% ì´ˆê³¼): {len(high_missing)}ê°œ ì»¬ëŸ¼")
print(f"ğŸŸ¡ ì¤‘ê°„ ê²°ì¸¡ì¹˜ (15-50%): {len(medium_missing)}ê°œ ì»¬ëŸ¼") 
print(f"ğŸŸ¢ ë‚®ì€ ê²°ì¸¡ì¹˜ (15% ì´í•˜): {len(low_missing)}ê°œ ì»¬ëŸ¼")
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `df.isnull().sum()`: ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ë¥¼ ê³„ì‚°
- `(missing_count / len(df)) * 100`: ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì„ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
- `sort_values('Missing_Percent', ascending=False)`: ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬

ì´ ë¶„ì„ì„ í†µí•´ ìš°ë¦¬ëŠ” ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ ë†’ì€ ê²°ì¸¡ì¹˜ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€, ê·¸ë¦¬ê³  ì´ê²ƒì´ ìš°ì—°í•œ ëˆ„ë½ì¸ì§€ ì•„ë‹ˆë©´ êµ¬ì¡°ì ì¸ íŠ¹ì„±ì¸ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ê²°ì¸¡ì¹˜ ì‹œê°í™”ì™€ íŒ¨í„´ ë¶„ì„

ìˆ«ìë¡œë§Œ ë³´ëŠ” ê²°ì¸¡ì¹˜ í˜„í™©ì„ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•˜ì—¬ ë” ê¹Šì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì–´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# ê²°ì¸¡ì¹˜ ì‹œê°í™” í•¨ìˆ˜
def visualize_missing_data(df, figsize=(15, 10)):
    """
    ê²°ì¸¡ì¹˜ë¥¼ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
    figsize (tuple): ê·¸ë˜í”„ í¬ê¸°
    """
    
    # ì„œë¸Œí”Œë¡¯ ì„¤ì •
    fig, axes = plt.subplots(2, 2, figsize=figsize)
    fig.suptitle('ğŸ” House Prices Dataset ê²°ì¸¡ì¹˜ ì¢…í•© ë¶„ì„', fontsize=16, fontweight='bold')
    
    # 1. ê²°ì¸¡ì¹˜ íˆíŠ¸ë§µ (ìƒìœ„ 20ê°œ ë³€ìˆ˜)
    missing_data = df.isnull().sum().sort_values(ascending=False)
    top_missing = missing_data[missing_data > 0].head(20)
    
    if len(top_missing) > 0:
        missing_matrix = df[top_missing.index].isnull()
        sns.heatmap(missing_matrix.T, 
                   cbar=True, 
                   cmap='YlOrRd',
                   ax=axes[0,0])
        axes[0,0].set_title('ê²°ì¸¡ì¹˜ íŒ¨í„´ íˆíŠ¸ë§µ\n(ë…¸ë€ìƒ‰: ê°’ ìˆìŒ, ë¹¨ê°„ìƒ‰: ê²°ì¸¡ì¹˜)', fontsize=12)
        axes[0,0].set_xlabel('ë°ì´í„° ì¸ë±ìŠ¤')
        axes[0,0].set_ylabel('ë³€ìˆ˜ëª…')
    
    # 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ë§‰ëŒ€ê·¸ë˜í”„
    missing_percent = (missing_data / len(df)) * 100
    top_missing_percent = missing_percent[missing_percent > 0].head(15)
    
    top_missing_percent.plot(kind='barh', ax=axes[0,1], color='coral')
    axes[0,1].set_title('ë³€ìˆ˜ë³„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (%)', fontsize=12)
    axes[0,1].set_xlabel('ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (%)')
    
    # 3. ê²°ì¸¡ì¹˜ ìˆ˜ì¤€ë³„ ë¶„í¬
    missing_levels = []
    if len(missing_percent[missing_percent > 50]) > 0:
        missing_levels.append(('50% ì´ˆê³¼', len(missing_percent[missing_percent > 50])))
    if len(missing_percent[(missing_percent > 15) & (missing_percent <= 50)]) > 0:
        missing_levels.append(('15-50%', len(missing_percent[(missing_percent > 15) & (missing_percent <= 50)])))
    if len(missing_percent[(missing_percent > 0) & (missing_percent <= 15)]) > 0:
        missing_levels.append(('15% ì´í•˜', len(missing_percent[(missing_percent > 0) & (missing_percent <= 15)])))
    if len(missing_percent[missing_percent == 0]) > 0:
        missing_levels.append(('ê²°ì¸¡ì¹˜ ì—†ìŒ', len(missing_percent[missing_percent == 0])))
    
    if missing_levels:
        levels, counts = zip(*missing_levels)
        colors = ['red', 'orange', 'yellow', 'lightgreen'][:len(levels)]
        axes[1,0].pie(counts, labels=levels, autopct='%1.1f%%', colors=colors)
        axes[1,0].set_title('ê²°ì¸¡ì¹˜ ìˆ˜ì¤€ë³„ ë³€ìˆ˜ ë¶„í¬', fontsize=12)
    
    # 4. ë°ì´í„° íƒ€ì…ë³„ ê²°ì¸¡ì¹˜ í˜„í™©
    type_missing = df.dtypes.to_frame('dtype').join(df.isnull().sum().to_frame('missing'))
    type_summary = type_missing.groupby('dtype')['missing'].agg(['count', 'sum', 'mean'])
    
    type_summary['mean'].plot(kind='bar', ax=axes[1,1], color='skyblue')
    axes[1,1].set_title('ë°ì´í„° íƒ€ì…ë³„ í‰ê·  ê²°ì¸¡ì¹˜ ìˆ˜', fontsize=12)
    axes[1,1].set_xlabel('ë°ì´í„° íƒ€ì…')
    axes[1,1].set_ylabel('í‰ê·  ê²°ì¸¡ì¹˜ ìˆ˜')
    axes[1,1].tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.show()

# ê²°ì¸¡ì¹˜ ì‹œê°í™” ì‹¤í–‰
visualize_missing_data(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `sns.heatmap()`: ê²°ì¸¡ì¹˜ íŒ¨í„´ì„ ìƒ‰ìƒìœ¼ë¡œ í‘œí˜„í•˜ëŠ” íˆíŠ¸ë§µ ìƒì„±
- `plot(kind='barh')`: ìˆ˜í‰ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ì‹œê°í™”  
- `plt.pie()`: ì›í˜• ì°¨íŠ¸ë¡œ ê²°ì¸¡ì¹˜ ìˆ˜ì¤€ë³„ ë¶„í¬ í‘œí˜„
- `groupby().agg()`: ë°ì´í„° íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì§‘ê³„ í†µê³„ ê³„ì‚°

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive data visualization dashboard showing missing data analysis for a house prices dataset. Include: 1) A heatmap showing missing data patterns across variables and observations with yellow for present values and red for missing values, 2) A horizontal bar chart showing missing data percentages by variable, 3) A pie chart showing distribution of variables by missing data levels (>50%, 15-50%, <15%, no missing), 4) A bar chart showing average missing data count by data type. Use professional color schemes with clear labels and titles in a 2x2 subplot layout."

### êµ¬ì¡°ì  ê²°ì¸¡ì¹˜ vs ì„ì˜ì  ê²°ì¸¡ì¹˜ êµ¬ë¶„

House Prices ë°ì´í„°ì—ì„œëŠ” ë§ì€ ê²°ì¸¡ì¹˜ê°€ **êµ¬ì¡°ì  íŠ¹ì„±**ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:

```python
# êµ¬ì¡°ì  ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„
def analyze_structural_missing(df):
    """
    êµ¬ì¡°ì  ê²°ì¸¡ì¹˜ íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    print("ğŸ—ï¸ êµ¬ì¡°ì  ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„:\n")
    
    # 1. ì§€í•˜ì‹¤ ê´€ë ¨ ë³€ìˆ˜ë“¤
    basement_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']
    basement_missing = df[basement_cols].isnull().sum()
    
    print("ğŸ”¸ ì§€í•˜ì‹¤ ê´€ë ¨ ë³€ìˆ˜ ê²°ì¸¡ì¹˜:")
    for col in basement_cols:
        if col in df.columns:
            missing_count = df[col].isnull().sum()
            missing_percent = (missing_count / len(df)) * 100
            print(f"   {col}: {missing_count}ê°œ ({missing_percent:.1f}%)")
    
    # 2. ì°¨ê³  ê´€ë ¨ ë³€ìˆ˜ë“¤  
    garage_cols = ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']
    print(f"\nğŸ”¸ ì°¨ê³  ê´€ë ¨ ë³€ìˆ˜ ê²°ì¸¡ì¹˜:")
    for col in garage_cols:
        if col in df.columns:
            missing_count = df[col].isnull().sum()
            missing_percent = (missing_count / len(df)) * 100
            print(f"   {col}: {missing_count}ê°œ ({missing_percent:.1f}%)")
    
    # 3. ìˆ˜ì˜ì¥ ê´€ë ¨ ë³€ìˆ˜ë“¤
    if 'PoolQC' in df.columns:
        pool_missing = df['PoolQC'].isnull().sum()
        pool_percent = (pool_missing / len(df)) * 100
        print(f"\nğŸ”¸ ìˆ˜ì˜ì¥ í’ˆì§ˆ(PoolQC): {pool_missing}ê°œ ({pool_percent:.1f}%)")
        
        # ìˆ˜ì˜ì¥ ë©´ì ê³¼ í’ˆì§ˆì˜ ê´€ê³„ í™•ì¸
        if 'PoolArea' in df.columns:
            pool_area_zero = (df['PoolArea'] == 0).sum()
            print(f"   ìˆ˜ì˜ì¥ ë©´ì ì´ 0ì¸ ê²½ìš°: {pool_area_zero}ê°œ")
            print(f"   ğŸ’¡ ìˆ˜ì˜ì¥ì´ ì—†ìœ¼ë©´ í’ˆì§ˆ í‰ê°€ë„ ë¶ˆê°€ëŠ¥í•¨ì„ ì˜ë¯¸")
    
    # 4. ë²½ë‚œë¡œ ê´€ë ¨ ë³€ìˆ˜ë“¤
    if 'FireplaceQu' in df.columns:
        fireplace_missing = df['FireplaceQu'].isnull().sum()
        fireplace_percent = (fireplace_missing / len(df)) * 100
        print(f"\nğŸ”¸ ë²½ë‚œë¡œ í’ˆì§ˆ(FireplaceQu): {fireplace_missing}ê°œ ({fireplace_percent:.1f}%)")
        
        if 'Fireplaces' in df.columns:
            no_fireplace = (df['Fireplaces'] == 0).sum()
            print(f"   ë²½ë‚œë¡œ ê°œìˆ˜ê°€ 0ì¸ ê²½ìš°: {no_fireplace}ê°œ")
            print(f"   ğŸ’¡ ë²½ë‚œë¡œê°€ ì—†ìœ¼ë©´ í’ˆì§ˆ í‰ê°€ê°€ ë¶ˆê°€ëŠ¥í•¨")

# êµ¬ì¡°ì  ê²°ì¸¡ì¹˜ ë¶„ì„ ì‹¤í–‰
analyze_structural_missing(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- êµ¬ì¡°ì  ê²°ì¸¡ì¹˜ëŠ” íŠ¹ì • ì‹œì„¤ì´ ì—†ì„ ë•Œ í•´ë‹¹ ì‹œì„¤ì˜ í’ˆì§ˆì´ë‚˜ íŠ¹ì„±ì„ í‰ê°€í•  ìˆ˜ ì—†ì–´ì„œ ë°œìƒ
- ì˜ˆ: ì§€í•˜ì‹¤ì´ ì—†ëŠ” ì£¼íƒì—ì„œëŠ” ì§€í•˜ì‹¤ í’ˆì§ˆ(BsmtQual)ì„ í‰ê°€í•  ìˆ˜ ì—†ìŒ
- ì´ëŸ° ê²½ìš° ê²°ì¸¡ì¹˜ëŠ” "ì •ë³´ì˜ ë¶€ì¬"ê°€ ì•„ë‹ˆë¼ "í•´ë‹¹ ì‹œì„¤ì˜ ë¶€ì¬"ë¥¼ ì˜ë¯¸

### ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ ë¶„ì„

ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ ê²°ì¸¡ì¹˜ê°€ í•¨ê»˜ ë°œìƒí•˜ëŠ” íŒ¨í„´ì„ ë¶„ì„í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ ë¶„ì„
def analyze_missing_correlation(df):
    """
    ë³€ìˆ˜ë“¤ ê°„ì˜ ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜ë“¤ë§Œ ì„ íƒ
    missing_cols = df.columns[df.isnull().any()].tolist()
    
    if len(missing_cols) > 1:
        # ê²°ì¸¡ì¹˜ íŒ¨í„´ì„ 0(ê°’ ìˆìŒ)ê³¼ 1(ê²°ì¸¡ì¹˜)ë¡œ ë³€í™˜
        missing_pattern = df[missing_cols].isnull().astype(int)
        
        # ê²°ì¸¡ì¹˜ ê°„ ìƒê´€ê´€ê³„ ê³„ì‚°
        missing_corr = missing_pattern.corr()
        
        # ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
        plt.figure(figsize=(12, 10))
        mask = np.triu(missing_corr)  # ìƒì‚¼ê° ë§ˆìŠ¤í¬ë¡œ ì¤‘ë³µ ì œê±°
        
        sns.heatmap(missing_corr, 
                   mask=mask,
                   annot=True, 
                   cmap='RdBu_r', 
                   center=0,
                   square=True,
                   fmt='.2f')
        
        plt.title('ğŸ“Š ë³€ìˆ˜ ê°„ ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„\n(1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ í•¨ê»˜ ê²°ì¸¡ë˜ëŠ” ê²½í–¥)', 
                 fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()
        
        # ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìŒ ì°¾ê¸°
        high_corr_pairs = []
        for i in range(len(missing_corr.columns)):
            for j in range(i+1, len(missing_corr.columns)):
                corr_value = missing_corr.iloc[i, j]
                if abs(corr_value) > 0.7:  # ìƒê´€ê³„ìˆ˜ 0.7 ì´ìƒ
                    high_corr_pairs.append((
                        missing_corr.columns[i], 
                        missing_corr.columns[j], 
                        corr_value
                    ))
        
        if high_corr_pairs:
            print("\nğŸ”— ë†’ì€ ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìŒ (|r| > 0.7):")
            for var1, var2, corr in high_corr_pairs:
                print(f"   {var1} â†” {var2}: {corr:.3f}")
        else:
            print("\nâœ… ë§¤ìš° ë†’ì€ ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìŒì€ ì—†ìŠµë‹ˆë‹¤.")

# ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤í–‰
analyze_missing_correlation(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `df.isnull().astype(int)`: True/Falseë¥¼ 1/0ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ìˆ˜ì¹˜ ê³„ì‚° ê°€ëŠ¥í•˜ê²Œ í•¨
- `np.triu()`: ìƒì‚¼ê° í–‰ë ¬ ë§ˆìŠ¤í¬ë¡œ ì¤‘ë³µë˜ëŠ” ìƒê´€ê³„ìˆ˜ ì œê±°
- `sns.heatmap(center=0)`: 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ëŠ” ìƒ‰ìƒ ìŠ¤ì¼€ì¼ ì ìš©

ì´ ë¶„ì„ì„ í†µí•´ ì–´ë–¤ ë³€ìˆ˜ë“¤ì´ í•¨ê»˜ ê²°ì¸¡ë˜ëŠ” ê²½í–¥ì´ ìˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì§€í•˜ì‹¤ ê´€ë ¨ ë³€ìˆ˜ë“¤ì€ ì„œë¡œ ë†’ì€ ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¼ ê²ƒì…ë‹ˆë‹¤.

---

## ğŸ“– 4.1.2 ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ë¡ ê³¼ ì‹¤ì œ êµ¬í˜„

ê²°ì¸¡ì¹˜ ë¶„ì„ì„ ë§ˆì³¤ìœ¼ë‹ˆ, ì´ì œ ì‹¤ì œë¡œ ì´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤. ê° ë°©ë²•ì˜ íŠ¹ì§•ê³¼ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì´í•´í•˜ê³ , House Prices ë°ì´í„°ì— ì‹¤ì œë¡œ ì ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.

### ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²• ë¶„ë¥˜

ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ì€ í¬ê²Œ **ì‚­ì œ(Deletion)**, **ëŒ€ì²´(Imputation)**, **ì˜ˆì¸¡(Prediction)** ë°©ë²•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **ëŒ€ì²´(Imputation)**: ê²°ì¸¡ì¹˜ë¥¼ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ì±„ì›Œë„£ëŠ” ê³¼ì •
> - **ë¦¬ìŠ¤íŠ¸ì™€ì´ì¦ˆ ì‚­ì œ(Listwise Deletion)**: ê²°ì¸¡ì¹˜ê°€ í•˜ë‚˜ë¼ë„ ìˆëŠ” í–‰ ì „ì²´ë¥¼ ì‚­ì œ
> - **í˜ì–´ì™€ì´ì¦ˆ ì‚­ì œ(Pairwise Deletion)**: ë¶„ì„ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤ì—ë§Œ ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ê²½ìš°ë§Œ ì‚¬ìš©

### ë°©ë²• 1: ì‚­ì œ ê¸°ë°˜ ì²˜ë¦¬ (Deletion Methods)

```python
# ì‚­ì œ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ë“¤
def deletion_methods_analysis(df):
    """
    ë‹¤ì–‘í•œ ì‚­ì œ ë°©ë²•ì˜ ì˜í–¥ì„ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    """
    original_shape = df.shape
    print(f"ğŸ  ì›ë³¸ ë°ì´í„°: {original_shape[0]:,}í–‰ Ã— {original_shape[1]:,}ì—´")
    
    # 1. ì™„ì „ ì‚­ì œ (Listwise Deletion)
    complete_cases = df.dropna()
    print(f"\n1ï¸âƒ£ ì™„ì „ ì‚­ì œ í›„: {complete_cases.shape[0]:,}í–‰ Ã— {complete_cases.shape[1]:,}ì—´")
    print(f"   ğŸ“‰ ì†ì‹¤ë¥ : {((original_shape[0] - complete_cases.shape[0]) / original_shape[0] * 100):.1f}%")
    
    # 2. ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì´ ë†’ì€ ë³€ìˆ˜ ì‚­ì œ
    missing_threshold = 50  # 50% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜ ì‚­ì œ
    missing_percent = (df.isnull().sum() / len(df)) * 100
    high_missing_cols = missing_percent[missing_percent > missing_threshold].index.tolist()
    
    df_reduced_cols = df.drop(columns=high_missing_cols)
    print(f"\n2ï¸âƒ£ ë†’ì€ ê²°ì¸¡ì¹˜ ë³€ìˆ˜({missing_threshold}% ì´ìƒ) ì‚­ì œ:")
    print(f"   ì‚­ì œëœ ë³€ìˆ˜: {len(high_missing_cols)}ê°œ")
    print(f"   ë‚¨ì€ ë³€ìˆ˜: {df_reduced_cols.shape[1]}ê°œ")
    if high_missing_cols:
        print(f"   ì‚­ì œëœ ë³€ìˆ˜ ëª©ë¡: {', '.join(high_missing_cols[:5])}...")
    
    # 3. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼: ë†’ì€ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ ì‚­ì œ í›„ í–‰ ì‚­ì œ
    df_hybrid = df_reduced_cols.dropna()
    print(f"\n3ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ (ë³€ìˆ˜ ì‚­ì œ + í–‰ ì‚­ì œ):")
    print(f"   ìµœì¢… ë°ì´í„°: {df_hybrid.shape[0]:,}í–‰ Ã— {df_hybrid.shape[1]:,}ì—´")
    print(f"   ğŸ“‰ ì „ì²´ ì†ì‹¤ë¥ : {((original_shape[0] - df_hybrid.shape[0]) / original_shape[0] * 100):.1f}%")
    
    return {
        'original': df,
        'complete_cases': complete_cases,
        'reduced_columns': df_reduced_cols,
        'hybrid': df_hybrid,
        'high_missing_cols': high_missing_cols
    }

# ì‚­ì œ ë°©ë²• ë¶„ì„ ì‹¤í–‰
deletion_results = deletion_methods_analysis(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `df.dropna()`: ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ëª¨ë“  í–‰ì„ ì‚­ì œ
- `missing_percent > missing_threshold`: ì„ê³„ê°’ë³´ë‹¤ ë†’ì€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì„ ê°€ì§„ ë³€ìˆ˜ í•„í„°ë§
- `df.drop(columns=...)`: ì§€ì •ëœ ì»¬ëŸ¼ë“¤ì„ ì‚­ì œ

### ë°©ë²• 2: í†µê³„ì  ëŒ€ì²´ (Statistical Imputation)

```python
from sklearn.impute import SimpleImputer

# í†µê³„ì  ëŒ€ì²´ ë°©ë²• êµ¬í˜„
def statistical_imputation(df):
    """
    ë‹¤ì–‘í•œ í†µê³„ì  ëŒ€ì²´ ë°©ë²•ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜
    """
    # ì›ë³¸ ë°ì´í„° ë³µì‚¬ (ì›ë³¸ ë³´ì¡´)
    df_imputed = df.copy()
    
    # ìˆ˜ì¹˜í˜•ê³¼ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ë¦¬
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if 'SalePrice' in numeric_cols:
        numeric_cols.remove('SalePrice')  # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸
    
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    
    print("ğŸ“Š í†µê³„ì  ëŒ€ì²´ ë°©ë²• ì ìš©:")
    
    # 1. ìˆ˜ì¹˜í˜• ë³€ìˆ˜: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
    if numeric_cols:
        numeric_imputer = SimpleImputer(strategy='median')
        df_imputed[numeric_cols] = numeric_imputer.fit_transform(df_imputed[numeric_cols])
        print(f"âœ… ìˆ˜ì¹˜í˜• ë³€ìˆ˜ {len(numeric_cols)}ê°œ: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´")
    
    # 2. ë²”ì£¼í˜• ë³€ìˆ˜: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´  
    if categorical_cols:
        categorical_imputer = SimpleImputer(strategy='most_frequent')
        df_imputed[categorical_cols] = categorical_imputer.fit_transform(df_imputed[categorical_cols])
        print(f"âœ… ë²”ì£¼í˜• ë³€ìˆ˜ {len(categorical_cols)}ê°œ: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´")
    
    # ëŒ€ì²´ ì „í›„ ë¹„êµ
    original_missing = df.isnull().sum().sum()
    final_missing = df_imputed.isnull().sum().sum()
    
    print(f"\nğŸ“ˆ ëŒ€ì²´ ê²°ê³¼:")
    print(f"   ëŒ€ì²´ ì „ ì´ ê²°ì¸¡ì¹˜: {original_missing:,}ê°œ")
    print(f"   ëŒ€ì²´ í›„ ì´ ê²°ì¸¡ì¹˜: {final_missing:,}ê°œ")
    print(f"   ì²˜ë¦¬ ì™„ë£Œìœ¨: {((original_missing - final_missing) / original_missing * 100):.1f}%")
    
    return df_imputed

# í†µê³„ì  ëŒ€ì²´ ì‹¤í–‰
imputed_data = statistical_imputation(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `SimpleImputer(strategy='median')`: scikit-learnì˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´ ë„êµ¬, ì¤‘ì•™ê°’ ì‚¬ìš©
- `strategy='most_frequent'`: ë²”ì£¼í˜• ë°ì´í„°ì—ì„œ ê°€ì¥ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ê°’ìœ¼ë¡œ ëŒ€ì²´
- `fit_transform()`: ëŒ€ì²´ ê·œì¹™ì„ í•™ìŠµ(fit)í•˜ê³  ì ìš©(transform)í•˜ëŠ” ê³¼ì •

### ë°©ë²• 3: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ëŒ€ì²´ (Domain-specific Imputation)

House Prices ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ì „ë¬¸ì ì¸ ëŒ€ì²´ ë°©ë²•ì„ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
def domain_specific_imputation(df):
    """
    ë¶€ë™ì‚° ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜
    """
    df_domain = df.copy()
    
    print("ğŸ¡ ë¶€ë™ì‚° ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬:")
    
    # 1. ì§€í•˜ì‹¤ ê´€ë ¨ ë³€ìˆ˜ë“¤ - "ì§€í•˜ì‹¤ ì—†ìŒ"ìœ¼ë¡œ ì²˜ë¦¬
    basement_quality_cols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']
    for col in basement_quality_cols:
        if col in df_domain.columns:
            before_count = df_domain[col].isnull().sum()
            df_domain[col] = df_domain[col].fillna('None')  # "ì—†ìŒ"ì„ ì˜ë¯¸í•˜ëŠ” ê°’
            after_count = df_domain[col].isnull().sum()
            print(f"   {col}: {before_count}ê°œ â†’ {after_count}ê°œ (Noneìœ¼ë¡œ ëŒ€ì²´)")
    
    # 2. ì°¨ê³  ê´€ë ¨ ë³€ìˆ˜ë“¤ - "ì°¨ê³  ì—†ìŒ"ìœ¼ë¡œ ì²˜ë¦¬
    garage_quality_cols = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']
    for col in garage_quality_cols:
        if col in df_domain.columns:
            before_count = df_domain[col].isnull().sum()
            df_domain[col] = df_domain[col].fillna('None')
            after_count = df_domain[col].isnull().sum()
            print(f"   {col}: {before_count}ê°œ â†’ {after_count}ê°œ (Noneìœ¼ë¡œ ëŒ€ì²´)")
    
    # 3. ì°¨ê³  ê±´ì„¤ì—°ë„ - 0ìœ¼ë¡œ ì²˜ë¦¬ (ì°¨ê³ ê°€ ì—†ìœ¼ë©´ ê±´ì„¤ì—°ë„ë„ ì—†ìŒ)
    if 'GarageYrBlt' in df_domain.columns:
        before_count = df_domain['GarageYrBlt'].isnull().sum()
        df_domain['GarageYrBlt'] = df_domain['GarageYrBlt'].fillna(0)
        after_count = df_domain['GarageYrBlt'].isnull().sum()
        print(f"   GarageYrBlt: {before_count}ê°œ â†’ {after_count}ê°œ (0ìœ¼ë¡œ ëŒ€ì²´)")
    
    # 4. ìˆ˜ì˜ì¥/ë²½ë‚œë¡œ í’ˆì§ˆ - "ì—†ìŒ"ìœ¼ë¡œ ì²˜ë¦¬
    facility_cols = ['PoolQC', 'FireplaceQu', 'Fence', 'MiscFeature']
    for col in facility_cols:
        if col in df_domain.columns:
            before_count = df_domain[col].isnull().sum()
            df_domain[col] = df_domain[col].fillna('None')
            after_count = df_domain[col].isnull().sum()
            print(f"   {col}: {before_count}ê°œ â†’ {after_count}ê°œ (Noneìœ¼ë¡œ ëŒ€ì²´)")
    
    # 5. ê³¨ëª©ê¸¸ ì ‘ê·¼ - ëŒ€ë¶€ë¶„ ê³¨ëª©ê¸¸ ì ‘ê·¼ì´ ì—†ìœ¼ë¯€ë¡œ 'No'ë¡œ ì²˜ë¦¬
    if 'Alley' in df_domain.columns:
        before_count = df_domain['Alley'].isnull().sum()
        df_domain['Alley'] = df_domain['Alley'].fillna('None')
        after_count = df_domain['Alley'].isnull().sum()
        print(f"   Alley: {before_count}ê°œ â†’ {after_count}ê°œ (Noneìœ¼ë¡œ ëŒ€ì²´)")
    
    # 6. ë§ˆê°ì¬ ê´€ë ¨ - ì¶”ê°€ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
    if 'MasVnrType' in df_domain.columns:
        before_count = df_domain['MasVnrType'].isnull().sum()
        df_domain['MasVnrType'] = df_domain['MasVnrType'].fillna('None')
        after_count = df_domain['MasVnrType'].isnull().sum()
        print(f"   MasVnrType: {before_count}ê°œ â†’ {after_count}ê°œ (Noneìœ¼ë¡œ ëŒ€ì²´)")
    
    if 'MasVnrArea' in df_domain.columns:
        before_count = df_domain['MasVnrArea'].isnull().sum()
        # ë§ˆê°ì¬ íƒ€ì…ì´ Noneì´ë©´ ë©´ì ë„ 0
        mask = df_domain['MasVnrType'] == 'None'
        df_domain.loc[mask, 'MasVnrArea'] = 0
        # ë‚˜ë¨¸ì§€ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´
        median_area = df_domain['MasVnrArea'].median()
        df_domain['MasVnrArea'] = df_domain['MasVnrArea'].fillna(median_area)
        after_count = df_domain['MasVnrArea'].isnull().sum()
        print(f"   MasVnrArea: {before_count}ê°œ â†’ {after_count}ê°œ (0 ë˜ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´)")
    
    # 7. ì „ê¸° ì‹œìŠ¤í…œ - í‘œì¤€ ì‹œìŠ¤í…œìœ¼ë¡œ ê°€ì •
    if 'Electrical' in df_domain.columns:
        before_count = df_domain['Electrical'].isnull().sum()
        mode_electrical = df_domain['Electrical'].mode()[0]
        df_domain['Electrical'] = df_domain['Electrical'].fillna(mode_electrical)
        after_count = df_domain['Electrical'].isnull().sum()
        print(f"   Electrical: {before_count}ê°œ â†’ {after_count}ê°œ ({mode_electrical}ìœ¼ë¡œ ëŒ€ì²´)")
    
    # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
    remaining_missing = df_domain.isnull().sum().sum()
    original_missing = df.isnull().sum().sum()
    
    print(f"\nğŸ“Š ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   ì²˜ë¦¬ ì „ ì´ ê²°ì¸¡ì¹˜: {original_missing:,}ê°œ")
    print(f"   ì²˜ë¦¬ í›„ ì´ ê²°ì¸¡ì¹˜: {remaining_missing:,}ê°œ")
    print(f"   ì²˜ë¦¬ ì™„ë£Œìœ¨: {((original_missing - remaining_missing) / original_missing * 100):.1f}%")
    
    return df_domain

# ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì²˜ë¦¬ ì‹¤í–‰
domain_imputed_data = domain_specific_imputation(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `fillna('None')`: ê²°ì¸¡ì¹˜ë¥¼ 'None' ë¬¸ìì—´ë¡œ ëŒ€ì²´ (ì‹œì„¤ì´ ì—†ìŒì„ ì˜ë¯¸)
- `df.mode()[0]`: ìµœë¹ˆê°’(ê°€ì¥ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ê°’)ì˜ ì²« ë²ˆì§¸ ê°’
- `df.loc[mask, column]`: ì¡°ê±´(mask)ì— ë§ëŠ” í–‰ì˜ íŠ¹ì • ì»¬ëŸ¼ì— ê°’ í• ë‹¹

ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ì²˜ë¦¬ì˜ í•µì‹¬ì€ **ê²°ì¸¡ì¹˜ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. "ì •ë³´ê°€ ì—†ë‹¤"ê°€ ì•„ë‹ˆë¼ "í•´ë‹¹ ì‹œì„¤ì´ ì—†ë‹¤"ëŠ” ì˜ë¯¸ë¡œ í•´ì„í•˜ì—¬ ì ì ˆí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.

### ë°©ë²• 4: ê³ ê¸‰ ì˜ˆì¸¡ ê¸°ë°˜ ëŒ€ì²´ (Advanced Predictive Imputation)

```python
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.impute import KNNImputer

# ì˜ˆì¸¡ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´
def predictive_imputation(df, target_col='SalePrice'):
    """
    ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ ì˜ˆì¸¡ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´
    """
    df_pred = df.copy()
    
    print("ğŸ¤– ì˜ˆì¸¡ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ëŒ€ì²´:")
    
    # 1. KNN ëŒ€ì²´ (K-Nearest Neighbors)
    print("\n1ï¸âƒ£ KNN ëŒ€ì²´ ë°©ë²•:")
    
    # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ì„ íƒ (KNNì€ ìˆ˜ì¹˜í˜• ë°ì´í„°ì— ì í•©)
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if target_col in numeric_cols:
        numeric_cols.remove(target_col)
    
    # KNN ëŒ€ì²´ê¸° ì ìš©
    knn_imputer = KNNImputer(n_neighbors=5)
    df_pred[numeric_cols] = knn_imputer.fit_transform(df_pred[numeric_cols])
    
    print(f"   âœ… {len(numeric_cols)}ê°œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì— KNN ëŒ€ì²´ ì ìš© (k=5)")
    
    # 2. ë²”ì£¼í˜• ë³€ìˆ˜ëŠ” Random Forestë¡œ ì˜ˆì¸¡
    print("\n2ï¸âƒ£ Random Forest ì˜ˆì¸¡ ëŒ€ì²´:")
    
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    processed_cols = []
    
    for col in categorical_cols:
        if df_pred[col].isnull().sum() > 0:
            # ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” í–‰ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ìƒì„±
            train_mask = df_pred[col].notnull()
            
            if train_mask.sum() > 10:  # ì¶©ë¶„í•œ í•™ìŠµ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                # íŠ¹ì„± ì„ íƒ (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ì¤‘ ê²°ì¸¡ì¹˜ê°€ ì ì€ ê²ƒë“¤)
                feature_cols = [c for c in numeric_cols 
                               if df_pred[c].isnull().sum() < len(df_pred) * 0.1][:10]
                
                if len(feature_cols) >= 3:  # ìµœì†Œ 3ê°œ íŠ¹ì„± í•„ìš”
                    X_train = df_pred.loc[train_mask, feature_cols]
                    y_train = df_pred.loc[train_mask, col]
                    
                    # Random Forest ë¶„ë¥˜ê¸° í•™ìŠµ
                    rf_classifier = RandomForestClassifier(
                        n_estimators=50, 
                        random_state=42,
                        max_depth=10
                    )
                    rf_classifier.fit(X_train, y_train)
                    
                    # ê²°ì¸¡ì¹˜ ì˜ˆì¸¡
                    missing_mask = df_pred[col].isnull()
                    if missing_mask.sum() > 0:
                        X_missing = df_pred.loc[missing_mask, feature_cols]
                        predicted_values = rf_classifier.predict(X_missing)
                        df_pred.loc[missing_mask, col] = predicted_values
                        
                        processed_cols.append(col)
                        print(f"   âœ… {col}: {missing_mask.sum()}ê°œ ê²°ì¸¡ì¹˜ ì˜ˆì¸¡ ëŒ€ì²´")
    
    print(f"\nğŸ“Š ì˜ˆì¸¡ ê¸°ë°˜ ì²˜ë¦¬ ì™„ë£Œ:")
    print(f"   KNN ëŒ€ì²´: {len(numeric_cols)}ê°œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜")
    print(f"   RF ì˜ˆì¸¡ ëŒ€ì²´: {len(processed_cols)}ê°œ ë²”ì£¼í˜• ë³€ìˆ˜")
    
    final_missing = df_pred.isnull().sum().sum()
    original_missing = df.isnull().sum().sum()
    
    print(f"   ì²˜ë¦¬ ì „ ì´ ê²°ì¸¡ì¹˜: {original_missing:,}ê°œ")
    print(f"   ì²˜ë¦¬ í›„ ì´ ê²°ì¸¡ì¹˜: {final_missing:,}ê°œ")
    print(f"   ì²˜ë¦¬ ì™„ë£Œìœ¨: {((original_missing - final_missing) / original_missing * 100):.1f}%")
    
    return df_pred

# ì˜ˆì¸¡ ê¸°ë°˜ ëŒ€ì²´ ì‹¤í–‰ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)
print("âš ï¸ ì˜ˆì¸¡ ê¸°ë°˜ ëŒ€ì²´ëŠ” ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
predictive_imputed_data = predictive_imputation(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `KNNImputer(n_neighbors=5)`: ê°€ì¥ ìœ ì‚¬í•œ 5ê°œ ë°ì´í„°ì˜ í‰ê· ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ëŒ€ì²´
- `RandomForestClassifier()`: ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ ì‚¬ìš©í•´ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ë³€ìˆ˜ì˜ ê°’ì„ ì˜ˆì¸¡
- `fit()`: ëª¨ë¸ í•™ìŠµ, `predict()`: ì˜ˆì¸¡ ìˆ˜í–‰

ì´ ë°©ë²•ì€ ê°€ì¥ ì •êµí•˜ì§€ë§Œ ê³„ì‚° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³ , ë•Œë¡œëŠ” ê³¼ì í•©ì˜ ìœ„í—˜ì´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“– 4.1.3 ì´ìƒì¹˜ íƒì§€ì™€ ì²˜ë¦¬

ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¥¼ ë§ˆì³¤ìœ¼ë‹ˆ, ì´ì œ ë°ì´í„°ì˜ ë˜ ë‹¤ë¥¸ ë„ì „ ê³¼ì œì¸ **ì´ìƒì¹˜(Outliers)**ë¥¼ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤. ì´ìƒì¹˜ëŠ” ë‹¤ë¥¸ ë°ì´í„°ì™€ í˜„ì €íˆ ë‹¤ë¥¸ ê°’ì„ ê°€ì§„ ê´€ì¸¡ì¹˜ë¡œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì´ìƒì¹˜ì˜ ì •ì˜ì™€ ìœ í˜•

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **ì´ìƒì¹˜(Outlier)**: ë‹¤ë¥¸ ê´€ì¸¡ì¹˜ë“¤ê³¼ í˜„ì €íˆ ë‹¤ë¥¸ íŒ¨í„´ì„ ë³´ì´ëŠ” ë°ì´í„° í¬ì¸íŠ¸
> - **IQR (Interquartile Range)**: ì œ3ì‚¬ë¶„ìœ„ìˆ˜ì—ì„œ ì œ1ì‚¬ë¶„ìœ„ìˆ˜ë¥¼ ëº€ ê°’ (ì¤‘ê°„ 50% ë°ì´í„°ì˜ ë²”ìœ„)
> - **Z-Score**: ë°ì´í„°ê°€ í‰ê· ìœ¼ë¡œë¶€í„° í‘œì¤€í¸ì°¨ì˜ ëª‡ ë°° ë–¨ì–´ì ¸ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ

ì´ìƒì¹˜ëŠ” ë°œìƒ ì›ì¸ì— ë”°ë¼ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¥˜ë©ë‹ˆë‹¤:

1. **ì¸¡ì • ì˜¤ë¥˜ ì´ìƒì¹˜**: ë°ì´í„° ì…ë ¥ ì‹¤ìˆ˜, ì„¼ì„œ ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ë°œìƒ
2. **ìì—° ë°œìƒ ì´ìƒì¹˜**: ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ê·¹ë‹¨ì ì¸ ì¼€ì´ìŠ¤ 
3. **ì²˜ë¦¬ ì˜¤ë¥˜ ì´ìƒì¹˜**: ë°ì´í„° ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë°œìƒí•œ ì˜¤ë¥˜

### ì´ìƒì¹˜ íƒì§€ ë°©ë²• êµ¬í˜„

```python
# ì´ìƒì¹˜ íƒì§€ ì¢…í•© í•¨ìˆ˜
def detect_outliers(df, columns=None):
    """
    ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ì¢…í•© í•¨ìˆ˜
    
    Parameters:
    df (pd.DataFrame): ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
    columns (list): ë¶„ì„í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)
    
    Returns:
    dict: ê° ë°©ë²•ë³„ ì´ìƒì¹˜ íƒì§€ ê²°ê³¼
    """
    if columns is None:
        columns = df.select_dtypes(include=[np.number]).columns.tolist()
        # íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸
        if 'SalePrice' in columns:
            columns.remove('SalePrice')
    
    outlier_results = {}
    
    print("ğŸ” ì´ìƒì¹˜ íƒì§€ ë°©ë²•ë³„ ê²°ê³¼:\n")
    
    for col in columns[:5]:  # ì²˜ìŒ 5ê°œ ë³€ìˆ˜ë§Œ ë¶„ì„ (ì˜ˆì‹œ)
        if col in df.columns:
            print(f"ğŸ“Š {col} ë³€ìˆ˜ ì´ìƒì¹˜ ë¶„ì„:")
            
            # ê¸°ë³¸ í†µê³„
            col_data = df[col].dropna()
            mean_val = col_data.mean()
            std_val = col_data.std()
            median_val = col_data.median()
            
            # 1. IQR ë°©ë²•
            Q1 = col_data.quantile(0.25)
            Q3 = col_data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            iqr_outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]
            
            # 2. Z-Score ë°©ë²• (ì ˆëŒ“ê°’ 3 ì´ìƒ)
            z_scores = np.abs((col_data - mean_val) / std_val)
            zscore_outliers = col_data[z_scores > 3]
            
            # 3. Modified Z-Score ë°©ë²• (ì¤‘ì•™ê°’ ê¸°ë°˜)
            median_absolute_deviation = np.median(np.abs(col_data - median_val))
            modified_z_scores = 0.6745 * (col_data - median_val) / median_absolute_deviation
            modified_zscore_outliers = col_data[np.abs(modified_z_scores) > 3.5]
            
            # ê²°ê³¼ ì €ì¥
            outlier_results[col] = {
                'iqr_outliers': iqr_outliers,
                'zscore_outliers': zscore_outliers,
                'modified_zscore_outliers': modified_zscore_outliers,
                'bounds': {'lower': lower_bound, 'upper': upper_bound}
            }
            
            print(f"   IQR ë°©ë²•: {len(iqr_outliers)}ê°œ ì´ìƒì¹˜ (ë²”ìœ„: {lower_bound:.1f} ~ {upper_bound:.1f})")
            print(f"   Z-Score ë°©ë²•: {len(zscore_outliers)}ê°œ ì´ìƒì¹˜")
            print(f"   Modified Z-Score ë°©ë²•: {len(modified_zscore_outliers)}ê°œ ì´ìƒì¹˜")
            print()
    
    return outlier_results

# ì´ìƒì¹˜ íƒì§€ ì‹¤í–‰
outlier_results = detect_outliers(train_data)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `quantile(0.25)`, `quantile(0.75)`: ì œ1ì‚¬ë¶„ìœ„ìˆ˜ì™€ ì œ3ì‚¬ë¶„ìœ„ìˆ˜ ê³„ì‚°
- `1.5 * IQR`: ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì´ìƒì¹˜ íŒë³„ ê¸°ì¤€
- `np.abs(z_scores) > 3`: Z-Scoreì˜ ì ˆëŒ“ê°’ì´ 3ë³´ë‹¤ í° ê²½ìš°ë¥¼ ì´ìƒì¹˜ë¡œ íŒë³„

### ì´ìƒì¹˜ ì‹œê°í™”

ì´ìƒì¹˜ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ì—¬ íŒ¨í„´ì„ íŒŒì•…í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# ì´ìƒì¹˜ ì‹œê°í™” í•¨ìˆ˜
def visualize_outliers(df, columns=None, figsize=(15, 12)):
    """
    ì´ìƒì¹˜ë¥¼ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    """
    if columns is None:
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if 'SalePrice' in numeric_cols:
            numeric_cols.remove('SalePrice')
        columns = numeric_cols[:6]  # ìƒìœ„ 6ê°œ ë³€ìˆ˜
    
    fig, axes = plt.subplots(2, 3, figsize=figsize)
    fig.suptitle('ğŸ” ì´ìƒì¹˜ ì‹œê°í™” ë¶„ì„', fontsize=16, fontweight='bold')
    
    for i, col in enumerate(columns):
        if i >= 6:  # ìµœëŒ€ 6ê°œë§Œ í‘œì‹œ
            break
            
        row = i // 3
        col_idx = i % 3
        
        if col in df.columns:
            # ë°•ìŠ¤í”Œë¡¯ìœ¼ë¡œ ì´ìƒì¹˜ ì‹œê°í™”
            df[col].dropna().plot(kind='box', ax=axes[row, col_idx])
            axes[row, col_idx].set_title(f'{col}\nì´ìƒì¹˜ íƒì§€', fontsize=10)
            axes[row, col_idx].grid(True, alpha=0.3)
    
    # ë¹ˆ ì„œë¸Œí”Œë¡¯ ì œê±°
    for i in range(len(columns), 6):
        row = i // 3
        col_idx = i % 3
        axes[row, col_idx].remove()
    
    plt.tight_layout()
    plt.show()
    
    # ìƒì„¸ ë¶„ì„: ëŒ€í‘œ ë³€ìˆ˜ í•˜ë‚˜ ì„ íƒ
    if 'GrLivArea' in df.columns:
        print("ğŸ“Š ìƒì„¸ ë¶„ì„: GrLivArea (ì§€ìƒì¸µ ê±°ì£¼ ë©´ì )")
        
        plt.figure(figsize=(15, 5))
        
        # íˆìŠ¤í† ê·¸ë¨
        plt.subplot(1, 3, 1)
        df['GrLivArea'].hist(bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        plt.title('ê±°ì£¼ ë©´ì  ë¶„í¬')
        plt.xlabel('ë©´ì  (sq ft)')
        plt.ylabel('ë¹ˆë„')
        plt.grid(True, alpha=0.3)
        
        # ë°•ìŠ¤í”Œë¡¯
        plt.subplot(1, 3, 2)
        df['GrLivArea'].plot(kind='box')
        plt.title('ê±°ì£¼ ë©´ì  ë°•ìŠ¤í”Œë¡¯')
        plt.ylabel('ë©´ì  (sq ft)')
        plt.grid(True, alpha=0.3)
        
        # ì‚°ì ë„ (SalePriceì™€ì˜ ê´€ê³„)
        plt.subplot(1, 3, 3)
        if 'SalePrice' in df.columns:
            plt.scatter(df['GrLivArea'], df['SalePrice'], alpha=0.6, color='coral')
            plt.title('ê±°ì£¼ ë©´ì  vs íŒë§¤ ê°€ê²©')
            plt.xlabel('ê±°ì£¼ ë©´ì  (sq ft)')
            plt.ylabel('íŒë§¤ ê°€ê²© ($)')
            plt.grid(True, alpha=0.3)
            
            # ì´ìƒì¹˜ë¡œ ë³´ì´ëŠ” í¬ì¸íŠ¸ ê°•ì¡°
            outlier_threshold = df['GrLivArea'].quantile(0.99)
            outlier_mask = df['GrLivArea'] > outlier_threshold
            if outlier_mask.sum() > 0:
                plt.scatter(df.loc[outlier_mask, 'GrLivArea'], 
                           df.loc[outlier_mask, 'SalePrice'], 
                           color='red', s=100, alpha=0.8, 
                           label=f'ìƒìœ„ 1% ì´ìƒì¹˜ ({outlier_mask.sum()}ê°œ)')
                plt.legend()
        
        plt.tight_layout()
        plt.show()

# ì´ìƒì¹˜ ì‹œê°í™” ì‹¤í–‰
visualize_outliers(train_data)
```

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive outlier detection visualization dashboard with: 1) A 2x3 grid of box plots showing outliers in different numerical variables (like GrLivArea, LotArea, TotalBsmtSF, etc.) from a house prices dataset, 2) A detailed analysis section with three plots for GrLivArea: histogram showing distribution with outliers highlighted, box plot with outlier points clearly marked, and scatter plot of GrLivArea vs SalePrice with extreme outliers highlighted in red. Use professional styling with clear labels, gridlines, and color coding to distinguish normal data points from outliers."

### ì´ìƒì¹˜ ì²˜ë¦¬ ì „ëµ

ì´ìƒì¹˜ë¥¼ íƒì§€í–ˆìœ¼ë‹ˆ, ì´ì œ ì´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í• ì§€ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤.

```python
# ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²•ë“¤
def handle_outliers(df, method='iqr', columns=None):
    """
    ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    method (str): 'remove', 'cap', 'transform', 'keep'
    """
    df_processed = df.copy()
    
    if columns is None:
        columns = ['GrLivArea', 'LotArea', 'TotalBsmtSF']  # ì˜ˆì‹œ ì»¬ëŸ¼ë“¤
    
    print(f"ğŸ”§ ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²•: {method}")
    
    for col in columns:
        if col in df_processed.columns:
            col_data = df_processed[col].dropna()
            
            # IQR ê¸°ë°˜ ì´ìƒì¹˜ ê²½ê³„ ê³„ì‚°
            Q1 = col_data.quantile(0.25)
            Q3 = col_data.quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # ì´ìƒì¹˜ ê°œìˆ˜ í™•ì¸
            outliers_before = ((df_processed[col] < lower_bound) | 
                              (df_processed[col] > upper_bound)).sum()
            
            if method == 'remove':
                # ë°©ë²• 1: ì´ìƒì¹˜ ì œê±°
                mask = (df_processed[col] >= lower_bound) & (df_processed[col] <= upper_bound)
                df_processed = df_processed[mask | df_processed[col].isnull()]
                outliers_after = 0
                
            elif method == 'cap':
                # ë°©ë²• 2: ì´ìƒì¹˜ ìº¡í•‘ (ê²½ê³„ê°’ìœ¼ë¡œ ì œí•œ)
                df_processed[col] = df_processed[col].clip(lower=lower_bound, upper=upper_bound)
                outliers_after = 0
                
            elif method == 'transform':
                # ë°©ë²• 3: ë¡œê·¸ ë³€í™˜ (ì–‘ì˜ ê°’ë§Œ ê°€ëŠ¥)
                if (df_processed[col] > 0).all():
                    df_processed[col + '_log'] = np.log1p(df_processed[col])
                    print(f"   {col}: ë¡œê·¸ ë³€í™˜ ì ìš© ({col}_log ì»¬ëŸ¼ ìƒì„±)")
                outliers_after = outliers_before  # ë³€í™˜ì€ ì œê±°ê°€ ì•„ë‹˜
                
            elif method == 'winsorize':
                # ë°©ë²• 4: ìœˆì €í™” (ìƒí•˜ìœ„ 5%ë¥¼ í•´ë‹¹ ë¶„ìœ„ìˆ˜ ê°’ìœ¼ë¡œ ëŒ€ì²´)
                from scipy.stats import mstats
                df_processed[col] = mstats.winsorize(df_processed[col], limits=[0.05, 0.05])
                outliers_after = 0
                
            else:  # method == 'keep'
                # ë°©ë²• 5: ì´ìƒì¹˜ ìœ ì§€ (ë¶„ì„ ëª©ì )
                outliers_after = outliers_before
            
            print(f"   {col}: {outliers_before}ê°œ â†’ {outliers_after}ê°œ")
    
    print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    print(f"   ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
    print(f"   ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {df_processed.shape}")
    
    return df_processed

# ê° ë°©ë²•ë³„ ì´ìƒì¹˜ ì²˜ë¦¬ ë¹„êµ
methods = ['keep', 'cap', 'remove', 'transform']
processed_data = {}

for method in methods:
    print(f"\n{'='*50}")
    processed_data[method] = handle_outliers(train_data, method=method)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `clip(lower, upper)`: ê°’ì„ ì§€ì •ëœ ë²”ìœ„ë¡œ ì œí•œí•˜ëŠ” í•¨ìˆ˜
- `np.log1p()`: log(1+x) ë³€í™˜ìœ¼ë¡œ 0ì„ í¬í•¨í•œ ì–‘ìˆ˜ì— ì•ˆì „í•˜ê²Œ ì ìš©
- `mstats.winsorize()`: ê·¹ë‹¨ê°’ì„ íŠ¹ì • ë¶„ìœ„ìˆ˜ ê°’ìœ¼ë¡œ ëŒ€ì²´

### ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì—ì„œì˜ ì´ìƒì¹˜ í•´ì„

```python
# ì´ìƒì¹˜ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  ì˜ë¯¸ ë¶„ì„
def analyze_outlier_business_impact(df):
    """
    ì´ìƒì¹˜ê°€ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ì–´ë–¤ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ ë¶„ì„
    """
    print("ğŸ¡ ì´ìƒì¹˜ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  í•´ì„:")
    
    if 'GrLivArea' in df.columns and 'SalePrice' in df.columns:
        # ê±°ì£¼ ë©´ì  ì´ìƒì¹˜ ë¶„ì„
        area_q99 = df['GrLivArea'].quantile(0.99)
        large_houses = df[df['GrLivArea'] > area_q99]
        
        print(f"\nğŸ  ëŒ€í˜• ì£¼íƒ ë¶„ì„ (ìƒìœ„ 1%, ë©´ì  > {area_q99:.0f} sq ft):")
        print(f"   ëŒ€í˜• ì£¼íƒ ìˆ˜: {len(large_houses)}ê°œ")
        
        if len(large_houses) > 0:
            avg_price_large = large_houses['SalePrice'].mean()
            avg_price_normal = df[df['GrLivArea'] <= area_q99]['SalePrice'].mean()
            
            print(f"   ëŒ€í˜• ì£¼íƒ í‰ê·  ê°€ê²©: ${avg_price_large:,.0f}")
            print(f"   ì¼ë°˜ ì£¼íƒ í‰ê·  ê°€ê²©: ${avg_price_normal:,.0f}")
            print(f"   ê°€ê²© í”„ë¦¬ë¯¸ì—„: {((avg_price_large / avg_price_normal - 1) * 100):.1f}%")
            
            # ëŒ€í˜• ì£¼íƒì˜ íŠ¹ì„± ë¶„ì„
            if 'Neighborhood' in df.columns:
                print(f"\n   ëŒ€í˜• ì£¼íƒì´ ë§ì€ ì§€ì—­:")
                neighborhood_dist = large_houses['Neighborhood'].value_counts().head(3)
                for neighborhood, count in neighborhood_dist.items():
                    pct = (count / len(large_houses)) * 100
                    print(f"   - {neighborhood}: {count}ê°œ ({pct:.1f}%)")
    
    # ê°€ê²© ì´ìƒì¹˜ ë¶„ì„
    if 'SalePrice' in df.columns:
        price_q99 = df['SalePrice'].quantile(0.99)
        price_q01 = df['SalePrice'].quantile(0.01)
        
        expensive_houses = df[df['SalePrice'] > price_q99]
        cheap_houses = df[df['SalePrice'] < price_q01]
        
        print(f"\nğŸ’° ê°€ê²© ì´ìƒì¹˜ ë¶„ì„:")
        print(f"   ê³ ê°€ ì£¼íƒ (ìƒìœ„ 1%): {len(expensive_houses)}ê°œ (${price_q99:,.0f} ì´ìƒ)")
        print(f"   ì €ê°€ ì£¼íƒ (í•˜ìœ„ 1%): {len(cheap_houses)}ê°œ (${price_q01:,.0f} ì´í•˜)")
        
        # ê³ ê°€ ì£¼íƒì˜ íŠ¹ì§•
        if len(expensive_houses) > 0 and 'OverallQual' in df.columns:
            avg_quality_expensive = expensive_houses['OverallQual'].mean()
            avg_quality_normal = df[(df['SalePrice'] >= price_q01) & 
                                   (df['SalePrice'] <= price_q99)]['OverallQual'].mean()
            
            print(f"   ê³ ê°€ ì£¼íƒ í‰ê·  í’ˆì§ˆ: {avg_quality_expensive:.1f}/10")
            print(f"   ì¼ë°˜ ì£¼íƒ í‰ê·  í’ˆì§ˆ: {avg_quality_normal:.1f}/10")

# ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ ë¶„ì„ ì‹¤í–‰
analyze_outlier_business_impact(train_data)
```

---

## ğŸ¯ ì§ì ‘ í•´ë³´ê¸° - ì—°ìŠµ ë¬¸ì œ

ì´ì œ ë°°ìš´ ë‚´ìš©ì„ ì‹¤ì œë¡œ ì ìš©í•´ë³´ëŠ” ì—°ìŠµ ë¬¸ì œë¥¼ í’€ì–´ë³´ê² ìŠµë‹ˆë‹¤.

### ì—°ìŠµ ë¬¸ì œ 1: ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„ â­â­
ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ê²°ì¸¡ì¹˜ íŒ¨í„´ì„ ë¶„ì„í•´ë³´ì„¸ìš”.

```python
# ì—°ìŠµ ë¬¸ì œ 1: ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„
def exercise_missing_pattern(df):
    """
    ê²°ì¸¡ì¹˜ íŒ¨í„´ì„ ë¶„ì„í•˜ê³  ì²˜ë¦¬ ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” í•¨ìˆ˜
    TODO: ë¹ˆ ë¶€ë¶„ì„ ì±„ì›Œë³´ì„¸ìš”
    """
    
    # 1. ê²°ì¸¡ì¹˜ê°€ 50% ì´ìƒì¸ ë³€ìˆ˜ ì°¾ê¸°
    high_missing_vars = # TODO: ì—¬ê¸°ë¥¼ ì™„ì„±í•˜ì„¸ìš”
    
    # 2. ì§€í•˜ì‹¤ ê´€ë ¨ ë³€ìˆ˜ë“¤ì˜ ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ í™•ì¸
    basement_vars = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1']
    basement_corr = # TODO: ê²°ì¸¡ì¹˜ ìƒê´€ê´€ê³„ ê³„ì‚°
    
    # 3. ì²˜ë¦¬ ì „ëµ ì¶œë ¥
    print("ğŸ“‹ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ:")
    print(f"ë†’ì€ ê²°ì¸¡ì¹˜ ë³€ìˆ˜ ({len(high_missing_vars)}ê°œ): ì‚­ì œ ê³ ë ¤")
    print(f"ì§€í•˜ì‹¤ ë³€ìˆ˜ ìƒê´€ê´€ê³„: {basement_corr:.3f}")
    
    return high_missing_vars

# íŒíŠ¸: isnull(), sum(), corr() í•¨ìˆ˜ë“¤ì„ í™œìš©í•˜ì„¸ìš”
```

### ì—°ìŠµ ë¬¸ì œ 2: ë§ì¶¤í˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬ â­â­â­
ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”.

```python
# ì—°ìŠµ ë¬¸ì œ 2: ë§ì¶¤í˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬
def exercise_custom_imputation(df):
    """
    ë¶€ë™ì‚° ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ë§ì¶¤í˜• ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    """
    df_processed = df.copy()
    
    # TODO: ë‹¤ìŒ ê·œì¹™ì— ë”°ë¼ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ì„¸ìš”
    # 1. ìˆ˜ì˜ì¥ í’ˆì§ˆ(PoolQC)ì´ ê²°ì¸¡ì´ë©´ 'No Pool'ë¡œ ëŒ€ì²´
    # 2. ì°¨ê³  ê±´ì„¤ì—°ë„(GarageYrBlt)ê°€ ê²°ì¸¡ì´ë©´ ì£¼íƒ ê±´ì„¤ì—°ë„(YearBuilt)ë¡œ ëŒ€ì²´
    # 3. ì§€í•˜ì‹¤ ë©´ì (TotalBsmtSF)ì´ ê²°ì¸¡ì´ë©´ 0ìœ¼ë¡œ ëŒ€ì²´
    
    # ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
    
    return df_processed
```

### ì—°ìŠµ ë¬¸ì œ 3: ì´ìƒì¹˜ ì˜í–¥ ë¶„ì„ â­â­â­
ì´ìƒì¹˜ê°€ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ë³´ì„¸ìš”.

```python
# ì—°ìŠµ ë¬¸ì œ 3: ì´ìƒì¹˜ ì˜í–¥ ë¶„ì„
def exercise_outlier_impact(df):
    """
    ì´ìƒì¹˜ê°€ ê°€ê²© ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
    """
    from sklearn.linear_model import LinearRegression
    from sklearn.metrics import mean_squared_error
    
    # TODO: ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”
    # 1. GrLivAreaë¥¼ ì‚¬ìš©í•œ ë‹¨ìˆœ ì„ í˜• íšŒê·€ ëª¨ë¸ êµ¬ì¶•
    # 2. ì´ìƒì¹˜ ì œê±° ì „í›„ì˜ ì˜ˆì¸¡ ì„±ëŠ¥(RMSE) ë¹„êµ
    # 3. ê²°ê³¼ í•´ì„ ë° ê¶Œê³ ì‚¬í•­ ì œì‹œ
    
    # íŒíŠ¸: IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ì •ì˜í•˜ê³ ,
    # ì´ìƒì¹˜ í¬í•¨/ì œì™¸ ë°ì´í„°ë¡œ ê°ê° ëª¨ë¸ì„ í•™ìŠµí•´ë³´ì„¸ìš”
    
    pass
```

---

## ğŸ“š í•µì‹¬ ì •ë¦¬

ì´ë²ˆ Partì—ì„œ ë°°ìš´ í•µì‹¬ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### âœ… ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í•µì‹¬ í¬ì¸íŠ¸

1. **ê²°ì¸¡ì¹˜ ìœ í˜• ì´í•´**: MCAR, MAR, MNAR êµ¬ë¶„í•˜ì—¬ ì ì ˆí•œ ì²˜ë¦¬ ë°©ë²• ì„ íƒ
2. **êµ¬ì¡°ì  vs ì„ì˜ì  ê²°ì¸¡**: ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ê²°ì¸¡ì¹˜ì˜ ì˜ë¯¸ íŒŒì•…
3. **ì²˜ë¦¬ ë°©ë²• ì„ íƒ**: 
   - ë‹¨ìˆœ ì‚­ì œ â† ë°ì´í„° ì†ì‹¤ ìœ„í—˜
   - í†µê³„ì  ëŒ€ì²´ â† ë¹ ë¥´ê³  ì•ˆì „í•œ ê¸°ë³¸ ë°©ë²•  
   - ë„ë©”ì¸ ê¸°ë°˜ ëŒ€ì²´ â† ê°€ì¥ ì •í™•í•˜ê³  ì˜ë¯¸ìˆëŠ” ë°©ë²•
   - ì˜ˆì¸¡ ê¸°ë°˜ ëŒ€ì²´ â† ì •êµí•˜ì§€ë§Œ ë³µì¡í•˜ê³  ê³¼ì í•© ìœ„í—˜

### âœ… ì´ìƒì¹˜ ì²˜ë¦¬ í•µì‹¬ í¬ì¸íŠ¸

1. **íƒì§€ ë°©ë²•**: IQR, Z-Score, Modified Z-Score ë“± ë‹¤ì–‘í•œ ë°©ë²• ë³‘í–‰
2. **ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„**: ì´ìƒì¹˜ê°€ ì˜¤ë¥˜ì¸ì§€ ì‹¤ì œ ê·¹ë‹¨ê°’ì¸ì§€ íŒë‹¨
3. **ì²˜ë¦¬ ì „ëµ**:
   - ì œê±° â† ëª…í™•í•œ ì˜¤ë¥˜ì¸ ê²½ìš°
   - ìº¡í•‘/ìœˆì €í™” â† ê·¹ë‹¨ê°’ ì˜í–¥ ì™„í™”
   - ë³€í™˜ â† ë¶„í¬ ì •ê·œí™” 
   - ìœ ì§€ â† ì¤‘ìš”í•œ ì •ë³´ì¸ ê²½ìš°

### ğŸ’¡ ì‹¤ë¬´ ì ìš© íŒ

- **í•­ìƒ ì›ë³¸ ë°ì´í„° ë³´ì¡´**: ì²˜ë¦¬ ê³¼ì •ì„ ë˜ëŒë¦´ ìˆ˜ ìˆë„ë¡ ë³µì‚¬ë³¸ ì‚¬ìš©
- **ì²˜ë¦¬ íš¨ê³¼ ê²€ì¦**: ì²˜ë¦¬ ì „í›„ ë°ì´í„° ë¶„í¬ì™€ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
- **ë¬¸ì„œí™”**: ì²˜ë¦¬ ì´ìœ ì™€ ë°©ë²•ì„ ëª…í™•íˆ ê¸°ë¡
- **ë‹¨ê³„ì  ì ‘ê·¼**: í•œ ë²ˆì— ëª¨ë“  ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ í•˜ì§€ ë§ê³  ë‹¨ê³„ë³„ ì²˜ë¦¬

---

## ğŸ¤” ìƒê°í•´ë³´ê¸°

1. **êµ¬ì¡°ì  ê²°ì¸¡ì¹˜ì˜ ì •ë³´ì  ê°€ì¹˜**: ì§€í•˜ì‹¤ì´ë‚˜ ì°¨ê³ ê°€ ì—†ëŠ” ì£¼íƒì˜ ì •ë³´ê°€ ì–´ë–»ê²Œ ìœ ìš©í•  ìˆ˜ ìˆì„ê¹Œìš”? ì´ëŸ° ì •ë³´ë¥¼ ìƒˆë¡œìš´ íŠ¹ì„±ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¼ê¹Œìš”?

2. **ì´ìƒì¹˜ì˜ ì–‘ë©´ì„±**: ë§¤ìš° í° ì£¼íƒì´ë‚˜ ë§¤ìš° ë¹„ì‹¼ ì£¼íƒì´ ì´ìƒì¹˜ë¡œ íƒì§€ë˜ì—ˆì„ ë•Œ, ì´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì´ í•­ìƒ ì˜³ì€ ì„ íƒì¼ê¹Œìš”? ì–´ë–¤ ê²½ìš°ì— ì´ìƒì¹˜ë¥¼ ë³´ì¡´í•´ì•¼ í• ê¹Œìš”?

3. **ì²˜ë¦¬ ë°©ë²•ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„**: ì •êµí•œ ì˜ˆì¸¡ ê¸°ë°˜ ëŒ€ì²´ ë°©ë²•ì´ í•­ìƒ ë‹¨ìˆœí•œ í‰ê· ê°’ ëŒ€ì²´ë³´ë‹¤ ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì¥í• ê¹Œìš”? ê° ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ì‹¤ë¬´ ê´€ì ì—ì„œ ìƒê°í•´ë³´ì„¸ìš”.

---

## ğŸ”œ ë‹¤ìŒ Part ì˜ˆê³ : ë°ì´í„° ë³€í™˜ê³¼ ì •ê·œí™”

ë‹¤ìŒ Partì—ì„œëŠ” ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì— ì í•©í•˜ë„ë¡ ë³€í™˜í•˜ëŠ” ê¸°ë²•ë“¤ì„ ë°°ì›ë‹ˆë‹¤:

- **ìŠ¤ì¼€ì¼ë§ ê¸°ë²•**: ë³€ìˆ˜ ê°„ í¬ê¸° ì°¨ì´ í•´ê²° (StandardScaler, MinMaxScaler, RobustScaler)
- **ì¸ì½”ë”© ê¸°ë²•**: ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ (One-Hot, Label, Target Encoding)  
- **ë¶„í¬ ë³€í™˜**: ì™œë„ ì œê±°ì™€ ì •ê·œë¶„í¬ ê·¼ì‚¬ (ë¡œê·¸ë³€í™˜, Box-Cox ë³€í™˜)
- **ì‹¤ì „ íŒŒì´í”„ë¼ì¸**: scikit-learn Pipelineì„ í™œìš©í•œ ì²´ê³„ì  ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°

ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì „ë¬¸ì ì¸ ë³€í™˜ ê¸°ë²•ë“¤ì„ ë§ˆìŠ¤í„°í•´ë³´ê² ìŠµë‹ˆë‹¤!

---

*"ì¢‹ì€ ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì¢‹ì€ ëª¨ë¸ì˜ ì²«ê±¸ìŒì…ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ë¥¼ ë‹¨ìˆœí•œ ë¬¸ì œê°€ ì•„ë‹Œ ë°ì´í„°ê°€ ì „í•˜ëŠ” ì •ë³´ë¡œ í•´ì„í•˜ëŠ” ì•ˆëª©ì„ ê¸°ë¥´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."*

