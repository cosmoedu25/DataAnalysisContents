# 7ì¥ Part 2: AI ìƒì„± ì½”ë“œ ê²€ì¦ ë° ìµœì í™”
**ë¶€ì œ: AI ì½”ë“œë¥¼ ë¯¿ì–´ë„ ë ê¹Œ? - ì²´ê³„ì ì¸ ê²€ì¦ê³¼ ê°œì„  ì „ëµ**

## í•™ìŠµ ëª©í‘œ
ì´ Partë¥¼ ì™„ë£Œí•œ í›„, ì—¬ëŸ¬ë¶„ì€ ë‹¤ìŒì„ í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤:
- AIê°€ ìƒì„±í•œ ì½”ë“œì˜ ì¼ë°˜ì ì¸ ì˜¤ë¥˜ íŒ¨í„´ì„ ì‹ë³„í•˜ê³  íƒì§€í•  ìˆ˜ ìˆë‹¤
- ì½”ë“œ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ê¸°ì¤€ê³¼ ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤
- ì„±ëŠ¥, ê°€ë…ì„±, ì•ˆì •ì„± ê´€ì ì—ì„œ ì½”ë“œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆë‹¤
- ë³´ì•ˆê³¼ ì•ˆì •ì„±ì„ ê³ ë ¤í•œ ì½”ë“œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜ë¦½í•  ìˆ˜ ìˆë‹¤

## ì´ë²ˆ Part ë¯¸ë¦¬ë³´ê¸°
AIê°€ ìƒì„±í•œ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§ˆì¹˜ ìë™ì°¨ì˜ ììœ¨ì£¼í–‰ ê¸°ëŠ¥ì„ ë§¹ì‹ í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì˜ ì‘ë™í•˜ì§€ë§Œ, ì˜ˆìƒì¹˜ ëª»í•œ ìƒí™©ì—ì„œëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 

ë˜‘ë˜‘í•œ ìš´ì „ìê°€ ììœ¨ì£¼í–‰ ì¤‘ì—ë„ í•­ìƒ ì£¼ì˜ë¥¼ ê¸°ìš¸ì´ë“¯ì´, ë°ì´í„° ë¶„ì„ê°€ë„ AIê°€ ìƒì„±í•œ ì½”ë“œë¥¼ ë¹„íŒì ìœ¼ë¡œ ê²€í† í•˜ê³  ê°œì„ í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

ì´ë²ˆ Partì—ì„œëŠ” AI ìƒì„± ì½”ë“œì˜ ìˆ¨ê²¨ì§„ í•¨ì •ë“¤ì„ ë°œê²¬í•˜ê³ , ì²´ê³„ì ì¸ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ì½”ë“œë¡œ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. SMS ìŠ¤íŒ¸ íƒì§€ í”„ë¡œì íŠ¸ë¥¼ ê³„ì† í™œìš©í•˜ì—¬ ì‹¤ì œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ì½”ë“œ í’ˆì§ˆ ì´ìŠˆë“¤ì„ ì§ì ‘ ê²½í—˜í•´ë³´ê² ìŠµë‹ˆë‹¤.

---

> ğŸ“ **ì¤‘ìš” ìš©ì–´**: **ì½”ë“œ ê²€ì¦(Code Verification)**
> 
> ì‘ì„±ëœ ì½”ë“œê°€ ì˜ë„í•œ ëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€, íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œì§€ë¥¼ ì²´ê³„ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ê¸°ëŠ¥ì„±, ì„±ëŠ¥, ê°€ë…ì„±, ë³´ì•ˆ, ìœ ì§€ë³´ìˆ˜ì„± ë“± ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ì½”ë“œë¥¼ í‰ê°€í•˜ê³  ê°œì„ ì ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

## 1. AI ìƒì„± ì½”ë“œì˜ ì¼ë°˜ì  ì˜¤ë¥˜ íŒ¨í„´

AIëŠ” ë§¤ìš° ë˜‘ë˜‘í•˜ì§€ë§Œ ì™„ë²½í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ë§ˆì¹˜ ì¬ëŠ¥ ìˆëŠ” ì‹ ì… ê°œë°œìì²˜ëŸ¼, ê¸°ë³¸ì ì¸ ì½”ë”©ì€ ì˜í•˜ì§€ë§Œ ì„¸ë¶€ì ì¸ ë¶€ë¶„ì—ì„œ ì‹¤ìˆ˜ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” AIê°€ ìì£¼ ë²”í•˜ëŠ” ì˜¤ë¥˜ íŒ¨í„´ë“¤ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### 1.1 ê¸°ëŠ¥ì  ì˜¤ë¥˜ (Functional Errors)

#### **ì˜¤ë¥˜ ìœ í˜• 1: ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ë¶€ì¡±**

AIëŠ” ì¼ë°˜ì ì¸ ìƒí™©ì— ëŒ€í•´ì„œëŠ” ì˜ ì‘ë™í•˜ëŠ” ì½”ë“œë¥¼ ìƒì„±í•˜ì§€ë§Œ, ê·¹ë‹¨ì ì´ê±°ë‚˜ ì˜ˆì™¸ì ì¸ ìƒí™©(ì—£ì§€ ì¼€ì´ìŠ¤)ì„ ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

**âŒ AI ìƒì„± ì½”ë“œ ì˜ˆì‹œ (ë¬¸ì œê°€ ìˆëŠ” ë²„ì „):**
```python
def calculate_sms_statistics(messages):
    """SMS ë©”ì‹œì§€ í†µê³„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
    total_chars = sum(len(msg) for msg in messages)
    avg_length = total_chars / len(messages)
    
    word_counts = [len(msg.split()) for msg in messages]
    avg_words = sum(word_counts) / len(word_counts)
    
    return {
        'total_messages': len(messages),
        'avg_char_length': avg_length,
        'avg_word_count': avg_words
    }

# í…ŒìŠ¤íŠ¸
messages = ["Hello world", "How are you?", "FREE money now!!!"]
stats = calculate_sms_statistics(messages)
print(stats)
```

**ğŸš¨ ë¬¸ì œì  ë¶„ì„:**
1. **ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì•ˆë¨**: `messages`ê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ `ZeroDivisionError` ë°œìƒ
2. **None ê°’ ì²˜ë¦¬ ì•ˆë¨**: ë©”ì‹œì§€ ì¤‘ `None`ì´ ìˆìœ¼ë©´ `TypeError` ë°œìƒ
3. **ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬ ë¯¸í¡**: ë¹ˆ ë©”ì‹œì§€ê°€ ìˆì–´ë„ ì—ëŸ¬ëŠ” ì—†ì§€ë§Œ ë¶€ì •í™•í•œ í†µê³„

**âœ… ê°œì„ ëœ ë²„ì „:**
```python
def calculate_sms_statistics(messages):
    """
    SMS ë©”ì‹œì§€ í†µê³„ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ (ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ í¬í•¨)
    
    Args:
        messages (list): SMS ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        dict: í†µê³„ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        
    Raises:
        ValueError: ì…ë ¥ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    
    # ì…ë ¥ ê²€ì¦
    if not isinstance(messages, list):
        raise ValueError("messagesëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    if len(messages) == 0:
        return {
            'total_messages': 0,
            'avg_char_length': 0,
            'avg_word_count': 0,
            'warning': 'Empty message list'
        }
    
    # None ê°’ê³¼ ë¹„ë¬¸ìì—´ ì œê±°
    valid_messages = []
    for msg in messages:
        if msg is not None and isinstance(msg, str):
            valid_messages.append(msg)
    
    if len(valid_messages) == 0:
        return {
            'total_messages': len(messages),
            'valid_messages': 0,
            'avg_char_length': 0,
            'avg_word_count': 0,
            'warning': 'No valid string messages found'
        }
    
    # í†µê³„ ê³„ì‚°
    total_chars = sum(len(msg) for msg in valid_messages)
    avg_length = total_chars / len(valid_messages)
    
    word_counts = [len(msg.split()) for msg in valid_messages if len(msg.strip()) > 0]
    avg_words = sum(word_counts) / len(word_counts) if word_counts else 0
    
    return {
        'total_messages': len(messages),
        'valid_messages': len(valid_messages),
        'avg_char_length': round(avg_length, 2),
        'avg_word_count': round(avg_words, 2)
    }

# ë‹¤ì–‘í•œ ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
test_cases = [
    [],  # ë¹ˆ ë¦¬ìŠ¤íŠ¸
    [None, "", "Hello"],  # Noneê³¼ ë¹ˆ ë¬¸ìì—´ í¬í•¨
    ["", "   ", "\n"],  # ê³µë°±ë§Œ ìˆëŠ” ë©”ì‹œì§€ë“¤
    ["Normal message", "Another one"]  # ì •ìƒ ì¼€ì´ìŠ¤
]

for i, test_case in enumerate(test_cases, 1):
    try:
        result = calculate_sms_statistics(test_case)
        print(f"í…ŒìŠ¤íŠ¸ {i}: {result}")
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ {i} ì—ëŸ¬: {e}")
```

**ì½”ë“œ í•´ì„¤:**
- `isinstance()` ì²´í¬ë¡œ íƒ€ì… ì•ˆì „ì„± í™•ë³´
- ë‹¨ê³„ë³„ ê²€ì¦ìœ¼ë¡œ ê° ì—£ì§€ ì¼€ì´ìŠ¤ ëŒ€ì‘
- ì˜ë¯¸ ìˆëŠ” ì—ëŸ¬ ë©”ì‹œì§€ì™€ ê²½ê³  ì •ë³´ ì œê³µ
- `round()` í•¨ìˆ˜ë¡œ ê°€ë…ì„± ìˆëŠ” ì†Œìˆ˜ì  í‘œì‹œ

#### **ì˜¤ë¥˜ ìœ í˜• 2: ë°ì´í„° íƒ€ì… í˜¼ë™**

AIëŠ” ë•Œë•Œë¡œ pandas DataFrameê³¼ ë¦¬ìŠ¤íŠ¸, ë˜ëŠ” numpy ë°°ì—´ê³¼ ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í˜¼ë™í•˜ì—¬ ì˜ëª»ëœ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤.

**âŒ ë¬¸ì œê°€ ìˆëŠ” AI ì½”ë“œ:**
```python
def preprocess_sms_data(data):
    """SMS ë°ì´í„° ì „ì²˜ë¦¬ (ì˜ëª»ëœ ë²„ì „)"""
    
    # ë°ì´í„° íƒ€ì…ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ì²˜ë¦¬
    cleaned_messages = []
    for message in data:
        # DataFrameì¸ ê²½ìš°ì™€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°ë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠìŒ
        clean_msg = message.lower().strip()  # AttributeError ìœ„í—˜!
        cleaned_messages.append(clean_msg)
    
    return cleaned_messages
```

**âœ… ê°œì„ ëœ ë²„ì „:**
```python
import pandas as pd
import numpy as np

def preprocess_sms_data(data, text_column='message'):
    """
    SMS ë°ì´í„° ì „ì²˜ë¦¬ (íƒ€ì… ì•ˆì „ ë²„ì „)
    
    Args:
        data: pandas DataFrame, list, numpy array, ë˜ëŠ” ë‹¨ì¼ ë¬¸ìì—´
        text_column: DataFrameì¸ ê²½ìš° í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì»¬ëŸ¼ëª…
        
    Returns:
        list: ì „ì²˜ë¦¬ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    """
    
    cleaned_messages = []
    
    # ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬
    if isinstance(data, pd.DataFrame):
        if text_column not in data.columns:
            raise ValueError(f"ì»¬ëŸ¼ '{text_column}'ê°€ DataFrameì— ì—†ìŠµë‹ˆë‹¤")
        messages = data[text_column].fillna('').astype(str)
        
    elif isinstance(data, (list, np.ndarray)):
        messages = [str(msg) if msg is not None else '' for msg in data]
        
    elif isinstance(data, str):
        messages = [data]
        
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data)}")
    
    # ì•ˆì „í•œ ì „ì²˜ë¦¬
    for message in messages:
        try:
            clean_msg = str(message).lower().strip()
            # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            if clean_msg:
                cleaned_messages.append(clean_msg)
        except Exception as e:
            print(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {message} -> {e}")
            continue
    
    return cleaned_messages

# ë‹¤ì–‘í•œ íƒ€ì… í…ŒìŠ¤íŠ¸
import pandas as pd

# DataFrame í…ŒìŠ¤íŠ¸
df_data = pd.DataFrame({'message': ['Hello', 'WORLD', '  spaces  '], 'label': [0, 1, 0]})
result1 = preprocess_sms_data(df_data, 'message')
print(f"DataFrame ê²°ê³¼: {result1}")

# ë¦¬ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸  
list_data = ['Hello', 'WORLD', None, '  spaces  ']
result2 = preprocess_sms_data(list_data)
print(f"ë¦¬ìŠ¤íŠ¸ ê²°ê³¼: {result2}")

# ë‹¨ì¼ ë¬¸ìì—´ í…ŒìŠ¤íŠ¸
single_data = "Single Message"
result3 = preprocess_sms_data(single_data)
print(f"ë‹¨ì¼ ë¬¸ìì—´ ê²°ê³¼: {result3}")
```

**ì½”ë“œ í•´ì„¤:**
- `isinstance()` ì²´í¬ë¡œ ì…ë ¥ ë°ì´í„° íƒ€ì… í™•ì¸
- íƒ€ì…ë³„ ì ì ˆí•œ ì²˜ë¦¬ ë¡œì§ ë¶„ë¦¬
- ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬ì™€ ì˜ë¯¸ ìˆëŠ” ì—ëŸ¬ ë©”ì‹œì§€
- `fillna()`, `astype()` ë“± pandas ë©”ì„œë“œì˜ ì•ˆì „í•œ ì‚¬ìš©

### 1.2 ì„±ëŠ¥ ê´€ë ¨ ì˜¤ë¥˜

#### **ì˜¤ë¥˜ ìœ í˜• 3: ë¹„íš¨ìœ¨ì ì¸ ë°˜ë³µë¬¸**

AIëŠ” ì¢…ì¢… ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ì½”ë“œë¥¼ ìƒì„±í•˜ì§€ë§Œ, ì„±ëŠ¥ ìµœì í™”ëŠ” ë†“ì¹˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.

**âŒ ë¹„íš¨ìœ¨ì ì¸ AI ì½”ë“œ:**
```python
def extract_spam_keywords(messages, spam_keywords):
    """ìŠ¤íŒ¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ë¹„íš¨ìœ¨ì  ë²„ì „)"""
    
    results = []
    for message in messages:
        keyword_count = 0
        found_keywords = []
        
        # ì´ì¤‘ ë°˜ë³µë¬¸ - O(n*m) ë³µì¡ë„
        for keyword in spam_keywords:
            if keyword.lower() in message.lower():
                keyword_count += 1
                found_keywords.append(keyword)
        
        results.append({
            'message': message,
            'keyword_count': keyword_count,
            'found_keywords': found_keywords
        })
    
    return results

# í…ŒìŠ¤íŠ¸ (ì„±ëŠ¥ ì¸¡ì •)
import time

spam_keywords = ['free', 'win', 'cash', 'prize', 'urgent', 'limited', 'offer', 'click', 'call', 'now']
sample_messages = ["FREE cash prize! Call NOW!", "Hello, how are you?"] * 1000  # í° ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜

start_time = time.time()
result = extract_spam_keywords(sample_messages, spam_keywords)
end_time = time.time()

print(f"ë¹„íš¨ìœ¨ì  ë²„ì „ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")
```

**âœ… ìµœì í™”ëœ ë²„ì „:**
```python
import re
from collections import defaultdict

def extract_spam_keywords_optimized(messages, spam_keywords):
    """
    ìŠ¤íŒ¸ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœì í™” ë²„ì „)
    
    ì •ê·œí‘œí˜„ì‹ê³¼ ì§‘í•© ì—°ì‚°ì„ í™œìš©í•œ ì„±ëŠ¥ ê°œì„ 
    """
    
    # í‚¤ì›Œë“œë¥¼ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ìœ¼ë¡œ ì»´íŒŒì¼ (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    keyword_pattern = re.compile(
        r'\b(' + '|'.join(re.escape(keyword.lower()) for keyword in spam_keywords) + r')\b',
        re.IGNORECASE
    )
    
    results = []
    
    for message in messages:
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í•œ ë²ˆì— ëª¨ë“  í‚¤ì›Œë“œ ì°¾ê¸°
        found_matches = keyword_pattern.findall(message.lower())
        
        # ì¤‘ë³µ ì œê±° ë° ì¹´ìš´íŠ¸
        unique_keywords = list(set(found_matches))
        keyword_count = len(found_matches)
        
        results.append({
            'message': message,
            'keyword_count': keyword_count,
            'found_keywords': unique_keywords
        })
    
    return results

# ë” ë‚˜ì€ ìµœì í™” ë²„ì „ (ë²¡í„°í™”)
import pandas as pd

def extract_spam_keywords_vectorized(messages, spam_keywords):
    """
    ë²¡í„°í™”ëœ ìŠ¤íŒ¸ í‚¤ì›Œë“œ ì¶”ì¶œ (pandas í™œìš©)
    
    ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ìµœì í™”ëœ ë²„ì „
    """
    
    df = pd.DataFrame({'message': messages})
    df['message_lower'] = df['message'].str.lower()
    
    # ê° í‚¤ì›Œë“œë³„ í¬í•¨ ì—¬ë¶€ ì²´í¬ (ë²¡í„°í™”)
    keyword_columns = {}
    for keyword in spam_keywords:
        col_name = f'has_{keyword}'
        keyword_columns[col_name] = df['message_lower'].str.contains(
            r'\b' + re.escape(keyword.lower()) + r'\b', 
            regex=True, 
            na=False
        )
    
    keyword_df = pd.DataFrame(keyword_columns)
    
    # í†µê³„ ê³„ì‚°
    df['keyword_count'] = keyword_df.sum(axis=1)
    
    # ì°¾ì€ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    def get_found_keywords(row_idx):
        found = []
        for i, keyword in enumerate(spam_keywords):
            col_name = f'has_{keyword}'
            if keyword_columns[col_name].iloc[row_idx]:
                found.append(keyword)
        return found
    
    df['found_keywords'] = [get_found_keywords(i) for i in range(len(df))]
    
    return df[['message', 'keyword_count', 'found_keywords']].to_dict('records')

# ì„±ëŠ¥ ë¹„êµ
start_time = time.time()
result_optimized = extract_spam_keywords_optimized(sample_messages, spam_keywords)
end_time = time.time()
print(f"ìµœì í™” ë²„ì „ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")

start_time = time.time()
result_vectorized = extract_spam_keywords_vectorized(sample_messages, spam_keywords)
end_time = time.time()
print(f"ë²¡í„°í™” ë²„ì „ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")

# ê²°ê³¼ í™•ì¸ (ì²« 3ê°œë§Œ)
print("\nê²°ê³¼ ì˜ˆì‹œ:")
for i in range(3):
    print(f"ë©”ì‹œì§€ {i+1}: {result_optimized[i]}")
```

**ì½”ë“œ í•´ì„¤:**
- **ì •ê·œí‘œí˜„ì‹ ì»´íŒŒì¼**: íŒ¨í„´ì„ í•œ ë²ˆë§Œ ì»´íŒŒì¼í•˜ì—¬ ë°˜ë³µ ì‚¬ìš©
- **ë²¡í„°í™” ì—°ì‚°**: pandasì˜ ë²¡í„°í™”ëœ ë¬¸ìì—´ ì—°ì‚° í™œìš©
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ë¶ˆí•„ìš”í•œ ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ìµœì†Œí™”
- **ì‹œê°„ ë³µì¡ë„ ê°œì„ **: O(n*m)ì—ì„œ O(n)ìœ¼ë¡œ ê°œì„ 

### 1.3 ë¡œì§ ì˜¤ë¥˜ (Logic Errors)

#### **ì˜¤ë¥˜ ìœ í˜• 4: ì˜ëª»ëœ ê°€ì •**

AIëŠ” ë¬¸ì œë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ í•´ê²°í•˜ë ¤ëŠ” ê²½í–¥ì´ ìˆì–´, ì‹¤ì œ ë°ì´í„°ì˜ ë³µì¡ì„±ì„ ê°„ê³¼í•˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤.

**âŒ ì˜ëª»ëœ ê°€ì •ì„ í¬í•¨í•œ ì½”ë“œ:**
```python
def classify_sms_simple(message):
    """
    ê°„ë‹¨í•œ SMS ë¶„ë¥˜ê¸° (ì˜ëª»ëœ ê°€ì • í¬í•¨)
    
    ê°€ì •: ìŠ¤íŒ¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ìŠ¤íŒ¸, ì—†ìœ¼ë©´ ì •ìƒ
    """
    
    spam_keywords = ['free', 'win', 'money', 'prize', 'call']
    
    message_lower = message.lower()
    
    # ë„ˆë¬´ ë‹¨ìˆœí•œ ê·œì¹™
    for keyword in spam_keywords:
        if keyword in message_lower:
            return 'spam'
    
    return 'ham'

# ë¬¸ì œê°€ ë˜ëŠ” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
test_messages = [
    "I won the lottery!",  # 'win' ë•Œë¬¸ì— ìŠ¤íŒ¸ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜
    "Call me when you're free",  # 'call', 'free' ë•Œë¬¸ì— ìŠ¤íŒ¸ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜  
    "The money is in the bank",  # 'money' ë•Œë¬¸ì— ìŠ¤íŒ¸ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜
    "Win the game tonight!",  # 'win' ë•Œë¬¸ì— ìŠ¤íŒ¸ìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜
    "FREE unlimited data plan"  # ì‹¤ì œ ìŠ¤íŒ¸ì´ì§€ë§Œ ë„ˆë¬´ ë‹¨ìˆœí•œ ê·œì¹™
]

print("ê°„ë‹¨í•œ ë¶„ë¥˜ê¸° ê²°ê³¼:")
for msg in test_messages:
    result = classify_sms_simple(msg)
    print(f"'{msg}' -> {result}")
```

**âœ… ê°œì„ ëœ ë²„ì „ (ë§¥ë½ ê³ ë ¤):**
```python
import re
from collections import defaultdict

def classify_sms_advanced(message):
    """
    ê³ ê¸‰ SMS ë¶„ë¥˜ê¸° (ë§¥ë½ê³¼ íŒ¨í„´ ê³ ë ¤)
    
    ì—¬ëŸ¬ ì‹ í˜¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ë¶„ë¥˜
    """
    
    # ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ìŠ¤íŒ¸ ì‹ í˜¸ë“¤
    spam_indicators = {
        'urgent_words': {
            'words': ['urgent', 'immediate', 'expires', 'limited time', 'act now'],
            'weight': 3
        },
        'money_related': {
            'words': ['free', 'cash', 'money', 'prize', '

, 'Â£', 'â‚¬'],
            'weight': 2
        },
        'action_words': {
            'words': ['call now', 'click here', 'text back', 'reply stop'],
            'weight': 2
        },
        'suspicious_patterns': {
            'patterns': [
                r'\b(win|won)\s+[$Â£â‚¬]\d+',  # "win $100" íŒ¨í„´
                r'\d{4,}',  # ê¸´ ìˆ«ì (ì „í™”ë²ˆí˜¸ ë“±)
                r'[A-Z]{3,}',  # ì—°ì† ëŒ€ë¬¸ì
                r'[!]{2,}',  # ì—°ì† ëŠë‚Œí‘œ
            ],
            'weight': 2
        }
    }
    
    message_lower = message.lower()
    spam_score = 0
    found_indicators = []
    
    # ë‹¨ì–´ ê¸°ë°˜ ì‹ í˜¸ í™•ì¸
    for category, info in spam_indicators.items():
        if 'words' in info:
            for word in info['words']:
                # ë‹¨ì–´ ê²½ê³„ ê³ ë ¤ ì •ê·œí‘œí˜„ì‹
                pattern = r'\b' + re.escape(word.lower()) + r'\b'
                if re.search(pattern, message_lower):
                    spam_score += info['weight']
                    found_indicators.append(f"{category}: {word}")
        
        # íŒ¨í„´ ê¸°ë°˜ ì‹ í˜¸ í™•ì¸
        elif 'patterns' in info:
            for pattern in info['patterns']:
                if re.search(pattern, message, re.IGNORECASE):
                    spam_score += info['weight']
                    found_indicators.append(f"{category}: pattern match")
    
    # ì¶”ê°€ íœ´ë¦¬ìŠ¤í‹±
    # ë©”ì‹œì§€ ê¸¸ì´ ê³ ë ¤
    if len(message) > 100:
        spam_score += 1
        found_indicators.append("length: long message")
    
    # ëŒ€ë¬¸ì ë¹„ìœ¨ ê³ ë ¤  
    upper_ratio = sum(1 for c in message if c.isupper()) / len(message)
    if upper_ratio > 0.3:
        spam_score += 2
        found_indicators.append("style: high uppercase ratio")
    
    # ìˆ«ì ë°€ë„ ê³ ë ¤
    digit_ratio = sum(1 for c in message if c.isdigit()) / len(message)
    if digit_ratio > 0.1:
        spam_score += 1
        found_indicators.append("content: high digit ratio")
    
    # ë¶„ë¥˜ ê²°ì • (ì„ê³„ê°’ ê¸°ë°˜)
    threshold = 4
    classification = 'spam' if spam_score >= threshold else 'ham'
    
    return {
        'classification': classification,
        'spam_score': spam_score,
        'indicators': found_indicators,
        'confidence': min(1.0, spam_score / 10)  # 0-1 ì‹ ë¢°ë„
    }

# í…ŒìŠ¤íŠ¸
print("ê³ ê¸‰ ë¶„ë¥˜ê¸° ê²°ê³¼:")
for msg in test_messages:
    result = classify_sms_advanced(msg)
    print(f"'{msg}'")
    print(f"  -> {result['classification']} (ì ìˆ˜: {result['spam_score']}, ì‹ ë¢°ë„: {result['confidence']:.2f})")
    print(f"  -> ì§€í‘œ: {result['indicators']}")
    print()
```

**ì½”ë“œ í•´ì„¤:**
- **ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ**: ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ë§¤ì¹­ ëŒ€ì‹  ì‹ í˜¸ì˜ ì¤‘ìš”ë„ ì°¨ë³„í™”
- **íŒ¨í„´ ì¸ì‹**: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ë³µì¡í•œ ìŠ¤íŒ¸ íŒ¨í„´ íƒì§€
- **ë§¥ë½ ê³ ë ¤**: ë‹¨ì–´ ê²½ê³„, ë©”ì‹œì§€ êµ¬ì¡° ë“± ì¢…í•©ì  íŒë‹¨
- **ì‹ ë¢°ë„ ì œê³µ**: ë¶„ë¥˜ ê²°ê³¼ì˜ í™•ì‹¤ì„± ì •ë„ í‘œì‹œ

> ğŸ’¡ **AI ì½”ë“œ ì˜¤ë¥˜ íƒì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸**
> 
> **ê¸°ëŠ¥ì  ì¸¡ë©´:**
> - [ ] ë¹ˆ ì…ë ¥, None ê°’ ì²˜ë¦¬
> - [ ] ë°ì´í„° íƒ€ì… ê²€ì¦
> - [ ] ì˜ˆì™¸ ìƒí™© í•¸ë“¤ë§
> 
> **ì„±ëŠ¥ì  ì¸¡ë©´:**
> - [ ] ë°˜ë³µë¬¸ íš¨ìœ¨ì„±
> - [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
> - [ ] ë²¡í„°í™” ê°€ëŠ¥ì„±
> 
> **ë…¼ë¦¬ì  ì¸¡ë©´:**
> - [ ] ê°€ì •ì˜ íƒ€ë‹¹ì„±
> - [ ] ì—£ì§€ ì¼€ì´ìŠ¤ ê³ ë ¤
> - [ ] ê²°ê³¼ì˜ í˜„ì‹¤ì„±

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "AI ì½”ë“œ ì˜¤ë¥˜ íŒ¨í„´ì„ ë³´ì—¬ì£¼ëŠ” ì¸í¬ê·¸ë˜í”½. ê¸°ëŠ¥ì  ì˜¤ë¥˜(ë¹¨ê°„ìƒ‰), ì„±ëŠ¥ ì˜¤ë¥˜(ì£¼í™©ìƒ‰), ë…¼ë¦¬ ì˜¤ë¥˜(ë…¸ë€ìƒ‰)ë¡œ êµ¬ë¶„ë˜ì–´ ìˆê³ , ê° ì˜¤ë¥˜ ìœ í˜•ë³„ë¡œ ëŒ€í‘œì ì¸ ì˜ˆì‹œ ì•„ì´ì½˜ê³¼ í•´ê²°ì±…ì´ í‘œì‹œëœ ëª¨ë˜í•œ ë‹¤ì´ì–´ê·¸ë¨"

## 2. ì½”ë“œ í’ˆì§ˆ í‰ê°€ ê¸°ì¤€ê³¼ ìë™í™” ë„êµ¬

ì½”ë“œ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ë§ˆì¹˜ ìŒì‹ì˜ ë§›ì„ í‰ê°€í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ë§›, ì˜ì–‘, ëª¨ì–‘, ê°€ê²© ë“± ì—¬ëŸ¬ ê¸°ì¤€ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì½”ë“œë„ ë§ˆì°¬ê°€ì§€ë¡œ ê¸°ëŠ¥ì„±, ì„±ëŠ¥, ê°€ë…ì„±, ìœ ì§€ë³´ìˆ˜ì„± ë“± ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

### 2.1 ì½”ë“œ í’ˆì§ˆ í‰ê°€ì˜ 5ê°€ì§€ ì°¨ì›

#### **ì°¨ì› 1: ê¸°ëŠ¥ì  ì •í™•ì„± (Functional Correctness)**
ì½”ë“œê°€ ì˜ë„í•œ ëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.

```python
class CodeFunctionalityTester:
    """ì½”ë“œì˜ ê¸°ëŠ¥ì  ì •í™•ì„±ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.test_results = []
    
    def test_function(self, func, test_cases, description=""):
        """
        í•¨ìˆ˜ì˜ ê¸°ëŠ¥ì„ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ê²€ì¦
        
        Args:
            func: í…ŒìŠ¤íŠ¸í•  í•¨ìˆ˜
            test_cases: [(ì…ë ¥, ì˜ˆìƒì¶œë ¥), ...] í˜•íƒœì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            description: í…ŒìŠ¤íŠ¸ ì„¤ëª…
        """
        print(f"\nğŸ§ª {description} í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 50)
        
        passed = 0
        failed = 0
        
        for i, (inputs, expected) in enumerate(test_cases, 1):
            try:
                # í•¨ìˆ˜ ì‹¤í–‰
                if isinstance(inputs, tuple):
                    result = func(*inputs)
                else:
                    result = func(inputs)
                
                # ê²°ê³¼ ë¹„êµ
                if self._compare_results(result, expected):
                    print(f"âœ… í…ŒìŠ¤íŠ¸ {i}: PASS")
                    passed += 1
                else:
                    print(f"âŒ í…ŒìŠ¤íŠ¸ {i}: FAIL")
                    print(f"   ì…ë ¥: {inputs}")
                    print(f"   ì˜ˆìƒ: {expected}")
                    print(f"   ì‹¤ì œ: {result}")
                    failed += 1
                    
            except Exception as e:
                print(f"ğŸ’¥ í…ŒìŠ¤íŠ¸ {i}: ERROR - {e}")
                print(f"   ì…ë ¥: {inputs}")
                failed += 1
        
        # ê²°ê³¼ ìš”ì•½
        total = passed + failed
        success_rate = passed / total if total > 0 else 0
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼: {passed}/{total} í†µê³¼ ({success_rate:.1%})")
        
        self.test_results.append({
            'description': description,
            'passed': passed,
            'failed': failed,
            'success_rate': success_rate
        })
        
        return success_rate
    
    def _compare_results(self, actual, expected):
        """ê²°ê³¼ ë¹„êµ (íƒ€ì…ê³¼ ê°’ ëª¨ë‘ ê³ ë ¤)"""
        if type(actual) != type(expected):
            return False
        
        if isinstance(expected, dict):
            return all(actual.get(k) == v for k, v in expected.items())
        elif isinstance(expected, (list, tuple)):
            return len(actual) == len(expected) and all(a == e for a, e in zip(actual, expected))
        else:
            return actual == expected

# SMS ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ
def calculate_message_length(message):
    """SMS ë©”ì‹œì§€ ê¸¸ì´ ê³„ì‚°"""
    if not isinstance(message, str):
        raise ValueError("ë©”ì‹œì§€ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    return len(message.strip())

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
tester = CodeFunctionalityTester()

# ì •ìƒ ì¼€ì´ìŠ¤ë¶€í„° ì—£ì§€ ì¼€ì´ìŠ¤ê¹Œì§€ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸
test_cases = [
    ("Hello", 5),                    # ì •ìƒ ì¼€ì´ìŠ¤
    ("  Hello  ", 5),                # ê³µë°± ì œê±°
    ("", 0),                         # ë¹ˆ ë¬¸ìì—´
    ("   ", 0),                      # ê³µë°±ë§Œ
    ("HiğŸ‘‹", 3),                     # ì´ëª¨í‹°ì½˜ í¬í•¨
    ("ì•ˆë…•í•˜ì„¸ìš”", 5),                # í•œê¸€
]

# ì—ëŸ¬ ì¼€ì´ìŠ¤ (ì˜ˆì™¸ ë°œìƒ ì˜ˆìƒ)
error_cases = [
    (None, ValueError),              # None ì…ë ¥
    (123, ValueError),               # ìˆ«ì ì…ë ¥
    ([], ValueError),                # ë¦¬ìŠ¤íŠ¸ ì…ë ¥
]

# ì •ìƒ í…ŒìŠ¤íŠ¸
success_rate = tester.test_function(
    calculate_message_length, 
    test_cases, 
    "SMS ê¸¸ì´ ê³„ì‚° í•¨ìˆ˜"
)

# ì—ëŸ¬ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
print(f"\nğŸš¨ ì—ëŸ¬ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸")
for inputs, expected_error in error_cases:
    try:
        result = calculate_message_length(inputs)
        print(f"âŒ ì˜ˆìƒ ì—ëŸ¬ ë°œìƒ ì•ˆí•¨: {inputs} -> {result}")
    except expected_error:
        print(f"âœ… ì˜ˆìƒ ì—ëŸ¬ ë°œìƒ: {inputs} -> {expected_error.__name__}")
    except Exception as e:
        print(f"âš ï¸ ë‹¤ë¥¸ ì—ëŸ¬ ë°œìƒ: {inputs} -> {type(e).__name__}: {e}")
```

**ì½”ë“œ í•´ì„¤:**
- **í¬ê´„ì  í…ŒìŠ¤íŠ¸**: ì •ìƒ ì¼€ì´ìŠ¤ë¶€í„° ì—£ì§€ ì¼€ì´ìŠ¤, ì—ëŸ¬ ì¼€ì´ìŠ¤ê¹Œì§€ ì²´ê³„ì  ê²€ì¦
- **ìë™í™”ëœ ë¹„êµ**: ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì…ì— ëŒ€í•œ ê²°ê³¼ ë¹„êµ ë¡œì§
- **ëª…í™•í•œ í”¼ë“œë°±**: ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ì— ëŒ€í•œ ìƒì„¸í•œ ì •ë³´ ì œê³µ

#### **ì°¨ì› 2: ì„±ëŠ¥ íš¨ìœ¨ì„± (Performance Efficiency)**
ì½”ë“œì˜ ì‹¤í–‰ ì†ë„ì™€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¸¡ì •í•©ë‹ˆë‹¤.

```python
import time
import psutil
import memory_profiler
from functools import wraps
import matplotlib.pyplot as plt

class PerformanceProfiler:
    """ì½”ë“œ ì„±ëŠ¥ì„ í”„ë¡œíŒŒì¼ë§í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.profile_results = []
    
    def profile_function(self, func, test_data, description=""):
        """
        í•¨ìˆ˜ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤
        
        Args:
            func: ì¸¡ì •í•  í•¨ìˆ˜
            test_data: í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ë°ì´í„°
            description: í•¨ìˆ˜ ì„¤ëª…
        """
        print(f"\nâš¡ {description} ì„±ëŠ¥ ì¸¡ì •")
        print("=" * 50)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ì‹œì‘)
        process = psutil.Process()
        memory_before = process.memory_info().rss / 1024 / 1024  # MB
        
        # ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
        start_time = time.perf_counter()
        
        try:
            result = func(test_data)
            execution_successful = True
        except Exception as e:
            print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            execution_successful = False
            result = None
        
        end_time = time.perf_counter()
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ì¢…ë£Œ)
        memory_after = process.memory_info().rss / 1024 / 1024  # MB
        
        # ê²°ê³¼ ê³„ì‚°
        execution_time = end_time - start_time
        memory_used = memory_after - memory_before
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“Š ì‹¤í–‰ ì‹œê°„: {execution_time:.4f}ì´ˆ")
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {memory_used:.2f}MB")
        
        if execution_successful:
            print(f"ğŸ“‹ ê²°ê³¼ í¬ê¸°: {len(result) if hasattr(result, '__len__') else 'N/A'}")
            print(f"âœ… ì‹¤í–‰ ì„±ê³µ")
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°
        performance_grade = self._calculate_performance_grade(execution_time, memory_used)
        print(f"ğŸ† ì„±ëŠ¥ ë“±ê¸‰: {performance_grade}")
        
        # ê²°ê³¼ ì €ì¥
        profile_data = {
            'description': description,
            'execution_time': execution_time,
            'memory_used': memory_used,
            'success': execution_successful,
            'grade': performance_grade
        }
        self.profile_results.append(profile_data)
        
        return profile_data
    
    def _calculate_performance_grade(self, time_sec, memory_mb):
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚° (A-F)"""
        # ì‹œê°„ ì ìˆ˜ (5ì´ˆ ê¸°ì¤€)
        time_score = max(0, 100 - (time_sec / 5.0) * 100)
        
        # ë©”ëª¨ë¦¬ ì ìˆ˜ (100MB ê¸°ì¤€)  
        memory_score = max(0, 100 - (memory_mb / 100.0) * 100)
        
        # ì¢…í•© ì ìˆ˜
        total_score = (time_score + memory_score) / 2
        
        if total_score >= 90: return "A"
        elif total_score >= 80: return "B"
        elif total_score >= 70: return "C"
        elif total_score >= 60: return "D"
        else: return "F"
    
    def compare_functions(self, functions_and_data, test_size=1000):
        """ì—¬ëŸ¬ í•¨ìˆ˜ì˜ ì„±ëŠ¥ì„ ë¹„êµí•©ë‹ˆë‹¤"""
        
        print(f"\nğŸ†š í•¨ìˆ˜ ì„±ëŠ¥ ë¹„êµ (í…ŒìŠ¤íŠ¸ í¬ê¸°: {test_size})")
        print("=" * 60)
        
        results = []
        
        for func, data, description in functions_and_data:
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸° ì¡°ì •
            if hasattr(data, '__len__') and len(data) > test_size:
                test_data = data[:test_size]
            else:
                test_data = data
            
            result = self.profile_function(func, test_data, description)
            results.append(result)
        
        # ë¹„êµ ì°¨íŠ¸ ìƒì„±
        self._create_comparison_chart(results)
        
        return results
    
    def _create_comparison_chart(self, results):
        """ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
        
        descriptions = [r['description'] for r in results]
        times = [r['execution_time'] for r in results]
        memories = [r['memory_used'] for r in results]
        
        # ì‹¤í–‰ ì‹œê°„ ë¹„êµ
        bars1 = ax1.bar(descriptions, times, color='skyblue', alpha=0.7)
        ax1.set_ylabel('ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')
        ax1.set_title('ì‹¤í–‰ ì‹œê°„ ë¹„êµ')
        ax1.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, time_val in zip(bars1, times):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{time_val:.3f}s', ha='center', va='bottom')
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        bars2 = ax2.bar(descriptions, memories, color='lightcoral', alpha=0.7)
        ax2.set_ylabel('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)')
        ax2.set_title('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ')
        ax2.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, mem_val in zip(bars2, memories):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{mem_val:.1f}MB', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì˜ˆì‹œ: ë‹¤ì–‘í•œ SMS ì²˜ë¦¬ í•¨ìˆ˜ ë¹„êµ
def process_sms_basic(messages):
    """ê¸°ë³¸ì ì¸ SMS ì²˜ë¦¬"""
    results = []
    for msg in messages:
        if isinstance(msg, str):
            clean = msg.lower().strip()
            word_count = len(clean.split())
            char_count = len(clean)
            results.append({
                'message': clean,
                'word_count': word_count,
                'char_count': char_count
            })
    return results

def process_sms_optimized(messages):
    """ìµœì í™”ëœ SMS ì²˜ë¦¬ (ë²¡í„°í™”)"""
    import pandas as pd
    
    # pandasë¡œ ë²¡í„°í™”ëœ ì²˜ë¦¬
    df = pd.DataFrame({'message': messages})
    df = df[df['message'].notna() & (df['message'] != '')]
    
    df['clean_message'] = df['message'].str.lower().str.strip()
    df['word_count'] = df['clean_message'].str.split().str.len()
    df['char_count'] = df['clean_message'].str.len()
    
    return df[['clean_message', 'word_count', 'char_count']].rename(
        columns={'clean_message': 'message'}
    ).to_dict('records')

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
sample_messages = [
    "Hello world",
    "FREE money NOW!!!",
    "How are you today?",
    "Win $1000 cash prize",
    "Call me when you're free"
] * 200  # 1000ê°œ ë©”ì‹œì§€ ìƒì„±

# ì„±ëŠ¥ ë¹„êµ ì‹¤í–‰
profiler = PerformanceProfiler()

comparison_data = [
    (process_sms_basic, sample_messages, "ê¸°ë³¸ ì²˜ë¦¬"),
    (process_sms_optimized, sample_messages, "ìµœì í™” ì²˜ë¦¬"),
]

comparison_results = profiler.compare_functions(comparison_data, test_size=1000)
```

**ì½”ë“œ í•´ì„¤:**
- **ë‹¤ì°¨ì› ì„±ëŠ¥ ì¸¡ì •**: ì‹¤í–‰ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë™ì‹œ ì¸¡ì •
- **ì„±ëŠ¥ ë“±ê¸‰ ì‹œìŠ¤í…œ**: A-F ë“±ê¸‰ìœ¼ë¡œ ì„±ëŠ¥ì„ ì§ê´€ì ìœ¼ë¡œ í‘œí˜„
- **ì‹œê°ì  ë¹„êµ**: matplotlibë¥¼ ì‚¬ìš©í•œ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±
- **ì‹¤ì œì  í…ŒìŠ¤íŠ¸**: ë™ì¼í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë‹¤ë¥¸ êµ¬í˜„ ë°©ì‹ ë¹„êµ

#### **ì°¨ì› 3: ì½”ë“œ í’ˆì§ˆ (Code Quality)**
ì½”ë“œì˜ ê°€ë…ì„±, ìœ ì§€ë³´ìˆ˜ì„±, ìŠ¤íƒ€ì¼ ì¼ê´€ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.

```python
import ast
import re
from collections import defaultdict

class CodeQualityAnalyzer:
    """ì½”ë“œ í’ˆì§ˆì„ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.quality_metrics = defaultdict(int)
        self.warnings = []
        self.suggestions = []
    
    def analyze_code_quality(self, code_string, filename="<string>"):
        """
        íŒŒì´ì¬ ì½”ë“œì˜ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„
        
        Args:
            code_string: ë¶„ì„í•  íŒŒì´ì¬ ì½”ë“œ ë¬¸ìì—´
            filename: íŒŒì¼ëª… (ì„ íƒì‚¬í•­)
        """
        print(f"\nğŸ“‹ ì½”ë“œ í’ˆì§ˆ ë¶„ì„: {filename}")
        print("=" * 50)
        
        # ì½”ë“œ íŒŒì‹±
        try:
            tree = ast.parse(code_string)
        except SyntaxError as e:
            print(f"âŒ êµ¬ë¬¸ ì˜¤ë¥˜: {e}")
            return {'overall_grade': 'F', 'error': str(e)}
        
        # ê° í’ˆì§ˆ ì§€í‘œ ë¶„ì„
        self._analyze_complexity(tree)
        self._analyze_naming(tree)
        self._analyze_structure(tree)
        self._analyze_documentation(code_string)
        self._analyze_style(code_string)
        
        # ì¢…í•© í‰ê°€
        overall_grade = self._calculate_overall_grade()
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_analysis_results(overall_grade)
        
        return {
            'overall_grade': overall_grade,
            'metrics': dict(self.quality_metrics),
            'warnings': self.warnings,
            'suggestions': self.suggestions
        }
    
    def _analyze_complexity(self, tree):
        """ë³µì¡ë„ ë¶„ì„ (ìˆœí™˜ ë³µì¡ë„, ì¤‘ì²© ê¹Šì´ ë“±)"""
        
        class ComplexityVisitor(ast.NodeVisitor):
            def __init__(self):
                self.max_depth = 0
                self.current_depth = 0
                self.decision_points = 0
                self.function_count = 0
                self.long_functions = 0
            
            def visit_FunctionDef(self, node):
                self.function_count += 1
                
                # í•¨ìˆ˜ ê¸¸ì´ ì²´í¬ (20ì¤„ ì´ìƒì´ë©´ ê²½ê³ )
                if len(node.body) > 20:
                    self.long_functions += 1
                
                self.generic_visit(node)
            
            def visit_If(self, node):
                self.decision_points += 1
                self._visit_nested(node)
            
            def visit_For(self, node):
                self.decision_points += 1
                self._visit_nested(node)
            
            def visit_While(self, node):
                self.decision_points += 1
                self._visit_nested(node)
            
            def _visit_nested(self, node):
                self.current_depth += 1
                self.max_depth = max(self.max_depth, self.current_depth)
                self.generic_visit(node)
                self.current_depth -= 1
        
        visitor = ComplexityVisitor()
        visitor.visit(tree)
        
        # ë³µì¡ë„ ì§€í‘œ ì €ì¥
        self.quality_metrics['max_nesting_depth'] = visitor.max_depth
        self.quality_metrics['cyclomatic_complexity'] = visitor.decision_points + 1
        self.quality_metrics['function_count'] = visitor.function_count
        self.quality_metrics['long_functions'] = visitor.long_functions
        
        # ê²½ê³  ìƒì„±
        if visitor.max_depth > 4:
            self.warnings.append(f"ì¤‘ì²© ê¹Šì´ê°€ ê¹ŠìŒ ({visitor.max_depth}ë ˆë²¨)")
        
        if visitor.decision_points > 10:
            self.warnings.append(f"ìˆœí™˜ ë³µì¡ë„ê°€ ë†’ìŒ ({visitor.decision_points + 1})")
        
        if visitor.long_functions > 0:
            self.warnings.append(f"ê¸´ í•¨ìˆ˜ ë°œê²¬ ({visitor.long_functions}ê°œ)")
    
    def _analyze_naming(self, tree):
        """ë„¤ì´ë° ì»¨ë²¤ì…˜ ë¶„ì„"""
        
        class NamingVisitor(ast.NodeVisitor):
            def __init__(self):
                self.function_names = []
                self.variable_names = []
                self.class_names = []
            
            def visit_FunctionDef(self, node):
                self.function_names.append(node.name)
                self.generic_visit(node)
            
            def visit_ClassDef(self, node):
                self.class_names.append(node.name)
                self.generic_visit(node)
            
            def visit_Name(self, node):
                if isinstance(node.ctx, ast.Store):
                    self.variable_names.append(node.id)
        
        visitor = NamingVisitor()
        visitor.visit(tree)
        
        # ë„¤ì´ë° ê·œì¹™ ì²´í¬
        naming_score = 0
        total_names = 0
        
        # í•¨ìˆ˜ëª… ì²´í¬ (snake_case)
        for name in visitor.function_names:
            total_names += 1
            if re.match(r'^[a-z][a-z0-9_]*

, name):
                naming_score += 1
            else:
                self.warnings.append(f"í•¨ìˆ˜ëª… '{name}'ì´ snake_case ê·œì¹™ì— ë§ì§€ ì•ŠìŒ")
        
        # í´ë˜ìŠ¤ëª… ì²´í¬ (PascalCase)
        for name in visitor.class_names:
            total_names += 1
            if re.match(r'^[A-Z][a-zA-Z0-9]*

, name):
                naming_score += 1
            else:
                self.warnings.append(f"í´ë˜ìŠ¤ëª… '{name}'ì´ PascalCase ê·œì¹™ì— ë§ì§€ ì•ŠìŒ")
        
        # ë³€ìˆ˜ëª… ì²´í¬ (ì˜ë¯¸ìˆëŠ” ì´ë¦„)
        short_names = [name for name in visitor.variable_names if len(name) < 3 and name not in ['i', 'j', 'k', 'x', 'y', 'z']]
        if short_names:
            self.warnings.append(f"ì˜ë¯¸ì—†ëŠ” ì§§ì€ ë³€ìˆ˜ëª…: {short_names}")
        
        # ë„¤ì´ë° ì ìˆ˜ ì €ì¥
        self.quality_metrics['naming_score'] = (naming_score / total_names * 100) if total_names > 0 else 100
    
    def _analyze_structure(self, tree):
        """ì½”ë“œ êµ¬ì¡° ë¶„ì„"""
        
        # í•¨ìˆ˜ë‹¹ í‰ê·  ë¼ì¸ ìˆ˜
        function_lines = []
        
        class StructureVisitor(ast.NodeVisitor):
            def visit_FunctionDef(self, node):
                # í•¨ìˆ˜ ë‚´ë¶€ ë¼ì¸ ìˆ˜ ê³„ì‚°
                if hasattr(node, 'lineno') and hasattr(node, 'end_lineno'):
                    lines = node.end_lineno - node.lineno + 1
                    function_lines.append(lines)
                self.generic_visit(node)
        
        visitor = StructureVisitor()
        visitor.visit(tree)
        
        if function_lines:
            avg_function_length = sum(function_lines) / len(function_lines)
            self.quality_metrics['avg_function_length'] = avg_function_length
            
            if avg_function_length > 30:
                self.suggestions.append("í•¨ìˆ˜ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”")
    
    def _analyze_documentation(self, code_string):
        """ë¬¸ì„œí™” ë¶„ì„"""
        
        lines = code_string.split('\n')
        
        # ì£¼ì„ ë¼ì¸ ìˆ˜ ê³„ì‚°
        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))
        docstring_lines = code_string.count('"""') // 2 * 3  # ê°„ë‹¨í•œ ì¶”ì •
        
        total_lines = len([line for line in lines if line.strip()])
        
        if total_lines > 0:
            comment_ratio = (comment_lines + docstring_lines) / total_lines * 100
            self.quality_metrics['documentation_ratio'] = comment_ratio
            
            if comment_ratio < 10:
                self.suggestions.append("ì½”ë“œì— ë” ë§ì€ ì£¼ì„ê³¼ ë¬¸ì„œí™”ë¥¼ ì¶”ê°€í•˜ì„¸ìš”")
            elif comment_ratio > 50:
                self.warnings.append("ì£¼ì„ì´ ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½”ë“œì˜ ëª…í™•ì„±ì„ í™•ì¸í•˜ì„¸ìš”")
    
    def _analyze_style(self, code_string):
        """ì½”ë”© ìŠ¤íƒ€ì¼ ë¶„ì„"""
        
        lines = code_string.split('\n')
        
        # ê¸´ ë¼ì¸ ì²´í¬ (80ì ì´ˆê³¼)
        long_lines = [i+1 for i, line in enumerate(lines) if len(line) > 80]
        if long_lines:
            self.warnings.append(f"ê¸´ ë¼ì¸ ë°œê²¬ (80ì ì´ˆê³¼): ë¼ì¸ {long_lines}")
        
        # ë¹ˆ ë¼ì¸ ì²´í¬
        consecutive_empty = 0
        max_consecutive_empty = 0
        
        for line in lines:
            if line.strip() == '':
                consecutive_empty += 1
                max_consecutive_empty = max(max_consecutive_empty, consecutive_empty)
            else:
                consecutive_empty = 0
        
        if max_consecutive_empty > 2:
            self.suggestions.append("ì—°ì†ëœ ë¹ˆ ë¼ì¸ì´ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤")
        
        # ìŠ¤íƒ€ì¼ ì ìˆ˜ ê³„ì‚°
        style_score = 100
        style_score -= len(long_lines) * 5  # ê¸´ ë¼ì¸ë§ˆë‹¤ 5ì  ê°ì 
        style_score -= max(0, max_consecutive_empty - 2) * 10  # ê³¼ë„í•œ ë¹ˆ ë¼ì¸ ê°ì 
        
        self.quality_metrics['style_score'] = max(0, style_score)
    
    def _calculate_overall_grade(self):
        """ì¢…í•© ë“±ê¸‰ ê³„ì‚°"""
        
        # ê°€ì¤‘ì¹˜ ì ìš© ì ìˆ˜ ê³„ì‚°
        weights = {
            'complexity': 0.3,
            'naming': 0.2,
            'documentation': 0.2,
            'style': 0.3
        }
        
        # ë³µì¡ë„ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        complexity_score = max(0, 100 - self.quality_metrics.get('cyclomatic_complexity', 1) * 5)
        complexity_score -= self.quality_metrics.get('max_nesting_depth', 0) * 10
        complexity_score = max(0, complexity_score)
        
        # ê° ì˜ì—­ë³„ ì ìˆ˜
        scores = {
            'complexity': complexity_score,
            'naming': self.quality_metrics.get('naming_score', 80),
            'documentation': min(100, self.quality_metrics.get('documentation_ratio', 10) * 5),
            'style': self.quality_metrics.get('style_score', 80)
        }
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_score = sum(scores[area] * weights[area] for area in weights)
        
        # ê²½ê³ ì— ë”°ë¥¸ ê°ì 
        weighted_score -= len(self.warnings) * 5
        weighted_score = max(0, weighted_score)
        
        # ë“±ê¸‰ ê²°ì •
        if weighted_score >= 90: return 'A'
        elif weighted_score >= 80: return 'B'
        elif weighted_score >= 70: return 'C'
        elif weighted_score >= 60: return 'D'
        else: return 'F'
    
    def _print_analysis_results(self, overall_grade):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        
        print(f"ğŸ“Š ë³µì¡ë„:")
        print(f"  - ìˆœí™˜ ë³µì¡ë„: {self.quality_metrics.get('cyclomatic_complexity', 'N/A')}")
        print(f"  - ìµœëŒ€ ì¤‘ì²© ê¹Šì´: {self.quality_metrics.get('max_nesting_depth', 'N/A')}")
        print(f"  - í•¨ìˆ˜ ê°œìˆ˜: {self.quality_metrics.get('function_count', 'N/A')}")
        
        print(f"\nğŸ“ ë„¤ì´ë°:")
        print(f"  - ë„¤ì´ë° ì ìˆ˜: {self.quality_metrics.get('naming_score', 'N/A'):.1f}%")
        
        print(f"\nğŸ“š ë¬¸ì„œí™”:")
        print(f"  - ë¬¸ì„œí™” ë¹„ìœ¨: {self.quality_metrics.get('documentation_ratio', 'N/A'):.1f}%")
        
        print(f"\nğŸ¨ ìŠ¤íƒ€ì¼:")
        print(f"  - ìŠ¤íƒ€ì¼ ì ìˆ˜: {self.quality_metrics.get('style_score', 'N/A'):.1f}%")
        
        print(f"\nğŸ† ì¢…í•© ë“±ê¸‰: {overall_grade}")
        
        if self.warnings:
            print(f"\nâš ï¸ ê²½ê³ ì‚¬í•­:")
            for warning in self.warnings:
                print(f"  - {warning}")
        
        if self.suggestions:
            print(f"\nğŸ’¡ ê°œì„  ì œì•ˆ:")
            for suggestion in self.suggestions:
                print(f"  - {suggestion}")

# ì½”ë“œ í’ˆì§ˆ ë¶„ì„ ì˜ˆì‹œ
sample_code = '''
def processMessages(msgs):
    result=[]
    for m in msgs:
        if m!=None:
            if len(m)>0:
                if isinstance(m,str):
                    clean=m.lower().strip()
                    if len(clean)>0:
                        wc=len(clean.split())
                        cc=len(clean)
                        result.append({"msg":clean,"wc":wc,"cc":cc})
    return result

class dataProcessor:
    def __init__(self):
        self.data=[]
    
    def add(self,item):
        self.data.append(item)
'''

# ë¶„ì„ ì‹¤í–‰
analyzer = CodeQualityAnalyzer()
quality_result = analyzer.analyze_code_quality(sample_code, "sample_sms_processor.py")
```

**ì½”ë“œ í•´ì„¤:**
- **AST íŒŒì‹±**: Pythonì˜ Abstract Syntax Treeë¥¼ í™œìš©í•œ ì •ì  ì½”ë“œ ë¶„ì„
- **ë‹¤ì°¨ì› í‰ê°€**: ë³µì¡ë„, ë„¤ì´ë°, ë¬¸ì„œí™”, ìŠ¤íƒ€ì¼ ë“± ì—¬ëŸ¬ ê´€ì ì—ì„œ ì¢…í•© í‰ê°€
- **ì‹¤ìš©ì  í”¼ë“œë°±**: êµ¬ì²´ì ì¸ ë¬¸ì œì ê³¼ ê°œì„  ë°©ì•ˆ ì œì‹œ
- **ì ìˆ˜í™” ì‹œìŠ¤í…œ**: ê°ê´€ì ì¸ í’ˆì§ˆ ì§€í‘œë¥¼ í†µí•œ ë“±ê¸‰ ì‚°ì •

> ğŸ’¡ **ì½”ë“œ í’ˆì§ˆ ìë™ í‰ê°€ ë„êµ¬ ì¶”ì²œ**
> 
> **ì •ì  ë¶„ì„ ë„êµ¬:**
> - **pylint**: ì¢…í•©ì ì¸ ì½”ë“œ í’ˆì§ˆ ë¶„ì„
> - **flake8**: PEP 8 ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì¤€ìˆ˜ í™•ì¸
> - **mypy**: íƒ€ì… íŒíŠ¸ ê²€ì¦
> 
> **ë³µì¡ë„ ë¶„ì„:**
> - **radon**: ìˆœí™˜ ë³µì¡ë„, ìœ ì§€ë³´ìˆ˜ì„± ì§€ìˆ˜ ê³„ì‚°
> - **xenon**: ë³µì¡ë„ ê¸°ë°˜ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
> 
> **ë³´ì•ˆ ë¶„ì„:**
> - **bandit**: ë³´ì•ˆ ì·¨ì•½ì  ìŠ¤ìº”
> - **safety**: ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ë³´ì•ˆ ê²€ì‚¬

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "ì½”ë“œ í’ˆì§ˆ í‰ê°€ì˜ 5ì°¨ì›ì„ ë³´ì—¬ì£¼ëŠ” ë ˆì´ë” ì°¨íŠ¸. ê¸°ëŠ¥ì  ì •í™•ì„±, ì„±ëŠ¥ íš¨ìœ¨ì„±, ì½”ë“œ í’ˆì§ˆ, ë³´ì•ˆì„±, ìœ ì§€ë³´ìˆ˜ì„±ì´ 5ê°í˜•ìœ¼ë¡œ ë°°ì¹˜ë˜ê³ , ê° ì°¨ì›ë§ˆë‹¤ ì ìˆ˜ê°€ í‘œì‹œëœ ëª¨ë˜í•œ ëŒ€ì‹œë³´ë“œ ìŠ¤íƒ€ì¼"

## 3. ì„±ëŠ¥ ìµœì í™” ì „ëµê³¼ ë¦¬íŒ©í† ë§ ê¸°ë²•

ì½”ë“œ ìµœì í™”ëŠ” ë§ˆì¹˜ ìë™ì°¨ íŠœë‹ê³¼ ê°™ìŠµë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì˜ ë‹¬ë¦¬ëŠ” ì°¨ë„ ì „ë¬¸ê°€ì˜ ì†ê¸¸ì„ ê±°ì¹˜ë©´ ë”ìš± ë¹ ë¥´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë‹¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. AIê°€ ìƒì„±í•œ ì½”ë“œë„ ë§ˆì°¬ê°€ì§€ë¡œ ì²´ê³„ì ì¸ ìµœì í™”ë¥¼ í†µí•´ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3.1 ë°ì´í„° êµ¬ì¡° ìµœì í™”

#### **ìµœì í™” ì „ëµ 1: ì í•©í•œ ìë£Œêµ¬ì¡° ì„ íƒ**

AIëŠ” ì¢…ì¢… ê°€ì¥ ì§ê´€ì ì¸ ìë£Œêµ¬ì¡°(ë¦¬ìŠ¤íŠ¸)ë¥¼ ì„ íƒí•˜ì§€ë§Œ, ì‘ì—…ì˜ íŠ¹ì„±ì— ë”°ë¼ ë” íš¨ìœ¨ì ì¸ ìë£Œêµ¬ì¡°ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**âŒ ë¹„íš¨ìœ¨ì ì¸ ë°ì´í„° êµ¬ì¡° ì‚¬ìš©:**
```python
def find_spam_keywords_inefficient(messages, keywords):
    """
    ìŠ¤íŒ¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (ë¹„íš¨ìœ¨ì  - ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©)
    ì‹œê°„ ë³µì¡ë„: O(n * m * k) where n=ë©”ì‹œì§€ ìˆ˜, m=í‚¤ì›Œë“œ ìˆ˜, k=í‰ê·  ë©”ì‹œì§€ ê¸¸ì´
    """
    spam_messages = []
    
    for message in messages:
        found_keywords = []
        
        # ë¦¬ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰ - O(m) ì‹œê°„ ì†Œìš”
        for keyword in keywords:
            if keyword.lower() in message.lower():
                found_keywords.append(keyword)
        
        if found_keywords:
            spam_messages.append({
                'message': message,
                'keywords': found_keywords,
                'spam_score': len(found_keywords)
            })
    
    return spam_messages

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
import time
import random

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
keywords = ['free', 'win', 'cash', 'prize', 'urgent', 'limited', 'offer', 'call', 'click', 'now'] * 10  # 100ê°œ í‚¤ì›Œë“œ
messages = [
    f"This is message {i} with some free offers and win prizes" 
    for i in range(1000)
]

start_time = time.time()
result_inefficient = find_spam_keywords_inefficient(messages, keywords)
time_inefficient = time.time() - start_time
print(f"ë¹„íš¨ìœ¨ì  ë²„ì „: {time_inefficient:.4f}ì´ˆ")
```

**âœ… ìµœì í™”ëœ ë°ì´í„° êµ¬ì¡°:**
```python
def find_spam_keywords_optimized(messages, keywords):
    """
    ìŠ¤íŒ¸ í‚¤ì›Œë“œ ê²€ìƒ‰ (ìµœì í™” - ì§‘í•©ê³¼ íŠ¸ë¼ì´ ì‚¬ìš©)
    ì‹œê°„ ë³µì¡ë„: O(n * k) where n=ë©”ì‹œì§€ ìˆ˜, k=í‰ê·  ë©”ì‹œì§€ ê¸¸ì´
    """
    
    # í‚¤ì›Œë“œë¥¼ ì§‘í•©ìœ¼ë¡œ ë³€í™˜ - O(1) ê²€ìƒ‰ ì‹œê°„
    keyword_set = {keyword.lower() for keyword in keywords}
    
    spam_messages = []
    
    for message in messages:
        # ë©”ì‹œì§€ë¥¼ ë‹¨ì–´ë¡œ ë¶„í• í•˜ê³  ì§‘í•©ìœ¼ë¡œ ë³€í™˜
        message_words = set(word.lower().strip('.,!?;:') for word in message.split())
        
        # êµì§‘í•©ìœ¼ë¡œ í•œ ë²ˆì— ì°¾ê¸° - O(min(len(keyword_set), len(message_words)))
        found_keywords = list(message_words.intersection(keyword_set))
        
        if found_keywords:
            spam_messages.append({
                'message': message,
                'keywords': found_keywords,
                'spam_score': len(found_keywords)
            })
    
    return spam_messages

# ë” ê³ ê¸‰ ìµœì í™”: íŠ¸ë¼ì´(Trie) ìë£Œêµ¬ì¡° í™œìš©
class TrieNode:
    def __init__(self):
        self.children = {}
        self.is_word = False
        self.word = ""

class SpamKeywordTrie:
    """íŠ¸ë¼ì´ë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    
    def __init__(self, keywords):
        self.root = TrieNode()
        self._build_trie(keywords)
    
    def _build_trie(self, keywords):
        """í‚¤ì›Œë“œë“¤ë¡œ íŠ¸ë¼ì´ êµ¬ì¶•"""
        for keyword in keywords:
            self._insert(keyword.lower())
    
    def _insert(self, word):
        """íŠ¸ë¼ì´ì— ë‹¨ì–´ ì‚½ì…"""
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.is_word = True
        node.word = word
    
    def find_keywords_in_text(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  í‚¤ì›Œë“œ ì°¾ê¸°"""
        text = text.lower()
        found_keywords = []
        
        # ëª¨ë“  ì‹œì‘ ìœ„ì¹˜ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        for i in range(len(text)):
            keywords = self._search_from_position(text, i)
            found_keywords.extend(keywords)
        
        return list(set(found_keywords))  # ì¤‘ë³µ ì œê±°
    
    def _search_from_position(self, text, start_pos):
        """íŠ¹ì • ìœ„ì¹˜ì—ì„œ ì‹œì‘í•˜ëŠ” í‚¤ì›Œë“œë“¤ ì°¾ê¸°"""
        node = self.root
        found_keywords = []
        
        for i in range(start_pos, len(text)):
            char = text[i]
            if char not in node.children:
                break
            
            node = node.children[char]
            if node.is_word:
                # ë‹¨ì–´ ê²½ê³„ í™•ì¸ (ê°„ë‹¨í•œ ë²„ì „)
                if (start_pos == 0 or not text[start_pos-1].isalnum()) and \
                   (i == len(text)-1 or not text[i+1].isalnum()):
                    found_keywords.append(node.word)
        
        return found_keywords

def find_spam_keywords_trie(messages, keywords):
    """íŠ¸ë¼ì´ë¥¼ í™œìš©í•œ ìŠ¤íŒ¸ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    
    # íŠ¸ë¼ì´ êµ¬ì¶• (í•œ ë²ˆë§Œ ìˆ˜í–‰)
    trie = SpamKeywordTrie(keywords)
    
    spam_messages = []
    
    for message in messages:
        found_keywords = trie.find_keywords_in_text(message)
        
        if found_keywords:
            spam_messages.append({
                'message': message,
                'keywords': found_keywords,
                'spam_score': len(found_keywords)
            })
    
    return spam_messages

# ì„±ëŠ¥ ë¹„êµ
start_time = time.time()
result_optimized = find_spam_keywords_optimized(messages, keywords)
time_optimized = time.time() - start_time
print(f"ì§‘í•© ìµœì í™” ë²„ì „: {time_optimized:.4f}ì´ˆ")

start_time = time.time()
result_trie = find_spam_keywords_trie(messages, keywords)
time_trie = time.time() - start_time
print(f"íŠ¸ë¼ì´ ìµœì í™” ë²„ì „: {time_trie:.4f}ì´ˆ")

# ì„±ëŠ¥ ê°œì„  ë¹„ìœ¨ ê³„ì‚°
improvement_set = (time_inefficient - time_optimized) / time_inefficient * 100
improvement_trie = (time_inefficient - time_trie) / time_inefficient * 100

print(f"\nğŸ“ˆ ì„±ëŠ¥ ê°œì„ :")
print(f"ì§‘í•© ì‚¬ìš©: {improvement_set:.1f}% ê°œì„ ")
print(f"íŠ¸ë¼ì´ ì‚¬ìš©: {improvement_trie:.1f}% ê°œì„ ")
```

**ì½”ë“œ í•´ì„¤:**
- **ì§‘í•©(Set) í™œìš©**: O(n) ê²€ìƒ‰ì„ O(1)ìœ¼ë¡œ ê°œì„ 
- **êµì§‘í•© ì—°ì‚°**: ë²¡í„°í™”ëœ ì§‘í•© ì—°ì‚°ìœ¼ë¡œ íš¨ìœ¨ì„± ì¦ëŒ€
- **íŠ¸ë¼ì´ ìë£Œêµ¬ì¡°**: ë¬¸ìì—´ ê²€ìƒ‰ì— íŠ¹í™”ëœ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ì ‘ë‘ì‚¬ ë§¤ì¹­ ìµœì í™”
- **ë©”ëª¨ë¦¬ vs ì‹œê°„ íŠ¸ë ˆì´ë“œì˜¤í”„**: ì´ˆê¸° êµ¬ì¶• ë¹„ìš©ì€ ìˆì§€ë§Œ ë°˜ë³µ ê²€ìƒ‰ì—ì„œ í° ì´ë“

#### **ìµœì í™” ì „ëµ 2: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬**

ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

**âŒ ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì ì¸ ì½”ë“œ:**
```python
def process_large_sms_dataset_inefficient(file_path):
    """
    ëŒ€ìš©ëŸ‰ SMS ë°ì´í„°ì…‹ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ë¹„íš¨ìœ¨ì )
    ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    """
    
    # ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë©”ëª¨ë¦¬ì— ë¡œë“œ
    with open(file_path, 'r', encoding='utf-8') as f:
        all_lines = f.readlines()  # ì „ì²´ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
    
    processed_data = []
    
    for line in all_lines:
        # ê° ì¤„ ì²˜ë¦¬
        parts = line.strip().split('\t')
        if len(parts) >= 2:
            label, message = parts[0], parts[1]
            
            # ë³µì¡í•œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€)
            processed_message = {
                'original': message,
                'clean': message.lower().strip(),
                'words': message.split(),
                'char_count': len(message),
                'word_count': len(message.split()),
                'label': label
            }
            processed_data.append(processed_message)
    
    return processed_data
```

**âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì½”ë“œ:**
```python
def process_large_sms_dataset_efficient(file_path, batch_size=1000):
    """
    ëŒ€ìš©ëŸ‰ SMS ë°ì´í„°ì…‹ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ì™€ ë°°ì¹˜ ì²˜ë¦¬ ê²°í•©
    """
    
    def process_batch(batch):
        """ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬"""
        processed_batch = []
        
        for line in batch:
            parts = line.strip().split('\t')
            if len(parts) >= 2:
                label, message = parts[0], parts[1]
                
                # í•„ìš”í•œ ì •ë³´ë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
                processed_message = {
                    'message': message.lower().strip(),
                    'length': len(message),
                    'word_count': len(message.split()),
                    'label': label
                }
                processed_batch.append(processed_message)
        
        return processed_batch
    
    # ì œë„ˆë ˆì´í„°ë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    def data_generator():
        with open(file_path, 'r', encoding='utf-8') as f:
            batch = []
            
            for line in f:  # í•œ ì¤„ì”© ì½ê¸°
                batch.append(line)
                
                if len(batch) >= batch_size:
                    yield process_batch(batch)
                    batch = []  # ë°°ì¹˜ ì´ˆê¸°í™”ë¡œ ë©”ëª¨ë¦¬ í•´ì œ
            
            # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
            if batch:
                yield process_batch(batch)
    
    # ê²°ê³¼ ìˆ˜ì§‘ (í•„ìš”ì‹œ íŒŒì¼ë¡œ ì§ì ‘ ì €ì¥ ê°€ëŠ¥)
    all_processed = []
    for batch_result in data_generator():
        all_processed.extend(batch_result)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ (ì„ íƒì‚¬í•­)
        if len(all_processed) % 10000 == 0:
            print(f"ì²˜ë¦¬ ì™„ë£Œ: {len(all_processed)}ê°œ ë©”ì‹œì§€")
    
    return all_processed

# ë” ê³ ê¸‰: pandasë¥¼ í™œìš©í•œ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
import pandas as pd

def process_with_pandas_chunks(file_path, chunk_size=5000):
    """
    pandas ì²­í¬ë¥¼ í™œìš©í•œ íš¨ìœ¨ì  ì²˜ë¦¬
    """
    
    results = []
    
    # ì²­í¬ ë‹¨ìœ„ë¡œ íŒŒì¼ ì½ê¸°
    for chunk in pd.read_csv(file_path, sep='\t', header=None, 
                            names=['label', 'message'], 
                            chunksize=chunk_size):
        
        # ë²¡í„°í™”ëœ ì²˜ë¦¬
        chunk['clean_message'] = chunk['message'].str.lower().str.strip()
        chunk['length'] = chunk['message'].str.len()
        chunk['word_count'] = chunk['message'].str.split().str.len()
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        processed_chunk = chunk[['label', 'clean_message', 'length', 'word_count']]
        
        results.append(processed_chunk)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
        print(f"ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: {len(processed_chunk)}ê°œ í–‰")
    
    # ëª¨ë“  ì²­í¬ ê²°í•©
    final_result = pd.concat(results, ignore_index=True)
    return final_result

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ í…ŒìŠ¤íŠ¸
import psutil
import os

def monitor_memory_usage(func, *args, **kwargs):
    """í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"""
    
    process = psutil.Process(os.getpid())
    
    # ì‹œì‘ ë©”ëª¨ë¦¬
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # í•¨ìˆ˜ ì‹¤í–‰
    result = func(*args, **kwargs)
    
    # ì¢…ë£Œ ë©”ëª¨ë¦¬
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    
    memory_used = memory_after - memory_before
    
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.2f}MB")
    print(f"ì²˜ë¦¬ëœ ë°ì´í„° í¬ê¸°: {len(result)}ê°œ")
    
    return result, memory_used

# ê°€ìƒì˜ ëŒ€ìš©ëŸ‰ SMS íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
def create_large_sms_file(file_path, num_records=50000):
    """í…ŒìŠ¤íŠ¸ìš© ëŒ€ìš©ëŸ‰ SMS íŒŒì¼ ìƒì„±"""
    
    import random
    
    spam_templates = [
        "FREE money! Call {} NOW!",
        "Win ${} cash prize! Text back!",
        "URGENT: Claim your {} reward!",
        "Limited time offer: {} discount!"
    ]
    
    ham_templates = [
        "Hey, how are you doing today?",
        "Meeting at {} tomorrow, see you there",
        "Thanks for the {} yesterday",
        "Can you pick up {} from the store?"
    ]
    
    with open(file_path, 'w', encoding='utf-8') as f:
        for i in range(num_records):
            if random.random() < 0.2:  # 20% ìŠ¤íŒ¸
                label = "spam"
                template = random.choice(spam_templates)
                message = template.format(random.randint(100, 9999))
            else:  # 80% ì •ìƒ
                label = "ham" 
                template = random.choice(ham_templates)
                message = template.format(random.choice(["lunch", "coffee", "milk", "3pm"]))
            
            f.write(f"{label}\t{message}\n")

# í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
test_file = "large_sms_test.txt"
create_large_sms_file(test_file, 20000)

print("ğŸ§ª ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸")
print("=" * 40)

# ë¹„íš¨ìœ¨ì  ë°©ë²• (ì£¼ì˜: ë©”ëª¨ë¦¬ ë¶€ì¡± ê°€ëŠ¥)
try:
    result1, memory1 = monitor_memory_usage(
        process_large_sms_dataset_inefficient, test_file
    )
    print(f"ë¹„íš¨ìœ¨ì  ë°©ë²•: {memory1:.2f}MB")
except Exception as e:
    print(f"ë¹„íš¨ìœ¨ì  ë°©ë²• ì‹¤íŒ¨: {e}")

# íš¨ìœ¨ì  ë°©ë²•
result2, memory2 = monitor_memory_usage(
    process_large_sms_dataset_efficient, test_file, 1000
)
print(f"íš¨ìœ¨ì  ë°©ë²•: {memory2:.2f}MB")

# pandas ì²­í¬ ë°©ë²•
result3, memory3 = monitor_memory_usage(
    process_with_pandas_chunks, test_file, 2000
)
print(f"pandas ì²­í¬ ë°©ë²•: {memory3:.2f}MB")

# íŒŒì¼ ì •ë¦¬
os.remove(test_file)
```

**ì½”ë“œ í•´ì„¤:**
- **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ì§€ ì•Šê³  ìˆœì°¨ì  ì²˜ë¦¬
- **ë°°ì¹˜ ì²˜ë¦¬**: ì ë‹¹í•œ í¬ê¸°ì˜ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œì–´
- **ì œë„ˆë ˆì´í„° í™œìš©**: `yield`ë¥¼ ì‚¬ìš©í•œ ì§€ì—° í‰ê°€ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€
- **pandas ì²­í‚¹**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™”ëœ pandas ê¸°ëŠ¥ í™œìš©

### 3.2 ì•Œê³ ë¦¬ì¦˜ ìµœì í™”

#### **ìµœì í™” ì „ëµ 3: ì‹œê°„ ë³µì¡ë„ ê°œì„ **

**âŒ ë¹„íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜:**
```python
def find_similar_messages_naive(messages, threshold=0.8):
    """
    ìœ ì‚¬í•œ ë©”ì‹œì§€ ì°¾ê¸° (ìˆœì§„í•œ ë°©ë²•)
    ì‹œê°„ ë³µì¡ë„: O(nÂ²) - ëª¨ë“  ìŒì„ ë¹„êµ
    """
    
    def jaccard_similarity(text1, text2):
        """ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚°"""
        set1 = set(text1.lower().split())
        set2 = set(text2.lower().split())
        
        intersection = len(set1.intersection(set2))
        union = len(set1.union(set2))
        
        return intersection / union if union > 0 else 0
    
    similar_pairs = []
    
    # ëª¨ë“  ìŒì„ ë¹„êµ - O(nÂ²)
    for i in range(len(messages)):
        for j in range(i + 1, len(messages)):
            similarity = jaccard_similarity(messages[i], messages[j])
            
            if similarity >= threshold:
                similar_pairs.append({
                    'message1': messages[i],
                    'message2': messages[j],
                    'similarity': similarity
                })
    
    return similar_pairs
```

**âœ… ìµœì í™”ëœ ì•Œê³ ë¦¬ì¦˜:**
```python
from collections import defaultdict
import hashlib

def find_similar_messages_optimized(messages, threshold=0.8):
    """
    ìœ ì‚¬í•œ ë©”ì‹œì§€ ì°¾ê¸° (ìµœì í™”ëœ ë°©ë²•)
    LSH(Locality Sensitive Hashing) ì‚¬ìš©
    """
    
    def get_shingles(text, k=3):
        """k-shingle ìƒì„±"""
        words = text.lower().split()
        if len(words) < k:
            return {' '.join(words)}
        
        shingles = set()
        for i in range(len(words) - k + 1):
            shingle = ' '.join(words[i:i+k])
            shingles.add(shingle)
        return shingles
    
    def minhash_signature(shingles, num_hashes=100):
        """MinHash ì‹œê·¸ë‹ˆì²˜ ìƒì„±"""
        signature = []
        
        for i in range(num_hashes):
            min_hash = float('inf')
            
            for shingle in shingles:
                # í•´ì‹œ í•¨ìˆ˜ (ië¥¼ ì‹œë“œë¡œ ì‚¬ìš©)
                hash_val = int(hashlib.md5((shingle + str(i)).encode()).hexdigest(), 16)
                min_hash = min(min_hash, hash_val)
            
            signature.append(min_hash)
        
        return signature
    
    def estimate_jaccard_similarity(sig1, sig2):
        """MinHash ì‹œê·¸ë‹ˆì²˜ë¡œ ìì¹´ë“œ ìœ ì‚¬ë„ ì¶”ì •"""
        matches = sum(1 for a, b in zip(sig1, sig2) if a == b)
        return matches / len(sig1)
    
    # 1ë‹¨ê³„: ëª¨ë“  ë©”ì‹œì§€ì˜ MinHash ì‹œê·¸ë‹ˆì²˜ ê³„ì‚°
    signatures = []
    message_data = []
    
    for i, message in enumerate(messages):
        shingles = get_shingles(message)
        if shingles:  # ë¹ˆ shingle ì§‘í•© ì œì™¸
            signature = minhash_signature(shingles)
            signatures.append(signature)
            message_data.append((i, message))
    
    # 2ë‹¨ê³„: LSH ë²„í‚·íŒ… (ì„ íƒì‚¬í•­ - ë” í° ìµœì í™”)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë²„ì „ìœ¼ë¡œ ëª¨ë“  ì‹œê·¸ë‹ˆì²˜ ë¹„êµ
    
    similar_pairs = []
    
    # ì‹œê·¸ë‹ˆì²˜ ë¹„êµ (ì—¬ì „íˆ O(nÂ²)ì´ì§€ë§Œ ì‹¤ì œ í…ìŠ¤íŠ¸ ë¹„êµë³´ë‹¤ í›¨ì”¬ ë¹ ë¦„)
    for i in range(len(signatures)):
        for j in range(i + 1, len(signatures)):
            estimated_similarity = estimate_jaccard_similarity(signatures[i], signatures[j])
            
            if estimated_similarity >= threshold:
                # ì •í™•í•œ ìœ ì‚¬ë„ ê³„ì‚° (í•„ìš”ì‹œ)
                actual_similarity = jaccard_similarity(
                    message_data[i][1], message_data[j][1]
                )
                
                if actual_similarity >= threshold:
                    similar_pairs.append({
                        'message1': message_data[i][1],
                        'message2': message_data[j][1],
                        'similarity': actual_similarity,
                        'estimated_similarity': estimated_similarity
                    })
    
    return similar_pairs

def jaccard_similarity(text1, text2):
    """ìì¹´ë“œ ìœ ì‚¬ë„ ê³„ì‚° (ê³µí†µ í•¨ìˆ˜)"""
    set1 = set(text1.lower().split())
    set2 = set(text2.lower().split())
    
    intersection = len(set1.intersection(set2))
    union = len(set1.union(set2))
    
    return intersection / union if union > 0 else 0

# ë” ê³ ê¸‰: sklearnì„ í™œìš©í•œ ë²¡í„°í™” ì ‘ê·¼
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def find_similar_messages_vectorized(messages, threshold=0.8):
    """
    ë²¡í„°í™”ë¥¼ í™œìš©í•œ ìœ ì‚¬ ë©”ì‹œì§€ ì°¾ê¸°
    TF-IDF + ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
    """
    
    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 3),  # 1-gram to 3-gram
        min_df=1,
        max_features=10000,
        stop_words='english'
    )
    
    # ë²¡í„° í–‰ë ¬ ìƒì„±
    tfidf_matrix = vectorizer.fit_transform(messages)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (í•œ ë²ˆì— ëª¨ë“  ìŒ)
    similarity_matrix = cosine_similarity(tfidf_matrix)
    
    # ì„ê³„ê°’ ì´ìƒì¸ ìŒ ì°¾ê¸°
    similar_pairs = []
    
    # ìƒì‚¼ê° í–‰ë ¬ë§Œ í™•ì¸ (ëŒ€ì¹­ì´ë¯€ë¡œ)
    rows, cols = np.where((similarity_matrix >= threshold) & 
                         (similarity_matrix < 1.0))  # ìê¸° ìì‹  ì œì™¸
    
    for row, col in zip(rows, cols):
        if row < col:  # ì¤‘ë³µ ì œê±°
            similar_pairs.append({
                'message1': messages[row],
                'message2': messages[col],
                'similarity': similarity_matrix[row, col]
            })
    
    return similar_pairs

# ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸
test_messages = [
    "Free money call now",
    "Call now for free money",
    "Win cash prize today",
    "Hello how are you",
    "Hi how are you doing",
    "Get free cash now",
    "Today win cash prize",
    "Are you doing well",
    "Free money available call",
    "Cash prize win today"
] * 50  # 500ê°œ ë©”ì‹œì§€ë¡œ í™•ì¥

print("ğŸš€ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ")
print("=" * 40)

# ìˆœì§„í•œ ë°©ë²•
import time
start = time.time()
result_naive = find_similar_messages_naive(test_messages[:100], 0.6)  # ì‘ì€ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
time_naive = time.time() - start
print(f"ìˆœì§„í•œ ë°©ë²• (100ê°œ): {time_naive:.4f}ì´ˆ, {len(result_naive)}ê°œ ìŒ")

# ìµœì í™” ë°©ë²•
start = time.time()
result_optimized = find_similar_messages_optimized(test_messages, 0.6)
time_optimized = time.time() - start
print(f"MinHash ë°©ë²• (500ê°œ): {time_optimized:.4f}ì´ˆ, {len(result_optimized)}ê°œ ìŒ")

# ë²¡í„°í™” ë°©ë²•
start = time.time()
result_vectorized = find_similar_messages_vectorized(test_messages, 0.6)
time_vectorized = time.time() - start
print(f"ë²¡í„°í™” ë°©ë²• (500ê°œ): {time_vectorized:.4f}ì´ˆ, {len(result_vectorized)}ê°œ ìŒ")

# ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
print(f"\nğŸ“‹ ìœ ì‚¬ ë©”ì‹œì§€ ì˜ˆì‹œ:")
for i, pair in enumerate(result_vectorized[:3]):
    print(f"{i+1}. '{pair['message1']}' â†” '{pair['message2']}' (ìœ ì‚¬ë„: {pair['similarity']:.3f})")
```

**ì½”ë“œ í•´ì„¤:**
- **MinHash**: ì§‘í•© ìœ ì‚¬ë„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” í™•ë¥ ì  ì•Œê³ ë¦¬ì¦˜
- **LSH**: ìœ ì‚¬í•œ í•­ëª©ë“¤ì„ ê°™ì€ ë²„í‚·ì— ë°°ì¹˜í•˜ì—¬ ë¹„êµ íšŸìˆ˜ ê°ì†Œ
- **ë²¡í„°í™”**: NumPy/scikit-learnì˜ ìµœì í™”ëœ í–‰ë ¬ ì—°ì‚° í™œìš©
- **ì¡°ê¸° ì¢…ë£Œ**: ì¶”ì •ê°’ìœ¼ë¡œ ë¨¼ì € í•„í„°ë§ í›„ ì •í™•í•œ ê³„ì‚° ìˆ˜í–‰

> ğŸ’¡ **ì„±ëŠ¥ ìµœì í™” ì²´í¬ë¦¬ìŠ¤íŠ¸**
> 
> **ë°ì´í„° êµ¬ì¡°:**
> - [ ] ì í•©í•œ ìë£Œêµ¬ì¡° ì„ íƒ (ë¦¬ìŠ¤íŠ¸ vs ì§‘í•© vs ë”•ì…”ë„ˆë¦¬)
> - [ ] ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë°, ë°°ì¹˜)
> - [ ] ìºì‹± í™œìš© (ì¤‘ë³µ ê³„ì‚° ë°©ì§€)
> 
> **ì•Œê³ ë¦¬ì¦˜:**
> - [ ] ì‹œê°„ ë³µì¡ë„ ê°œì„  (O(nÂ²) â†’ O(n log n))
> - [ ] ë²¡í„°í™” ì—°ì‚° í™œìš©
> - [ ] í™•ë¥ ì  ì•Œê³ ë¦¬ì¦˜ ì ìš©
> 
> **êµ¬í˜„:**
> - [ ] ë¶ˆí•„ìš”í•œ ê°ì²´ ìƒì„± ìµœì†Œí™”
> - [ ] ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ í™œìš©
> - [ ] ë³‘ë ¬ ì²˜ë¦¬ ê³ ë ¤

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "ì½”ë“œ ìµœì í™” ê³¼ì •ì„ ë³´ì—¬ì£¼ëŠ” í”Œë¡œìš°ì°¨íŠ¸. ì›ë³¸ AI ì½”ë“œ â†’ ë¬¸ì œ ì§„ë‹¨ â†’ ë°ì´í„° êµ¬ì¡° ìµœì í™” â†’ ì•Œê³ ë¦¬ì¦˜ ê°œì„  â†’ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ â†’ ìµœì¢… ìµœì í™” ì½”ë“œ ë‹¨ê³„ë¡œ ì´ì–´ì§€ë©°, ê° ë‹¨ê³„ë§ˆë‹¤ ì„±ëŠ¥ ì§€í‘œ(ì‹œê°„, ë©”ëª¨ë¦¬)ê°€ í‘œì‹œëœ ëª¨ë˜í•œ ë‹¤ì´ì–´ê·¸ë¨"

## 4. ë³´ì•ˆê³¼ ì•ˆì •ì„±ì„ ê³ ë ¤í•œ ì½”ë“œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤

AIê°€ ìƒì„±í•œ ì½”ë“œë¥¼ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ê¸° ì „ì—ëŠ” ì² ì €í•œ ë³´ì•ˆ ë° ì•ˆì •ì„± ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. ë§ˆì¹˜ ìƒˆë¡œìš´ ìë™ì°¨ê°€ ë„ë¡œì— ë‚˜ê°€ê¸° ì „ì— ì•ˆì „ì„± í…ŒìŠ¤íŠ¸ë¥¼ ê±°ì¹˜ëŠ” ê²ƒì²˜ëŸ¼, ì½”ë“œë„ ë‹¤ì–‘í•œ ê²€ì¦ ê³¼ì •ì„ í†µê³¼í•´ì•¼ í•©ë‹ˆë‹¤.

### 4.1 ì…ë ¥ ê²€ì¦ê³¼ ë°ì´í„° ë³´ì•ˆ

#### **ë³´ì•ˆ ì›ì¹™ 1: ì…ë ¥ ë°ì´í„° ê²€ì¦**

AIëŠ” ì¢…ì¢… ì‚¬ìš©ì ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì‹ ë¢°í•˜ëŠ” ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì•…ì˜ì ì´ê±°ë‚˜ ì˜ˆìƒì¹˜ ëª»í•œ ì…ë ¥ì´ ë“¤ì–´ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**âŒ ë³´ì•ˆ ì·¨ì•½ì ì´ ìˆëŠ” AI ì½”ë“œ:**
```python
def analyze_sms_content(user_input):
    """
    ì‚¬ìš©ì ì…ë ¥ SMS ë¶„ì„ (ë³´ì•ˆ ì·¨ì•½ì  í¬í•¨)
    """
    
    # 1. ì…ë ¥ ê²€ì¦ ì—†ìŒ - SQL Injection ìœ„í—˜
    query = f"SELECT * FROM sms_data WHERE content LIKE '%{user_input}%'"
    
    # 2. íŒŒì¼ ê²½ë¡œ ê²€ì¦ ì—†ìŒ - Path Traversal ìœ„í—˜
    log_file = f"logs/{user_input}_analysis.log"
    
    # 3. ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ í‰ê°€ - Code Injection ìœ„í—˜
    result = eval(f"len('{user_input}')")  # ë§¤ìš° ìœ„í—˜!
    
    # 4. ë¯¼ê°í•œ ì •ë³´ ë¡œê¹…
    print(f"ì‚¬ìš©ì ì…ë ¥: {user_input}")  # ë¡œê·¸ì— ë¯¼ê° ì •ë³´ ë…¸ì¶œ
    
    return {
        'query': query,
        'log_file': log_file,
        'length': result
    }

# ìœ„í—˜í•œ ì…ë ¥ ì˜ˆì‹œ
malicious_inputs = [
    "'; DROP TABLE sms_data; --",  # SQL Injection
    "../../../etc/passwd",  # Path Traversal
    "1'); __import__('os').system('rm -rf /'); ('",  # Code Injection
    "sensitive_password_123"  # ë¯¼ê°í•œ ì •ë³´
]

print("ğŸš¨ ë³´ì•ˆ ì·¨ì•½ì  í…ŒìŠ¤íŠ¸:")
for dangerous_input in malicious_inputs:
    try:
        result = analyze_sms_content(dangerous_input)
        print(f"ìœ„í—˜í•œ ì…ë ¥ ì²˜ë¦¬ë¨: {dangerous_input[:20]}...")
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
```

**âœ… ë³´ì•ˆì´ ê°•í™”ëœ ì½”ë“œ:**
```python
import re
import hashlib
import os
import sqlite3
from pathlib import Path
import logging

class SecureSMSAnalyzer:
    """ë³´ì•ˆì´ ê°•í™”ëœ SMS ë¶„ì„ê¸°"""
    
    def __init__(self, db_path="sms_database.db", log_dir="logs"):
        self.db_path = db_path
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì • (ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_dir / 'sms_analyzer.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _validate_input(self, user_input):
        """
        ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ì •í™”
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥ ë¬¸ìì—´
            
        Returns:
            dict: ê²€ì¦ ê²°ê³¼ì™€ ì •í™”ëœ ì…ë ¥
            
        Raises:
            ValueError: ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥
        """
        
        # 1. ê¸°ë³¸ íƒ€ì… ê²€ì¦
        if not isinstance(user_input, str):
            raise ValueError("ì…ë ¥ì€ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
        
        # 2. ê¸¸ì´ ì œí•œ
        if len(user_input) > 1000:
            raise ValueError("ì…ë ¥ ê¸¸ì´ê°€ 1000ìë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # 3. í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë¬¸ì ê²€ì¦
        forbidden_patterns = [
            r"[;<>\"'`]",  # SQL/Script Injection ë¬¸ì
            r"\.\./",      # Path Traversal íŒ¨í„´
            r"__\w+__",    # Python ë§¤ì§ ë©”ì„œë“œ
            r"import\s+\w+",  # import ë¬¸
            r"exec\s*\(",     # exec í•¨ìˆ˜
            r"eval\s*\(",     # eval í•¨ìˆ˜
        ]
        
        for pattern in forbidden_patterns:
            if re.search(pattern, user_input, re.IGNORECASE):
                raise ValueError(f"í—ˆìš©ë˜ì§€ ì•ŠëŠ” íŒ¨í„´ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤: {pattern}")
        
        # 4. ì…ë ¥ ì •í™” (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°©ì‹)
        # ì˜ë¬¸ì, ìˆ«ì, ê³µë°±, ê¸°ë³¸ êµ¬ë‘ì ë§Œ í—ˆìš©
        sanitized_input = re.sub(r'[^a-zA-Z0-9\s.,!?-]', '', user_input)
        
        # 5. ì…ë ¥ í•´ì‹œ ìƒì„± (ë¡œê¹…ìš© - ì›ë³¸ ë…¸ì¶œ ë°©ì§€)
        input_hash = hashlib.sha256(user_input.encode()).hexdigest()[:16]
        
        return {
            'original_length': len(user_input),
            'sanitized_input': sanitized_input.strip(),
            'input_hash': input_hash,
            'is_modified': user_input != sanitized_input
        }
    
    def analyze_sms_content(self, user_input):
        """
        ë³´ì•ˆì´ ê°•í™”ëœ SMS ë‚´ìš© ë¶„ì„
        
        Args:
            user_input: ë¶„ì„í•  SMS ë‚´ìš©
            
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        
        try:
            # 1. ì…ë ¥ ê²€ì¦
            validation_result = self._validate_input(user_input)
            
            # 2. ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹ ë¡œê¹…
            self.logger.info(f"SMS ë¶„ì„ ìš”ì²­ - í•´ì‹œ: {validation_result['input_hash']}, "
                           f"ê¸¸ì´: {validation_result['original_length']}")
            
            if validation_result['is_modified']:
                self.logger.warning(f"ì…ë ¥ì´ ì •í™”ë¨ - í•´ì‹œ: {validation_result['input_hash']}")
            
            # 3. ì•ˆì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ (Parameterized Query)
            analysis_result = self._safe_database_query(validation_result['sanitized_input'])
            
            # 4. ì•ˆì „í•œ íŒŒì¼ ì²˜ë¦¬
            log_file_path = self._safe_log_file_creation(validation_result['input_hash'])
            
            # 5. ì•ˆì „í•œ ê¸¸ì´ ê³„ì‚° (eval ëŒ€ì‹ )
            content_stats = self._calculate_content_stats(validation_result['sanitized_input'])
            
            return {
                'input_hash': validation_result['input_hash'],
                'analysis': analysis_result,
                'log_file': str(log_file_path),
                'stats': content_stats,
                'security_status': 'validated'
            }
            
        except ValueError as e:
            self.logger.error(f"ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
            return {
                'error': str(e),
                'security_status': 'rejected'
            }
        except Exception as e:
            self.logger.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return {
                'error': 'ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
                'security_status': 'error'
            }
    
    def _safe_database_query(self, sanitized_input):
        """ì•ˆì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ (Parameterized Query ì‚¬ìš©)"""
        
        # ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ëŒ€ì‹  ì‹œë®¬ë ˆì´ì…˜
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” parameterized query ì‚¬ìš©
        query_params = {
            'search_term': f"%{sanitized_input}%"
        }
        
        # Parameterized query ì˜ˆì‹œ (SQLite)
        query = "SELECT COUNT(*) FROM sms_data WHERE content LIKE ?"
        
        # ì‹¤ì œë¡œëŠ” ì´ë ‡ê²Œ ì‹¤í–‰:
        # cursor.execute(query, (query_params['search_term'],))
        
        return {
            'query_type': 'parameterized',
            'search_term_length': len(sanitized_input),
            'matched_count': 42  # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼
        }
    
    def _safe_log_file_creation(self, input_hash):
        """ì•ˆì „í•œ ë¡œê·¸ íŒŒì¼ ìƒì„± (Path Traversal ë°©ì§€)"""
        
        # íŒŒì¼ëª…ì— ì•ˆì „í•œ í•´ì‹œê°’ë§Œ ì‚¬ìš©
        safe_filename = f"analysis_{input_hash}.log"
        
        # ì§€ì •ëœ ë””ë ‰í† ë¦¬ ë‚´ì—ì„œë§Œ íŒŒì¼ ìƒì„±
        log_file_path = self.log_dir / safe_filename
        
        # ìƒìœ„ ë””ë ‰í† ë¦¬ ì ‘ê·¼ ì‹œë„ ì°¨ë‹¨
        try:
            log_file_path = log_file_path.resolve()
            if not str(log_file_path).startswith(str(self.log_dir.resolve())):
                raise ValueError("í—ˆìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ê²½ë¡œ")
        except Exception:
            raise ValueError("ìœ íš¨í•˜ì§€ ì•Šì€ íŒŒì¼ ê²½ë¡œ")
        
        # ì•ˆì „í•œ ë¡œê·¸ ì‘ì„±
        with open(log_file_path, 'w', encoding='utf-8') as f:
            f.write(f"ë¶„ì„ ì‹œì‘: {input_hash}\n")
            f.write(f"íƒ€ì„ìŠ¤íƒ¬í”„: {logging.Formatter().formatTime(logging.LogRecord('', 0, '', 0, '', (), None))}\n")
        
        return log_file_path
    
    def _calculate_content_stats(self, content):
        """ì•ˆì „í•œ ì½˜í…ì¸  í†µê³„ ê³„ì‚° (eval ì‚¬ìš© ì•ˆí•¨)"""
        
        return {
            'length': len(content),
            'word_count': len(content.split()),
            'char_freq': {char: content.count(char) for char in set(content) if char.isalnum()},
            'has_numbers': any(char.isdigit() for char in content),
            'has_uppercase': any(char.isupper() for char in content)
        }

# ë³´ì•ˆ ê°•í™” í…ŒìŠ¤íŠ¸
print("ğŸ”’ ë³´ì•ˆ ê°•í™” í…ŒìŠ¤íŠ¸:")
print("=" * 40)

analyzer = SecureSMSAnalyzer()

# ì •ìƒ ì…ë ¥ í…ŒìŠ¤íŠ¸
normal_input = "Hello, this is a normal SMS message!"
result = analyzer.analyze_sms_content(normal_input)
print(f"ì •ìƒ ì…ë ¥ ê²°ê³¼: {result['security_status']}")

# ì•…ì˜ì  ì…ë ¥ í…ŒìŠ¤íŠ¸
malicious_inputs = [
    "'; DROP TABLE sms_data; --",
    "../../../etc/passwd",
    "1'); __import__('os').system('ls'); ('",
    "normal text with <script>alert('xss')</script>"
]

for malicious_input in malicious_inputs:
    result = analyzer.analyze_sms_content(malicious_input)
    print(f"ì•…ì˜ì  ì…ë ¥ '{malicious_input[:20]}...' -> {result['security_status']}")
```

**ì½”ë“œ í•´ì„¤:**
- **ì…ë ¥ ê²€ì¦**: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°©ì‹ìœ¼ë¡œ í—ˆìš©ë˜ëŠ” ë¬¸ìë§Œ í†µê³¼
- **Parameterized Query**: SQL Injection ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „í•œ ì¿¼ë¦¬ ë°©ì‹
- **Path Traversal ë°©ì§€**: íŒŒì¼ ê²½ë¡œ ì •ê·œí™”ì™€ ë²”ìœ„ ì œí•œ
- **ë¯¼ê°í•œ ì •ë³´ ë³´í˜¸**: í•´ì‹œë¥¼ ì‚¬ìš©í•œ ë¡œê¹…ìœ¼ë¡œ ì›ë³¸ ì •ë³´ ë…¸ì¶œ ë°©ì§€

#### **ë³´ì•ˆ ì›ì¹™ 2: ê¶Œí•œ ê´€ë¦¬ì™€ ì ‘ê·¼ ì œì–´**

```python
import functools
import time
from enum import Enum
from collections import defaultdict, deque

class UserRole(Enum):
    """ì‚¬ìš©ì ì—­í•  ì •ì˜"""
    ADMIN = "admin"
    ANALYST = "analyst"
    VIEWER = "viewer"
    GUEST = "guest"

class PermissionLevel(Enum):
    """ê¶Œí•œ ë ˆë²¨ ì •ì˜"""
    READ = "read"
    WRITE = "write"
    DELETE = "delete"
    ADMIN = "admin"

class AccessControlManager:
    """ì ‘ê·¼ ì œì–´ ê´€ë¦¬ì"""
    
    def __init__(self):
        # ì—­í• ë³„ ê¶Œí•œ ë§¤í•‘
        self.role_permissions = {
            UserRole.ADMIN: [PermissionLevel.READ, PermissionLevel.WRITE, 
                           PermissionLevel.DELETE, PermissionLevel.ADMIN],
            UserRole.ANALYST: [PermissionLevel.READ, PermissionLevel.WRITE],
            UserRole.VIEWER: [PermissionLevel.READ],
            UserRole.GUEST: []
        }
        
        # ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…ì„ ìœ„í•œ ìš”ì²­ ì¶”ì 
        self.request_history = defaultdict(lambda: deque(maxlen=100))
        
        # ì‹¤íŒ¨í•œ ë¡œê·¸ì¸ ì‹œë„ ì¶”ì 
        self.failed_attempts = defaultdict(int)
        self.lockout_time = defaultdict(float)
    
    def require_permission(self, required_permission):
        """ê¶Œí•œ í™•ì¸ ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @functools.wraps(func)
            def wrapper(self, user_role, *args, **kwargs):
                # ê¶Œí•œ í™•ì¸
                if not self._check_permission(user_role, required_permission):
                    raise PermissionError(f"{required_permission.value} ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
                
                return func(self, user_role, *args, **kwargs)
            return wrapper
        return decorator
    
    def rate_limit(self, max_requests=10, time_window=60):
        """ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ë°ì½”ë ˆì´í„°"""
        def decorator(func):
            @functools.wraps(func)
            def wrapper(self, user_id, *args, **kwargs):
                current_time = time.time()
                user_requests = self.request_history[user_id]
                
                # ì‹œê°„ ìœˆë„ìš° ë‚´ì˜ ìš”ì²­ë§Œ ìœ ì§€
                while user_requests and current_time - user_requests[0] > time_window:
                    user_requests.popleft()
                
                # ìš”ì²­ ìˆ˜ í™•ì¸
                if len(user_requests) >= max_requests:
                    raise Exception(f"ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ì´ˆê³¼: {max_requests}íšŒ/{time_window}ì´ˆ")
                
                # í˜„ì¬ ìš”ì²­ ê¸°ë¡
                user_requests.append(current_time)
                
                return func(self, user_id, *args, **kwargs)
            return wrapper
        return decorator
    
    def _check_permission(self, user_role, required_permission):
        """ì‚¬ìš©ì ê¶Œí•œ í™•ì¸"""
        if not isinstance(user_role, UserRole):
            return False
        
        user_permissions = self.role_permissions.get(user_role, [])
        return required_permission in user_permissions
    
    def authenticate_user(self, user_id, password_hash):
        """ì‚¬ìš©ì ì¸ì¦ (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)"""
        
        # ê³„ì • ì ê¸ˆ í™•ì¸
        if self._is_account_locked(user_id):
            remaining_time = self.lockout_time[user_id] - time.time()
            raise Exception(f"ê³„ì •ì´ ì ê²¨ìˆìŠµë‹ˆë‹¤. {remaining_time:.0f}ì´ˆ í›„ ì¬ì‹œë„í•˜ì„¸ìš”")
        
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•´ì‹œ ë¹„êµ
        valid_users = {
            "admin": ("hashed_admin_password", UserRole.ADMIN),
            "analyst1": ("hashed_analyst_password", UserRole.ANALYST),
            "viewer1": ("hashed_viewer_password", UserRole.VIEWER)
        }
        
        if user_id in valid_users and valid_users[user_id][0] == password_hash:
            # ì„±ê³µ ì‹œ ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
            self.failed_attempts[user_id] = 0
            return valid_users[user_id][1]
        else:
            # ì‹¤íŒ¨ ì‹œ ì¹´ìš´í„° ì¦ê°€
            self.failed_attempts[user_id] += 1
            
            # 5íšŒ ì‹¤íŒ¨ ì‹œ 30ë¶„ ì ê¸ˆ
            if self.failed_attempts[user_id] >= 5:
                self.lockout_time[user_id] = time.time() + 1800  # 30ë¶„
                raise Exception("ë¡œê·¸ì¸ ì‹œë„ 5íšŒ ì‹¤íŒ¨ë¡œ ê³„ì •ì´ ì ê²¼ìŠµë‹ˆë‹¤")
            
            raise Exception("ì˜ëª»ëœ ì¸ì¦ ì •ë³´ì…ë‹ˆë‹¤")
    
    def _is_account_locked(self, user_id):
        """ê³„ì • ì ê¸ˆ ìƒíƒœ í™•ì¸"""
        if user_id in self.lockout_time:
            return time.time() < self.lockout_time[user_id]
        return False

class SecureSMSAnalysisAPI:
    """ë³´ì•ˆì´ ì ìš©ëœ SMS ë¶„ì„ API"""
    
    def __init__(self):
        self.access_manager = AccessControlManager()
        self.sms_data = {
            'total_messages': 10000,
            'spam_count': 1500,
            'ham_count': 8500
        }
    
    @AccessControlManager().require_permission(PermissionLevel.READ)
    def get_sms_statistics(self, user_role, user_id):
        """SMS í†µê³„ ì¡°íšŒ (ì½ê¸° ê¶Œí•œ í•„ìš”)"""
        
        return {
            'action': 'get_statistics',
            'user_role': user_role.value,
            'data': self.sms_data.copy()
        }
    
    @AccessControlManager().require_permission(PermissionLevel.WRITE)
    @AccessControlManager().rate_limit(max_requests=5, time_window=60)
    def analyze_new_message(self, user_role, user_id, message):
        """ìƒˆ ë©”ì‹œì§€ ë¶„ì„ (ì“°ê¸° ê¶Œí•œ + ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…)"""
        
        # ì‹¤ì œ ë¶„ì„ ë¡œì§ (ì‹œë®¬ë ˆì´ì…˜)
        analysis_result = {
            'message_id': f"msg_{int(time.time())}",
            'content_length': len(message),
            'predicted_label': 'ham' if len(message) < 50 else 'spam',
            'confidence': 0.85,
            'analyzed_by': user_role.value
        }
        
        return {
            'action': 'analyze_message',
            'result': analysis_result
        }
    
    @AccessControlManager().require_permission(PermissionLevel.DELETE)
    def delete_message(self, user_role, user_id, message_id):
        """ë©”ì‹œì§€ ì‚­ì œ (ì‚­ì œ ê¶Œí•œ í•„ìš”)"""
        
        return {
            'action': 'delete_message',
            'message_id': message_id,
            'deleted_by': user_role.value,
            'status': 'success'
        }
    
    @AccessControlManager().require_permission(PermissionLevel.ADMIN)
    def get_system_logs(self, user_role, user_id):
        """ì‹œìŠ¤í…œ ë¡œê·¸ ì¡°íšŒ (ê´€ë¦¬ì ê¶Œí•œ í•„ìš”)"""
        
        return {
            'action': 'get_system_logs',
            'logs': [
                {'timestamp': time.time(), 'event': 'user_login', 'user': user_id},
                {'timestamp': time.time()-100, 'event': 'message_analyzed', 'count': 50}
            ]
        }

# ë³´ì•ˆ í…ŒìŠ¤íŠ¸
print("\nğŸ” ê¶Œí•œ ê´€ë¦¬ ë° ì ‘ê·¼ ì œì–´ í…ŒìŠ¤íŠ¸:")
print("=" * 50)

api = SecureSMSAnalysisAPI()
access_manager = AccessControlManager()

# ë‹¤ì–‘í•œ ì—­í• ë¡œ í…ŒìŠ¤íŠ¸
test_scenarios = [
    (UserRole.VIEWER, "viewer1", "get_sms_statistics"),
    (UserRole.VIEWER, "viewer1", "analyze_new_message"),  # ê¶Œí•œ ë¶€ì¡±
    (UserRole.ANALYST, "analyst1", "analyze_new_message"),
    (UserRole.ANALYST, "analyst1", "delete_message"),  # ê¶Œí•œ ë¶€ì¡±
    (UserRole.ADMIN, "admin", "get_system_logs"),
]

for user_role, user_id, action in test_scenarios:
    try:
        if action == "get_sms_statistics":
            result = api.get_sms_statistics(user_role, user_id)
        elif action == "analyze_new_message":
            result = api.analyze_new_message(user_role, user_id, "Test SMS message")
        elif action == "delete_message":
            result = api.delete_message(user_role, user_id, "msg_123")
        elif action == "get_system_logs":
            result = api.get_system_logs(user_role, user_id)
        
        print(f"âœ… {user_role.value} - {action}: ì„±ê³µ")
        
    except PermissionError as e:
        print(f"âŒ {user_role.value} - {action}: ê¶Œí•œ ì—†ìŒ ({e})")
    except Exception as e:
        print(f"âš ï¸ {user_role.value} - {action}: ì˜¤ë¥˜ ({e})")

# ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… í…ŒìŠ¤íŠ¸
print(f"\nğŸš¦ ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… í…ŒìŠ¤íŠ¸:")
for i in range(7):  # 5íšŒ ì œí•œì„ ì´ˆê³¼í•˜ì—¬ í…ŒìŠ¤íŠ¸
    try:
        result = api.analyze_new_message(UserRole.ANALYST, "analyst1", f"Message {i}")
        print(f"âœ… ìš”ì²­ {i+1}: ì„±ê³µ")
    except Exception as e:
        print(f"âŒ ìš”ì²­ {i+1}: ì œí•œë¨ ({e})")
```

**ì½”ë“œ í•´ì„¤:**
- **ì—­í•  ê¸°ë°˜ ì ‘ê·¼ ì œì–´(RBAC)**: ì‚¬ìš©ì ì—­í• ì— ë”°ë¥¸ ê¶Œí•œ ì°¨ë³„í™”
- **ë°ì½”ë ˆì´í„° íŒ¨í„´**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ê¶Œí•œ í™•ì¸ ë¡œì§
- **ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…**: DoS ê³µê²© ë°©ì§€ë¥¼ ìœ„í•œ ìš”ì²­ íšŸìˆ˜ ì œí•œ
- **ê³„ì • ì ê¸ˆ**: ë¸Œë£¨íŠ¸ í¬ìŠ¤ ê³µê²© ë°©ì§€ë¥¼ ìœ„í•œ ì‹¤íŒ¨ ì‹œë„ ì œí•œ

### 4.2 ì½”ë“œ ì•ˆì •ì„± ê²€ì¦

#### **ì•ˆì •ì„± ì›ì¹™ 1: ì˜ˆì™¸ ì²˜ë¦¬ì™€ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜**

```python
import logging
import traceback
import time
from contextlib import contextmanager
from typing import Any, Dict, Optional, Callable

class RobustSMSProcessor:
    """ì•ˆì •ì„±ì´ ê°•í™”ëœ SMS ì²˜ë¦¬ê¸°"""
    
    def __init__(self, max_retries=3, backoff_factor=2):
        self.max_retries = max_retries
        self.backoff_factor = backoff_factor
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_metrics = {
            'total_processed': 0,
            'successful_processed': 0,
            'failed_processed': 0,
            'avg_processing_time': 0.0
        }
    
    @contextmanager
    def error_boundary(self, operation_name: str):
        """ì—ëŸ¬ ê²½ê³„ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        start_time = time.time()
        
        try:
            self.logger.info(f"{operation_name} ì‹œì‘")
            yield
            self.logger.info(f"{operation_name} ì„±ê³µ")
            
        except Exception as e:
            # ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹…
            error_details = {
                'operation': operation_name,
                'error_type': type(e).__name__,
                'error_message': str(e),
                'traceback': traceback.format_exc()
            }
            
            self.logger.error(f"{operation_name} ì‹¤íŒ¨: {error_details}")
            
            # ë³µêµ¬ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ì§€ í™•ì¸
            if self._is_recoverable_error(e):
                self.logger.info(f"{operation_name} ë³µêµ¬ ì‹œë„ ì¤‘...")
                raise  # ì¬ì‹œë„ë¥¼ ìœ„í•´ ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒ
            else:
                self.logger.critical(f"{operation_name} ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜")
                raise
                
        finally:
            processing_time = time.time() - start_time
            self.logger.info(f"{operation_name} ì™„ë£Œ ì‹œê°„: {processing_time:.3f}ì´ˆ")
    
    def _is_recoverable_error(self, error: Exception) -> bool:
        """ë³µêµ¬ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ì§€ íŒë‹¨"""
        
        recoverable_errors = [
            ConnectionError,
            TimeoutError,
            MemoryError,  # ë©”ëª¨ë¦¬ ë¶€ì¡±ì€ ì¬ì‹œë„ë¡œ í•´ê²°ë  ìˆ˜ ìˆìŒ
        ]
        
        # íŠ¹ì • ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ í™•ì¸
        recoverable_messages = [
            "connection timeout",
            "temporary failure",
            "server overloaded"
        ]
        
        error_message = str(error).lower()
        
        return (any(isinstance(error, err_type) for err_type in recoverable_errors) or
                any(msg in error_message for msg in recoverable_messages))
    
    def retry_with_backoff(self, operation: Callable, *args, **kwargs) -> Any:
        """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì ìš©í•œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜"""
        
        last_exception = None
        
        for attempt in range(self.max_retries + 1):
            try:
                return operation(*args, **kwargs)
                
            except Exception as e:
                last_exception = e
                
                if attempt == self.max_retries:
                    self.logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ {self.max_retries} ë„ë‹¬")
                    break
                
                if not self._is_recoverable_error(e):
                    self.logger.error("ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜, ì¬ì‹œë„ ì¤‘ë‹¨")
                    break
                
                # ì§€ìˆ˜ ë°±ì˜¤í”„ ì ìš©
                sleep_time = self.backoff_factor ** attempt
                self.logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{self.max_retries} "
                                  f"({sleep_time}ì´ˆ í›„): {str(e)}")
                time.sleep(sleep_time)
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        raise last_exception
    
    def process_sms_safely(self, messages: list) -> Dict[str, Any]:
        """ì•ˆì „í•œ SMS ì¼ê´„ ì²˜ë¦¬"""
        
        with self.error_boundary("SMS ì¼ê´„ ì²˜ë¦¬"):
            results = {
                'processed': [],
                'failed': [],
                'summary': {}
            }
            
            for i, message in enumerate(messages):
                try:
                    # ê°œë³„ ë©”ì‹œì§€ ì²˜ë¦¬
                    processed_msg = self.retry_with_backoff(
                        self._process_single_message, 
                        message, 
                        index=i
                    )
                    
                    results['processed'].append(processed_msg)
                    self.performance_metrics['successful_processed'] += 1
                    
                except Exception as e:
                    # ê°œë³„ ì‹¤íŒ¨ëŠ” ì „ì²´ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
                    failed_msg = {
                        'index': i,
                        'original_message': message,
                        'error': str(e),
                        'error_type': type(e).__name__
                    }
                    
                    results['failed'].append(failed_msg)
                    self.performance_metrics['failed_processed'] += 1
                    
                    self.logger.warning(f"ë©”ì‹œì§€ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                
                finally:
                    self.performance_metrics['total_processed'] += 1
            
            # ì²˜ë¦¬ ìš”ì•½
            total = len(messages)
            success_rate = len(results['processed']) / total if total > 0 else 0
            
            results['summary'] = {
                'total_messages': total,
                'successful_count': len(results['processed']),
                'failed_count': len(results['failed']),
                'success_rate': f"{success_rate:.1%}",
                'performance_metrics': self.performance_metrics.copy()
            }
            
            self.logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {len(results['processed'])}ê°œ, "
                           f"ì‹¤íŒ¨ {len(results['failed'])}ê°œ")
            
            return results
    
    def _process_single_message(self, message: str, index: int) -> Dict[str, Any]:
        """ê°œë³„ ë©”ì‹œì§€ ì²˜ë¦¬ (ì˜ˆì™¸ ë°œìƒ ê°€ëŠ¥)"""
        
        # ì‹œë®¬ë ˆì´ì…˜: ëœë¤í•˜ê²Œ ì—ëŸ¬ ë°œìƒ
        import random
        
        if random.random() < 0.1:  # 10% í™•ë¥ ë¡œ ë³µêµ¬ ê°€ëŠ¥í•œ ì—ëŸ¬
            raise ConnectionError("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì˜¤ë¥˜")
        
        if random.random() < 0.05:  # 5% í™•ë¥ ë¡œ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì—ëŸ¬
            raise ValueError(f"ì˜ëª»ëœ ë©”ì‹œì§€ í˜•ì‹: {message}")
        
        # ì •ìƒ ì²˜ë¦¬
        return {
            'index': index,
            'original': message,
            'cleaned': message.lower().strip(),
            'length': len(message),
            'timestamp': time.time()
        }
    
    def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ í™•ì¸"""
        
        health_status = {
            'status': 'healthy',
            'checks': {},
            'metrics': self.performance_metrics.copy()
        }
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        import psutil
        memory_percent = psutil.virtual_memory().percent
        health_status['checks']['memory'] = {
            'status': 'ok' if memory_percent < 80 else 'warning',
            'usage_percent': memory_percent
        }
        
        # ì„±ê³µë¥  í™•ì¸
        total_processed = self.performance_metrics['total_processed']
        if total_processed > 0:
            success_rate = (self.performance_metrics['successful_processed'] / 
                          total_processed)
            
            health_status['checks']['success_rate'] = {
                'status': 'ok' if success_rate > 0.9 else 'degraded',
                'rate': success_rate
            }
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        check_statuses = [check['status'] for check in health_status['checks'].values()]
        
        if any(status == 'degraded' for status in check_statuses):
            health_status['status'] = 'degraded'
        elif any(status == 'warning' for status in check_statuses):
            health_status['status'] = 'warning'
        
        return health_status

# ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
print("\nğŸ›¡ï¸ ì½”ë“œ ì•ˆì •ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸:")
print("=" * 50)

processor = RobustSMSProcessor(max_retries=2, backoff_factor=1.5)

# í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ (ì¼ë¶€ëŠ” ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìŒ)
test_messages = [
    "Hello, this is a normal message",
    "Another normal SMS text",
    "Free money call now!",
    "How are you doing today?",
    "Win cash prize immediately",
    "Normal conversation text",
    "Emergency notification",
    "Regular update message"
] * 3  # 24ê°œ ë©”ì‹œì§€ë¡œ í™•ì¥

# ì•ˆì „í•œ ì¼ê´„ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
start_time = time.time()
processing_result = processor.process_sms_safely(test_messages)
total_time = time.time() - start_time

print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
print(f"- ì´ ë©”ì‹œì§€: {processing_result['summary']['total_messages']}ê°œ")
print(f"- ì„±ê³µ: {processing_result['summary']['successful_count']}ê°œ")
print(f"- ì‹¤íŒ¨: {processing_result['summary']['failed_count']}ê°œ")
print(f"- ì„±ê³µë¥ : {processing_result['summary']['success_rate']}")
print(f"- ì´ ì†Œìš” ì‹œê°„: {total_time:.3f}ì´ˆ")

# ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ë¶„ì„
if processing_result['failed']:
    print(f"\nâŒ ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ë¶„ì„:")
    for failed in processing_result['failed'][:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
        print(f"- ì¸ë±ìŠ¤ {failed['index']}: {failed['error_type']} - {failed['error']}")

# ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ í™•ì¸
health_status = processor.health_check()
print(f"\nğŸ©º ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ: {health_status['status'].upper()}")

for check_name, check_result in health_status['checks'].items():
    status_emoji = {"ok": "âœ…", "warning": "âš ï¸", "degraded": "âŒ"}
    emoji = status_emoji.get(check_result['status'], "â“")
    print(f"{emoji} {check_name}: {check_result['status']}")
```

**ì½”ë“œ í•´ì„¤:**
- **ì—ëŸ¬ ê²½ê³„**: ì˜ˆì™¸ë¥¼ ê²©ë¦¬í•˜ì—¬ ì „ì²´ ì‹œìŠ¤í…œ ì¥ì•  ë°©ì§€
- **ì§€ìˆ˜ ë°±ì˜¤í”„**: ì¼ì‹œì  ì¥ì• ì— ëŒ€í•œ ì§€ëŠ¥ì  ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
- **íšŒë¡œ ì°¨ë‹¨ê¸°**: ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ ì¡°ê¸° ê°ì§€ ë° ì°¨ë‹¨
- **ê±´ê°• ìƒíƒœ ëª¨ë‹ˆí„°ë§**: ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ì¶”ì 

> ğŸ’¡ **ë³´ì•ˆ ë° ì•ˆì •ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸**
> 
> **ì…ë ¥ ë³´ì•ˆ:**
> - [ ] ì…ë ¥ ê²€ì¦ ë° ì •í™”
> - [ ] SQL/Code Injection ë°©ì§€
> - [ ] Path Traversal ë°©ì§€
> - [ ] ë¯¼ê°í•œ ì •ë³´ ë§ˆìŠ¤í‚¹
> 
> **ì ‘ê·¼ ì œì–´:**
> - [ ] ì—­í•  ê¸°ë°˜ ê¶Œí•œ ê´€ë¦¬
> - [ ] ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… êµ¬í˜„
> - [ ] ê³„ì • ì ê¸ˆ ë©”ì»¤ë‹ˆì¦˜
> - [ ] ì„¸ì…˜ ê´€ë¦¬
> 
> **ì•ˆì •ì„±:**
> - [ ] ì˜ˆì™¸ ì²˜ë¦¬ ë° ë³µêµ¬
> - [ ] ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
> - [ ] íšŒë¡œ ì°¨ë‹¨ê¸° íŒ¨í„´
> - [ ] ê±´ê°• ìƒíƒœ ëª¨ë‹ˆí„°ë§

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "ë³´ì•ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ê³„ì¸µì  ë‹¤ì´ì–´ê·¸ë¨. ì…ë ¥ ê²€ì¦ â†’ ê¶Œí•œ í™•ì¸ â†’ ì•ˆì „í•œ ì²˜ë¦¬ â†’ ì˜ˆì™¸ ì²˜ë¦¬ â†’ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§ ë‹¨ê³„ê°€ ë°©íŒ¨ì™€ ìë¬¼ì‡  ì•„ì´ì½˜ìœ¼ë¡œ í‘œí˜„ëœ ë³´ì•ˆ ì¤‘ì‹¬ì˜ ì¸í¬ê·¸ë˜í”½"

## 5. ì‹¤ì „ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: AI ìƒì„± ì½”ë“œ ê²€ì¦ ë° ê°œì„  ì›Œí¬í”Œë¡œìš°

ì´ì œ ë°°ìš´ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ì‹¤ì œ AIê°€ ìƒì„±í•œ SMS ë¶„ë¥˜ ì½”ë“œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê²€ì¦í•˜ê³  ê°œì„ í•˜ëŠ” ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

### 5.1 í”„ë¡œì íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

**ìƒí™©**: AIê°€ SMS ìŠ¤íŒ¸ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì½”ë“œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤. ì´ ì½”ë“œë¥¼ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ê¸° ì „ì— ì² ì €í•œ ê²€ì¦ê³¼ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.

**ëª©í‘œ**: AI ìƒì„± ì½”ë“œë¥¼ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì´ë©° ìœ ì§€ë³´ìˆ˜ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ê°œì„ í•˜ê¸°

### 5.2 AIê°€ ìƒì„±í•œ ì›ë³¸ ì½”ë“œ (ê²€ì¦ ëŒ€ìƒ)

ë¨¼ì € AIê°€ ìƒì„±í•œ ì›ë³¸ ì½”ë“œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:

```python
# AIê°€ ìƒì„±í•œ ì›ë³¸ ì½”ë“œ (ì—¬ëŸ¬ ë¬¸ì œì  í¬í•¨)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import pickle

def train_spam_classifier(data_file):
    # ë°ì´í„° ë¡œë“œ
    data = pd.read_csv(data_file)
    X = data['message']
    y = data['label']
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # ë²¡í„°í™”
    vectorizer = TfidfVectorizer()
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    
    # ëª¨ë¸ í›ˆë ¨
    model = MultinomialNB()
    model.fit(X_train_vec, y_train)
    
    # í‰ê°€
    predictions = model.predict(X_test_vec)
    accuracy = accuracy_score(y_test, predictions)
    print(f"Accuracy: {accuracy}")
    
    # ëª¨ë¸ ì €ì¥
    pickle.dump(model, open('spam_model.pkl', 'wb'))
    pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))
    
    return model, vectorizer

def classify_message(message):
    # ëª¨ë¸ ë¡œë“œ
    model = pickle.load(open('spam_model.pkl', 'rb'))
    vectorizer = pickle.load(open('vectorizer.pkl', 'rb'))
    
    # ì˜ˆì¸¡
    message_vec = vectorizer.transform([message])
    prediction = model.predict(message_vec)[0]
    probability = model.predict_proba(message_vec)[0]
    
    return prediction, max(probability)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    model, vectorizer = train_spam_classifier("spam_data.csv")
    result = classify_message("FREE money! Call now!")
    print(f"Classification: {result[0]}, Confidence: {result[1]}")
```

### 5.3 ì²´ê³„ì  ì½”ë“œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤

ì´ì œ ìš°ë¦¬ê°€ ë°°ìš´ ê¸°ë²•ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì´ ì½”ë“œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê°œì„ í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
import ast
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib  # pickleë³´ë‹¤ ì•ˆì „
import re
import hashlib
from dataclasses import dataclass

@dataclass
class CodeIssue:
    """ì½”ë“œ ì´ìŠˆë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    severity: str  # 'critical', 'high', 'medium', 'low'
    category: str  # 'security', 'performance', 'reliability', 'maintainability'
    description: str
    location: str
    recommendation: str

class AICodeValidator:
    """AI ìƒì„± ì½”ë“œ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.issues = []
        self.logger = logging.getLogger(__name__)
    
    def validate_code(self, code_string: str, filename: str = "<string>") -> List[CodeIssue]:
        """ì½”ë“œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦"""
        
        self.issues = []
        
        print(f"ğŸ” AI ìƒì„± ì½”ë“œ ê²€ì¦ ì‹œì‘: {filename}")
        print("=" * 60)
        
        # 1. êµ¬ë¬¸ ë¶„ì„
        try:
            tree = ast.parse(code_string)
            self._check_syntax_issues(tree)
        except SyntaxError as e:
            self.issues.append(CodeIssue(
                severity='critical',
                category='reliability',
                description=f"êµ¬ë¬¸ ì˜¤ë¥˜: {e}",
                location=f"ë¼ì¸ {e.lineno}",
                recommendation="êµ¬ë¬¸ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”"
            ))
            return self.issues
        
        # 2. ë³´ì•ˆ ì·¨ì•½ì  ê²€ì‚¬
        self._check_security_issues(code_string, tree)
        
        # 3. ì„±ëŠ¥ ë¬¸ì œ ê²€ì‚¬
        self._check_performance_issues(code_string, tree)
        
        # 4. ì•ˆì •ì„± ë¬¸ì œ ê²€ì‚¬
        self._check_reliability_issues(code_string, tree)
        
        # 5. ìœ ì§€ë³´ìˆ˜ì„± ë¬¸ì œ ê²€ì‚¬
        self._check_maintainability_issues(code_string, tree)
        
        # ê²°ê³¼ ìš”ì•½
        self._print_validation_summary()
        
        return self.issues
    
    def _check_security_issues(self, code_string: str, tree: ast.AST):
        """ë³´ì•ˆ ì·¨ì•½ì  ê²€ì‚¬"""
        
        # pickle ì‚¬ìš© ê²€ì‚¬
        if 'pickle.load' in code_string or 'pickle.dump' in code_string:
            self.issues.append(CodeIssue(
                severity='high',
                category='security',
                description="pickle ëª¨ë“ˆ ì‚¬ìš©ìœ¼ë¡œ ì¸í•œ ë³´ì•ˆ ìœ„í—˜",
                location="pickle.load/dump í˜¸ì¶œ",
                recommendation="joblib ë˜ëŠ” ë‹¤ë¥¸ ì•ˆì „í•œ ì§ë ¬í™” ë°©ë²• ì‚¬ìš©"
            ))
        
        # íŒŒì¼ ê²½ë¡œ í•˜ë“œì½”ë”© ê²€ì‚¬
        hardcoded_paths = re.findall(r'["\'][^"\']*\.(pkl|csv|txt)["\']', code_string)
        if hardcoded_paths:
            self.issues.append(CodeIssue(
                severity='medium',
                category='security',
                description="í•˜ë“œì½”ë”©ëœ íŒŒì¼ ê²½ë¡œ ë°œê²¬",
                location=f"ê²½ë¡œ: {hardcoded_paths}",
                recommendation="ì„¤ì • íŒŒì¼ì´ë‚˜ í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©"
            ))
        
        # ì˜ˆì™¸ ì²˜ë¦¬ ëˆ„ë½ ê²€ì‚¬
        class ExceptionHandlerChecker(ast.NodeVisitor):
            def __init__(self):
                self.has_file_operations = False
                self.has_exception_handling = False
            
            def visit_Call(self, node):
                if (hasattr(node.func, 'attr') and 
                    node.func.attr in ['read_csv', 'open', 'load']):
                    self.has_file_operations = True
                self.generic_visit(node)
            
            def visit_Try(self, node):
                self.has_exception_handling = True
                self.generic_visit(node)
        
        checker = ExceptionHandlerChecker()
        checker.visit(tree)
        
        if checker.has_file_operations and not checker.has_exception_handling:
            self.issues.append(CodeIssue(
                severity='medium',
                category='reliability',
                description="íŒŒì¼ I/O ì‘ì—…ì— ì˜ˆì™¸ ì²˜ë¦¬ ëˆ„ë½",
                location="íŒŒì¼ ì½ê¸°/ì“°ê¸° í•¨ìˆ˜",
                recommendation="try-except ë¸”ë¡ìœ¼ë¡œ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€"
            ))
    
    def _check_performance_issues(self, code_string: str, tree: ast.AST):
        """ì„±ëŠ¥ ë¬¸ì œ ê²€ì‚¬"""
        
        # ë§¤ë²ˆ ëª¨ë¸ ë¡œë“œí•˜ëŠ” ë¬¸ì œ
        if code_string.count('pickle.load') > 1 or code_string.count('joblib.load') > 1:
            self.issues.append(CodeIssue(
                severity='high',
                category='performance',
                description="ë§¤ë²ˆ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ëŠ” ë¹„íš¨ìœ¨ì  êµ¬ì¡°",
                location="classify_message í•¨ìˆ˜",
                recommendation="ëª¨ë¸ì„ í•œ ë²ˆ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ì— ìºì‹±"
            ))
        
        # ë°ì´í„° ê²€ì¦ ëˆ„ë½
        if 'train_test_split' in code_string and 'random_state' not in code_string:
            self.issues.append(CodeIssue(
                severity='low',
                category='reliability',
                description="ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ random_state ëˆ„ë½",
                location="train_test_split í˜¸ì¶œ",
                recommendation="random_state ë§¤ê°œë³€ìˆ˜ ì¶”ê°€"
            ))
    
    def _check_reliability_issues(self, code_string: str, tree: ast.AST):
        """ì•ˆì •ì„± ë¬¸ì œ ê²€ì‚¬"""
        
        # ì…ë ¥ ê²€ì¦ ëˆ„ë½
        class InputValidationChecker(ast.NodeVisitor):
            def __init__(self):
                self.function_params = []
                self.has_input_validation = False
            
            def visit_FunctionDef(self, node):
                self.function_params.extend([arg.arg for arg in node.args.args])
                
                # í•¨ìˆ˜ ë‚´ì—ì„œ isinstance, len ë“±ì˜ ê²€ì¦ í•¨ìˆ˜ í™•ì¸
                for child in ast.walk(node):
                    if (isinstance(child, ast.Call) and 
                        hasattr(child.func, 'id') and
                        child.func.id in ['isinstance', 'len', 'type']):
                        self.has_input_validation = True
                
                self.generic_visit(node)
        
        checker = InputValidationChecker()
        checker.visit(tree)
        
        if checker.function_params and not checker.has_input_validation:
            self.issues.append(CodeIssue(
                severity='medium',
                category='reliability',
                description="í•¨ìˆ˜ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì…ë ¥ ê²€ì¦ ëˆ„ë½",
                location="ëª¨ë“  í•¨ìˆ˜",
                recommendation="íƒ€ì… ì²´í¬ ë° ë²”ìœ„ ê²€ì¦ ì¶”ê°€"
            ))
    
    def _check_maintainability_issues(self, code_string: str, tree: ast.AST):
        """ìœ ì§€ë³´ìˆ˜ì„± ë¬¸ì œ ê²€ì‚¬"""
        
        # í•˜ë“œì½”ë”©ëœ ì„¤ì •ê°’ ê²€ì‚¬
        magic_numbers = re.findall(r'\b0\.[0-9]+\b|\b[1-9][0-9]*\b', code_string)
        if len(magic_numbers) > 3:  # 0.2, ë“±ì˜ ë§¤ì§ ë„˜ë²„
            self.issues.append(CodeIssue(
                severity='low',
                category='maintainability',
                description="í•˜ë“œì½”ë”©ëœ ì„¤ì •ê°’(ë§¤ì§ ë„˜ë²„) ë‹¤ìˆ˜ ë°œê²¬",
                location="ì „ì²´ ì½”ë“œ",
                recommendation="ì„¤ì • ìƒìˆ˜ë¡œ ë¶„ë¦¬í•˜ê±°ë‚˜ ì„¤ì • íŒŒì¼ ì‚¬ìš©"
            ))
        
        # í•¨ìˆ˜ ê¸¸ì´ ê²€ì‚¬
        class FunctionLengthChecker(ast.NodeVisitor):
            def visit_FunctionDef(self, node):
                if hasattr(node, 'end_lineno') and hasattr(node, 'lineno'):
                    length = node.end_lineno - node.lineno
                    if length > 30:
                        self.issues.append(CodeIssue(
                            severity='low',
                            category='maintainability',
                            description=f"í•¨ìˆ˜ '{node.name}'ê°€ ë„ˆë¬´ ê¹€ ({length}ì¤„)",
                            location=f"ë¼ì¸ {node.lineno}-{node.end_lineno}",
                            recommendation="í•¨ìˆ˜ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬"
                        ))
                self.generic_visit(node)
        
        checker = FunctionLengthChecker()
        checker.visit(tree)
    
    def _check_syntax_issues(self, tree: ast.AST):
        """êµ¬ë¬¸ ê´€ë ¨ ë¬¸ì œ ê²€ì‚¬"""
        # ASTê°€ ì •ìƒì ìœ¼ë¡œ íŒŒì‹±ë˜ì—ˆìœ¼ë¯€ë¡œ êµ¬ë¬¸ ì˜¤ë¥˜ëŠ” ì—†ìŒ
        pass
    
    def _print_validation_summary(self):
        """ê²€ì¦ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        
        if not self.issues:
            print("âœ… ê²€ì¦ ì™„ë£Œ: ë°œê²¬ëœ ë¬¸ì œì  ì—†ìŒ")
            return
        
        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        severity_counts = {}
        category_counts = {}
        
        for issue in self.issues:
            severity_counts[issue.severity] = severity_counts.get(issue.severity, 0) + 1
            category_counts[issue.category] = category_counts.get(issue.category, 0) + 1
        
        print(f"\nğŸ“‹ ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
        print(f"ì´ ë°œê²¬ëœ ë¬¸ì œ: {len(self.issues)}ê°œ")
        
        print(f"\nì‹¬ê°ë„ë³„ ë¶„í¬:")
        for severity in ['critical', 'high', 'medium', 'low']:
            count = severity_counts.get(severity, 0)
            if count > 0:
                emoji = {'critical': 'ğŸ”´', 'high': 'ğŸŸ ', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
                print(f"  {emoji[severity]} {severity.upper()}: {count}ê°œ")
        
        print(f"\nì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        for category, count in category_counts.items():
            print(f"  - {category}: {count}ê°œ")
        
        print(f"\nğŸ“ ìƒì„¸ ë¬¸ì œì :")
        for i, issue in enumerate(self.issues, 1):
            emoji = {'critical': 'ğŸ”´', 'high': 'ğŸŸ ', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}
            print(f"\n{i}. {emoji[issue.severity]} [{issue.category.upper()}] {issue.description}")
            print(f"   ìœ„ì¹˜: {issue.location}")
            print(f"   ê¶Œì¥ì‚¬í•­: {issue.recommendation}")

class ImprovedSMSClassifier:
    """ê°œì„ ëœ SMS ë¶„ë¥˜ê¸°"""
    
    def __init__(self, config: Optional[Dict] = None):
        """
        SMS ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
        
        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬ (ì„ íƒì‚¬í•­)
        """
        
        # ê¸°ë³¸ ì„¤ì •
        self.config = {
            'test_size': 0.2,
            'random_state': 42,
            'max_features': 5000,
            'ngram_range': (1, 2),
            'model_dir': Path('models'),
            'cross_validation_folds': 5
        }
        
        if config:
            self.config.update(config)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.config['model_dir'].mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë¸ ìºì‹±
        self.model = None
        self.vectorizer = None
        self.is_trained = False
    
    def validate_input_data(self, data: pd.DataFrame) -> Tuple[bool, str]:
        """ì…ë ¥ ë°ì´í„° ê²€ì¦"""
        
        try:
            # ê¸°ë³¸ ê²€ì¦
            if data is None or data.empty:
                return False, "ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['message', 'label']
            missing_columns = [col for col in required_columns if col not in data.columns]
            
            if missing_columns:
                return False, f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}"
            
            # ë°ì´í„° íƒ€ì… í™•ì¸
            if not data['message'].dtype == 'object':
                return False, "ë©”ì‹œì§€ ì»¬ëŸ¼ì´ ë¬¸ìì—´ íƒ€ì…ì´ ì•„ë‹™ë‹ˆë‹¤"
            
            # ê²°ì¸¡ì¹˜ í™•ì¸
            if data['message'].isnull().any():
                return False, "ë©”ì‹œì§€ì— ê²°ì¸¡ì¹˜ê°€ ìˆìŠµë‹ˆë‹¤"
            
            # ë¼ë²¨ í™•ì¸
            unique_labels = data['label'].unique()
            if len(unique_labels) < 2:
                return False, "ìµœì†Œ 2ê°œì˜ ë‹¤ë¥¸ ë¼ë²¨ì´ í•„ìš”í•©ë‹ˆë‹¤"
            
            self.logger.info(f"ë°ì´í„° ê²€ì¦ í†µê³¼: {len(data)}ê°œ ìƒ˜í”Œ, ë¼ë²¨: {unique_labels}")
            return True, "ê²€ì¦ í†µê³¼"
            
        except Exception as e:
            return False, f"ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def train_classifier(self, data_file: str) -> Dict[str, Any]:
        """
        ê°œì„ ëœ ë¶„ë¥˜ê¸° í›ˆë ¨
        
        Args:
            data_file: í›ˆë ¨ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            
        Returns:
            dict: í›ˆë ¨ ê²°ê³¼
        """
        
        try:
            self.logger.info(f"ëª¨ë¸ í›ˆë ¨ ì‹œì‘: {data_file}")
            
            # 1. ì•ˆì „í•œ ë°ì´í„° ë¡œë“œ
            if not Path(data_file).exists():
                raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
            
            data = pd.read_csv(data_file)
            
            # 2. ë°ì´í„° ê²€ì¦
            is_valid, message = self.validate_input_data(data)
            if not is_valid:
                raise ValueError(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {message}")
            
            # 3. ì „ì²˜ë¦¬
            X = data['message'].astype(str)
            y = data['label']
            
            # 4. ë°ì´í„° ë¶„í•  (ì¬í˜„ ê°€ëŠ¥ì„± ë³´ì¥)
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, 
                test_size=self.config['test_size'],
                random_state=self.config['random_state'],
                stratify=y  # ê³„ì¸µí™” ë¶„í• 
            )
            
            # 5. ì•ˆì „í•œ ë²¡í„°í™”
            self.vectorizer = TfidfVectorizer(
                max_features=self.config['max_features'],
                ngram_range=self.config['ngram_range'],
                stop_words='english',
                lowercase=True,
                strip_accents='ascii'
            )
            
            X_train_vec = self.vectorizer.fit_transform(X_train)
            X_test_vec = self.vectorizer.transform(X_test)
            
            # 6. ëª¨ë¸ í›ˆë ¨
            self.model = MultinomialNB(alpha=1.0)
            self.model.fit(X_train_vec, y_train)
            
            # 7. ì¢…í•© í‰ê°€
            train_score = self.model.score(X_train_vec, y_train)
            test_score = self.model.score(X_test_vec, y_test)
            
            # êµì°¨ ê²€ì¦
            cv_scores = cross_val_score(
                self.model, X_train_vec, y_train,
                cv=self.config['cross_validation_folds']
            )
            
            # ìƒì„¸ í‰ê°€
            y_pred = self.model.predict(X_test_vec)
            
            # 8. ì•ˆì „í•œ ëª¨ë¸ ì €ì¥
            model_path = self.config['model_dir'] / 'sms_model.joblib'
            vectorizer_path = self.config['model_dir'] / 'vectorizer.joblib'
            
            joblib.dump(self.model, model_path)
            joblib.dump(self.vectorizer, vectorizer_path)
            
            self.is_trained = True
            
            # 9. ê²°ê³¼ ë°˜í™˜
            results = {
                'train_accuracy': train_score,
                'test_accuracy': test_score,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std(),
                'classification_report': classification_report(y_test, y_pred, output_dict=True),
                'model_path': str(model_path),
                'vectorizer_path': str(vectorizer_path),
                'training_samples': len(X_train),
                'test_samples': len(X_test)
            }
            
            self.logger.info(f"í›ˆë ¨ ì™„ë£Œ - í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.3f}")
            return results
            
        except Exception as e:
            self.logger.error(f"í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def classify_message(self, message: str) -> Dict[str, Any]:
        """
        ì•ˆì „í•œ ë©”ì‹œì§€ ë¶„ë¥˜
        
        Args:
            message: ë¶„ë¥˜í•  ë©”ì‹œì§€
            
        Returns:
            dict: ë¶„ë¥˜ ê²°ê³¼
        """
        
        try:
            # 1. ì…ë ¥ ê²€ì¦
            if not isinstance(message, str):
                raise ValueError("ë©”ì‹œì§€ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
            
            if len(message.strip()) == 0:
                raise ValueError("ë¹ˆ ë©”ì‹œì§€ëŠ” ë¶„ë¥˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # 2. ëª¨ë¸ ë¡œë“œ (ìºì‹±ëœ ëª¨ë¸ ì‚¬ìš©)
            if not self.is_trained:
                self._load_models()
            
            # 3. ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡
            message_clean = message.strip()
            message_vec = self.vectorizer.transform([message_clean])
            
            prediction = self.model.predict(message_vec)[0]
            probabilities = self.model.predict_proba(message_vec)[0]
            
            # 4. ê²°ê³¼ êµ¬ì„±
            result = {
                'message': message_clean,
                'prediction': prediction,
                'confidence': float(max(probabilities)),
                'probabilities': {
                    label: float(prob) 
                    for label, prob in zip(self.model.classes_, probabilities)
                },
                'message_hash': hashlib.md5(message_clean.encode()).hexdigest()[:16],
                'timestamp': time.time()
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise
    
    def _load_models(self):
        """ëª¨ë¸ê³¼ ë²¡í„°ë¼ì´ì € ë¡œë“œ"""
        
        model_path = self.config['model_dir'] / 'sms_model.joblib'
        vectorizer_path = self.config['model_dir'] / 'vectorizer.joblib'
        
        if not model_path.exists() or not vectorizer_path.exists():
            raise FileNotFoundError("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”.")
        
        self.model = joblib.load(model_path)
        self.vectorizer = joblib.load(vectorizer_path)
        self.is_trained = True
        
        self.logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ê²€ì¦ ë° ê°œì„  í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
print("ğŸ” AI ì½”ë“œ ê²€ì¦ ë° ê°œì„  í”„ë¡œì„¸ìŠ¤")
print("=" * 60)

# 1. ì›ë³¸ AI ì½”ë“œ ê²€ì¦
original_code = '''
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import pickle

def train_spam_classifier(data_file):
    data = pd.read_csv(data_file)
    X = data['message']
    y = data['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    vectorizer = TfidfVectorizer()
    X_train_vec = vectorizer.fit_transform(X_train)
    X_test_vec = vectorizer.transform(X_test)
    model = MultinomialNB()
    model.fit(X_train_vec, y_train)
    predictions = model.predict(X_test_vec)
    accuracy = accuracy_score(y_test, predictions)
    print(f"Accuracy: {accuracy}")
    pickle.dump(model, open('spam_model.pkl', 'wb'))
    pickle.dump(vectorizer, open('vectorizer.pkl', 'wb'))
    return model, vectorizer

def classify_message(message):
    model = pickle.load(open('spam_model.pkl', 'rb'))
    vectorizer = pickle.load(open('vectorizer.pkl', 'rb'))
    message_vec = vectorizer.transform([message])
    prediction = model.predict(message_vec)[0]
    probability = model.predict_proba(message_vec)[0]
    return prediction, max(probability)
'''

# AI ì½”ë“œ ê²€ì¦ ìˆ˜í–‰
validator = AICodeValidator()
issues = validator.validate_code(original_code, "original_ai_code.py")

print(f"\nâœ¨ ê°œì„ ëœ ì½”ë“œì˜ ì¥ì :")
print("=" * 30)
print("âœ… ì•ˆì „í•œ joblib ì§ë ¬í™” ì‚¬ìš©")
print("âœ… í¬ê´„ì ì¸ ì…ë ¥ ê²€ì¦")
print("âœ… ì„¤ì • ê¸°ë°˜ êµ¬ì¡°ë¡œ ìœ ì—°ì„± ì¦ëŒ€")  
print("âœ… ëª¨ë¸ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ ")
print("âœ… ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹… ì¶”ê°€")
print("âœ… êµì°¨ ê²€ì¦ìœ¼ë¡œ ì‹ ë¢°ì„± í™•ë³´")
print("âœ… ìƒì„¸í•œ í‰ê°€ ì§€í‘œ ì œê³µ")
print("âœ… ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ë³´ì¥")

# ê°œì„ ëœ ë¶„ë¥˜ê¸° ì‚¬ìš© ì˜ˆì‹œ (ì‹¤ì œ ë°ì´í„°ê°€ ìˆë‹¤ë©´)
print(f"\nğŸš€ ê°œì„ ëœ ë¶„ë¥˜ê¸° ì‚¬ìš©ë²•:")
print("=" * 30)
print("""
# ê°œì„ ëœ ë¶„ë¥˜ê¸° ì‚¬ìš©
classifier = ImprovedSMSClassifier({
    'test_size': 0.25,
    'max_features': 10000,
    'cross_validation_folds': 10
})

# ì•ˆì „í•œ í›ˆë ¨
try:
    results = classifier.train_classifier('sms_data.csv')
    print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {results['test_accuracy']:.3f}")
    print(f"êµì°¨ ê²€ì¦ í‰ê· : {results['cv_mean']:.3f} Â± {results['cv_std']:.3f}")
except Exception as e:
    print(f"í›ˆë ¨ ì‹¤íŒ¨: {e}")

# ì•ˆì „í•œ ì˜ˆì¸¡
try:
    result = classifier.classify_message("FREE money! Call now!")
    print(f"ì˜ˆì¸¡: {result['prediction']}")
    print(f"ì‹ ë¢°ë„: {result['confidence']:.3f}")
except Exception as e:
    print(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
""")

## 6. ì§ì ‘ í•´ë³´ê¸°: ì—°ìŠµ ë¬¸ì œ

ë‹¤ìŒ ì—°ìŠµ ë¬¸ì œë“¤ì„ í†µí•´ AI ìƒì„± ì½”ë“œ ê²€ì¦ ë° ìµœì í™” ê¸°ìˆ ì„ ì—°ë§ˆí•´ë³´ì„¸ìš”.

### **ì—°ìŠµ ë¬¸ì œ 1: ì½”ë“œ í’ˆì§ˆ ìë™ í‰ê°€ê¸° êµ¬í˜„**

ë‹¤ìŒ AI ìƒì„± ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ ë¬¸ì œì ì„ ì°¾ê³  ê°œì„  ë°©ì•ˆì„ ì œì‹œí•˜ëŠ” ìë™ í‰ê°€ê¸°ë¥¼ êµ¬í˜„í•˜ì„¸ìš”:

```python
# í‰ê°€ ëŒ€ìƒ ì½”ë“œ
def process_data(data):
    result = []
    for i in range(len(data)):
        if data[i] != None:
            if type(data[i]) == str:
                if len(data[i]) > 0:
                    clean = data[i].lower()
                    words = clean.split()
                    for word in words:
                        if word not in ["the", "and", "or"]:
                            result.append(word)
    return result

# ê³¼ì œ:
# 1. ìœ„ ì½”ë“œì˜ ë¬¸ì œì ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ëŠ” í•¨ìˆ˜ ì‘ì„±
# 2. ê° ë¬¸ì œì ì— ëŒ€í•œ ì‹¬ê°ë„ í‰ê°€ (1-10ì )
# 3. êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ ì œì‹œ
# 4. ê°œì„ ëœ ë²„ì „ì˜ ì½”ë“œ ì‘ì„±

def analyze_code_quality(code_string):
    """
    ì½”ë“œ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”
    
    ë°˜í™˜ê°’ ì˜ˆì‹œ:
    {
        'overall_score': 6.5,
        'issues': [
            {
                'type': 'performance',
                'severity': 7,
                'description': 'ë¹„íš¨ìœ¨ì ì¸ ë°˜ë³µë¬¸ êµ¬ì¡°',
                'line': 3,
                'suggestion': 'list comprehension ì‚¬ìš© ê¶Œì¥'
            }
        ],
        'improved_code': 'ê°œì„ ëœ ì½”ë“œ ë¬¸ìì—´'
    }
    """
    # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
    pass

# ì‘ì„± ê³µê°„:
[ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”]
```

### **ì—°ìŠµ ë¬¸ì œ 2: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹ ì‹œìŠ¤í…œ**

ë‘ ê°€ì§€ ë‹¤ë¥¸ êµ¬í˜„ ë°©ì‹ì˜ ì„±ëŠ¥ì„ ìë™ìœ¼ë¡œ ë¹„êµí•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”:

```python
# ë¹„êµ ëŒ€ìƒ í•¨ìˆ˜ 1 (AI ìƒì„±)
def find_duplicates_v1(texts):
    duplicates = []
    for i in range(len(texts)):
        for j in range(i+1, len(texts)):
            if texts[i] == texts[j]:
                if texts[i] not in duplicates:
                    duplicates.append(texts[i])
    return duplicates

# ë¹„êµ ëŒ€ìƒ í•¨ìˆ˜ 2 (ê°œì„ ëœ ë²„ì „)
def find_duplicates_v2(texts):
    seen = set()
    duplicates = set()
    for text in texts:
        if text in seen:
            duplicates.add(text)
        else:
            seen.add(text)
    return list(duplicates)

# ê³¼ì œ:
# 1. ë‹¤ì–‘í•œ í¬ê¸°ì˜ ë°ì´í„°ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
# 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
# 3. ì‹œê°„ ë³µì¡ë„ ë¶„ì„
# 4. ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” ì°¨íŠ¸ ìƒì„±
# 5. ì–´ëŠ ìƒí™©ì—ì„œ ì–´ë–¤ ë°©ë²•ì´ ë” ì¢‹ì€ì§€ ë¶„ì„

class PerformanceBenchmark:
    def __init__(self):
        pass
    
    def compare_functions(self, func1, func2, test_data_sizes=[100, 1000, 10000]):
        """
        ë‘ í•¨ìˆ˜ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ë©”ì„œë“œë¥¼ êµ¬í˜„í•˜ì„¸ìš”
        
        Args:
            func1, func2: ë¹„êµí•  í•¨ìˆ˜ë“¤
            test_data_sizes: í…ŒìŠ¤íŠ¸í•  ë°ì´í„° í¬ê¸° ë¦¬ìŠ¤íŠ¸
            
        Returns:
            dict: ì„±ëŠ¥ ë¹„êµ ê²°ê³¼
        """
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass
    
    def plot_comparison(self, results):
        """ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” ë©”ì„œë“œ"""
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass

# ì‘ì„± ê³µê°„:
[ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”]
```

### **ì—°ìŠµ ë¬¸ì œ 3: ë³´ì•ˆ ì·¨ì•½ì  ìŠ¤ìºë„ˆ**

AIê°€ ìƒì„±í•œ ë°ì´í„° ì²˜ë¦¬ ì½”ë“œì—ì„œ ë³´ì•ˆ ì·¨ì•½ì ì„ ìë™ìœ¼ë¡œ íƒì§€í•˜ëŠ” ìŠ¤ìºë„ˆë¥¼ êµ¬í˜„í•˜ì„¸ìš”:

```python
# ìŠ¤ìº” ëŒ€ìƒ ì½”ë“œë“¤
vulnerable_codes = [
    '''
def load_user_data(user_id):
    query = f"SELECT * FROM users WHERE id = {user_id}"
    return database.execute(query)
    ''',
    
    '''
def save_file(filename, content):
    with open(f"uploads/{filename}", "w") as f:
        f.write(content)
    ''',
    
    '''
def calculate_result(expression):
    return eval(expression)
    ''',
    
    '''
def log_activity(user_input):
    print(f"User activity: {user_input}")
    with open("activity.log", "a") as f:
        f.write(f"{user_input}\\n")
    '''
]

# ê³¼ì œ:
# 1. SQL Injection ìœ„í—˜ íƒì§€
# 2. Path Traversal ì·¨ì•½ì  ì°¾ê¸°
# 3. Code Injection ê°€ëŠ¥ì„± í™•ì¸
# 4. ì •ë³´ ë…¸ì¶œ ìœ„í—˜ í‰ê°€
# 5. ê° ì·¨ì•½ì ì— ëŒ€í•œ ìˆ˜ì • ë°©ì•ˆ ì œì‹œ

class SecurityScanner:
    def __init__(self):
        self.vulnerability_patterns = {
            # ì—¬ê¸°ì— ì·¨ì•½ì  íŒ¨í„´ë“¤ì„ ì •ì˜í•˜ì„¸ìš”
        }
    
    def scan_code(self, code_string):
        """
        ì½”ë“œì—ì„œ ë³´ì•ˆ ì·¨ì•½ì ì„ ìŠ¤ìº”í•˜ëŠ” ë©”ì„œë“œ
        
        Returns:
            dict: ë°œê²¬ëœ ì·¨ì•½ì ê³¼ ê¶Œì¥ì‚¬í•­
        """
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass
    
    def generate_security_report(self, scan_results):
        """ë³´ì•ˆ ìŠ¤ìº” ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass

# ì‘ì„± ê³µê°„:
[ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”]
```

### **ì—°ìŠµ ë¬¸ì œ 4: ì¢…í•© ì½”ë“œ ê°œì„  í”„ë¡œì íŠ¸**

ë‹¤ìŒ AI ìƒì„± ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì¢…í•©ì ìœ¼ë¡œ ê°œì„ í•˜ì„¸ìš”:

```python
# AIê°€ ìƒì„±í•œ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ë¬¸ì œì  ë‹¤ìˆ˜ í¬í•¨)
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import pickle

def analyze_customer_data():
    # ë°ì´í„° ë¡œë“œ
    data = pd.read_csv("customer_data.csv")
    
    # ì „ì²˜ë¦¬
    data = data.dropna()
    X = data.drop('target', axis=1)
    y = data['target']
    
    # ëª¨ë¸ í›ˆë ¨
    model = RandomForestClassifier()
    model.fit(X, y)
    
    # ì˜ˆì¸¡
    predictions = model.predict(X)
    accuracy = (predictions == y).mean()
    
    # ê²°ê³¼ ì €ì¥
    pickle.dump(model, open("model.pkl", "wb"))
    
    print(f"Accuracy: {accuracy}")
    return model

def predict_customer(customer_data):
    model = pickle.load(open("model.pkl", "rb"))
    prediction = model.predict([customer_data])
    return prediction[0]

# ê³¼ì œ:
# 1. ëª¨ë“  ë¬¸ì œì ì„ ì‹ë³„í•˜ê³  ë¶„ë¥˜í•˜ì„¸ìš”
# 2. ìš°ì„ ìˆœìœ„ë¥¼ ë§¤ê²¨ ê°œì„  ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”
# 3. ì™„ì „íˆ ê°œì„ ëœ ë²„ì „ì„ êµ¬í˜„í•˜ì„¸ìš”
# 4. ê°œì„  ì „í›„ì˜ ì°¨ì´ì ì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ì„¸ìš”
# 5. í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì™„ì„±í•˜ì„¸ìš”

class ImprovedDataPipeline:
    """ê°œì„ ëœ ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config=None):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        - ì„¤ì • ê´€ë¦¬
        - ë¡œê¹… ì„¤ì •
        - ê²€ì¦ ì²´ê³„ êµ¬ì¶•
        """
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass
    
    def validate_data(self, data):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass
    
    def preprocess_data(self, data):
        """ì•ˆì „í•œ ë°ì´í„° ì „ì²˜ë¦¬"""
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass
    
    def train_model(self, data_path):
        """ê°œì„ ëœ ëª¨ë¸ í›ˆë ¨"""
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass
    
    def predict(self, customer_data):
        """ì•ˆì „í•œ ì˜ˆì¸¡"""
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass
    
    def evaluate_model(self, test_data):
        """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
        # ì—¬ê¸°ì— êµ¬í˜„í•˜ì„¸ìš”
        pass

# ì‘ì„± ê³µê°„:
[ì—¬ê¸°ì— ì™„ì „íˆ ê°œì„ ëœ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•˜ì„¸ìš”]
```

## 7. í•µì‹¬ ì •ë¦¬ ë° ìš”ì•½

### ğŸ¯ **ì´ë²ˆ Partì—ì„œ ë°°ìš´ í•µì‹¬ ë‚´ìš©**

1. **AI ìƒì„± ì½”ë“œì˜ ì¼ë°˜ì  ì˜¤ë¥˜ íŒ¨í„´**
   - **ê¸°ëŠ¥ì  ì˜¤ë¥˜**: ì—£ì§€ ì¼€ì´ìŠ¤ ì²˜ë¦¬ ë¶€ì¡±, ë°ì´í„° íƒ€ì… í˜¼ë™
   - **ì„±ëŠ¥ ê´€ë ¨ ì˜¤ë¥˜**: ë¹„íš¨ìœ¨ì ì¸ ë°˜ë³µë¬¸, ë¶€ì ì ˆí•œ ìë£Œêµ¬ì¡° ì„ íƒ
   - **ë…¼ë¦¬ì  ì˜¤ë¥˜**: ì˜ëª»ëœ ê°€ì •, ë‹¨ìˆœí™”ëœ ì‚¬ê³ ë°©ì‹

2. **ì½”ë“œ í’ˆì§ˆ í‰ê°€ì˜ 5ê°€ì§€ ì°¨ì›**
   - **ê¸°ëŠ¥ì  ì •í™•ì„±**: ì˜ë„í•œ ëŒ€ë¡œ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
   - **ì„±ëŠ¥ íš¨ìœ¨ì„±**: ì‹¤í–‰ ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
   - **ì½”ë“œ í’ˆì§ˆ**: ê°€ë…ì„±, ìœ ì§€ë³´ìˆ˜ì„±, ì¼ê´€ì„± í‰ê°€
   - **ë³´ì•ˆì„±**: ì·¨ì•½ì  íƒì§€ ë° ë³´ì•ˆ ê°•í™”
   - **ì•ˆì •ì„±**: ì˜ˆì™¸ ì²˜ë¦¬ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜

3. **ì„±ëŠ¥ ìµœì í™” ì „ëµ**
   - **ë°ì´í„° êµ¬ì¡° ìµœì í™”**: ì í•©í•œ ìë£Œêµ¬ì¡° ì„ íƒ (ì§‘í•©, íŠ¸ë¼ì´ ë“±)
   - **ì•Œê³ ë¦¬ì¦˜ ê°œì„ **: ì‹œê°„ ë³µì¡ë„ ìµœì í™”, ë²¡í„°í™” ì—°ì‚° í™œìš©
   - **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬, ë°°ì¹˜ ì²˜ë¦¬ í™œìš©

4. **ë³´ì•ˆ ë° ì•ˆì •ì„± ê²€ì¦**
   - **ì…ë ¥ ê²€ì¦**: í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë°©ì‹, Parameterized Query
   - **ê¶Œí•œ ê´€ë¦¬**: ì—­í•  ê¸°ë°˜ ì ‘ê·¼ ì œì–´, ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…
   - **ì˜ˆì™¸ ì²˜ë¦¬**: ì—ëŸ¬ ê²½ê³„, ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„

### ğŸ’¡ **ì‹¤ë¬´ ì ìš© ê°€ì´ë“œë¼ì¸**

**ë‹¨ê³„ë³„ ì½”ë“œ ê²€ì¦ í”„ë¡œì„¸ìŠ¤:**
1. **êµ¬ë¬¸ ë° ë…¼ë¦¬ ê²€ì¦** â†’ AST íŒŒì‹±, ì •ì  ë¶„ì„
2. **ë³´ì•ˆ ì·¨ì•½ì  ìŠ¤ìº”** â†’ íŒ¨í„´ ë§¤ì¹­, ë³´ì•ˆ ë„êµ¬ í™œìš©
3. **ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§** â†’ ì‹¤í–‰ ì‹œê°„, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
4. **ì•ˆì •ì„± í…ŒìŠ¤íŠ¸** â†’ ì˜ˆì™¸ ìƒí™©, ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
5. **ì¢…í•© í’ˆì§ˆ í‰ê°€** â†’ ë‹¤ì°¨ì› ì§€í‘œ ê¸°ë°˜ ë“±ê¸‰ ì‚°ì •

**ì½”ë“œ ê°œì„  ìš°ì„ ìˆœìœ„:**
1. **ğŸ”´ Critical**: ë³´ì•ˆ ì·¨ì•½ì , ì¹˜ëª…ì  ë²„ê·¸
2. **ğŸŸ  High**: ì„±ëŠ¥ ë³‘ëª©, ì•ˆì •ì„± ì´ìŠˆ  
3. **ğŸŸ¡ Medium**: ê°€ë…ì„±, ìœ ì§€ë³´ìˆ˜ì„± ë¬¸ì œ
4. **ğŸŸ¢ Low**: ì½”ë“œ ìŠ¤íƒ€ì¼, ìµœì í™” ì—¬ì§€

### ğŸ”„ **ì§€ì†ì  ê°œì„  ì „ëµ**

1. **ìë™í™”ëœ ê²€ì¦ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
   - CI/CDì— ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬ í†µí•©
   - ìë™í™”ëœ ë³´ì•ˆ ìŠ¤ìº” ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
   - í’ˆì§ˆ ì§€í‘œ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

2. **íŒ€ ì½”ë“œ ë¦¬ë·° ë¬¸í™” ì •ì°©**
   - AI ìƒì„± ì½”ë“œ ë¦¬ë·° ì²´í¬ë¦¬ìŠ¤íŠ¸ í™œìš©
   - ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ ê°œë°œì ê°„ í˜‘ì—… ê°•í™”
   - ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ê³µìœ  ë° í•™ìŠµ

3. **í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§**
   - ì„±ëŠ¥ ì§€í‘œ ì‹¤ì‹œê°„ ì¶”ì 
   - ì˜¤ë¥˜ìœ¨ ë° ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§
   - ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê°œì„ 

### ğŸ› ï¸ **ì¶”ì²œ ë„êµ¬ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬**

**ì •ì  ë¶„ì„ ë„êµ¬:**
- **pylint**: ì¢…í•©ì ì¸ ì½”ë“œ í’ˆì§ˆ ë¶„ì„
- **flake8**: PEP 8 ìŠ¤íƒ€ì¼ ê°€ì´ë“œ ì¤€ìˆ˜
- **mypy**: íƒ€ì… íŒíŠ¸ ê²€ì¦
- **bandit**: ë³´ì•ˆ ì·¨ì•½ì  ìŠ¤ìº”

**ì„±ëŠ¥ ë¶„ì„ ë„êµ¬:**
- **memory_profiler**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
- **line_profiler**: ë¼ì¸ë³„ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
- **py-spy**: í”„ë¡œë•ì…˜ í™˜ê²½ í”„ë¡œíŒŒì¼ë§
- **locust**: ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

**ë³´ì•ˆ ë„êµ¬:**
- **safety**: ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ë³´ì•ˆ ê²€ì‚¬
- **semgrep**: ì •ì  ë³´ì•ˆ ë¶„ì„
- **codecov**: ì½”ë“œ ì»¤ë²„ë¦¬ì§€ ì¸¡ì •

## 8. ë‹¤ìŒ Part ì˜ˆê³ : ìë™í™”ì™€ ìˆ˜ë™ ì‘ì—…ì˜ ê· í˜• ì°¾ê¸°

ë‹¤ìŒ Partì—ì„œëŠ” ë°ì´í„° ë¶„ì„ ì‘ì—…ì—ì„œ ì–´ë–¤ ë¶€ë¶„ì„ AIì—ê²Œ ë§¡ê¸°ê³ , ì–´ë–¤ ë¶€ë¶„ì€ ì¸ê°„ì´ ì§ì ‘ í•´ì•¼ í•˜ëŠ”ì§€ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.

**ë‹¤ìŒ Part ì£¼ìš” ë‚´ìš©:**
- AI ìë™í™”ì— ì í•©í•œ ì‘ì—…ê³¼ ì¸ê°„ ê°œì…ì´ í•„ìš”í•œ ì‘ì—… êµ¬ë¶„
- íš¨ìœ¨ì ì¸ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ì™€ ì—­í•  ë¶„ë‹´ ì „ëµ
- í’ˆì§ˆ ê´€ë¦¬ë¥¼ ìœ„í•œ ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ë°©ë²•
- ì¸ê°„-AI í˜‘ì—…ì˜ ìµœì í™” ê¸°ë²•

**ë¯¸ë¦¬ ìƒê°í•´ë³¼ ì§ˆë¬¸:**
- ë°ì´í„° ë¶„ì„ ê³¼ì •ì—ì„œ AIê°€ ì˜í•˜ëŠ” ê²ƒê³¼ ëª»í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¼ê¹Œìš”?
- ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ìë™í™”í•  ì‘ì—…ì„ ì„ ë³„í•´ì•¼ í• ê¹Œìš”?
- í’ˆì§ˆì„ ë³´ì¥í•˜ë©´ì„œë„ íš¨ìœ¨ì„±ì„ ë†’ì´ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?

> ğŸ–¼ï¸ **ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸**: 
> "AI ì½”ë“œ ê²€ì¦ ë° ê°œì„  ì›Œí¬í”Œë¡œìš°ë¥¼ ë³´ì—¬ì£¼ëŠ” ì¢…í•© ë‹¤ì´ì–´ê·¸ë¨. ì›ë³¸ AI ì½”ë“œì—ì„œ ì‹œì‘í•˜ì—¬ ê²€ì¦ â†’ ë¬¸ì œ íƒì§€ â†’ ìš°ì„ ìˆœìœ„ ì„¤ì • â†’ ê°œì„  â†’ í…ŒìŠ¤íŠ¸ â†’ ë°°í¬ ë‹¨ê³„ê°€ ìˆœí™˜í•˜ë©°, ê° ë‹¨ê³„ë§ˆë‹¤ ì²´í¬ë¦¬ìŠ¤íŠ¸ì™€ ë„êµ¬ë“¤ì´ í‘œì‹œëœ í”„ë¡œí˜ì…”ë„í•œ í”„ë¡œì„¸ìŠ¤ ì°¨íŠ¸"

---

**ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! 7ì¥ Part 2ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!**

AI ìƒì„± ì½”ë“œë¥¼ ê²€ì¦í•˜ê³  ê°œì„ í•˜ëŠ” ê²ƒì€ í˜„ëŒ€ ë°ì´í„° ë¶„ì„ê°€ì˜ í•„ìˆ˜ ìŠ¤í‚¬ì…ë‹ˆë‹¤. ì´ë²ˆ Partì—ì„œ ë°°ìš´ ì²´ê³„ì ì¸ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ì™€ ìµœì í™” ê¸°ë²•ì„ í†µí•´ AIì˜ ì¥ì ì„ í™œìš©í•˜ë©´ì„œë„ ì•ˆì „í•˜ê³  íš¨ìœ¨ì ì¸ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. 

ì½”ë“œ í’ˆì§ˆì€ í•œ ë²ˆì— ì™„ì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì§€ì†ì ì¸ ê²€ì¦ê³¼ ê°œì„ ì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ ë°œì „ì‹œì¼œ ë‚˜ê°€ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ë‹¤ìŒ Partì—ì„œëŠ” AIì™€ ì¸ê°„ì˜ ì—­í• ì„ ì–´ë–»ê²Œ íš¨ê³¼ì ìœ¼ë¡œ ë¶„ë‹´í• ì§€ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤!

