# 4ì¥ Part 4: AI ë„êµ¬ë¥¼ í™œìš©í•œ ìë™ ì „ì²˜ë¦¬ì™€ í•œê³„ì 
## ì¸ê³µì§€ëŠ¥ì˜ í˜ì„ í™œìš©í•˜ë©´ì„œë„ ê·¸ í•œê³„ë¥¼ ì´í•´í•˜ëŠ” ê· í˜•ì¡íŒ ì ‘ê·¼

---

## ğŸ“š í•™ìŠµ ëª©í‘œ

ì´ë²ˆ Partì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤:

âœ… **AI ìë™ ì „ì²˜ë¦¬ ë„êµ¬ì˜ ì¥ì ê³¼ í•œê³„ì ì„ ëª…í™•íˆ ì´í•´í•  ìˆ˜ ìˆë‹¤**
âœ… **AutoML í”Œë«í¼ì˜ ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤**  
âœ… **AI ê¸°ë°˜ íŠ¹ì„± ìƒì„± ë„êµ¬ì˜ ì›ë¦¬ì™€ ì ìš© ë°©ë²•ì„ ìµí ìˆ˜ ìˆë‹¤**
âœ… **ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹ì˜ ì‘ë™ ë°©ì‹ê³¼ ê²€ì¦ ë°©ë²•ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤**
âœ… **ì¸ê°„-AI í˜‘ì—… ì „ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°ë¥¼ ì„¤ê³„í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆë‹¤**

---

## ğŸ¯ ì´ë²ˆ Part ë¯¸ë¦¬ë³´ê¸°

**AI ê¸°ìˆ ì˜ ë°œì „**ìœ¼ë¡œ ë°ì´í„° ì „ì²˜ë¦¬ë„ ìë™í™”ì˜ ì‹œëŒ€ë¥¼ ë§ì´í–ˆìŠµë‹ˆë‹¤. AutoML í”Œë«í¼ë¶€í„° ì§€ëŠ¥í˜• íŠ¹ì„± ìƒì„± ë„êµ¬ê¹Œì§€, ì´ì œ AIê°€ ë°ì´í„° ì „ì²˜ë¦¬ì˜ ë§ì€ ë¶€ë¶„ì„ ëŒ€ì‹  ì²˜ë¦¬í•´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ **"AIê°€ ëª¨ë“  ê²ƒì„ í•´ê²°í•´ì¤€ë‹¤"**ëŠ” ìƒê°ì€ ìœ„í—˜í•©ë‹ˆë‹¤. AI ë„êµ¬ë“¤ì€ ë¶„ëª… ê°•ë ¥í•˜ì§€ë§Œ, **ë¸”ë™ë°•ìŠ¤ ë¬¸ì œ**, **í¸í–¥ì„± ì¦í­**, **ë„ë©”ì¸ ì§€ì‹ ë¶€ì¡±** ë“±ì˜ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ì´ë²ˆ Partì—ì„œëŠ” **AIì˜ í˜ì„ ìµœëŒ€í•œ í™œìš©í•˜ë©´ì„œë„ ê·¸ í•œê³„ë¥¼ ì •í™•íˆ ì´í•´**í•˜ì—¬, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤. 

íŠ¹íˆ **7ì¥ì—ì„œ í•™ìŠµí•œ AI í˜‘ì—… ê¸°ë²•**ì„ ì „ì²˜ë¦¬ ì˜ì—­ì— íŠ¹í™”í•˜ì—¬ ì ìš©í•˜ê³ , ì‹¤ì œ House Prices ë°ì´í„°ë¥¼ í†µí•´ **ì¸ê°„ì˜ íŒë‹¨ë ¥ê³¼ AIì˜ íš¨ìœ¨ì„±ì„ ê²°í•©**í•œ ìµœì ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•´ë³´ê² ìŠµë‹ˆë‹¤.

> **ğŸ’¡ Part 4ì˜ í•µì‹¬ í¬ì¸íŠ¸**  
> "AIëŠ” ê°•ë ¥í•œ ë„êµ¬ì´ì§€ë§Œ ë§ŒëŠ¥ì´ ì•„ë‹™ë‹ˆë‹¤. ì¸ê°„ì˜ ì§€í˜œì™€ AIì˜ íš¨ìœ¨ì„±ì„ ì¡°í™”ë¡­ê²Œ ê²°í•©í•  ë•Œ ìµœê³ ì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

---

## ğŸ“– 4.4.1 AutoML ì „ì²˜ë¦¬ ë„êµ¬ ì†Œê°œì™€ í™œìš©

### AutoML ì „ì²˜ë¦¬ì˜ í˜ì‹ 

**AutoML(Automated Machine Learning)**ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì „ ê³¼ì •ì„ ìë™í™”í•˜ëŠ” ê¸°ìˆ ë¡œ, ì „ì²˜ë¦¬ ì˜ì—­ì—ì„œë„ í˜ì‹ ì ì¸ ë³€í™”ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **AutoML**: ë¨¸ì‹ ëŸ¬ë‹ íŒŒì´í”„ë¼ì¸ì˜ ìë™í™” ê¸°ìˆ 
> - **Auto-preprocessing**: ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì˜ ìë™í™”
> - **Meta-learning**: ê³¼ê±° ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” AI ê¸°ë²•
> - **Pipeline optimization**: ì „ì²˜ë¦¬ ë‹¨ê³„ë“¤ì˜ ìµœì  ì¡°í•© íƒìƒ‰

### ì£¼ìš” AutoML í”Œë«í¼ ë¹„êµ

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# AutoML í”Œë«í¼ ë¹„êµë¥¼ ìœ„í•œ ë°ëª¨ í•¨ìˆ˜
def compare_automl_platforms():
    """
    ì£¼ìš” AutoML í”Œë«í¼ë“¤ì˜ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ë¹„êµ
    """
    print("ğŸ¤– ì£¼ìš” AutoML í”Œë«í¼ ì „ì²˜ë¦¬ ê¸°ëŠ¥ ë¹„êµ:")
    
    # í”Œë«í¼ë³„ íŠ¹ì§• ì •ë¦¬
    platforms = {
        'H2O AutoML': {
            'strengths': ['ê°•ë ¥í•œ ìë™ íŠ¹ì„± ê³µí•™', 'ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬', 'ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ì§€ì›'],
            'preprocessing_features': ['ìë™ ì¸ì½”ë”©', 'ê²°ì¸¡ì¹˜ ì²˜ë¦¬', 'íŠ¹ì„± ì„ íƒ', 'ìŠ¤ì¼€ì¼ë§'],
            'limitations': ['ë³µì¡í•œ ì„¤ì •', 'í•´ì„ì„± ì œí•œ', 'ì»¤ìŠ¤í„°ë§ˆì´ì§• ì–´ë ¤ì›€'],
            'best_for': 'ëŒ€ê·œëª¨ ì •í˜• ë°ì´í„°, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘',
            'cost': 'Free + Enterprise ë²„ì „',
            'complexity': 'Medium-High'
        },
        'Google AutoML': {
            'strengths': ['í´ë¼ìš°ë“œ í†µí•©', 'ì‚¬ìš© í¸ì˜ì„±', 'í™•ì¥ì„±'],
            'preprocessing_features': ['ìë™ ë°ì´í„° ê²€ì¦', 'íŠ¹ì„± ì¤‘ìš”ë„', 'ìë™ ë³€í™˜'],
            'limitations': ['ë¹„ìš©', 'Google ìƒíƒœê³„ ì¢…ì†', 'ì œí•œì  ì»¤ìŠ¤í„°ë§ˆì´ì§•'],
            'best_for': 'í´ë¼ìš°ë“œ í™˜ê²½, ë¹„ê°œë°œì ì¹œí™”ì ',
            'cost': 'ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê³¼ê¸ˆ',
            'complexity': 'Low'
        },
        'DataRobot': {
            'strengths': ['ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ê¸°ëŠ¥', 'ëª¨ë¸ í•´ì„ì„±', 'ë°°í¬ ì§€ì›'],
            'preprocessing_features': ['ì§€ëŠ¥í˜• ì „ì²˜ë¦¬', 'ìë™ íŠ¹ì„± ë°œê²¬', 'ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬'],
            'limitations': ['ë†’ì€ ë¹„ìš©', 'ë³µì¡í•œ ì¸í„°í˜ì´ìŠ¤', 'í•™ìŠµ ê³¡ì„ '],
            'best_for': 'ëŒ€ê¸°ì—…, ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ',
            'cost': 'Enterprise ë¼ì´ì„ ìŠ¤',
            'complexity': 'High'
        },
        'AutoGluon': {
            'strengths': ['ì˜¤í”ˆì†ŒìŠ¤', 'ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì… ì§€ì›', 'AWS í†µí•©'],
            'preprocessing_features': ['ìë™ ì „ì²˜ë¦¬', 'í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì§€ì›', 'ì•™ìƒë¸” ìµœì í™”'],
            'limitations': ['ìƒëŒ€ì ìœ¼ë¡œ ìƒˆë¡œìš´ í”Œë«í¼', 'ì»¤ë®¤ë‹ˆí‹° í¬ê¸°', 'ë¬¸ì„œí™”'],
            'best_for': 'ì—°êµ¬, í”„ë¡œí† íƒ€ì´í•‘, AWS í™˜ê²½',
            'cost': 'Free (ì˜¤í”ˆì†ŒìŠ¤)',
            'complexity': 'Medium'
        },
        'TPOT': {
            'strengths': ['íŒŒì´ì¬ ë„¤ì´í‹°ë¸Œ', 'ìœ ì „ ì•Œê³ ë¦¬ì¦˜', 'íˆ¬ëª…ì„±'],
            'preprocessing_features': ['íŒŒì´í”„ë¼ì¸ ìµœì í™”', 'scikit-learn ê¸°ë°˜', 'ì½”ë“œ ìƒì„±'],
            'limitations': ['ëŠë¦° ì†ë„', 'ì œí•œì  ìŠ¤ì¼€ì¼ë§', 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰'],
            'best_for': 'ì—°êµ¬, êµìœ¡, ì‘ì€ ë°ì´í„°ì…‹',
            'cost': 'Free (ì˜¤í”ˆì†ŒìŠ¤)',
            'complexity': 'Medium'
        }
    }
    
    # ë¹„êµí‘œ ì¶œë ¥
    print(f"\nğŸ“Š í”Œë«í¼ë³„ ìƒì„¸ ë¹„êµ:")
    for platform, features in platforms.items():
        print(f"\nğŸ”¹ {platform}")
        print(f"   ğŸ’ª ì¥ì : {', '.join(features['strengths'])}")
        print(f"   âš™ï¸  ì „ì²˜ë¦¬ ê¸°ëŠ¥: {', '.join(features['preprocessing_features'])}")
        print(f"   âš ï¸  í•œê³„ì : {', '.join(features['limitations'])}")
        print(f"   ğŸ¯ ìµœì  ìš©ë„: {features['best_for']}")
        print(f"   ğŸ’° ë¹„ìš©: {features['cost']}")
        print(f"   ğŸ“ˆ ë³µì¡ë„: {features['complexity']}")
    
    return platforms

# í”Œë«í¼ ë¹„êµ ì‹¤í–‰
platform_comparison = compare_automl_platforms()
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- 5ê°€ì§€ ì£¼ìš” AutoML í”Œë«í¼ì˜ íŠ¹ì§•ì„ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
- ê° í”Œë«í¼ì˜ ì¥ë‹¨ì ê³¼ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ëª…í™•íˆ êµ¬ë¶„
- ë¹„ìš©ê³¼ ë³µì¡ë„ê¹Œì§€ ê³ ë ¤í•œ ì‹¤ë¬´ì  ì„ íƒ ê°€ì´ë“œ ì œê³µ

### ì‹¤ì œ AutoML ì „ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜

```python
# AutoML ìŠ¤íƒ€ì¼ ìë™ ì „ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
class AutoPreprocessor:
    """
    AutoML ë„êµ¬ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, aggressive_mode=False):
        self.aggressive_mode = aggressive_mode  # ê³µê²©ì  ì „ì²˜ë¦¬ ëª¨ë“œ
        self.preprocessing_steps = []
        self.feature_importance = {}
        self.warnings = []
    
    def analyze_data_quality(self, df):
        """ë°ì´í„° í’ˆì§ˆ ìë™ ë¶„ì„"""
        print("ğŸ” ìë™ ë°ì´í„° í’ˆì§ˆ ë¶„ì„:")
        
        quality_report = {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'missing_data': {},
            'data_types': {},
            'potential_issues': []
        }
        
        # ê²°ì¸¡ì¹˜ ë¶„ì„
        for col in df.columns:
            missing_pct = df[col].isnull().sum() / len(df) * 100
            if missing_pct > 0:
                quality_report['missing_data'][col] = missing_pct
                
                if missing_pct > 50:
                    quality_report['potential_issues'].append(f"{col}: 50% ì´ìƒ ê²°ì¸¡ì¹˜")
                elif missing_pct > 20:
                    quality_report['potential_issues'].append(f"{col}: 20% ì´ìƒ ê²°ì¸¡ì¹˜")
        
        # ë°ì´í„° íƒ€ì… ë¶„ì„
        for col in df.columns:
            dtype = str(df[col].dtype)
            quality_report['data_types'][col] = dtype
            
            # ì ì¬ì  ë¬¸ì œ íƒì§€
            if dtype == 'object':
                unique_ratio = df[col].nunique() / len(df)
                if unique_ratio > 0.95:  # ê±°ì˜ ëª¨ë“  ê°’ì´ ê³ ìœ í•œ ê²½ìš°
                    quality_report['potential_issues'].append(f"{col}: ë†’ì€ ì¹´ë””ë„ë¦¬í‹° (ID ì»¬ëŸ¼ ê°€ëŠ¥ì„±)")
                elif unique_ratio < 0.05:  # ë§¤ìš° ì ì€ ê³ ìœ ê°’
                    quality_report['potential_issues'].append(f"{col}: ë§¤ìš° ë‚®ì€ ë‹¤ì–‘ì„±")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"   ğŸ“Š ë°ì´í„° í¬ê¸°: {quality_report['total_rows']:,}í–‰ Ã— {quality_report['total_columns']}ì—´")
        print(f"   âŒ ê²°ì¸¡ì¹˜ ìˆëŠ” ì»¬ëŸ¼: {len(quality_report['missing_data'])}ê°œ")
        print(f"   âš ï¸  ì ì¬ì  ë¬¸ì œ: {len(quality_report['potential_issues'])}ê°œ")
        
        if quality_report['potential_issues']:
            print(f"\n   ğŸ“‹ ë°œê²¬ëœ ë¬¸ì œì ë“¤:")
            for issue in quality_report['potential_issues'][:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                print(f"      â€¢ {issue}")
        
        return quality_report
    
    def auto_missing_value_strategy(self, df, column):
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ëµ ìë™ ê²°ì •"""
        missing_pct = df[column].isnull().sum() / len(df) * 100
        dtype = df[column].dtype
        
        if missing_pct == 0:
            return 'no_action', 'No missing values'
        elif missing_pct > 70:
            return 'drop_column', f'Too many missing values ({missing_pct:.1f}%)'
        elif dtype in ['int64', 'float64']:
            if missing_pct < 5:
                return 'median_impute', 'Low missing rate - use median'
            elif missing_pct < 20:
                return 'mean_impute', 'Medium missing rate - use mean'
            else:
                return 'mode_impute', 'High missing rate - use mode'
        else:  # object dtype
            if missing_pct < 10:
                return 'mode_impute', 'Use most frequent value'
            else:
                return 'constant_impute', 'Use constant value (Unknown)'
    
    def auto_feature_engineering(self, df, target_col=None):
        """ìë™ íŠ¹ì„± ê³µí•™"""
        print("\nğŸ› ï¸ ìë™ íŠ¹ì„± ê³µí•™:")
        
        df_processed = df.copy()
        new_features = []
        
        # ë‚ ì§œ íŠ¹ì„± ìë™ ê°ì§€ ë° ì¶”ì¶œ
        for col in df.columns:
            if 'year' in col.lower() or 'date' in col.lower():
                if df[col].dtype in ['int64', 'float64'] and df[col].max() > 1900:
                    # ì—°ë„ë¡œ ì¶”ì •ë˜ëŠ” ì»¬ëŸ¼
                    current_year = 2023
                    age_col = f"{col}_Age"
                    df_processed[age_col] = current_year - df[col]
                    new_features.append(age_col)
                    print(f"   âœ… {age_col}: {col}ë¡œë¶€í„° ì—°ë ¹ ê³„ì‚°")
        
        # ìˆ˜ì¹˜í˜• íŠ¹ì„± ì¡°í•© ìë™ ìƒì„±
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if target_col and target_col in numeric_cols:
            numeric_cols.remove(target_col)
        
        if len(numeric_cols) >= 2:
            # ìƒìœ„ ìƒê´€ê´€ê³„ íŠ¹ì„±ë“¤ ì¡°í•©
            if target_col:
                correlations = df[numeric_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)
                top_features = correlations.head(3).index.tolist()
            else:
                top_features = numeric_cols[:3]
            
            # ë¹„ìœ¨ íŠ¹ì„± ìƒì„±
            if len(top_features) >= 2:
                ratio_col = f"{top_features[0]}_per_{top_features[1]}"
                df_processed[ratio_col] = df_processed[top_features[0]] / (df_processed[top_features[1]] + 1)
                new_features.append(ratio_col)
                print(f"   âœ… {ratio_col}: ë¹„ìœ¨ íŠ¹ì„± ìë™ ìƒì„±")
        
        # ë²”ì£¼í˜• íŠ¹ì„± ë¹ˆë„ ì¸ì½”ë”©
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        for col in categorical_cols:
            if df[col].nunique() > 2 and df[col].nunique() < 20:  # ì ì ˆí•œ ì¹´ë””ë„ë¦¬í‹°
                freq_col = f"{col}_Frequency"
                freq_map = df[col].value_counts().to_dict()
                df_processed[freq_col] = df[col].map(freq_map)
                new_features.append(freq_col)
                print(f"   âœ… {freq_col}: ë¹ˆë„ ì¸ì½”ë”© ì ìš©")
        
        print(f"\n   ğŸ“ˆ ì´ {len(new_features)}ê°œ ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±")
        return df_processed, new_features
    
    def auto_preprocessing_pipeline(self, df, target_col=None):
        """ì™„ì „ ìë™ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        print("ğŸ¤– ì™„ì „ ìë™ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰:")
        
        # 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ë¶„ì„
        quality_report = self.analyze_data_quality(df)
        
        # 2ë‹¨ê³„: ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        print("\nğŸ”§ ìë™ ê²°ì¸¡ì¹˜ ì²˜ë¦¬:")
        df_processed = df.copy()
        
        for col in df.columns:
            if col == target_col:
                continue
                
            strategy, reason = self.auto_missing_value_strategy(df, col)
            
            if strategy == 'drop_column':
                df_processed = df_processed.drop(columns=[col])
                print(f"   âŒ {col}: ì»¬ëŸ¼ ì œê±° ({reason})")
                self.warnings.append(f"ì»¬ëŸ¼ {col} ì œê±°ë¨: {reason}")
                
            elif strategy == 'median_impute':
                df_processed[col].fillna(df_processed[col].median(), inplace=True)
                print(f"   ğŸ“Š {col}: ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´")
                
            elif strategy == 'mean_impute':
                df_processed[col].fillna(df_processed[col].mean(), inplace=True)
                print(f"   ğŸ“Š {col}: í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´")
                
            elif strategy == 'mode_impute':
                df_processed[col].fillna(df_processed[col].mode().iloc[0], inplace=True)
                print(f"   ğŸ“Š {col}: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´")
                
            elif strategy == 'constant_impute':
                df_processed[col].fillna('Unknown', inplace=True)
                print(f"   ğŸ“Š {col}: 'Unknown'ìœ¼ë¡œ ëŒ€ì²´")
        
        # 3ë‹¨ê³„: ìë™ íŠ¹ì„± ê³µí•™
        df_processed, new_features = self.auto_feature_engineering(df_processed, target_col)
        
        # 4ë‹¨ê³„: ì¸ì½”ë”© ë° ìŠ¤ì¼€ì¼ë§
        print("\nğŸ”„ ìë™ ì¸ì½”ë”© ë° ìŠ¤ì¼€ì¼ë§:")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()
        
        for col in categorical_cols:
            if df_processed[col].nunique() <= 10:  # ë‚®ì€ ì¹´ë””ë„ë¦¬í‹°
                # ì›-í•« ì¸ì½”ë”©
                dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)
                df_processed = pd.concat([df_processed, dummies], axis=1)
                df_processed = df_processed.drop(columns=[col])
                print(f"   ğŸ”„ {col}: ì›-í•« ì¸ì½”ë”© ì ìš©")
            else:  # ë†’ì€ ì¹´ë””ë„ë¦¬í‹°
                # ë¼ë²¨ ì¸ì½”ë”©
                le = LabelEncoder()
                df_processed[col] = le.fit_transform(df_processed[col].astype(str))
                print(f"   ğŸ”„ {col}: ë¼ë²¨ ì¸ì½”ë”© ì ìš©")
                self.warnings.append(f"ë†’ì€ ì¹´ë””ë„ë¦¬í‹°ë¡œ ì¸í•´ {col}ì— ë¼ë²¨ ì¸ì½”ë”© ì‚¬ìš©")
        
        # 5ë‹¨ê³„: ìµœì¢… ê²€ì¦
        print(f"\nâœ… ìë™ ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   ğŸ“Š ìµœì¢… ë°ì´í„° í¬ê¸°: {df_processed.shape[0]:,}í–‰ Ã— {df_processed.shape[1]}ì—´")
        print(f"   ğŸ†• ìƒì„±ëœ íŠ¹ì„±: {len(new_features)}ê°œ")
        print(f"   âš ï¸  ê²½ê³ ì‚¬í•­: {len(self.warnings)}ê°œ")
        
        if self.warnings:
            print(f"\n   ğŸ“‹ ì£¼ìš” ê²½ê³ ì‚¬í•­:")
            for warning in self.warnings[:3]:
                print(f"      â€¢ {warning}")
        
        return df_processed, {
            'original_shape': df.shape,
            'final_shape': df_processed.shape,
            'new_features': new_features,
            'warnings': self.warnings,
            'quality_report': quality_report
        }

# House Prices ë°ì´í„°ë¡œ AutoML ì „ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
try:
    # ë°ì´í„° ë¡œë“œ
    train_data = pd.read_csv('datasets/house_prices/train.csv')
    print("âœ… House Prices ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
    
    # AutoML ì „ì²˜ë¦¬ ì‹¤í–‰
    auto_processor = AutoPreprocessor(aggressive_mode=False)
    processed_data, processing_report = auto_processor.auto_preprocessing_pipeline(
        train_data, target_col='SalePrice'
    )
    
    print(f"\nğŸ“‹ ì „ì²˜ë¦¬ ìš”ì•½ ë³´ê³ ì„œ:")
    print(f"   ì›ë³¸: {processing_report['original_shape'][0]:,}í–‰ Ã— {processing_report['original_shape'][1]}ì—´")
    print(f"   ì²˜ë¦¬ í›„: {processing_report['final_shape'][0]:,}í–‰ Ã— {processing_report['final_shape'][1]}ì—´")
    print(f"   ë³€í™”: {processing_report['final_shape'][1] - processing_report['original_shape'][1]:+}ì—´")
    
except FileNotFoundError:
    print("âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ House Prices Datasetì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ datasets/house_prices/ í´ë”ì— ì €ì¥í•˜ì„¸ìš”.")
    
    # ì˜ˆì‹œ ë°ì´í„°ë¡œ ì‹œë®¬ë ˆì´ì…˜
    print("\nğŸ”„ ì˜ˆì‹œ ë°ì´í„°ë¡œ AutoML ì „ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜:")
    
    # ê°€ìƒì˜ ë¶€ë™ì‚° ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 1000
    
    sample_data = pd.DataFrame({
        'GrLivArea': np.random.normal(1500, 500, n_samples),
        'YearBuilt': np.random.randint(1950, 2020, n_samples),
        'BedroomAbvGr': np.random.randint(1, 6, n_samples),
        'Neighborhood': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_samples),
        'SalePrice': np.random.normal(200000, 80000, n_samples)
    })
    
    # ì˜ë„ì ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì¶”ê°€
    sample_data.loc[np.random.choice(n_samples, 100, replace=False), 'GrLivArea'] = np.nan
    sample_data.loc[np.random.choice(n_samples, 50, replace=False), 'Neighborhood'] = np.nan
    
    auto_processor = AutoPreprocessor()
    processed_data, processing_report = auto_processor.auto_preprocessing_pipeline(
        sample_data, target_col='SalePrice'
    )
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `AutoPreprocessor` í´ë˜ìŠ¤ë¡œ AutoML ë„êµ¬ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì™„ì „ ì‹œë®¬ë ˆì´ì…˜
- ë°ì´í„° í’ˆì§ˆ ìë™ ë¶„ì„ë¶€í„° ê²°ì¸¡ì¹˜ ì²˜ë¦¬, íŠ¹ì„± ê³µí•™, ì¸ì½”ë”©ê¹Œì§€ ì „ ê³¼ì • ìë™í™”
- ê° ë‹¨ê³„ë³„ ê²°ì • ë…¼ë¦¬ì™€ ê²½ê³ ì‚¬í•­ê¹Œì§€ í¬í•¨í•œ ì‹¤ë¬´ì  êµ¬í˜„

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive AutoML preprocessing workflow visualization showing: 1) A flowchart of automated preprocessing steps (Data Quality Analysis â†’ Missing Value Handling â†’ Feature Engineering â†’ Encoding & Scaling â†’ Validation), 2) A comparison matrix of 5 major AutoML platforms with their strengths, limitations, and use cases, 3) Before and after data transformation metrics, 4) Warning and quality indicators. Use professional styling with clear icons for each step and platform logos."

---

## ğŸ“– 4.4.2 AI ê¸°ë°˜ íŠ¹ì„± ìƒì„± ë„êµ¬

### ìë™ íŠ¹ì„± ê³µí•™ì˜ í˜ì‹ 

AI ê¸°ë°˜ íŠ¹ì„± ìƒì„±ì€ **Featuretools**, **AutoFeat**, **OneBM** ë“±ì˜ ë„êµ¬ë¥¼ í†µí•´ ì¸ê°„ì´ ìƒê°í•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ íŠ¹ì„±ë“¤ì„ ìë™ìœ¼ë¡œ ë°œê²¬í•©ë‹ˆë‹¤.

```python
# AI ê¸°ë°˜ íŠ¹ì„± ìƒì„± ë„êµ¬ ì‹œë®¬ë ˆì´ì…˜
class AIFeatureGenerator:
    """
    AI ê¸°ë°˜ ìë™ íŠ¹ì„± ìƒì„±ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.generated_features = []
        self.feature_scores = {}
        self.generation_strategies = [
            'mathematical_combinations',
            'temporal_features', 
            'statistical_features',
            'interaction_features',
            'polynomial_features'
        ]
    
    def mathematical_combinations(self, df, max_features=20):
        """ìˆ˜í•™ì  ì¡°í•© ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
        print("ğŸ”¢ ìˆ˜í•™ì  ì¡°í•© íŠ¹ì„± ìë™ ìƒì„±:")
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        new_features = {}
        
        if len(numeric_cols) >= 2:
            # ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìŒì— ëŒ€í•´ ìˆ˜í•™ì  ì¡°í•© ì‹œë„
            feature_count = 0
            
            for i, col1 in enumerate(numeric_cols):
                for j, col2 in enumerate(numeric_cols[i+1:], i+1):
                    if feature_count >= max_features:
                        break
                    
                    # ë”í•˜ê¸°
                    add_name = f"ADD_{col1}_{col2}"
                    new_features[add_name] = df[col1] + df[col2]
                    
                    # ë¹¼ê¸°
                    sub_name = f"SUB_{col1}_{col2}"
                    new_features[sub_name] = df[col1] - df[col2]
                    
                    # ê³±í•˜ê¸°
                    mul_name = f"MUL_{col1}_{col2}"
                    new_features[mul_name] = df[col1] * df[col2]
                    
                    # ë‚˜ëˆ„ê¸° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
                    div_name = f"DIV_{col1}_{col2}"
                    new_features[div_name] = df[col1] / (df[col2] + 1e-8)
                    
                    feature_count += 4
                    
                    if feature_count >= max_features:
                        break
        
        print(f"   âœ… {len(new_features)}ê°œ ìˆ˜í•™ì  ì¡°í•© íŠ¹ì„± ìƒì„±")
        return new_features
    
    def statistical_features(self, df, window_sizes=[3, 5, 7]):
        """í†µê³„ì  íŠ¹ì„± ìë™ ìƒì„±"""
        print("ğŸ“Š í†µê³„ì  íŠ¹ì„± ìë™ ìƒì„±:")
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        new_features = {}
        
        for col in numeric_cols:
            # ê¸°ë³¸ í†µê³„ëŸ‰
            new_features[f"ZSCORE_{col}"] = (df[col] - df[col].mean()) / (df[col].std() + 1e-8)
            new_features[f"RANK_{col}"] = df[col].rank(pct=True)
            new_features[f"SQUARED_{col}"] = df[col] ** 2
            new_features[f"SQRT_{col}"] = np.sqrt(np.abs(df[col]))
            new_features[f"LOG_{col}"] = np.log1p(np.abs(df[col]))
            
            # ë¡¤ë§ í†µê³„ëŸ‰ (ë°ì´í„°ê°€ ì‹œê³„ì—´ì´ë¼ê³  ê°€ì •)
            for window in window_sizes:
                if len(df) > window:
                    new_features[f"ROLLING_MEAN_{col}_{window}"] = df[col].rolling(window).mean()
                    new_features[f"ROLLING_STD_{col}_{window}"] = df[col].rolling(window).std()
                    new_features[f"ROLLING_MIN_{col}_{window}"] = df[col].rolling(window).min()
                    new_features[f"ROLLING_MAX_{col}_{window}"] = df[col].rolling(window).max()
        
        print(f"   âœ… {len(new_features)}ê°œ í†µê³„ì  íŠ¹ì„± ìƒì„±")
        return new_features
    
    def interaction_features(self, df, target_col=None, top_k=5):
        """ìƒí˜¸ì‘ìš© íŠ¹ì„± ìë™ ë°œê²¬"""
        print("ğŸ”— ìƒí˜¸ì‘ìš© íŠ¹ì„± ìë™ ë°œê²¬:")
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if target_col and target_col in numeric_cols:
            numeric_cols.remove(target_col)
        
        new_features = {}
        
        if target_col and len(numeric_cols) >= 2:
            # íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê´€ê³„ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ íŠ¹ì„± ì„ ë³„
            correlations = df[numeric_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)
            top_features = correlations.head(top_k).index.tolist()
            
            # ìƒìœ„ íŠ¹ì„±ë“¤ ê°„ì˜ ìƒí˜¸ì‘ìš© ìƒì„±
            for i, col1 in enumerate(top_features):
                for j, col2 in enumerate(top_features[i+1:], i+1):
                    # ê³±ì…ˆ ìƒí˜¸ì‘ìš©
                    interaction_name = f"INTERACT_{col1}_{col2}"
                    new_features[interaction_name] = df[col1] * df[col2]
                    
                    # ì¡°ê±´ë¶€ íŠ¹ì„±
                    greater_name = f"GREATER_{col1}_{col2}"
                    new_features[greater_name] = (df[col1] > df[col2]).astype(int)
        
        print(f"   âœ… {len(new_features)}ê°œ ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„±")
        return new_features
    
    def evaluate_feature_importance(self, df, features_dict, target_col):
        """ìƒì„±ëœ íŠ¹ì„±ì˜ ì¤‘ìš”ë„ í‰ê°€"""
        print("\nâš–ï¸ íŠ¹ì„± ì¤‘ìš”ë„ ìë™ í‰ê°€:")
        
        if target_col not in df.columns:
            print("   âš ï¸ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì—†ì–´ ì¤‘ìš”ë„ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}
        
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.feature_selection import mutual_info_regression
        
        feature_scores = {}
        
        # ê° íŠ¹ì„±ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ì¤‘ìš”ë„ ê³„ì‚°
        for feature_name, feature_values in features_dict.items():
            try:
                # ê²°ì¸¡ì¹˜ ì œê±°
                valid_mask = ~(pd.isna(feature_values) | pd.isna(df[target_col]))
                if valid_mask.sum() < 10:  # ìœ íš¨í•œ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê±´ë„ˆëœ€
                    continue
                
                X = feature_values[valid_mask].values.reshape(-1, 1)
                y = df[target_col][valid_mask].values
                
                # ìƒí˜¸ì •ë³´ëŸ‰ ê³„ì‚°
                mi_score = mutual_info_regression(X, y)[0]
                
                # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
                corr_score = abs(np.corrcoef(X.flatten(), y)[0, 1])
                
                # ì¢…í•© ì ìˆ˜ (ìƒí˜¸ì •ë³´ëŸ‰ê³¼ ìƒê´€ê³„ìˆ˜ì˜ í‰ê· )
                combined_score = (mi_score + corr_score) / 2
                
                feature_scores[feature_name] = {
                    'mutual_info': mi_score,
                    'correlation': corr_score,
                    'combined_score': combined_score
                }
                
            except Exception as e:
                continue  # ì˜¤ë¥˜ê°€ ë°œìƒí•œ íŠ¹ì„±ì€ ê±´ë„ˆëœ€
        
        # ìƒìœ„ íŠ¹ì„±ë“¤ ì¶œë ¥
        sorted_features = sorted(feature_scores.items(), 
                               key=lambda x: x[1]['combined_score'], 
                               reverse=True)
        
        print(f"   ğŸ“ˆ í‰ê°€ëœ íŠ¹ì„±: {len(feature_scores)}ê°œ")
        print(f"   ğŸ† ìƒìœ„ 10ê°œ íŠ¹ì„±:")
        
        for i, (feature_name, scores) in enumerate(sorted_features[:10], 1):
            print(f"      {i:2d}. {feature_name}: {scores['combined_score']:.3f}")
        
        return feature_scores
    
    def generate_ai_features(self, df, target_col=None, max_features_per_type=15):
        """AI ê¸°ë°˜ íŠ¹ì„± ìƒì„± ë©”ì¸ í•¨ìˆ˜"""
        print("ğŸ¤– AI ê¸°ë°˜ ìë™ íŠ¹ì„± ìƒì„± ì‹œì‘:")
        print(f"   ğŸ“Š ì…ë ¥ ë°ì´í„°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        all_new_features = {}
        
        # 1. ìˆ˜í•™ì  ì¡°í•© íŠ¹ì„±
        math_features = self.mathematical_combinations(df, max_features_per_type)
        all_new_features.update(math_features)
        
        # 2. í†µê³„ì  íŠ¹ì„±
        stat_features = self.statistical_features(df)
        all_new_features.update(stat_features)
        
        # 3. ìƒí˜¸ì‘ìš© íŠ¹ì„±
        if target_col:
            interaction_features = self.interaction_features(df, target_col)
            all_new_features.update(interaction_features)
        
        # 4. íŠ¹ì„± ì¤‘ìš”ë„ í‰ê°€
        if target_col:
            feature_scores = self.evaluate_feature_importance(df, all_new_features, target_col)
            self.feature_scores = feature_scores
        
        # 5. ìƒìœ„ íŠ¹ì„±ë“¤ë§Œ ì„ ë³„
        if target_col and feature_scores:
            # ìƒìœ„ 30ê°œ íŠ¹ì„±ë§Œ ì„ íƒ
            top_features = sorted(feature_scores.items(), 
                                key=lambda x: x[1]['combined_score'], 
                                reverse=True)[:30]
            
            selected_features = {}
            for feature_name, _ in top_features:
                selected_features[feature_name] = all_new_features[feature_name]
            
            all_new_features = selected_features
        
        print(f"\nâœ… AI íŠ¹ì„± ìƒì„± ì™„ë£Œ:")
        print(f"   ğŸ†• ìµœì¢… ì„ íƒëœ íŠ¹ì„±: {len(all_new_features)}ê°œ")
        
        # ìƒˆë¡œìš´ íŠ¹ì„±ë“¤ì„ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€
        df_enhanced = df.copy()
        for feature_name, feature_values in all_new_features.items():
            df_enhanced[feature_name] = feature_values
        
        print(f"   ğŸ“Š ìµœì¢… ë°ì´í„°: {df_enhanced.shape[0]:,}í–‰ Ã— {df_enhanced.shape[1]}ì—´")
        
        return df_enhanced, all_new_features, getattr(self, 'feature_scores', {})

# AI íŠ¹ì„± ìƒì„± ì‹¤í–‰
try:
    # House Prices ë°ì´í„° ë¡œë“œ
    train_data = pd.read_csv('datasets/house_prices/train.csv')
    
    # ì£¼ìš” ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ (ì˜ˆì‹œë¥¼ ìœ„í•´)
    key_columns = ['GrLivArea', 'YearBuilt', 'TotalBsmtSF', 'GarageArea', 'SalePrice']
    available_columns = [col for col in key_columns if col in train_data.columns]
    
    if len(available_columns) >= 3:
        sample_data = train_data[available_columns].copy()
        
        # AI íŠ¹ì„± ìƒì„±ê¸° ì‹¤í–‰
        ai_generator = AIFeatureGenerator()
        enhanced_data, new_features, feature_scores = ai_generator.generate_ai_features(
            sample_data, target_col='SalePrice'
        )
        
        print(f"\nğŸ“‹ AI íŠ¹ì„± ìƒì„± ìš”ì•½:")
        print(f"   ì›ë³¸ íŠ¹ì„±: {len(available_columns)}ê°œ")
        print(f"   ìƒì„±ëœ íŠ¹ì„±: {len(new_features)}ê°œ")
        print(f"   ì¦ê°€ìœ¨: {len(new_features)/len(available_columns)*100:.1f}%")
        
    else:
        print("âš ï¸ ì¶©ë¶„í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
except FileNotFoundError:
    print("âŒ ì‹¤ì œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì˜ˆì‹œ ë°ì´í„°ë¡œ ì‹œì—°í•©ë‹ˆë‹¤.")
    
    # ì˜ˆì‹œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 500
    
    sample_data = pd.DataFrame({
        'feature_1': np.random.normal(100, 20, n_samples),
        'feature_2': np.random.normal(50, 10, n_samples),
        'feature_3': np.random.exponential(5, n_samples),
        'target': np.random.normal(200, 50, n_samples)
    })
    
    # íƒ€ê²Ÿê³¼ íŠ¹ì„± ê°„ ê´€ê³„ ê°•í™”
    sample_data['target'] += sample_data['feature_1'] * 0.5 + sample_data['feature_2'] * 0.3
    
    ai_generator = AIFeatureGenerator()
    enhanced_data, new_features, feature_scores = ai_generator.generate_ai_features(
        sample_data, target_col='target'
    )
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `AIFeatureGenerator`ë¡œ ì‹¤ì œ AI íŠ¹ì„± ìƒì„± ë„êµ¬ì˜ ì‘ë™ ë°©ì‹ ì‹œë®¬ë ˆì´ì…˜
- ìˆ˜í•™ì  ì¡°í•©, í†µê³„ì  ë³€í™˜, ìƒí˜¸ì‘ìš© íŠ¹ì„± ë“± ë‹¤ì–‘í•œ ìë™ ìƒì„± ì „ëµ êµ¬í˜„
- ìƒí˜¸ì •ë³´ëŸ‰ê³¼ ìƒê´€ê³„ìˆ˜ë¥¼ í™œìš©í•œ ê°ê´€ì  íŠ¹ì„± í‰ê°€ ì‹œìŠ¤í…œ

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive AI feature generation visualization showing: 1) A flowchart of mathematical feature combinations (addition, subtraction, multiplication, division), 2) Statistical transformations (z-score, rank, log, sqrt), 3) Interaction detection matrix with correlation coefficients, 4) Feature importance scoring system with mutual information and correlation metrics, 5) Before and after comparison of dataset dimensions (original vs generated features). Use modern data science styling with clear mathematical notation and performance metrics."

---

## ğŸ“– 4.4.3 ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹ì˜ ì‘ë™ ë°©ì‹ê³¼ ê²€ì¦ ë°©ë²•

### ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹ì˜ í˜ì‹ 

**ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹**ì€ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ê·œì¹™ ì—”ì§„ì„ ê²°í•©í•˜ì—¬ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ìë™ìœ¼ë¡œ íƒì§€í•˜ê³  ìˆ˜ì •í•˜ëŠ” ì°¨ì„¸ëŒ€ ê¸°ìˆ ì…ë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **Intelligent Data Cleaning**: AI ê¸°ë°˜ ìë™ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ
> - **Pattern-based Detection**: íŒ¨í„´ í•™ìŠµì„ í†µí•œ ì´ìƒ ë°ì´í„° ìë™ íƒì§€
> - **Self-learning Cleaning**: ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•´ ì§€ì† í•™ìŠµí•˜ëŠ” í´ë¦¬ë‹ ì‹œìŠ¤í…œ
> - **Quality Score**: ë°ì´í„° í’ˆì§ˆì„ ì •ëŸ‰í™”í•œ ì¢…í•© ì ìˆ˜

### ì§€ëŠ¥í˜• í´ë¦¬ë‹ ì‹œìŠ¤í…œ êµ¬ì¡°

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import re
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class IntelligentDataCleaner:
    """
    ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹ ì‹œìŠ¤í…œ
    AIì™€ ê·œì¹™ ì—”ì§„ì„ ê²°í•©í•œ ìë™ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
    """
    
    def __init__(self, learning_mode=True):
        self.learning_mode = learning_mode
        self.quality_rules = {}
        self.learned_patterns = {}
        self.cleaning_history = []
        self.quality_scores = {}
        
    def comprehensive_quality_assessment(self, df):
        """ì¢…í•©ì  ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        print("ğŸ” ì§€ëŠ¥í˜• ë°ì´í„° í’ˆì§ˆ ì¢…í•© í‰ê°€:")
        
        assessment = {
            'completeness': {},    # ì™„ì „ì„±
            'consistency': {},     # ì¼ê´€ì„±  
            'accuracy': {},        # ì •í™•ì„±
            'validity': {},        # ìœ íš¨ì„±
            'uniqueness': {},      # ê³ ìœ ì„±
            'overall_score': 0
        }
        
        # 1. ì™„ì „ì„± í‰ê°€ (ê²°ì¸¡ì¹˜ ë¶„ì„)
        print(f"\nğŸ“Š 1. ì™„ì „ì„± í‰ê°€:")
        for col in df.columns:
            missing_rate = df[col].isnull().sum() / len(df)
            assessment['completeness'][col] = {
                'missing_rate': missing_rate,
                'score': max(0, 100 - missing_rate * 100),
                'status': 'excellent' if missing_rate < 0.05 else 
                         'good' if missing_rate < 0.15 else 
                         'poor' if missing_rate < 0.50 else 'critical'
            }
            print(f"   {col}: {missing_rate:.1%} ê²°ì¸¡ -> {assessment['completeness'][col]['status']}")
        
        # 2. ì¼ê´€ì„± í‰ê°€ (ë°ì´í„° íƒ€ì…ê³¼ í˜•ì‹)
        print(f"\nğŸ“Š 2. ì¼ê´€ì„± í‰ê°€:")
        for col in df.columns:
            dtype = str(df[col].dtype)
            consistency_score = 100  # ê¸°ë³¸ ì ìˆ˜
            
            if dtype == 'object':  # ë¬¸ìì—´ ì»¬ëŸ¼ ì¼ê´€ì„± ê²€ì‚¬
                values = df[col].dropna()
                if len(values) > 0:
                    # í˜•ì‹ ì¼ê´€ì„± ì²´í¬
                    unique_patterns = set()
                    for val in values.head(100):  # ìƒ˜í”Œ 100ê°œë§Œ ê²€ì‚¬
                        pattern = self._extract_pattern(str(val))
                        unique_patterns.add(pattern)
                    
                    pattern_diversity = len(unique_patterns) / min(len(values), 100)
                    consistency_score = max(0, 100 - pattern_diversity * 50)
            
            assessment['consistency'][col] = {
                'data_type': dtype,
                'score': consistency_score,
                'status': 'excellent' if consistency_score >= 90 else
                         'good' if consistency_score >= 70 else
                         'poor' if consistency_score >= 50 else 'critical'
            }
            print(f"   {col}: {dtype} -> {assessment['consistency'][col]['status']}")
        
        # 3. ì •í™•ì„± í‰ê°€ (ì´ìƒì¹˜ íƒì§€)
        print(f"\nğŸ“Š 3. ì •í™•ì„± í‰ê°€:")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df[col].notna().sum() < 10:  # ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê±´ë„ˆëœ€
                continue
                
            # IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            outlier_mask = ((df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR))
            outlier_rate = outlier_mask.sum() / df[col].notna().sum()
            
            accuracy_score = max(0, 100 - outlier_rate * 200)  # ì´ìƒì¹˜ 1%ë‹¹ 2ì  ê°ì 
            
            assessment['accuracy'][col] = {
                'outlier_rate': outlier_rate,
                'score': accuracy_score,
                'status': 'excellent' if outlier_rate < 0.02 else
                         'good' if outlier_rate < 0.05 else
                         'poor' if outlier_rate < 0.10 else 'critical'
            }
            print(f"   {col}: {outlier_rate:.1%} ì´ìƒì¹˜ -> {assessment['accuracy'][col]['status']}")
        
        # 4. ìœ íš¨ì„± í‰ê°€ (ë„ë©”ì¸ ê·œì¹™ ê²€ì¦)
        print(f"\nğŸ“Š 4. ìœ íš¨ì„± í‰ê°€:")
        validity_issues = self._validate_domain_rules(df)
        
        for col, issues in validity_issues.items():
            invalid_rate = issues['count'] / len(df) if len(df) > 0 else 0
            validity_score = max(0, 100 - invalid_rate * 100)
            
            assessment['validity'][col] = {
                'invalid_rate': invalid_rate,
                'issues': issues['types'],
                'score': validity_score,
                'status': 'excellent' if invalid_rate < 0.01 else
                         'good' if invalid_rate < 0.05 else
                         'poor' if invalid_rate < 0.15 else 'critical'
            }
            print(f"   {col}: {invalid_rate:.1%} ìœ íš¨ì„± ë¬¸ì œ -> {assessment['validity'][col]['status']}")
        
        # 5. ê³ ìœ ì„± í‰ê°€ (ì¤‘ë³µ ë°ì´í„°)
        print(f"\nğŸ“Š 5. ê³ ìœ ì„± í‰ê°€:")
        duplicate_rate = df.duplicated().sum() / len(df)
        uniqueness_score = max(0, 100 - duplicate_rate * 100)
        
        assessment['uniqueness'] = {
            'duplicate_rate': duplicate_rate,
            'score': uniqueness_score,
            'status': 'excellent' if duplicate_rate < 0.01 else
                     'good' if duplicate_rate < 0.05 else
                     'poor' if duplicate_rate < 0.15 else 'critical'
        }
        print(f"   ì „ì²´ ë°ì´í„°: {duplicate_rate:.1%} ì¤‘ë³µ -> {assessment['uniqueness']['status']}")
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        all_scores = []
        
        # ê° ì°¨ì›ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
        if assessment['completeness']:
            completeness_avg = np.mean([v['score'] for v in assessment['completeness'].values()])
            all_scores.append(completeness_avg)
        
        if assessment['consistency']:
            consistency_avg = np.mean([v['score'] for v in assessment['consistency'].values()])
            all_scores.append(consistency_avg)
        
        if assessment['accuracy']:
            accuracy_avg = np.mean([v['score'] for v in assessment['accuracy'].values()])
            all_scores.append(accuracy_avg)
        
        if assessment['validity']:
            validity_avg = np.mean([v['score'] for v in assessment['validity'].values()])
            all_scores.append(validity_avg)
        
        all_scores.append(assessment['uniqueness']['score'])
        
        assessment['overall_score'] = np.mean(all_scores) if all_scores else 0
        
        print(f"\nğŸ† ì¢…í•© ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {assessment['overall_score']:.1f}/100")
        if assessment['overall_score'] >= 90:
            print("   ë“±ê¸‰: A (Excellent) âœ¨")
        elif assessment['overall_score'] >= 80:
            print("   ë“±ê¸‰: B (Good) âœ…")
        elif assessment['overall_score'] >= 70:
            print("   ë“±ê¸‰: C (Acceptable) âš ï¸")
        else:
            print("   ë“±ê¸‰: D (Poor) âŒ")
        
        return assessment
    
    def _extract_pattern(self, text):
        """ë¬¸ìì—´ íŒ¨í„´ ì¶”ì¶œ"""
        # ìˆ«ìëŠ” N, ë¬¸ìëŠ” A, íŠ¹ìˆ˜ë¬¸ìëŠ” Së¡œ ë³€í™˜
        pattern = ""
        for char in str(text):
            if char.isdigit():
                pattern += "N"
            elif char.isalpha():
                pattern += "A"
            elif char.isspace():
                pattern += " "
            else:
                pattern += "S"
        return pattern
    
    def _validate_domain_rules(self, df):
        """ë„ë©”ì¸ë³„ ìœ íš¨ì„± ê·œì¹™ ê²€ì¦"""
        issues = {}
        
        for col in df.columns:
            col_issues = {'count': 0, 'types': []}
            
            # ì¼ë°˜ì ì¸ ìœ íš¨ì„± ê·œì¹™ë“¤
            if 'price' in col.lower() or 'cost' in col.lower():
                # ê°€ê²©ì€ ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ìŒ
                if col in df.select_dtypes(include=[np.number]).columns:
                    negative_count = (df[col] < 0).sum()
                    if negative_count > 0:
                        col_issues['count'] += negative_count
                        col_issues['types'].append('negative_price')
            
            if 'age' in col.lower():
                # ë‚˜ì´ëŠ” 0-150 ë²”ìœ„
                if col in df.select_dtypes(include=[np.number]).columns:
                    invalid_age = ((df[col] < 0) | (df[col] > 150)).sum()
                    if invalid_age > 0:
                        col_issues['count'] += invalid_age
                        col_issues['types'].append('invalid_age_range')
            
            if 'email' in col.lower():
                # ì´ë©”ì¼ í˜•ì‹ ê²€ì¦
                if col in df.select_dtypes(include=['object']).columns:
                    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}


                    invalid_emails = df[col].dropna().apply(
                        lambda x: not re.match(email_pattern, str(x))
                    ).sum()
                    if invalid_emails > 0:
                        col_issues['count'] += invalid_emails
                        col_issues['types'].append('invalid_email_format')
            
            if 'year' in col.lower():
                # ì—°ë„ëŠ” 1900-2030 ë²”ìœ„
                if col in df.select_dtypes(include=[np.number]).columns:
                    invalid_year = ((df[col] < 1900) | (df[col] > 2030)).sum()
                    if invalid_year > 0:
                        col_issues['count'] += invalid_year
                        col_issues['types'].append('invalid_year_range')
            
            if col_issues['count'] > 0:
                issues[col] = col_issues
        
        return issues
    
    def intelligent_anomaly_detection(self, df):
        """ì§€ëŠ¥í˜• ì´ìƒ ë°ì´í„° íƒì§€"""
        print("\nğŸ¤– ì§€ëŠ¥í˜• ì´ìƒ ë°ì´í„° íƒì§€:")
        
        anomalies = {
            'isolation_forest': {},
            'dbscan_outliers': {},
            'statistical_outliers': {},
            'pattern_anomalies': {}
        }
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if len(numeric_cols) >= 2:
            # 1. Isolation Forest ì´ìƒì¹˜ íƒì§€
            print(f"\nğŸŒ² 1. Isolation Forest ì´ìƒì¹˜ íƒì§€:")
            iso_forest = IsolationForest(contamination=0.1, random_state=42)
            
            # ê²°ì¸¡ê°’ì´ ì—†ëŠ” ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì‚¬ìš©
            numeric_data = df[numeric_cols].fillna(df[numeric_cols].median())
            
            if len(numeric_data) > 10:  # ìµœì†Œ ë°ì´í„° ìˆ˜ í™•ì¸
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(numeric_data)
                
                anomaly_labels = iso_forest.fit_predict(scaled_data)
                anomaly_indices = np.where(anomaly_labels == -1)[0]
                
                anomalies['isolation_forest'] = {
                    'indices': anomaly_indices.tolist(),
                    'count': len(anomaly_indices),
                    'percentage': len(anomaly_indices) / len(df) * 100
                }
                
                print(f"   íƒì§€ëœ ì´ìƒ ë°ì´í„°: {len(anomaly_indices)}ê°œ ({len(anomaly_indices)/len(df)*100:.1f}%)")
            
            # 2. DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
            print(f"\nğŸ” 2. DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ì´ìƒì¹˜ íƒì§€:")
            if len(numeric_data) > 20:  # DBSCANì€ ë” ë§ì€ ë°ì´í„° í•„ìš”
                scaler = StandardScaler()
                scaled_data = scaler.fit_transform(numeric_data)
                
                dbscan = DBSCAN(eps=0.5, min_samples=5)
                cluster_labels = dbscan.fit_predict(scaled_data)
                
                outlier_indices = np.where(cluster_labels == -1)[0]
                
                anomalies['dbscan_outliers'] = {
                    'indices': outlier_indices.tolist(),
                    'count': len(outlier_indices),
                    'percentage': len(outlier_indices) / len(df) * 100,
                    'n_clusters': len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
                }
                
                print(f"   í´ëŸ¬ìŠ¤í„° ìˆ˜: {anomalies['dbscan_outliers']['n_clusters']}ê°œ")
                print(f"   ì´ìƒì¹˜: {len(outlier_indices)}ê°œ ({len(outlier_indices)/len(df)*100:.1f}%)")
        
        # 3. í†µê³„ì  ì´ìƒì¹˜ íƒì§€ (ê°œë³„ ì»¬ëŸ¼)
        print(f"\nğŸ“Š 3. í†µê³„ì  ì´ìƒì¹˜ íƒì§€:")
        for col in numeric_cols:
            if df[col].notna().sum() < 10:
                continue
                
            # Modified Z-Score ë°©ë²•
            median = df[col].median()
            mad = np.median(np.abs(df[col] - median))
            
            if mad != 0:  # MADê°€ 0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                modified_z_scores = 0.6745 * (df[col] - median) / mad
                outlier_mask = np.abs(modified_z_scores) > 3.5
                
                outlier_indices = df[outlier_mask].index.tolist()
                
                anomalies['statistical_outliers'][col] = {
                    'indices': outlier_indices,
                    'count': len(outlier_indices),
                    'percentage': len(outlier_indices) / df[col].notna().sum() * 100
                }
                
                print(f"   {col}: {len(outlier_indices)}ê°œ ì´ìƒì¹˜ ({len(outlier_indices)/df[col].notna().sum()*100:.1f}%)")
        
        # 4. íŒ¨í„´ ê¸°ë°˜ ì´ìƒ íƒì§€
        print(f"\nğŸ” 4. íŒ¨í„´ ê¸°ë°˜ ì´ìƒ íƒì§€:")
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        
        for col in categorical_cols:
            if df[col].notna().sum() < 10:
                continue
                
            # ë¹ˆë„ ê¸°ë°˜ ì´ìƒ íŒ¨í„´ íƒì§€
            value_counts = df[col].value_counts()
            total_count = value_counts.sum()
            
            # ì „ì²´ì˜ 1% ë¯¸ë§Œì¸ ê°’ë“¤ì„ í¬ê·€ íŒ¨í„´ìœ¼ë¡œ ê°„ì£¼
            rare_threshold = max(1, total_count * 0.01)
            rare_values = value_counts[value_counts < rare_threshold].index.tolist()
            
            if rare_values:
                rare_indices = df[df[col].isin(rare_values)].index.tolist()
                
                anomalies['pattern_anomalies'][col] = {
                    'rare_values': rare_values,
                    'indices': rare_indices,
                    'count': len(rare_indices),
                    'percentage': len(rare_indices) / df[col].notna().sum() * 100
                }
                
                print(f"   {col}: {len(rare_values)}ê°œ í¬ê·€ íŒ¨í„´, {len(rare_indices)}ê°œ ë°ì´í„° ({len(rare_indices)/df[col].notna().sum()*100:.1f}%)")
        
        return anomalies
    
    def automated_cleaning_recommendations(self, df, quality_assessment, anomalies):
        """ìë™ í´ë¦¬ë‹ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        print("\nğŸ› ï¸ ìë™ í´ë¦¬ë‹ ê¶Œê³ ì‚¬í•­:")
        
        recommendations = {
            'high_priority': [],    # ë†’ì€ ìš°ì„ ìˆœìœ„
            'medium_priority': [],  # ì¤‘ê°„ ìš°ì„ ìˆœìœ„  
            'low_priority': [],     # ë‚®ì€ ìš°ì„ ìˆœìœ„
            'automated_actions': []  # ìë™ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—…
        }
        
        # 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ê¶Œê³ 
        for col, info in quality_assessment['completeness'].items():
            missing_rate = info['missing_rate']
            
            if missing_rate > 0.5:
                recommendations['high_priority'].append({
                    'action': 'column_removal',
                    'column': col,
                    'reason': f'ë„ˆë¬´ ë§ì€ ê²°ì¸¡ê°’ ({missing_rate:.1%})',
                    'urgency': 'critical'
                })
            elif missing_rate > 0.2:
                recommendations['medium_priority'].append({
                    'action': 'missing_value_imputation',
                    'column': col,
                    'method': 'advanced' if df[col].dtype in ['int64', 'float64'] else 'mode',
                    'reason': f'ìƒë‹¹í•œ ê²°ì¸¡ê°’ ({missing_rate:.1%})',
                    'urgency': 'medium'
                })
            elif missing_rate > 0.05:
                recommendations['low_priority'].append({
                    'action': 'missing_value_imputation',
                    'column': col,
                    'method': 'simple',
                    'reason': f'ì†ŒëŸ‰ì˜ ê²°ì¸¡ê°’ ({missing_rate:.1%})',
                    'urgency': 'low'
                })
        
        # 2. ì´ìƒì¹˜ ì²˜ë¦¬ ê¶Œê³ 
        for col, info in quality_assessment.get('accuracy', {}).items():
            outlier_rate = info['outlier_rate']
            
            if outlier_rate > 0.1:
                recommendations['high_priority'].append({
                    'action': 'outlier_investigation',
                    'column': col,
                    'reason': f'ë§ì€ ì´ìƒì¹˜ ({outlier_rate:.1%})',
                    'urgency': 'high'
                })
            elif outlier_rate > 0.05:
                recommendations['medium_priority'].append({
                    'action': 'outlier_treatment',
                    'column': col,
                    'method': 'capping',
                    'reason': f'ë³´í†µ ìˆ˜ì¤€ì˜ ì´ìƒì¹˜ ({outlier_rate:.1%})',
                    'urgency': 'medium'
                })
        
        # 3. ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬ ê¶Œê³ 
        duplicate_info = quality_assessment['uniqueness']
        if duplicate_info['duplicate_rate'] > 0.01:
            priority = 'high_priority' if duplicate_info['duplicate_rate'] > 0.1 else 'medium_priority'
            recommendations[priority].append({
                'action': 'duplicate_removal',
                'reason': f'ì¤‘ë³µ ë°ì´í„° ({duplicate_info["duplicate_rate"]:.1%})',
                'urgency': 'high' if duplicate_info['duplicate_rate'] > 0.1 else 'medium'
            })
        
        # 4. ìë™ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—…ë“¤
        if duplicate_info['duplicate_rate'] > 0:
            recommendations['automated_actions'].append({
                'action': 'remove_exact_duplicates',
                'description': 'ì™„ì „íˆ ë™ì¼í•œ í–‰ ì œê±°',
                'safe': True
            })
        
        # ê¶Œê³ ì‚¬í•­ ì¶œë ¥
        for priority, items in recommendations.items():
            if items:
                priority_name = {
                    'high_priority': 'ğŸ”´ ë†’ì€ ìš°ì„ ìˆœìœ„',
                    'medium_priority': 'ğŸŸ¡ ì¤‘ê°„ ìš°ì„ ìˆœìœ„',
                    'low_priority': 'ğŸŸ¢ ë‚®ì€ ìš°ì„ ìˆœìœ„',
                    'automated_actions': 'ğŸ¤– ìë™ ì‹¤í–‰ ê°€ëŠ¥'
                }
                
                print(f"\n{priority_name[priority]}:")
                for i, item in enumerate(items[:5], 1):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                    print(f"   {i}. {item.get('action', 'unknown')}: {item.get('reason', 'N/A')}")
                    if 'column' in item:
                        print(f"      ëŒ€ìƒ: {item['column']}")
                    if 'method' in item:
                        print(f"      ë°©ë²•: {item['method']}")
        
        return recommendations
    
    def generate_cleaning_report(self, df, quality_assessment, anomalies, recommendations):
        """ì¢…í•© í´ë¦¬ë‹ ë³´ê³ ì„œ ìƒì„±"""
        print("\nğŸ“‹ ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹ ì¢…í•© ë³´ê³ ì„œ:")
        print("=" * 60)
        
        # ë°ì´í„° ê°œìš”
        print(f"\nğŸ“Š ë°ì´í„° ê°œìš”:")
        print(f"   í–‰ ìˆ˜: {len(df):,}")
        print(f"   ì—´ ìˆ˜: {len(df.columns)}")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # í’ˆì§ˆ ì ìˆ˜ ìš”ì•½
        print(f"\nğŸ† í’ˆì§ˆ í‰ê°€ ìš”ì•½:")
        print(f"   ì¢…í•© ì ìˆ˜: {quality_assessment['overall_score']:.1f}/100")
        
        # ì°¨ì›ë³„ ì ìˆ˜
        dimensions = ['completeness', 'consistency', 'accuracy', 'validity']
        for dim in dimensions:
            if quality_assessment[dim]:
                scores = [v['score'] for v in quality_assessment[dim].values()]
                avg_score = np.mean(scores)
                print(f"   {dim.capitalize()}: {avg_score:.1f}/100")
        
        print(f"   Uniqueness: {quality_assessment['uniqueness']['score']:.1f}/100")
        
        # ì£¼ìš” ë¬¸ì œì 
        print(f"\nâš ï¸ ì£¼ìš” ë¬¸ì œì :")
        problem_count = 0
        
        # ì‹¬ê°í•œ ê²°ì¸¡ì¹˜
        for col, info in quality_assessment['completeness'].items():
            if info['status'] in ['poor', 'critical']:
                problem_count += 1
                print(f"   â€¢ {col}: {info['missing_rate']:.1%} ê²°ì¸¡ì¹˜")
        
        # ë§ì€ ì´ìƒì¹˜
        for col, info in quality_assessment.get('accuracy', {}).items():
            if info['status'] in ['poor', 'critical']:
                problem_count += 1
                print(f"   â€¢ {col}: {info['outlier_rate']:.1%} ì´ìƒì¹˜")
        
        # ì¤‘ë³µ ë°ì´í„°
        if quality_assessment['uniqueness']['status'] in ['poor', 'critical']:
            problem_count += 1
            rate = quality_assessment['uniqueness']['duplicate_rate']
            print(f"   â€¢ ì „ì²´: {rate:.1%} ì¤‘ë³µ ë°ì´í„°")
        
        if problem_count == 0:
            print("   ë°œê²¬ëœ ì£¼ìš” ë¬¸ì œì  ì—†ìŒ âœ…")
        
        # ê¶Œê³ ì‚¬í•­ ìš”ì•½
        print(f"\nğŸ› ï¸ ê¶Œê³ ì‚¬í•­ ìš”ì•½:")
        total_actions = sum(len(recommendations[key]) for key in recommendations.keys())
        print(f"   ì´ ê¶Œê³ ì‚¬í•­: {total_actions}ê°œ")
        
        for priority in ['high_priority', 'medium_priority', 'low_priority']:
            count = len(recommendations[priority])
            if count > 0:
                priority_names = {
                    'high_priority': 'ë†’ì€ ìš°ì„ ìˆœìœ„',
                    'medium_priority': 'ì¤‘ê°„ ìš°ì„ ìˆœìœ„', 
                    'low_priority': 'ë‚®ì€ ìš°ì„ ìˆœìœ„'
                }
                print(f"   {priority_names[priority]}: {count}ê°œ")
        
        # ì˜ˆìƒ ê°œì„  íš¨ê³¼
        print(f"\nğŸ“ˆ ì˜ˆìƒ ê°œì„  íš¨ê³¼:")
        current_score = quality_assessment['overall_score']
        
        if current_score < 70:
            expected_improvement = 20
        elif current_score < 85:
            expected_improvement = 10
        else:
            expected_improvement = 5
        
        expected_score = min(100, current_score + expected_improvement)
        print(f"   ê¶Œê³ ì‚¬í•­ ì ìš© í›„ ì˜ˆìƒ ì ìˆ˜: {expected_score:.1f}/100")
        print(f"   ì˜ˆìƒ ê°œì„ í­: +{expected_improvement:.1f}ì ")
        
        print("=" * 60)
        
        return {
            'current_score': current_score,
            'expected_score': expected_score,
            'improvement': expected_improvement,
            'total_recommendations': total_actions,
            'problem_count': problem_count
        }

# House Prices ë°ì´í„°ë¡œ ì§€ëŠ¥í˜• í´ë¦¬ë‹ ì‹œì—°
try:
    # ë°ì´í„° ë¡œë“œ
    train_data = pd.read_csv('datasets/house_prices/train.csv')
    print("âœ… House Prices ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
    
    # ì£¼ìš” ì»¬ëŸ¼ë§Œ ì„ íƒ (ì‹œì—°ìš©)
    sample_columns = ['SalePrice', 'GrLivArea', 'YearBuilt', 'TotalBsmtSF', 
                     'GarageArea', 'LotArea', 'Neighborhood', 'BldgType']
    available_columns = [col for col in sample_columns if col in train_data.columns]
    
    if len(available_columns) >= 5:
        sample_data = train_data[available_columns].copy()
        
        # ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë„ˆ ì‹¤í–‰
        cleaner = IntelligentDataCleaner(learning_mode=True)
        
        # 1. ì¢…í•© í’ˆì§ˆ í‰ê°€
        quality_assessment = cleaner.comprehensive_quality_assessment(sample_data)
        
        # 2. ì§€ëŠ¥í˜• ì´ìƒ ë°ì´í„° íƒì§€
        anomalies = cleaner.intelligent_anomaly_detection(sample_data)
        
        # 3. ìë™ í´ë¦¬ë‹ ê¶Œê³ ì‚¬í•­
        recommendations = cleaner.automated_cleaning_recommendations(
            sample_data, quality_assessment, anomalies
        )
        
        # 4. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        report = cleaner.generate_cleaning_report(
            sample_data, quality_assessment, anomalies, recommendations
        )
        
        print(f"\nğŸ“‹ ì§€ëŠ¥í˜• í´ë¦¬ë‹ ì‹œìŠ¤í…œ ìš”ì•½:")
        print(f"   í˜„ì¬ í’ˆì§ˆ ì ìˆ˜: {report['current_score']:.1f}/100")
        print(f"   ì˜ˆìƒ ê°œì„  ì ìˆ˜: {report['expected_score']:.1f}/100")
        print(f"   ì´ ê¶Œê³ ì‚¬í•­: {report['total_recommendations']}ê°œ")
        
    else:
        print("âš ï¸ ì¶©ë¶„í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
except FileNotFoundError:
    print("âŒ ì‹¤ì œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê°€ìƒ ë°ì´í„°ë¡œ ì‹œì—°í•©ë‹ˆë‹¤.")
    
    # ê°€ìƒ ë°ì´í„° ìƒì„± (í’ˆì§ˆ ë¬¸ì œ í¬í•¨)
    np.random.seed(42)
    n_samples = 1000
    
    sample_data = pd.DataFrame({
        'price': np.random.lognormal(10, 1, n_samples),
        'area': np.random.normal(1500, 400, n_samples),
        'year': np.random.randint(1950, 2023, n_samples),
        'category': np.random.choice(['A', 'B', 'C', 'D'], n_samples),
        'email': ['user' + str(i) + '@email.com' for i in range(n_samples)]
    })
    
    # ì˜ë„ì ìœ¼ë¡œ í’ˆì§ˆ ë¬¸ì œ ì¶”ê°€
    # ê²°ì¸¡ì¹˜ ì¶”ê°€
    sample_data.loc[np.random.choice(n_samples, 100, replace=False), 'area'] = np.nan
    sample_data.loc[np.random.choice(n_samples, 50, replace=False), 'category'] = np.nan
    
    # ì´ìƒì¹˜ ì¶”ê°€
    sample_data.loc[np.random.choice(n_samples, 20, replace=False), 'price'] = -1000
    sample_data.loc[np.random.choice(n_samples, 30, replace=False), 'area'] = 10000
    
    # ì¤‘ë³µ ë°ì´í„° ì¶”ê°€
    duplicate_indices = np.random.choice(n_samples, 50, replace=False)
    for idx in duplicate_indices:
        sample_data.loc[n_samples + len(duplicate_indices)] = sample_data.loc[idx]
    
    # ì˜ëª»ëœ ì´ë©”ì¼ ì¶”ê°€
    sample_data.loc[np.random.choice(len(sample_data), 30, replace=False), 'email'] = 'invalid_email'
    
    # ì§€ëŠ¥í˜• í´ë¦¬ë„ˆ ì‹¤í–‰
    cleaner = IntelligentDataCleaner()
    quality_assessment = cleaner.comprehensive_quality_assessment(sample_data)
    anomalies = cleaner.intelligent_anomaly_detection(sample_data)
    recommendations = cleaner.automated_cleaning_recommendations(
        sample_data, quality_assessment, anomalies
    )
    report = cleaner.generate_cleaning_report(
        sample_data, quality_assessment, anomalies, recommendations
    )
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `IntelligentDataCleaner` í´ë˜ìŠ¤ë¡œ AI ê¸°ë°˜ ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹ ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„
- 5ì°¨ì› í’ˆì§ˆ í‰ê°€ (ì™„ì „ì„±, ì¼ê´€ì„±, ì •í™•ì„±, ìœ íš¨ì„±, ê³ ìœ ì„±)ì™€ ì¢…í•© ì ìˆ˜ ì‹œìŠ¤í…œ
- Isolation Forest, DBSCAN, í†µê³„ì  ë°©ë²•, íŒ¨í„´ ë¶„ì„ ë“± ë‹¤ì–‘í•œ ì´ìƒ íƒì§€ ê¸°ë²• í†µí•©
- ìë™ ê¶Œê³ ì‚¬í•­ ìƒì„±ê³¼ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í´ë¦¬ë‹ ì „ëµ ìˆ˜ë¦½

### AI í´ë¦¬ë‹ ê²°ê³¼ ê²€ì¦ ë°©ë²•ë¡ 

```python
class CleaningValidationFramework:
    """
    AI í´ë¦¬ë‹ ê²°ê³¼ì˜ ì‹ ë¢°ì„± ê²€ì¦ í”„ë ˆì„ì›Œí¬
    """
    
    def __init__(self):
        self.validation_history = []
        self.quality_metrics = {}
        
    def pre_post_comparison(self, original_df, cleaned_df):
        """ì „í›„ ë¹„êµ ë¶„ì„"""
        print("ğŸ” AI í´ë¦¬ë‹ ì „í›„ ë¹„êµ ë¶„ì„:")
        
        comparison = {
            'data_shape': {
                'before': original_df.shape,
                'after': cleaned_df.shape,
                'change': (cleaned_df.shape[0] - original_df.shape[0], 
                          cleaned_df.shape[1] - original_df.shape[1])
            },
            'missing_values': {},
            'data_types': {},
            'statistical_changes': {}
        }
        
        print(f"\nğŸ“Š ë°ì´í„° í¬ê¸° ë³€í™”:")
        print(f"   ì´ì „: {original_df.shape[0]:,}í–‰ Ã— {original_df.shape[1]}ì—´")
        print(f"   ì´í›„: {cleaned_df.shape[0]:,}í–‰ Ã— {cleaned_df.shape[1]}ì—´")
        print(f"   ë³€í™”: {comparison['data_shape']['change'][0]:+,}í–‰, {comparison['data_shape']['change'][1]:+}ì—´")
        
        # ê³µí†µ ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ ë¹„êµ
        common_cols = set(original_df.columns) & set(cleaned_df.columns)
        
        # ê²°ì¸¡ê°’ ë¹„êµ
        print(f"\nğŸ“Š ê²°ì¸¡ê°’ ë³€í™”:")
        for col in common_cols:
            before_missing = original_df[col].isnull().sum()
            after_missing = cleaned_df[col].isnull().sum()
            change = after_missing - before_missing
            
            comparison['missing_values'][col] = {
                'before': before_missing,
                'after': after_missing,
                'change': change,
                'improvement': change < 0
            }
            
            if change != 0:
                print(f"   {col}: {before_missing} â†’ {after_missing} ({change:+})")
        
        # í†µê³„ì  ë³€í™” ë¶„ì„ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)
        numeric_cols = original_df.select_dtypes(include=[np.number]).columns
        common_numeric = set(numeric_cols) & common_cols
        
        if common_numeric:
            print(f"\nğŸ“Š í†µê³„ì  íŠ¹ì„± ë³€í™”:")
            for col in common_numeric:
                if original_df[col].notna().sum() > 0 and cleaned_df[col].notna().sum() > 0:
                    before_mean = original_df[col].mean()
                    after_mean = cleaned_df[col].mean()
                    before_std = original_df[col].std()
                    after_std = cleaned_df[col].std()
                    
                    comparison['statistical_changes'][col] = {
                        'mean_change': after_mean - before_mean,
                        'std_change': after_std - before_std,
                        'mean_change_pct': (after_mean - before_mean) / before_mean * 100 if before_mean != 0 else 0,
                        'std_change_pct': (after_std - before_std) / before_std * 100 if before_std != 0 else 0
                    }
                    
                    print(f"   {col}:")
                    print(f"      í‰ê· : {before_mean:.2f} â†’ {after_mean:.2f} ({comparison['statistical_changes'][col]['mean_change_pct']:+.1f}%)")
                    print(f"      í‘œì¤€í¸ì°¨: {before_std:.2f} â†’ {after_std:.2f} ({comparison['statistical_changes'][col]['std_change_pct']:+.1f}%)")
        
        return comparison
    
    def validate_cleaning_quality(self, original_df, cleaned_df, cleaning_actions):
        """í´ë¦¬ë‹ í’ˆì§ˆ ê²€ì¦"""
        print("\nğŸ¯ AI í´ë¦¬ë‹ í’ˆì§ˆ ê²€ì¦:")
        
        validation_results = {
            'data_integrity': True,
            'logical_consistency': True,
            'business_rules': True,
            'statistical_validity': True,
            'issues': []
        }
        
        # 1. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        print(f"\nğŸ” 1. ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦:")
        
        # ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° ì†ì‹¤ í™•ì¸
        expected_loss = sum(1 for action in cleaning_actions if action.get('action') == 'row_removal')
        actual_loss = len(original_df) - len(cleaned_df)
        
        if abs(actual_loss - expected_loss) > len(original_df) * 0.01:  # 1% ì´ìƒ ì°¨ì´
            validation_results['data_integrity'] = False
            validation_results['issues'].append("ì˜ˆìƒë³´ë‹¤ ë§ì€ ë°ì´í„° ì†ì‹¤")
            print("   âŒ ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° ì†ì‹¤ ë°œìƒ")
        else:
            print("   âœ… ë°ì´í„° ë¬´ê²°ì„± ìœ ì§€")
        
        # 2. ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦
        print(f"\nğŸ” 2. ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦:")
        consistency_issues = 0
        
        for col in cleaned_df.columns:
            if col in original_df.columns:
                # ë°ì´í„° íƒ€ì… ë³€í™” í™•ì¸
                if original_df[col].dtype != cleaned_df[col].dtype:
                    # ì˜ë„ì ì¸ ë³€í™”ê°€ ì•„ë‹Œ ê²½ìš° ë¬¸ì œ
                    type_change_intended = any(
                        action.get('column') == col and action.get('action') == 'type_conversion'
                        for action in cleaning_actions
                    )
                    
                    if not type_change_intended:
                        consistency_issues += 1
                        validation_results['issues'].append(f"{col}: ì˜ˆìƒì¹˜ ëª»í•œ ë°ì´í„° íƒ€ì… ë³€í™”")
        
        if consistency_issues == 0:
            print("   âœ… ë…¼ë¦¬ì  ì¼ê´€ì„± ìœ ì§€")
        else:
            validation_results['logical_consistency'] = False
            print(f"   âŒ {consistency_issues}ê°œ ì¼ê´€ì„± ë¬¸ì œ ë°œê²¬")
        
        # 3. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
        print(f"\nğŸ” 3. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦:")
        business_violations = 0
        
        # ê°€ê²© ê´€ë ¨ ì»¬ëŸ¼ ê²€ì¦
        price_cols = [col for col in cleaned_df.columns if 'price' in col.lower() or 'cost' in col.lower()]
        for col in price_cols:
            if col in cleaned_df.select_dtypes(include=[np.number]).columns:
                negative_prices = (cleaned_df[col] < 0).sum()
                if negative_prices > 0:
                    business_violations += 1
                    validation_results['issues'].append(f"{col}: {negative_prices}ê°œ ìŒìˆ˜ ê°€ê²©")
        
        # ì—°ë„ ê´€ë ¨ ì»¬ëŸ¼ ê²€ì¦
        year_cols = [col for col in cleaned_df.columns if 'year' in col.lower()]
        for col in year_cols:
            if col in cleaned_df.select_dtypes(include=[np.number]).columns:
                invalid_years = ((cleaned_df[col] < 1900) | (cleaned_df[col] > 2030)).sum()
                if invalid_years > 0:
                    business_violations += 1
                    validation_results['issues'].append(f"{col}: {invalid_years}ê°œ ë¹„í˜„ì‹¤ì  ì—°ë„")
        
        if business_violations == 0:
            print("   âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¤€ìˆ˜")
        else:
            validation_results['business_rules'] = False
            print(f"   âŒ {business_violations}ê°œ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜")
        
        # 4. í†µê³„ì  ìœ íš¨ì„± ê²€ì¦
        print(f"\nğŸ” 4. í†µê³„ì  ìœ íš¨ì„± ê²€ì¦:")
        statistical_issues = 0
        
        numeric_cols = cleaned_df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if col in original_df.columns and cleaned_df[col].notna().sum() > 10:
                # ê·¹ë‹¨ì ì¸ í†µê³„ëŸ‰ ë³€í™” í™•ì¸
                original_mean = original_df[col].mean()
                cleaned_mean = cleaned_df[col].mean()
                
                if original_mean != 0:
                    mean_change_pct = abs(cleaned_mean - original_mean) / abs(original_mean)
                    
                    # í‰ê· ì´ 50% ì´ìƒ ë³€í–ˆë‹¤ë©´ ë¬¸ì œ ê°€ëŠ¥ì„±
                    if mean_change_pct > 0.5:
                        statistical_issues += 1
                        validation_results['issues'].append(f"{col}: í‰ê· ê°’ {mean_change_pct*100:.1f}% ë³€í™”")
        
        if statistical_issues == 0:
            validation_results['statistical_validity'] = True
            print("   âœ… í†µê³„ì  ìœ íš¨ì„± í™•ì¸")
        else:
            validation_results['statistical_validity'] = False
            print(f"   âŒ {statistical_issues}ê°œ í†µê³„ì  ì´ìƒ í˜„ìƒ")
        
        # ì¢…í•© íŒì •
        all_passed = all([
            validation_results['data_integrity'],
            validation_results['logical_consistency'], 
            validation_results['business_rules'],
            validation_results['statistical_validity']
        ])
        
        print(f"\nğŸ† ì¢…í•© ê²€ì¦ ê²°ê³¼:")
        if all_passed:
            print("   âœ… ëª¨ë“  ê²€ì¦ í†µê³¼ - AI í´ë¦¬ë‹ ê²°ê³¼ ì‹ ë¢°í•¨")
        else:
            print("   âš ï¸ ì¼ë¶€ ê²€ì¦ ì‹¤íŒ¨ - ì¶”ê°€ ê²€í†  í•„ìš”")
            print("   ì£¼ìš” ë¬¸ì œì :")
            for issue in validation_results['issues'][:5]:
                print(f"      â€¢ {issue}")
        
        return validation_results
    
    def generate_trust_score(self, validation_results, cleaner_reputation=0.8):
        """AI í´ë¦¬ë‹ ê²°ê³¼ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        print(f"\nğŸ¯ AI í´ë¦¬ë‹ ì‹ ë¢°ë„ ì ìˆ˜:")
        
        # ê¸°ë³¸ ì ìˆ˜ (ê²€ì¦ ê²°ê³¼ ê¸°ë°˜)
        validation_score = 0
        weights = {
            'data_integrity': 0.3,
            'logical_consistency': 0.25,
            'business_rules': 0.25,
            'statistical_validity': 0.2
        }
        
        for criterion, weight in weights.items():
            if validation_results[criterion]:
                validation_score += weight
        
        # í´ë¦¬ë„ˆ ì‹ ë¢°ë„ ë°˜ì˜ (ì´ì „ ì„±ëŠ¥ ê¸°ë°˜)
        trust_score = (validation_score * 0.7) + (cleaner_reputation * 0.3)
        
        # ì´ìŠˆ ìˆ˜ì— ë”°ë¥¸ ê°ì 
        issue_penalty = min(0.2, len(validation_results['issues']) * 0.05)
        trust_score = max(0, trust_score - issue_penalty)
        
        print(f"   ê²€ì¦ ì ìˆ˜: {validation_score:.2f}")
        print(f"   í´ë¦¬ë„ˆ ì‹ ë¢°ë„: {cleaner_reputation:.2f}")
        print(f"   ì´ìŠˆ ê°ì : -{issue_penalty:.2f}")
        print(f"   ìµœì¢… ì‹ ë¢°ë„: {trust_score:.2f}/1.0")
        
        if trust_score >= 0.9:
            print("   ë“±ê¸‰: A (ë§¤ìš° ì‹ ë¢°í•¨) âœ¨")
            recommendation = "ê²°ê³¼ë¥¼ ë°”ë¡œ ì‚¬ìš©í•´ë„ ì•ˆì „í•©ë‹ˆë‹¤."
        elif trust_score >= 0.8:
            print("   ë“±ê¸‰: B (ì‹ ë¢°í•¨) âœ…")
            recommendation = "ê°„ë‹¨í•œ ê²€í†  í›„ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        elif trust_score >= 0.7:
            print("   ë“±ê¸‰: C (ë³´í†µ) âš ï¸")
            recommendation = "ì‹ ì¤‘í•œ ê²€í†  í›„ ì„ ë³„ì  ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        else:
            print("   ë“±ê¸‰: D (ë‚®ìŒ) âŒ")
            recommendation = "ìˆ˜ë™ ê²€í†  ë° ì¬ì‘ì—…ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
        
        print(f"   ê¶Œê³ ì‚¬í•­: {recommendation}")
        
        return {
            'trust_score': trust_score,
            'validation_score': validation_score,
            'reputation_score': cleaner_reputation,
            'issue_penalty': issue_penalty,
            'recommendation': recommendation
        }

# ê²€ì¦ í”„ë ˆì„ì›Œí¬ ì‹œì—°
print("\n" + "="*60)
print("ğŸ¯ AI í´ë¦¬ë‹ ê²°ê³¼ ê²€ì¦ ì‹œì—°")
print("="*60)

# ê°€ìƒì˜ í´ë¦¬ë‹ ì „í›„ ë°ì´í„° ìƒì„±
np.random.seed(42)
n_samples = 500

# ì›ë³¸ ë°ì´í„° (ë¬¸ì œ í¬í•¨)
original_data = pd.DataFrame({
    'price': np.concatenate([np.random.lognormal(10, 0.5, 450), [-1000] * 10, [0] * 40]),
    'area': np.concatenate([np.random.normal(1500, 300, 480), [np.nan] * 20]),
    'year': np.concatenate([np.random.randint(1950, 2023, 490), [1800] * 10])
})

# í´ë¦¬ë‹ëœ ë°ì´í„° (ë¬¸ì œ í•´ê²°ë¨)
cleaned_data = pd.DataFrame({
    'price': np.random.lognormal(10, 0.5, 480),  # ìŒìˆ˜ ì œê±°
    'area': np.random.normal(1500, 300, 480),     # ê²°ì¸¡ì¹˜ ëŒ€ì²´
    'year': np.random.randint(1950, 2023, 480)    # ì´ìƒí•œ ì—°ë„ ì œê±°
})

# ê°€ìƒì˜ í´ë¦¬ë‹ ì•¡ì…˜ ë¡œê·¸
cleaning_actions = [
    {'action': 'row_removal', 'reason': 'negative_price', 'count': 10},
    {'action': 'row_removal', 'reason': 'zero_price', 'count': 40}, 
    {'action': 'missing_imputation', 'column': 'area', 'method': 'median', 'count': 20},
    {'action': 'row_removal', 'reason': 'invalid_year', 'count': 10}
]

# ê²€ì¦ í”„ë ˆì„ì›Œí¬ ì‹¤í–‰
validator = CleaningValidationFramework()

# 1. ì „í›„ ë¹„êµ
comparison = validator.pre_post_comparison(original_data, cleaned_data)

# 2. í’ˆì§ˆ ê²€ì¦
validation_results = validator.validate_cleaning_quality(
    original_data, cleaned_data, cleaning_actions
)

# 3. ì‹ ë¢°ë„ ì ìˆ˜
trust_results = validator.generate_trust_score(validation_results)
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `CleaningValidationFramework`ë¡œ AI í´ë¦¬ë‹ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ ê²€ì¦
- 4ì°¨ì› ê²€ì¦ (ë°ì´í„° ë¬´ê²°ì„±, ë…¼ë¦¬ì  ì¼ê´€ì„±, ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™, í†µê³„ì  ìœ íš¨ì„±)
- ì •ëŸ‰ì  ì‹ ë¢°ë„ ì ìˆ˜ì™€ ë“±ê¸‰ ì‹œìŠ¤í…œìœ¼ë¡œ ê°ê´€ì  í‰ê°€
- ì‹¤ë¬´ì  ê¶Œê³ ì‚¬í•­ê¹Œì§€ í¬í•¨í•œ ì™„ì „í•œ ê²€ì¦ í”„ë ˆì„ì›Œí¬

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive data cleaning validation framework visualization showing: 1) A before/after comparison dashboard with data quality metrics, 2) A 4-dimensional validation radar chart (data integrity, logical consistency, business rules, statistical validity), 3) A trust score calculation formula and grading system (A-D grades), 4) A workflow diagram of validation steps from data input to final recommendation. Use professional data science styling with clear metrics and validation indicators."

---

## ğŸ“– 4.4.4 ìë™í™” ì „ì²˜ë¦¬ì˜ í•¨ì •ê³¼ í•œê³„

### ìë™í™”ì˜ ê·¸ë¦¼ì: ë³´ì´ì§€ ì•ŠëŠ” ìœ„í—˜ë“¤

AI ê¸°ë°˜ ìë™ ì „ì²˜ë¦¬ëŠ” ê°•ë ¥í•œ ë„êµ¬ì´ì§€ë§Œ, **"ë§ˆë²•ì˜ ë²„íŠ¼"ì€ ì•„ë‹™ë‹ˆë‹¤**. ë¬´ë¶„ë³„í•œ ìë™í™”ëŠ” ì˜¤íˆë ¤ ë°ì´í„° í’ˆì§ˆì„ ì €í•˜ì‹œí‚¤ê³  ë¶„ì„ ê²°ê³¼ë¥¼ ì™œê³¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> **ğŸ” ì£¼ìš” ìš©ì–´ í•´ì„¤**
> - **Automation Bias**: ìë™í™” ê²°ê³¼ë¥¼ ë¬´ë¹„íŒì ìœ¼ë¡œ ìˆ˜ìš©í•˜ëŠ” í¸í–¥
> - **Black Box Problem**: ì²˜ë¦¬ ê³¼ì •ì„ ì´í•´í•  ìˆ˜ ì—†ëŠ” ë¸”ë™ë°•ìŠ¤ ë¬¸ì œ
> - **Context Ignorance**: ë„ë©”ì¸ ë§¥ë½ì„ ë¬´ì‹œí•˜ëŠ” ì¼ë°˜í™”ì˜ í•¨ì •
> - **Statistical Artifacts**: í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë¬´ì˜ë¯¸í•œ ê²°ê³¼

### ìë™í™” ì „ì²˜ë¦¬ì˜ ì£¼ìš” í•œê³„ì 

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class AutomationPitfallAnalyzer:
    """
    ìë™í™” ì „ì²˜ë¦¬ì˜ í•¨ì •ê³¼ í•œê³„ì ì„ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        self.pitfall_cases = {}
        self.demonstration_results = {}
        
    def demonstrate_context_ignorance(self):
        """ë„ë©”ì¸ ë§¥ë½ ë¬´ì‹œì˜ í•¨ì • ì‹œì—°"""
        print("âš ï¸ í•¨ì • 1: ë„ë©”ì¸ ë§¥ë½ ë¬´ì‹œ (Context Ignorance)")
        print("="*50)
        
        # ì˜ë£Œ ì§„ë‹¨ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(42)
        n_patients = 1000
        
        # í™˜ì ë°ì´í„° ìƒì„±
        medical_data = pd.DataFrame({
            'age': np.random.normal(50, 15, n_patients),
            'blood_pressure': np.random.normal(120, 20, n_patients),
            'cholesterol': np.random.normal(200, 40, n_patients),
            'test_result': np.random.choice([0, 1], n_patients, p=[0.8, 0.2]),  # 20% ì–‘ì„±
            'doctor_notes': ['normal'] * 800 + ['critical'] * 100 + ['urgent'] * 100
        })
        
        # ì˜ë£Œì§„ì´ ì˜ë„ì ìœ¼ë¡œ ë‚¨ê¸´ ê²°ì¸¡ì¹˜ (ì¤‘ìš”í•œ ì˜ë¯¸)
        # "í˜ˆì••ì´ ì •ìƒì¸ ê²½ìš° ì½œë ˆìŠ¤í…Œë¡¤ ì¸¡ì • ìƒëµ" ì •ì±…
        normal_bp_indices = medical_data[medical_data['blood_pressure'] < 130].index
        medical_data.loc[normal_bp_indices[:200], 'cholesterol'] = np.nan
        
        print(f"ğŸ“Š ì˜ë£Œ ë°ì´í„° í˜„í™©:")
        print(f"   í™˜ì ìˆ˜: {len(medical_data)}")
        print(f"   ì½œë ˆìŠ¤í…Œë¡¤ ê²°ì¸¡: {medical_data['cholesterol'].isnull().sum()}ê°œ")
        print(f"   ê²°ì¸¡ í™˜ìë“¤ì˜ í‰ê·  í˜ˆì••: {medical_data[medical_data['cholesterol'].isnull()]['blood_pressure'].mean():.1f}")
        
        # ì˜ëª»ëœ ìë™ ì „ì²˜ë¦¬: ë§¥ë½ ë¬´ì‹œí•˜ê³  ë‹¨ìˆœ ëŒ€ì²´
        print(f"\nğŸ¤– ìë™ ì „ì²˜ë¦¬ (ë§¥ë½ ë¬´ì‹œ):")
        auto_imputer = SimpleImputer(strategy='mean')
        auto_processed = medical_data.copy()
        auto_processed['cholesterol'] = auto_imputer.fit_transform(
            auto_processed[['cholesterol']]
        )
        
        print(f"   ê²°ì¸¡ì¹˜ë¥¼ ì „ì²´ í‰ê· ({medical_data['cholesterol'].mean():.1f})ìœ¼ë¡œ ëŒ€ì²´")
        
        # ì˜¬ë°”ë¥¸ ë„ë©”ì¸ ê¸°ë°˜ ì „ì²˜ë¦¬
        print(f"\nğŸ‘¨â€âš•ï¸ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì „ì²˜ë¦¬:")
        domain_processed = medical_data.copy()
        
        # ì •ìƒ í˜ˆì•• í™˜ìëŠ” ë‚®ì€ ì½œë ˆìŠ¤í…Œë¡¤ë¡œ ëŒ€ì²´
        normal_bp_mask = (domain_processed['blood_pressure'] < 130) & domain_processed['cholesterol'].isnull()
        domain_processed.loc[normal_bp_mask, 'cholesterol'] = 180  # ì •ìƒ ë²”ìœ„
        
        # ë‚˜ë¨¸ì§€ëŠ” ì—°ë ¹ëŒ€ë³„ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
        remaining_missing = domain_processed['cholesterol'].isnull()
        for age_group in [(0, 40), (40, 60), (60, 100)]:
            age_mask = (domain_processed['age'] >= age_group[0]) & (domain_processed['age'] < age_group[1])
            group_mean = medical_data.loc[age_mask & ~medical_data['cholesterol'].isnull(), 'cholesterol'].mean()
            domain_processed.loc[remaining_missing & age_mask, 'cholesterol'] = group_mean
        
        print(f"   ì •ìƒ í˜ˆì•• í™˜ì: ì •ìƒ ì½œë ˆìŠ¤í…Œë¡¤(180)ë¡œ ëŒ€ì²´")
        print(f"   ê¸°íƒ€: ì—°ë ¹ëŒ€ë³„ í‰ê· ìœ¼ë¡œ ëŒ€ì²´")
        
        # ë‘ ë°©ë²•ì˜ ì°¨ì´ì  ë¹„êµ
        print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ë¹„êµ:")
        auto_normal_bp_cholesterol = auto_processed[normal_bp_mask]['cholesterol'].mean()
        domain_normal_bp_cholesterol = domain_processed[normal_bp_mask]['cholesterol'].mean()
        
        print(f"   ì •ìƒ í˜ˆì•• í™˜ìë“¤ì˜ ì½œë ˆìŠ¤í…Œë¡¤:")
        print(f"   - ìë™ ì „ì²˜ë¦¬: {auto_normal_bp_cholesterol:.1f}")
        print(f"   - ë„ë©”ì¸ ê¸°ë°˜: {domain_normal_bp_cholesterol:.1f}")
        print(f"   - ì°¨ì´: {abs(auto_normal_bp_cholesterol - domain_normal_bp_cholesterol):.1f}")
        
        if abs(auto_normal_bp_cholesterol - domain_normal_bp_cholesterol) > 10:
            print("   âš ï¸ ìƒë‹¹í•œ ì°¨ì´ ë°œìƒ - ì§„ë‹¨ ê²°ê³¼ì— ì˜í–¥ ê°€ëŠ¥ì„± ë†’ìŒ")
        
        return {
            'original': medical_data,
            'auto_processed': auto_processed,
            'domain_processed': domain_processed,
            'difference': abs(auto_normal_bp_cholesterol - domain_normal_bp_cholesterol)
        }
    
    def demonstrate_overfitting_artifacts(self):
        """ê³¼ì í•©ê³¼ í—ˆìœ„ íŒ¨í„´ì˜ í•¨ì • ì‹œì—°"""
        print("\nâš ï¸ í•¨ì • 2: ê³¼ì í•©ê³¼ í—ˆìœ„ íŒ¨í„´ (Overfitting Artifacts)")
        print("="*50)
        
        # ì‘ì€ ë°ì´í„°ì…‹ì—ì„œ ê³¼ë„í•œ íŠ¹ì„± ìƒì„± ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(42)
        n_samples = 100  # ì˜ë„ì ìœ¼ë¡œ ì‘ì€ ë°ì´í„°ì…‹
        
        # ê¸°ë³¸ ë°ì´í„° ìƒì„±
        small_dataset = pd.DataFrame({
            'feature_1': np.random.normal(0, 1, n_samples),
            'feature_2': np.random.normal(0, 1, n_samples),
            'feature_3': np.random.normal(0, 1, n_samples),
            'target': np.random.normal(0, 1, n_samples)
        })
        
        # íƒ€ê²Ÿê³¼ ì•½ê°„ì˜ ê´€ê³„ ì¶”ê°€ (ì‹¤ì œ ì‹ í˜¸)
        small_dataset['target'] += 0.3 * small_dataset['feature_1'] + 0.2 * small_dataset['feature_2']
        
        print(f"ğŸ“Š ì‘ì€ ë°ì´í„°ì…‹ í˜„í™©:")
        print(f"   ìƒ˜í”Œ ìˆ˜: {n_samples}")
        print(f"   ê¸°ë³¸ íŠ¹ì„±: {len(small_dataset.columns) - 1}ê°œ")
        
        # ê³¼ë„í•œ ìë™ íŠ¹ì„± ìƒì„±
        print(f"\nğŸ¤– ê³µê²©ì ì¸ ìë™ íŠ¹ì„± ìƒì„±:")
        enhanced_dataset = small_dataset.copy()
        
        # ëª¨ë“  ê°€ëŠ¥í•œ ì¡°í•© ìƒì„±
        feature_cols = ['feature_1', 'feature_2', 'feature_3']
        generated_count = 0
        
        # 2ì°¨ íŠ¹ì„±ë“¤
        for i, col1 in enumerate(feature_cols):
            for j, col2 in enumerate(feature_cols[i+1:], i+1):
                enhanced_dataset[f'{col1}_{col2}_multiply'] = enhanced_dataset[col1] * enhanced_dataset[col2]
                enhanced_dataset[f'{col1}_{col2}_add'] = enhanced_dataset[col1] + enhanced_dataset[col2]
                enhanced_dataset[f'{col1}_{col2}_subtract'] = enhanced_dataset[col1] - enhanced_dataset[col2]
                enhanced_dataset[f'{col1}_{col2}_divide'] = enhanced_dataset[col1] / (enhanced_dataset[col2] + 1e-8)
                generated_count += 4
        
        # ê³ ì°¨ íŠ¹ì„±ë“¤
        for col in feature_cols:
            enhanced_dataset[f'{col}_squared'] = enhanced_dataset[col] ** 2
            enhanced_dataset[f'{col}_cubed'] = enhanced_dataset[col] ** 3
            enhanced_dataset[f'{col}_sqrt'] = np.sqrt(np.abs(enhanced_dataset[col]))
            enhanced_dataset[f'{col}_log'] = np.log1p(np.abs(enhanced_dataset[col]))
            generated_count += 4
        
        print(f"   ìƒì„±ëœ íŠ¹ì„±: {generated_count}ê°œ")
        print(f"   ì´ íŠ¹ì„±: {len(enhanced_dataset.columns) - 1}ê°œ")
        print(f"   íŠ¹ì„±/ìƒ˜í”Œ ë¹„ìœ¨: {(len(enhanced_dataset.columns) - 1) / n_samples:.2f}")
        
        # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
        
        # ê¸°ë³¸ ë°ì´í„°ì…‹
        X_basic = small_dataset.drop('target', axis=1)
        y = small_dataset['target']
        X_train_basic, X_test_basic, y_train, y_test = train_test_split(
            X_basic, y, test_size=0.3, random_state=42
        )
        
        basic_model = RandomForestRegressor(n_estimators=100, random_state=42)
        basic_model.fit(X_train_basic, y_train)
        basic_pred = basic_model.predict(X_test_basic)
        basic_r2 = r2_score(y_test, basic_pred)
        
        # ê³¼ë„í•œ íŠ¹ì„± ë°ì´í„°ì…‹
        X_enhanced = enhanced_dataset.drop('target', axis=1)
        X_train_enhanced, X_test_enhanced, _, _ = train_test_split(
            X_enhanced, y, test_size=0.3, random_state=42
        )
        
        enhanced_model = RandomForestRegressor(n_estimators=100, random_state=42)
        enhanced_model.fit(X_train_enhanced, y_train)
        enhanced_pred = enhanced_model.predict(X_test_enhanced)
        enhanced_r2 = r2_score(y_test, enhanced_pred)
        
        print(f"   ê¸°ë³¸ íŠ¹ì„± ëª¨ë¸ RÂ²: {basic_r2:.3f}")
        print(f"   ê³¼ë„í•œ íŠ¹ì„± ëª¨ë¸ RÂ²: {enhanced_r2:.3f}")
        
        # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í™•ì¸
        from sklearn.model_selection import cross_val_score
        
        basic_cv_scores = cross_val_score(basic_model, X_basic, y, cv=5, scoring='r2')
        enhanced_cv_scores = cross_val_score(enhanced_model, X_enhanced, y, cv=5, scoring='r2')
        
        print(f"\nğŸ”„ êµì°¨ ê²€ì¦ ê²°ê³¼:")
        print(f"   ê¸°ë³¸ íŠ¹ì„± í‰ê·  RÂ²: {basic_cv_scores.mean():.3f} (Â±{basic_cv_scores.std():.3f})")
        print(f"   ê³¼ë„í•œ íŠ¹ì„± í‰ê·  RÂ²: {enhanced_cv_scores.mean():.3f} (Â±{enhanced_cv_scores.std():.3f})")
        
        # ê³¼ì í•© ì§„ë‹¨
        if enhanced_cv_scores.mean() < basic_cv_scores.mean():
            print("   âš ï¸ ê³¼ì í•© ë°œìƒ - íŠ¹ì„± ì¦ê°€ê°€ ì„±ëŠ¥ ì €í•˜ ì•¼ê¸°")
        elif enhanced_cv_scores.std() > basic_cv_scores.std() * 1.5:
            print("   âš ï¸ ë¶ˆì•ˆì •í•œ ì„±ëŠ¥ - ê³¼ë„í•œ íŠ¹ì„±ìœ¼ë¡œ ì¸í•œ ë¶„ì‚° ì¦ê°€")
        
        return {
            'basic_r2': basic_r2,
            'enhanced_r2': enhanced_r2,
            'basic_cv_mean': basic_cv_scores.mean(),
            'enhanced_cv_mean': enhanced_cv_scores.mean(),
            'overfitting_detected': enhanced_cv_scores.mean() < basic_cv_scores.mean()
        }
    
    def demonstrate_automation_bias(self):
        """ìë™í™” í¸í–¥ì˜ í•¨ì • ì‹œì—°"""
        print("\nâš ï¸ í•¨ì • 3: ìë™í™” í¸í–¥ (Automation Bias)")
        print("="*50)
        
        # í¸í–¥ëœ ìë™ ë¶„ë¥˜ ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(42)
        n_samples = 1000
        
        # ì±„ìš© ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜ (í¸í–¥ í¬í•¨)
        hiring_data = pd.DataFrame({
            'experience_years': np.random.exponential(3, n_samples),
            'education_score': np.random.normal(75, 15, n_samples),
            'skills_test': np.random.normal(80, 12, n_samples),
            'previous_salary': np.random.lognormal(10, 0.5, n_samples),
            'gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.6, 0.4]),
            'university_tier': np.random.choice(['Top', 'Mid', 'Other'], n_samples, p=[0.2, 0.3, 0.5])
        })
        
        # í¸í–¥ëœ ê³¼ê±° ê²°ì • íŒ¨í„´ (ë‚¨ì„± ì„ í˜¸, ëª…ë¬¸ëŒ€ ì„ í˜¸)
        bias_factors = 1.0
        bias_factors += (hiring_data['gender'] == 'Male') * 0.3  # ë‚¨ì„± ì„ í˜¸ í¸í–¥
        bias_factors += (hiring_data['university_tier'] == 'Top') * 0.4  # ëª…ë¬¸ëŒ€ í¸í–¥
        bias_factors += np.random.normal(0, 0.1, n_samples)  # ë¬´ì‘ìœ„ ìš”ì†Œ
        
        # ì‹¤ì œ ì„±ê³¼ëŠ” í¸í–¥ê³¼ ë¬´ê´€í•˜ê²Œ ê²°ì •
        actual_performance = (
            0.3 * hiring_data['experience_years'] + 
            0.4 * (hiring_data['skills_test'] - 80) / 12 +
            0.3 * (hiring_data['education_score'] - 75) / 15 +
            np.random.normal(0, 0.2, n_samples)
        )
        
        # í¸í–¥ëœ ê³¼ê±° ì±„ìš© ê²°ì •
        biased_decisions = (bias_factors + np.random.normal(0, 0.3, n_samples)) > 1.2
        
        hiring_data['hired_historically'] = biased_decisions
        hiring_data['actual_performance'] = actual_performance
        
        print(f"ğŸ“Š ì±„ìš© ë°ì´í„° í˜„í™©:")
        print(f"   ì „ì²´ ì§€ì›ì: {n_samples}")
        male_hire_rate = hiring_data[hiring_data['gender'] == 'Male']['hired_historically'].mean()
        female_hire_rate = hiring_data[hiring_data['gender'] == 'Female']['hired_historically'].mean()
        print(f"   ë‚¨ì„± ì±„ìš©ë¥ : {male_hire_rate:.1%}")
        print(f"   ì—¬ì„± ì±„ìš©ë¥ : {female_hire_rate:.1%}")
        
        top_uni_hire_rate = hiring_data[hiring_data['university_tier'] == 'Top']['hired_historically'].mean()
        other_uni_hire_rate = hiring_data[hiring_data['university_tier'] == 'Other']['hired_historically'].mean()
        print(f"   ëª…ë¬¸ëŒ€ ì±„ìš©ë¥ : {top_uni_hire_rate:.1%}")
        print(f"   ê¸°íƒ€ëŒ€ ì±„ìš©ë¥ : {other_uni_hire_rate:.1%}")
        
        # ìë™í™” ì‹œìŠ¤í…œì´ í¸í–¥ì„ í•™ìŠµ
        print(f"\nğŸ¤– ìë™í™” AI ì±„ìš© ì‹œìŠ¤í…œ:")
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import LabelEncoder
        
        # íŠ¹ì„± ì¤€ë¹„
        features = hiring_data[['experience_years', 'education_score', 'skills_test', 'previous_salary']].copy()
        
        # í¸í–¥ ìš”ì†Œë“¤ì„ íŠ¹ì„±ì— í¬í•¨ (ì‹¤ì œë¡œëŠ” AIê°€ íŒ¨í„´ì„ ì°¾ì•„ëƒ„)
        le_gender = LabelEncoder()
        le_uni = LabelEncoder()
        features['gender_encoded'] = le_gender.fit_transform(hiring_data['gender'])
        features['university_encoded'] = le_uni.fit_transform(hiring_data['university_tier'])
        
        # í¸í–¥ëœ ê³¼ê±° ë°ì´í„°ë¡œ í•™ìŠµ
        ai_model = RandomForestClassifier(n_estimators=100, random_state=42)
        ai_model.fit(features, hiring_data['hired_historically'])
        
        # ìƒˆë¡œìš´ ì§€ì›ìì— ëŒ€í•œ ì˜ˆì¸¡
        new_candidates = pd.DataFrame({
            'experience_years': [3, 3],
            'education_score': [82, 82],
            'skills_test': [85, 85],
            'previous_salary': [50000, 50000],
            'gender': ['Male', 'Female'],
            'university_tier': ['Mid', 'Mid']
        })
        
        new_features = new_candidates[['experience_years', 'education_score', 'skills_test', 'previous_salary']].copy()
        new_features['gender_encoded'] = le_gender.transform(new_candidates['gender'])
        new_features['university_encoded'] = le_uni.transform(new_candidates['university_tier'])
        
        ai_predictions = ai_model.predict_proba(new_features)[:, 1]
        
        print(f"   ë™ì¼í•œ ìŠ¤í™ì˜ ì§€ì›ì 2ëª…:")
        print(f"   - ë‚¨ì„± ì§€ì›ì ì±„ìš© í™•ë¥ : {ai_predictions[0]:.1%}")
        print(f"   - ì—¬ì„± ì§€ì›ì ì±„ìš© í™•ë¥ : {ai_predictions[1]:.1%}")
        print(f"   - ì°¨ì´: {abs(ai_predictions[0] - ai_predictions[1]):.1%}")
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        feature_importance = ai_model.feature_importances_
        feature_names = ['ê²½ë ¥', 'í•™ì ', 'ìŠ¤í‚¬í…ŒìŠ¤íŠ¸', 'ì´ì „ì—°ë´‰', 'ì„±ë³„', 'ëŒ€í•™ë“±ê¸‰']
        
        print(f"\nğŸ“Š AI ëª¨ë¸ì˜ ì¤‘ìš”ë„ ìˆœìœ„:")
        importance_pairs = list(zip(feature_names, feature_importance))
        importance_pairs.sort(key=lambda x: x[1], reverse=True)
        
        for i, (name, importance) in enumerate(importance_pairs, 1):
            print(f"   {i}. {name}: {importance:.3f}")
            if name in ['ì„±ë³„', 'ëŒ€í•™ë“±ê¸‰'] and importance > 0.1:
                print(f"      âš ï¸ í¸í–¥ ìš”ì†Œê°€ ë†’ì€ ì¤‘ìš”ë„ë¥¼ ê°€ì§")
        
        return {
            'gender_bias': abs(ai_predictions[0] - ai_predictions[1]),
            'male_hire_rate': male_hire_rate,
            'female_hire_rate': female_hire_rate,
            'feature_importance': dict(importance_pairs),
            'bias_detected': abs(ai_predictions[0] - ai_predictions[1]) > 0.1
        }
    
    def demonstrate_black_box_problems(self):
        """ë¸”ë™ë°•ìŠ¤ ë¬¸ì œì˜ í•¨ì • ì‹œì—°"""
        print("\nâš ï¸ í•¨ì • 4: ë¸”ë™ë°•ìŠ¤ ë¬¸ì œ (Black Box Problem)")
        print("="*50)
        
        # ë³µì¡í•œ ìë™ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œë®¬ë ˆì´ì…˜
        np.random.seed(42)
        n_samples = 500
        
        # ì›ë³¸ ë°ì´í„°
        raw_data = pd.DataFrame({
            'feature_A': np.random.normal(100, 20, n_samples),
            'feature_B': np.random.exponential(2, n_samples),
            'feature_C': np.random.choice(['Low', 'Medium', 'High'], n_samples),
            'target': np.random.normal(50, 10, n_samples)
        })
        
        # ë¸”ë™ë°•ìŠ¤ ìë™ ì „ì²˜ë¦¬ (ê³¼ì • ìˆ¨ê²¨ì§)
        print(f"ğŸ”® ë¸”ë™ë°•ìŠ¤ ìë™ ì „ì²˜ë¦¬ ì‹¤í–‰...")
        
        def mysterious_automl_preprocessing(data):
            """ë³µì¡í•˜ê³  ì„¤ëª…í•˜ê¸° ì–´ë ¤ìš´ ìë™ ì „ì²˜ë¦¬"""
            processed = data.copy()
            
            # 1. ë¹„ì„ í˜• ë³€í™˜ (ì‚¬ìš©ì ëª¨ë¥´ê²Œ)
            processed['feature_A'] = np.log1p(np.abs(processed['feature_A'] - 100))
            
            # 2. ì´ìƒí•œ ìŠ¤ì¼€ì¼ë§ (ë…ìì  ì•Œê³ ë¦¬ì¦˜)
            processed['feature_B'] = (processed['feature_B'] - processed['feature_B'].quantile(0.1)) / (
                processed['feature_B'].quantile(0.9) - processed['feature_B'].quantile(0.1)
            )
            
            # 3. ë³µì¡í•œ ë²”ì£¼í˜• ì¸ì½”ë”©
            category_map = {'Low': -0.7, 'Medium': 0.1, 'High': 1.3}  # ë¹„ì§ê´€ì  ë§¤í•‘
            processed['feature_C'] = processed['feature_C'].map(category_map)
            
            # 4. ìˆ¨ê²¨ì§„ íŠ¹ì„± ìƒì„±
            processed['hidden_feature_1'] = (
                processed['feature_A'] * processed['feature_B'] + 
                processed['feature_C'] ** 2
            )
            processed['hidden_feature_2'] = np.sin(processed['feature_A']) * np.exp(processed['feature_C'])
            
            # 5. ë¹„ë°€ ê°€ì¤‘ì¹˜ ì ìš©
            secret_weights = [0.3, 0.2, 0.15, 0.2, 0.15]
            feature_cols = ['feature_A', 'feature_B', 'feature_C', 'hidden_feature_1', 'hidden_feature_2']
            
            for i, col in enumerate(feature_cols):
                processed[col] *= secret_weights[i]
            
            return processed
        
        processed_data = mysterious_automl_preprocessing(raw_data)
        
        print(f"   âœ… ì „ì²˜ë¦¬ ì™„ë£Œ")
        print(f"   ì›ë³¸ íŠ¹ì„±: {len(raw_data.columns)-1}ê°œ â†’ ì²˜ë¦¬ëœ íŠ¹ì„±: {len(processed_data.columns)-1}ê°œ")
        
        # ì‚¬ìš©ìê°€ ë³€í™”ë¥¼ íŒŒì•…í•˜ë ¤ê³  ì‹œë„
        print(f"\nğŸ” ì‚¬ìš©ìì˜ ë³€í™” íŒŒì•… ì‹œë„:")
        
        original_stats = raw_data.describe()
        processed_stats = processed_data.describe()
        
        print(f"   Feature A í†µê³„ ë³€í™”:")
        print(f"   - ì›ë³¸ í‰ê· : {original_stats.loc['mean', 'feature_A']:.2f}")
        print(f"   - ì²˜ë¦¬ í›„ í‰ê· : {processed_stats.loc['mean', 'feature_A']:.2f}")
        print(f"   - ì›ë³¸ í‘œì¤€í¸ì°¨: {original_stats.loc['std', 'feature_A']:.2f}")
        print(f"   - ì²˜ë¦¬ í›„ í‘œì¤€í¸ì°¨: {processed_stats.loc['std', 'feature_A']:.2f}")
        print(f"   â“ ì–´ë–¤ ë³€í™˜ì´ ì ìš©ë˜ì—ˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ìŒ")
        
        print(f"\n   Feature B ë²”ìœ„ ë³€í™”:")
        print(f"   - ì›ë³¸ ë²”ìœ„: [{original_stats.loc['min', 'feature_B']:.2f}, {original_stats.loc['max', 'feature_B']:.2f}]")
        print(f"   - ì²˜ë¦¬ í›„ ë²”ìœ„: [{processed_stats.loc['min', 'feature_B']:.2f}, {processed_stats.loc['max', 'feature_B']:.2f}]")
        print(f"   â“ í‘œì¤€í™”ì¸ì§€ ì •ê·œí™”ì¸ì§€ ë‹¤ë¥¸ ë°©ë²•ì¸ì§€ ë¶ˆëª…")
        
        print(f"\n   ìƒˆë¡œìš´ íŠ¹ì„±ë“¤:")
        new_features = set(processed_data.columns) - set(raw_data.columns)
        for feature in new_features:
            print(f"   - {feature}: ìƒì„± ë°©ë²• ì•Œ ìˆ˜ ì—†ìŒ")
        
        # ë¬¸ì œ ìƒí™©: ê²°ê³¼ í•´ì„ ë¶ˆê°€ëŠ¥
        print(f"\nâŒ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤:")
        print(f"   1. ì „ì²˜ë¦¬ ê³¼ì • ì¶”ì  ë¶ˆê°€ëŠ¥")
        print(f"   2. ìƒˆë¡œìš´ ë°ì´í„° ì ìš© ì‹œ ë™ì¼í•œ ë³€í™˜ ë³´ì¥ ì–´ë ¤ì›€") 
        print(f"   3. ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ì˜ ì†Œí†µ ë¶ˆê°€")
        print(f"   4. ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ì¸ íŒŒì•… ë¶ˆê°€")
        print(f"   5. ê·œì œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡± ì–´ë ¤ì›€")
        
        # íˆ¬ëª…í•œ ì „ì²˜ë¦¬ì™€ ë¹„êµ
        print(f"\nâœ… íˆ¬ëª…í•œ ì „ì²˜ë¦¬ ëŒ€ì•ˆ:")
        
        def transparent_preprocessing(data):
            """ë‹¨ê³„ë³„ë¡œ ì„¤ëª… ê°€ëŠ¥í•œ ì „ì²˜ë¦¬"""
            processed = data.copy()
            steps = []
            
            # 1. Feature A: í‘œì¤€í™”
            scaler = StandardScaler()
            processed['feature_A'] = scaler.fit_transform(processed[['feature_A']])
            steps.append("Feature A: í‘œì¤€í™” ì ìš© (í‰ê· =0, í‘œì¤€í¸ì°¨=1)")
            
            # 2. Feature B: Min-Max ì •ê·œí™”
            min_val = processed['feature_B'].min()
            max_val = processed['feature_B'].max()
            processed['feature_B'] = (processed['feature_B'] - min_val) / (max_val - min_val)
            steps.append(f"Feature B: Min-Max ì •ê·œí™” ì ìš© (ë²”ìœ„: 0-1)")
            
            # 3. Feature C: ìˆœì„œí˜• ì¸ì½”ë”©
            order_map = {'Low': 0, 'Medium': 1, 'High': 2}
            processed['feature_C'] = processed['feature_C'].map(order_map)
            steps.append("Feature C: ìˆœì„œí˜• ì¸ì½”ë”© (Low=0, Medium=1, High=2)")
            
            return processed, steps
        
        transparent_data, steps = transparent_preprocessing(raw_data)
        
        print(f"   ì ìš©ëœ ì „ì²˜ë¦¬ ë‹¨ê³„:")
        for i, step in enumerate(steps, 1):
            print(f"   {i}. {step}")
        
        return {
            'original_shape': raw_data.shape,
            'blackbox_shape': processed_data.shape,
            'transparent_shape': transparent_data.shape,
            'new_features_count': len(new_features),
            'transparency_advantage': len(steps)
        }
    
    def generate_pitfall_prevention_guide(self):
        """í•¨ì • ë°©ì§€ ê°€ì´ë“œë¼ì¸ ìƒì„±"""
        print("\nğŸ›¡ï¸ ìë™í™” ì „ì²˜ë¦¬ í•¨ì • ë°©ì§€ ê°€ì´ë“œë¼ì¸")
        print("="*50)
        
        guidelines = {
            'context_preservation': [
                "ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ ê¸´ë°€íˆ í˜‘ë ¥í•˜ì—¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ íŒŒì•…",
                "ë°ì´í„° ìƒì„± ê³¼ì •ê³¼ ì˜ë¯¸ë¥¼ ì¶©ë¶„íˆ ì´í•´í•œ í›„ ì „ì²˜ë¦¬ ì ìš©", 
                "ìë™ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „ ê²°ì¸¡ íŒ¨í„´ì˜ ì˜ë¯¸ ë¶„ì„",
                "ì—…ê³„ í‘œì¤€ê³¼ ê·œì œ ìš”êµ¬ì‚¬í•­ì„ ì „ì²˜ë¦¬ ê·œì¹™ì— ë°˜ì˜"
            ],
            'overfitting_prevention': [
                "ë°ì´í„° í¬ê¸° ëŒ€ë¹„ ì ì ˆí•œ ìˆ˜ì¤€ì˜ íŠ¹ì„± ìƒì„±",
                "êµì°¨ ê²€ì¦ì„ í†µí•œ ì¼ë°˜í™” ì„±ëŠ¥ ì§€ì†ì  ëª¨ë‹ˆí„°ë§",
                "íŠ¹ì„± ì„ íƒ ê¸°ë²•ìœ¼ë¡œ ë¶ˆí•„ìš”í•œ íŠ¹ì„± ì œê±°",
                "ì¡°ê¸° ì¤‘ë‹¨ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ê³¼ë„í•œ ë³µì¡ì„± ë°©ì§€"
            ],
            'bias_mitigation': [
                "í›ˆë ¨ ë°ì´í„°ì˜ í¸í–¥ì„±ì„ ì‚¬ì „ì— ë¶„ì„í•˜ê³  ë³´ì •",
                "ê³µì •ì„± ì§€í‘œë¥¼ ì •ì˜í•˜ê³  ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§",
                "ë‹¤ì–‘í•œ ê·¸ë£¹ì— ëŒ€í•œ ì„±ëŠ¥ì„ ê°œë³„ì ìœ¼ë¡œ í‰ê°€",
                "í¸í–¥ ì™„í™” ê¸°ë²•ì„ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì— í†µí•©"
            ],
            'transparency_maintenance': [
                "ëª¨ë“  ì „ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ë¬¸ì„œí™”í•˜ê³  ë²„ì „ ê´€ë¦¬",
                "ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì¬í˜„ ê°€ëŠ¥í•˜ê²Œ êµ¬ì„±",
                "ê° ë³€í™˜ì˜ ëª©ì ê³¼ íš¨ê³¼ë¥¼ ëª…í™•íˆ ê¸°ë¡",
                "ìë™í™”ëœ ë¶€ë¶„ê³¼ ìˆ˜ë™ ê°œì…ì´ í•„ìš”í•œ ë¶€ë¶„ êµ¬ë¶„"
            ]
        }
        
        print(f"\nğŸ“‹ í•µì‹¬ ê°€ì´ë“œë¼ì¸:")
        
        for category, items in guidelines.items():
            category_names = {
                'context_preservation': '1. ë§¥ë½ ë³´ì¡´',
                'overfitting_prevention': '2. ê³¼ì í•© ë°©ì§€',
                'bias_mitigation': '3. í¸í–¥ ì™„í™”',
                'transparency_maintenance': '4. íˆ¬ëª…ì„± ìœ ì§€'
            }
            
            print(f"\n{category_names[category]}:")
            for i, item in enumerate(items, 1):
                print(f"   {i}) {item}")
        
        # ì²´í¬ë¦¬ìŠ¤íŠ¸ ì œê³µ
        print(f"\nâœ… ìë™í™” ì „ì²˜ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
        checklist = [
            "ë„ë©”ì¸ ì „ë¬¸ê°€ ê²€í† ë¥¼ ë°›ì•˜ëŠ”ê°€?",
            "ì „ì²˜ë¦¬ ê³¼ì •ì´ íˆ¬ëª…í•˜ê³  ì„¤ëª… ê°€ëŠ¥í•œê°€?",
            "í¸í–¥ì„± ê²€ì‚¬ë¥¼ ìˆ˜í–‰í–ˆëŠ”ê°€?",
            "ê³¼ì í•© ìœ„í—˜ì„ í‰ê°€í–ˆëŠ”ê°€?",
            "ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ”ê°€?",
            "ì¬í˜„ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ì¸ê°€?",
            "ì˜ˆì™¸ ìƒí™©ì— ëŒ€í•œ ëŒ€ì‘ì±…ì´ ìˆëŠ”ê°€?",
            "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì²´ê³„ê°€ êµ¬ì¶•ë˜ì–´ ìˆëŠ”ê°€?"
        ]
        
        for i, item in enumerate(checklist, 1):
            print(f"   â–¡ {item}")
        
        return guidelines

# í•¨ì • ë¶„ì„ê¸° ì‹¤í–‰
analyzer = AutomationPitfallAnalyzer()

# ê° í•¨ì • ì‹œì—°
context_results = analyzer.demonstrate_context_ignorance()
overfitting_results = analyzer.demonstrate_overfitting_artifacts()
bias_results = analyzer.demonstrate_automation_bias()
blackbox_results = analyzer.demonstrate_black_box_problems()

# ë°©ì§€ ê°€ì´ë“œë¼ì¸
guidelines = analyzer.generate_pitfall_prevention_guide()

print(f"\n" + "="*60)
print("ğŸ“Š ìë™í™” ì „ì²˜ë¦¬ í•¨ì • ë¶„ì„ ìš”ì•½")
print("="*60)
print(f"âœ… ë„ë©”ì¸ ë§¥ë½ ì°¨ì´: {context_results['difference']:.1f}")
print(f"âœ… ê³¼ì í•© íƒì§€: {'ì˜ˆ' if overfitting_results['overfitting_detected'] else 'ì•„ë‹ˆì˜¤'}")
print(f"âœ… í¸í–¥ íƒì§€: {'ì˜ˆ' if bias_results['bias_detected'] else 'ì•„ë‹ˆì˜¤'} ({bias_results['gender_bias']:.1%} ì°¨ì´)")
print(f"âœ… ë¸”ë™ë°•ìŠ¤ íŠ¹ì„± ìˆ˜: {blackbox_results['new_features_count']}ê°œ")
print(f"âœ… íˆ¬ëª…ì„± ê°œì„ : {blackbox_results['transparency_advantage']}ë‹¨ê³„ ëª…ì‹œ")
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `AutomationPitfallAnalyzer` í´ë˜ìŠ¤ë¡œ ìë™í™” ì „ì²˜ë¦¬ì˜ 4ê°€ì§€ ì£¼ìš” í•¨ì • ì‹¤ì¦ì  ì‹œì—°
- ë„ë©”ì¸ ë§¥ë½ ë¬´ì‹œ, ê³¼ì í•©, ìë™í™” í¸í–¥, ë¸”ë™ë°•ìŠ¤ ë¬¸ì œì˜ êµ¬ì²´ì  ì‚¬ë¡€ì™€ í•´ê²°ì±… ì œì‹œ
- ê° í•¨ì •ë³„ íƒì§€ ë°©ë²•ê³¼ ë°©ì§€ ê°€ì´ë“œë¼ì¸, ì²´í¬ë¦¬ìŠ¤íŠ¸ê¹Œì§€ í¬í•¨í•œ ì‹¤ë¬´ì  ì ‘ê·¼

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive automation pitfalls visualization showing: 1) Four main pitfall categories (context ignorance, overfitting artifacts, automation bias, black box problems) with warning icons, 2) Before/after comparison charts showing the impact of each pitfall, 3) A prevention framework diagram with checkpoints and validation steps, 4) A traffic light system (green/yellow/red) for automation safety assessment. Use professional warning colors and clear visual hierarchy to emphasize the risks."

---

## ğŸ“– 4.4.5 í”„ë¡œì íŠ¸: AI ë³´ì¡° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

### ì¢…í•© í”„ë¡œì íŠ¸ ê°œìš”

ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì§€ê¸ˆê¹Œì§€ í•™ìŠµí•œ ë‚´ìš©ì„ í†µí•©í•˜ì—¬ **ì¸ê°„ì˜ ì§€í˜œì™€ AIì˜ íš¨ìœ¨ì„±ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

**ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ:**
- AutoMLê³¼ ìˆ˜ë™ ì „ì²˜ë¦¬ì˜ ìµœì  ì¡°í•© ì„¤ê³„
- AI ê²°ê³¼ ê²€ì¦ê³¼ ì‹ ë¢°ë„ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•  
- í•¨ì • ë°©ì§€ì™€ í’ˆì§ˆ ê´€ë¦¬ ë©”ì»¤ë‹ˆì¦˜ í†µí•©
- ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬ ê°€ëŠ¥í•œ íŒŒì´í”„ë¼ì¸ ì™„ì„±

### í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì„¤ê³„

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class HybridPreprocessingPipeline:
    """
    ì¸ê°„-AI í˜‘ì—… ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self, domain_expertise=None, safety_level='high'):
        self.domain_expertise = domain_expertise or {}
        self.safety_level = safety_level  # 'low', 'medium', 'high'
        self.processing_history = []
        self.quality_metrics = {}
        self.ai_suggestions = {}
        self.human_overrides = {}
        self.validation_results = {}
        
    def phase1_ai_exploration(self, df, target_col=None):
        """Phase 1: AI ê¸°ë°˜ ì´ˆê¸° íƒìƒ‰"""
        print("ğŸ¤– Phase 1: AI ê¸°ë°˜ ì´ˆê¸° ë°ì´í„° íƒìƒ‰")
        print("="*50)
        
        ai_insights = {
            'data_overview': {},
            'quality_issues': {},
            'suggestions': {},
            'risk_flags': []
        }
        
        # 1.1 ê¸°ë³¸ ë°ì´í„° ê°œìš”
        print(f"\nğŸ“Š 1.1 ìë™ ë°ì´í„° ê°œìš” ìƒì„±:")
        ai_insights['data_overview'] = {
            'shape': df.shape,
            'memory_usage': df.memory_usage(deep=True).sum() / 1024**2,  # MB
            'data_types': df.dtypes.value_counts().to_dict(),
            'missing_percentage': (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
        }
        
        print(f"   ë°ì´í„° í¬ê¸°: {ai_insights['data_overview']['shape'][0]:,}í–‰ Ã— {ai_insights['data_overview']['shape'][1]}ì—´")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {ai_insights['data_overview']['memory_usage']:.1f} MB")
        print(f"   ì „ì²´ ê²°ì¸¡ë¥ : {ai_insights['data_overview']['missing_percentage']:.2f}%")
        
        # 1.2 í’ˆì§ˆ ë¬¸ì œ ìë™ íƒì§€
        print(f"\nğŸ” 1.2 ìë™ í’ˆì§ˆ ë¬¸ì œ íƒì§€:")
        quality_issues = {}
        
        # ê²°ì¸¡ì¹˜ íŒ¨í„´ ë¶„ì„
        missing_by_col = df.isnull().sum()
        high_missing_cols = missing_by_col[missing_by_col > len(df) * 0.5].index.tolist()
        moderate_missing_cols = missing_by_col[(missing_by_col > len(df) * 0.2) & 
                                             (missing_by_col <= len(df) * 0.5)].index.tolist()
        
        quality_issues['high_missing'] = high_missing_cols
        quality_issues['moderate_missing'] = moderate_missing_cols
        
        # ì´ìƒì¹˜ íƒì§€ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        outlier_cols = []
        
        for col in numeric_cols:
            if df[col].notna().sum() > 10:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)).sum()
                outlier_rate = outliers / df[col].notna().sum()
                
                if outlier_rate > 0.05:  # 5% ì´ìƒ ì´ìƒì¹˜
                    outlier_cols.append((col, outlier_rate))
        
        quality_issues['outlier_columns'] = outlier_cols
        
        # ì¤‘ë³µ ë°ì´í„° í™•ì¸
        duplicate_count = df.duplicated().sum()
        quality_issues['duplicates'] = duplicate_count
        
        ai_insights['quality_issues'] = quality_issues
        
        if high_missing_cols:
            print(f"   âš ï¸ ì‹¬ê°í•œ ê²°ì¸¡ì¹˜: {len(high_missing_cols)}ê°œ ì»¬ëŸ¼")
            for col in high_missing_cols[:3]:
                print(f"      - {col}: {missing_by_col[col]/len(df)*100:.1f}% ê²°ì¸¡")
        
        if outlier_cols:
            print(f"   âš ï¸ ì´ìƒì¹˜ ë¬¸ì œ: {len(outlier_cols)}ê°œ ì»¬ëŸ¼")
            for col, rate in outlier_cols[:3]:
                print(f"      - {col}: {rate*100:.1f}% ì´ìƒì¹˜")
        
        if duplicate_count > 0:
            print(f"   âš ï¸ ì¤‘ë³µ ë°ì´í„°: {duplicate_count}ê°œ ({duplicate_count/len(df)*100:.1f}%)")
        
        # 1.3 AI ìë™ ì œì•ˆ ìƒì„±
        print(f"\nğŸ’¡ 1.3 AI ìë™ ì œì•ˆ ìƒì„±:")
        suggestions = []
        
        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì œì•ˆ
        for col in high_missing_cols:
            suggestions.append({
                'type': 'removal',
                'target': col,
                'reason': f'ë„ˆë¬´ ë§ì€ ê²°ì¸¡ê°’ ({missing_by_col[col]/len(df)*100:.1f}%)',
                'confidence': 0.9,
                'risk': 'low'
            })
        
        for col in moderate_missing_cols:
            if col in numeric_cols:
                suggestions.append({
                    'type': 'imputation',
                    'target': col,
                    'method': 'median',
                    'reason': f'ì¤‘ê°„ ìˆ˜ì¤€ ê²°ì¸¡ê°’ ({missing_by_col[col]/len(df)*100:.1f}%)',
                    'confidence': 0.7,
                    'risk': 'medium'
                })
        
        # ì´ìƒì¹˜ ì²˜ë¦¬ ì œì•ˆ
        for col, rate in outlier_cols:
            if rate > 0.15:  # 15% ì´ìƒ
                suggestions.append({
                    'type': 'outlier_investigation',
                    'target': col,
                    'reason': f'ë§ì€ ì´ìƒì¹˜ ({rate*100:.1f}%)',
                    'confidence': 0.8,
                    'risk': 'high'
                })
            else:
                suggestions.append({
                    'type': 'outlier_capping',
                    'target': col,
                    'method': 'iqr',
                    'reason': f'ë³´í†µ ìˆ˜ì¤€ ì´ìƒì¹˜ ({rate*100:.1f}%)',
                    'confidence': 0.6,
                    'risk': 'medium'
                })
        
        # ì¤‘ë³µ ì œê±° ì œì•ˆ
        if duplicate_count > 0:
            suggestions.append({
                'type': 'duplicate_removal',
                'target': 'all',
                'reason': f'{duplicate_count}ê°œ ì¤‘ë³µ ë ˆì½”ë“œ',
                'confidence': 0.95,
                'risk': 'low'
            })
        
        ai_insights['suggestions'] = suggestions
        
        print(f"   ì´ {len(suggestions)}ê°œ ì œì•ˆ ìƒì„±")
        for i, suggestion in enumerate(suggestions[:5], 1):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
            confidence_emoji = "ğŸŸ¢" if suggestion['confidence'] > 0.8 else "ğŸŸ¡" if suggestion['confidence'] > 0.6 else "ğŸ”´"
            print(f"   {i}. {suggestion['type']}: {suggestion['target']} {confidence_emoji}")
            print(f"      ì´ìœ : {suggestion['reason']}")
        
        # 1.4 ìœ„í—˜ ì‹ í˜¸ íƒì§€
        print(f"\nğŸš¨ 1.4 ìœ„í—˜ ì‹ í˜¸ íƒì§€:")
        risk_flags = []
        
        # ë°ì´í„° í¬ê¸° vs ë³µì¡ì„±
        complexity_ratio = len(df.columns) / len(df)
        if complexity_ratio > 0.1:  # ì»¬ëŸ¼ì´ ë„ˆë¬´ ë§ìŒ
            risk_flags.append(f"ë†’ì€ ì°¨ì›: ì»¬ëŸ¼/í–‰ ë¹„ìœ¨ {complexity_ratio:.3f}")
        
        # ê²°ì¸¡ì¹˜ íŒ¨í„´
        if ai_insights['data_overview']['missing_percentage'] > 20:
            risk_flags.append(f"ë†’ì€ ê²°ì¸¡ë¥ : {ai_insights['data_overview']['missing_percentage']:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        if ai_insights['data_overview']['memory_usage'] > 500:  # 500MB ì´ìƒ
            risk_flags.append(f"í° ë°ì´í„°: {ai_insights['data_overview']['memory_usage']:.1f}MB")
        
        ai_insights['risk_flags'] = risk_flags
        
        if risk_flags:
            for flag in risk_flags:
                print(f"   âš ï¸ {flag}")
        else:
            print("   âœ… íŠ¹ë³„í•œ ìœ„í—˜ ì‹ í˜¸ ì—†ìŒ")
        
        return ai_insights
    
    def phase2_human_review(self, df, ai_insights, domain_rules=None):
        """Phase 2: ì¸ê°„ ì „ë¬¸ê°€ ê²€í†  ë° ê°œì…"""
        print(f"\nğŸ‘¨â€ğŸ’¼ Phase 2: ì¸ê°„ ì „ë¬¸ê°€ ê²€í†  ë° ê°œì…")
        print("="*50)
        
        human_decisions = {
            'approved_suggestions': [],
            'rejected_suggestions': [],
            'custom_rules': [],
            'domain_overrides': {}
        }
        
        # 2.1 ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê²€í† 
        print(f"\nğŸ§  2.1 ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê²€í† :")
        
        # ì˜ˆì‹œ: ë¶€ë™ì‚° ë„ë©”ì¸ ê·œì¹™ë“¤
        default_domain_rules = {
            'house_prices': {
                'SalePrice': {'min': 10000, 'max': 1000000, 'negative_allowed': False},
                'YearBuilt': {'min': 1800, 'max': 2030},
                'GrLivArea': {'min': 300, 'max': 5000, 'unit': 'sqft'},
                'LotArea': {'min': 1000, 'max': 100000, 'unit': 'sqft'},
                'missing_patterns': {
                    'GarageArea': 'if no garage, missing is expected',
                    'PoolArea': 'if no pool, missing is expected'
                }
            }
        }
        
        applied_rules = domain_rules or default_domain_rules.get('house_prices', {})
        
        if applied_rules:
            print(f"   âœ… ë„ë©”ì¸ ê·œì¹™ ì ìš©: {len(applied_rules)}ê°œ ê·œì¹™")
            
            # ë„ë©”ì¸ ê·œì¹™ ìœ„ë°˜ ê²€ì‚¬
            violations = []
            for col, rules in applied_rules.items():
                if col in df.columns and 'min' in rules and 'max' in rules:
                    if df[col].dtype in ['int64', 'float64']:
                        min_violations = (df[col] < rules['min']).sum()
                        max_violations = (df[col] > rules['max']).sum()
                        
                        if min_violations > 0 or max_violations > 0:
                            violations.append({
                                'column': col,
                                'min_violations': min_violations,
                                'max_violations': max_violations,
                                'rules': rules
                            })
            
            if violations:
                print(f"   âš ï¸ ë„ë©”ì¸ ê·œì¹™ ìœ„ë°˜ ë°œê²¬:")
                for violation in violations:
                    print(f"      - {violation['column']}: {violation['min_violations']}ê°œ ìµœì†Ÿê°’ ìœ„ë°˜, {violation['max_violations']}ê°œ ìµœëŒ“ê°’ ìœ„ë°˜")
        else:
            print(f"   ğŸ“ ë„ë©”ì¸ ê·œì¹™ ì—†ìŒ - ì¼ë°˜ì  ê·œì¹™ ì ìš©")
        
        # 2.2 AI ì œì•ˆ ê²€í† 
        print(f"\nğŸ” 2.2 AI ì œì•ˆ ê²€í† :")
        
        for i, suggestion in enumerate(ai_insights['suggestions']):
            print(f"\n   ì œì•ˆ {i+1}: {suggestion['type']} - {suggestion['target']}")
            print(f"   ì´ìœ : {suggestion['reason']}")
            print(f"   AI ì‹ ë¢°ë„: {suggestion['confidence']:.1%}")
            print(f"   ìœ„í—˜ë„: {suggestion['risk']}")
            
            # ì•ˆì „ ìˆ˜ì¤€ì— ë”°ë¥¸ ìë™ ìŠ¹ì¸/ê±°ë¶€
            if self.safety_level == 'high':
                # ë†’ì€ ì•ˆì „: ì‹ ë¢°ë„ 90% ì´ìƒì´ê³  ìœ„í—˜ë„ ë‚®ì€ ê²ƒë§Œ ìë™ ìŠ¹ì¸
                if suggestion['confidence'] >= 0.9 and suggestion['risk'] == 'low':
                    decision = 'approved'
                    print(f"   ğŸ‘ ìë™ ìŠ¹ì¸ (ë†’ì€ ì‹ ë¢°ë„ + ë‚®ì€ ìœ„í—˜)")
                else:
                    decision = 'human_review'
                    print(f"   ğŸ‘€ ì¸ê°„ ê²€í†  í•„ìš”")
            elif self.safety_level == 'medium':
                # ì¤‘ê°„ ì•ˆì „: ì‹ ë¢°ë„ 80% ì´ìƒì´ê³  ìœ„í—˜ë„ ì¤‘ê°„ ì´í•˜
                if suggestion['confidence'] >= 0.8 and suggestion['risk'] in ['low', 'medium']:
                    decision = 'approved'
                    print(f"   ğŸ‘ ìë™ ìŠ¹ì¸ (ì¤‘ê°„ ì‹ ë¢°ë„)")
                else:
                    decision = 'human_review'
                    print(f"   ğŸ‘€ ì¸ê°„ ê²€í†  í•„ìš”")
            else:  # low safety
                # ë‚®ì€ ì•ˆì „: ì‹ ë¢°ë„ 70% ì´ìƒì€ ëª¨ë‘ ìŠ¹ì¸
                if suggestion['confidence'] >= 0.7:
                    decision = 'approved'
                    print(f"   ğŸ‘ ìë™ ìŠ¹ì¸ (ê¸°ë³¸ ì‹ ë¢°ë„)")
                else:
                    decision = 'rejected'
                    print(f"   ğŸ‘ ìë™ ê±°ë¶€ (ë‚®ì€ ì‹ ë¢°ë„)")
            
            # ë„ë©”ì¸ ì§€ì‹ê³¼ ì¶©ëŒí•˜ëŠ” ê²½ìš° ì¸ê°„ ê²€í†  ìš”êµ¬
            if suggestion['target'] in applied_rules:
                if suggestion['type'] == 'removal':
                    decision = 'human_review'
                    print(f"   ğŸ¤” ë„ë©”ì¸ ê·œì¹™ ì¶©ëŒ - ì¸ê°„ ê²€í†  í•„ìš”")
            
            # ê²°ì • ê¸°ë¡
            if decision == 'approved':
                human_decisions['approved_suggestions'].append(suggestion)
            elif decision == 'rejected':
                human_decisions['rejected_suggestions'].append(suggestion)
            else:
                # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ì¸ê°„ì´ ê°œì…í•˜ì—¬ ê²°ì •
                # ì‹œë®¬ë ˆì´ì…˜ì—ì„œëŠ” ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ìë™ ê²°ì •
                if suggestion['risk'] == 'high' or suggestion['confidence'] < 0.5:
                    human_decisions['rejected_suggestions'].append(suggestion)
                    print(f"   ğŸ‘ ì¸ê°„ì´ ê±°ë¶€")
                else:
                    human_decisions['approved_suggestions'].append(suggestion)
                    print(f"   ğŸ‘ ì¸ê°„ì´ ìŠ¹ì¸")
        
        # 2.3 ì»¤ìŠ¤í…€ ê·œì¹™ ì¶”ê°€
        print(f"\nâš™ï¸ 2.3 ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ê·œì¹™ ì¶”ê°€:")
        custom_rules = [
            {
                'name': 'business_hour_normalization',
                'description': 'ì˜ì—…ì‹œê°„ ë°ì´í„°ë¥¼ 24ì‹œê°„ í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”',
                'apply_if': 'time_related_columns_exist',
                'priority': 'high'
            },
            {
                'name': 'currency_standardization', 
                'description': 'ë‹¤ì–‘í•œ í†µí™”ë¥¼ USDë¡œ í†µì¼',
                'apply_if': 'currency_columns_exist',
                'priority': 'medium'
            },
            {
                'name': 'outlier_business_validation',
                'description': 'ì´ìƒì¹˜ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì—ì„œ ì¬ê²€í† ',
                'apply_if': 'outliers_detected',
                'priority': 'high'
            }
        ]
        
        human_decisions['custom_rules'] = custom_rules
        
        for rule in custom_rules:
            print(f"   + {rule['name']}: {rule['description']}")
        
        print(f"\nğŸ“Š ì¸ê°„ ê²€í†  ê²°ê³¼:")
        print(f"   ìŠ¹ì¸ëœ AI ì œì•ˆ: {len(human_decisions['approved_suggestions'])}ê°œ")
        print(f"   ê±°ë¶€ëœ AI ì œì•ˆ: {len(human_decisions['rejected_suggestions'])}ê°œ") 
        print(f"   ì¶”ê°€ ì»¤ìŠ¤í…€ ê·œì¹™: {len(human_decisions['custom_rules'])}ê°œ")
        
        return human_decisions
    
    def phase3_hybrid_execution(self, df, ai_insights, human_decisions):
        """Phase 3: í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ ì‹¤í–‰"""
        print(f"\nâš™ï¸ Phase 3: í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ ì‹¤í–‰")
        print("="*50)
        
        processed_df = df.copy()
        execution_log = []
        
        # 3.1 ì•ˆì „í•œ ìë™ ì²˜ë¦¬ (ìŠ¹ì¸ëœ AI ì œì•ˆ)
        print(f"\nğŸ¤– 3.1 ì•ˆì „í•œ ìë™ ì²˜ë¦¬:")
        
        for suggestion in human_decisions['approved_suggestions']:
            try:
                if suggestion['type'] == 'duplicate_removal':
                    before_count = len(processed_df)
                    processed_df = processed_df.drop_duplicates()
                    after_count = len(processed_df)
                    
                    execution_log.append({
                        'step': 'duplicate_removal',
                        'target': 'all',
                        'before_count': before_count,
                        'after_count': after_count,
                        'removed': before_count - after_count
                    })
                    print(f"   âœ… ì¤‘ë³µ ì œê±°: {before_count - after_count}ê°œ í–‰ ì œê±°")
                
                elif suggestion['type'] == 'removal' and suggestion['target'] != 'all':
                    col = suggestion['target']
                    if col in processed_df.columns:
                        processed_df = processed_df.drop(columns=[col])
                        execution_log.append({
                            'step': 'column_removal',
                            'target': col,
                            'reason': suggestion['reason']
                        })
                        print(f"   âœ… ì»¬ëŸ¼ ì œê±°: {col}")
                
                elif suggestion['type'] == 'imputation':
                    col = suggestion['target']
                    method = suggestion.get('method', 'median')
                    
                    if col in processed_df.columns:
                        missing_before = processed_df[col].isnull().sum()
                        
                        if method == 'median':
                            fill_value = processed_df[col].median()
                        elif method == 'mean':
                            fill_value = processed_df[col].mean()
                        else:
                            fill_value = processed_df[col].mode().iloc[0] if len(processed_df[col].mode()) > 0 else 0
                        
                        processed_df[col].fillna(fill_value, inplace=True)
                        missing_after = processed_df[col].isnull().sum()
                        
                        execution_log.append({
                            'step': 'imputation',
                            'target': col,
                            'method': method,
                            'fill_value': fill_value,
                            'missing_before': missing_before,
                            'missing_after': missing_after
                        })
                        print(f"   âœ… ê²°ì¸¡ì¹˜ ëŒ€ì²´: {col} ({method}) - {missing_before - missing_after}ê°œ ëŒ€ì²´")
                
                elif suggestion['type'] == 'outlier_capping':
                    col = suggestion['target']
                    if col in processed_df.columns and processed_df[col].dtype in ['int64', 'float64']:
                        Q1 = processed_df[col].quantile(0.25)
                        Q3 = processed_df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR
                        
                        outliers_before = ((processed_df[col] < lower_bound) | (processed_df[col] > upper_bound)).sum()
                        
                        processed_df[col] = processed_df[col].clip(lower_bound, upper_bound)
                        
                        outliers_after = ((processed_df[col] < lower_bound) | (processed_df[col] > upper_bound)).sum()
                        
                        execution_log.append({
                            'step': 'outlier_capping',
                            'target': col,
                            'lower_bound': lower_bound,
                            'upper_bound': upper_bound,
                            'outliers_before': outliers_before,
                            'outliers_after': outliers_after
                        })
                        print(f"   âœ… ì´ìƒì¹˜ ì œí•œ: {col} - {outliers_before}ê°œ â†’ {outliers_after}ê°œ")
                
            except Exception as e:
                print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {suggestion['target']} - {str(e)}")
                execution_log.append({
                    'step': 'error',
                    'target': suggestion['target'],
                    'error': str(e)
                })
        
        # 3.2 ìˆ˜ë™ ê²€ì¦ì´ í•„ìš”í•œ ì²˜ë¦¬
        print(f"\nğŸ‘¨â€ğŸ’¼ 3.2 ìˆ˜ë™ ê²€ì¦ ì²˜ë¦¬:")
        
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ì¼ë¶€ ìˆ˜ë™ ì²˜ë¦¬ ì‹œì—°
        manual_steps = [
            'ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜ ë°ì´í„° ê°œë³„ ê²€í† ',
            'ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ ì´ìƒì¹˜ ì¼€ì´ìŠ¤ ë…¼ì˜',
            'ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±ì„ ìœ„í•œ ë„ë©”ì¸ ì§€ì‹ ì ìš©'
        ]
        
        for step in manual_steps:
            print(f"   ğŸ” {step}")
        
        # 3.3 í’ˆì§ˆ ê²€ì¦
        print(f"\nğŸ¯ 3.3 ì²˜ë¦¬ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦:")
        
        quality_check = {
            'data_integrity': True,
            'missing_reduction': 0,
            'outlier_reduction': 0,
            'issues': []
        }
        
        # ê²°ì¸¡ì¹˜ ê°ì†Œ í™•ì¸
        original_missing = df.isnull().sum().sum()
        processed_missing = processed_df.isnull().sum().sum()
        quality_check['missing_reduction'] = original_missing - processed_missing
        
        # ì´ìƒì¹˜ ê°ì†Œ í™•ì¸ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)
        numeric_cols = processed_df.select_dtypes(include=[np.number]).columns
        original_outliers = 0
        processed_outliers = 0
        
        for col in numeric_cols:
            if col in df.columns and df[col].notna().sum() > 10:
                # ì›ë³¸ ì´ìƒì¹˜ ìˆ˜
                Q1_orig = df[col].quantile(0.25)
                Q3_orig = df[col].quantile(0.75)
                IQR_orig = Q3_orig - Q1_orig
                original_outliers += ((df[col] < Q1_orig - 1.5 * IQR_orig) | 
                                    (df[col] > Q3_orig + 1.5 * IQR_orig)).sum()
                
                # ì²˜ë¦¬ëœ ì´ìƒì¹˜ ìˆ˜
                if col in processed_df.columns:
                    Q1_proc = processed_df[col].quantile(0.25)
                    Q3_proc = processed_df[col].quantile(0.75)
                    IQR_proc = Q3_proc - Q1_proc
                    processed_outliers += ((processed_df[col] < Q1_proc - 1.5 * IQR_proc) | 
                                         (processed_df[col] > Q3_proc + 1.5 * IQR_proc)).sum()
        
        quality_check['outlier_reduction'] = original_outliers - processed_outliers
        
        # ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
        if len(processed_df) == 0:
            quality_check['data_integrity'] = False
            quality_check['issues'].append("ëª¨ë“  ë°ì´í„°ê°€ ì œê±°ë¨")
        
        if processed_df.shape[1] == 0:
            quality_check['data_integrity'] = False
            quality_check['issues'].append("ëª¨ë“  ì»¬ëŸ¼ì´ ì œê±°ë¨")
        
        print(f"   ì›ë³¸: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        print(f"   ì²˜ë¦¬ í›„: {processed_df.shape[0]:,}í–‰ Ã— {processed_df.shape[1]}ì—´")
        print(f"   ê²°ì¸¡ì¹˜ ê°ì†Œ: {quality_check['missing_reduction']:,}ê°œ")
        print(f"   ì´ìƒì¹˜ ê°ì†Œ: {quality_check['outlier_reduction']:,}ê°œ")
        
        if quality_check['data_integrity']:
            print(f"   âœ… ë°ì´í„° ë¬´ê²°ì„± ìœ ì§€")
        else:
            print(f"   âŒ ë°ì´í„° ë¬´ê²°ì„± ë¬¸ì œ:")
            for issue in quality_check['issues']:
                print(f"      - {issue}")
        
        return processed_df, execution_log, quality_check
    
    def phase4_performance_validation(self, original_df, processed_df, target_col=None):
        """Phase 4: ì„±ëŠ¥ ê²€ì¦ ë° ë¹„êµ"""
        print(f"\nğŸ“Š Phase 4: ì„±ëŠ¥ ê²€ì¦ ë° ë¹„êµ")
        print("="*50)
        
        validation_results = {
            'model_performance': {},
            'statistical_comparison': {},
            'recommendation': ''
        }
        
        if target_col and target_col in original_df.columns and target_col in processed_df.columns:
            print(f"\nğŸ¯ 4.1 ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:")
            
            try:
                # ì›ë³¸ ë°ì´í„° ëª¨ë¸
                orig_features = original_df.drop(columns=[target_col]).select_dtypes(include=[np.number])
                orig_target = original_df[target_col]
                
                # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±° (ì›ë³¸)
                orig_complete_mask = orig_features.notna().all(axis=1) & orig_target.notna()
                orig_X = orig_features[orig_complete_mask]
                orig_y = orig_target[orig_complete_mask]
                
                if len(orig_X) > 10:
                    orig_model = RandomForestRegressor(n_estimators=50, random_state=42)
                    orig_scores = cross_val_score(orig_model, orig_X, orig_y, cv=5, scoring='r2')
                    orig_mean_score = orig_scores.mean()
                    orig_std_score = orig_scores.std()
                    
                    print(f"   ì›ë³¸ ë°ì´í„° ì„±ëŠ¥: RÂ² = {orig_mean_score:.3f} (Â±{orig_std_score:.3f})")
                    validation_results['model_performance']['original'] = {
                        'r2_mean': orig_mean_score,
                        'r2_std': orig_std_score,
                        'sample_size': len(orig_X)
                    }
                else:
                    print(f"   ì›ë³¸ ë°ì´í„°: ì™„ì „í•œ ì¼€ì´ìŠ¤ ë¶€ì¡± ({len(orig_X)}ê°œ)")
                    orig_mean_score = 0
                
                # ì²˜ë¦¬ëœ ë°ì´í„° ëª¨ë¸
                proc_features = processed_df.drop(columns=[target_col], errors='ignore').select_dtypes(include=[np.number])
                proc_target = processed_df[target_col] if target_col in processed_df.columns else None
                
                if proc_target is not None and len(proc_features) > 10:
                    # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±° (ì²˜ë¦¬ëœ ë°ì´í„°)
                    proc_complete_mask = proc_features.notna().all(axis=1) & proc_target.notna()
                    proc_X = proc_features[proc_complete_mask]
                    proc_y = proc_target[proc_complete_mask]
                    
                    if len(proc_X) > 10:
                        proc_model = RandomForestRegressor(n_estimators=50, random_state=42)
                        proc_scores = cross_val_score(proc_model, proc_X, proc_y, cv=5, scoring='r2')
                        proc_mean_score = proc_scores.mean()
                        proc_std_score = proc_scores.std()
                        
                        print(f"   ì²˜ë¦¬ëœ ë°ì´í„° ì„±ëŠ¥: RÂ² = {proc_mean_score:.3f} (Â±{proc_std_score:.3f})")
                        validation_results['model_performance']['processed'] = {
                            'r2_mean': proc_mean_score,
                            'r2_std': proc_std_score,
                            'sample_size': len(proc_X)
                        }
                        
                        # ì„±ëŠ¥ ë¹„êµ
                        improvement = proc_mean_score - orig_mean_score
                        print(f"   ì„±ëŠ¥ ë³€í™”: {improvement:+.3f}")
                        
                        if improvement > 0.05:
                            print(f"   âœ… ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒ")
                        elif improvement > 0.01:
                            print(f"   âœ… ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒ")
                        elif improvement > -0.01:
                            print(f"   â– ì„±ëŠ¥ ìœ ì§€")
                        else:
                            print(f"   âŒ ì„±ëŠ¥ ì €í•˜")
                    else:
                        print(f"   ì²˜ë¦¬ëœ ë°ì´í„°: ì™„ì „í•œ ì¼€ì´ìŠ¤ ë¶€ì¡± ({len(proc_X)}ê°œ)")
                else:
                    print(f"   ì²˜ë¦¬ëœ ë°ì´í„°ì—ì„œ íƒ€ê²Ÿ ì»¬ëŸ¼ ì—†ìŒ")
                
            except Exception as e:
                print(f"   âŒ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹¤íŒ¨: {str(e)}")
        
        # 4.2 í†µê³„ì  íŠ¹ì„± ë¹„êµ
        print(f"\nğŸ“ˆ 4.2 í†µê³„ì  íŠ¹ì„± ë¹„êµ:")
        
        # ê³µí†µ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ì— ëŒ€í•´ í†µê³„ ë¹„êµ
        orig_numeric = original_df.select_dtypes(include=[np.number])
        proc_numeric = processed_df.select_dtypes(include=[np.number])
        common_cols = set(orig_numeric.columns) & set(proc_numeric.columns)
        
        stat_comparison = {}
        for col in common_cols:
            if original_df[col].notna().sum() > 0 and processed_df[col].notna().sum() > 0:
                orig_mean = original_df[col].mean()
                proc_mean = processed_df[col].mean()
                orig_std = original_df[col].std()
                proc_std = processed_df[col].std()
                
                mean_change_pct = abs(proc_mean - orig_mean) / abs(orig_mean) * 100 if orig_mean != 0 else 0
                std_change_pct = abs(proc_std - orig_std) / abs(orig_std) * 100 if orig_std != 0 else 0
                
                stat_comparison[col] = {
                    'mean_change_pct': mean_change_pct,
                    'std_change_pct': std_change_pct
                }
                
                if mean_change_pct > 10 or std_change_pct > 10:
                    print(f"   âš ï¸ {col}: í‰ê·  {mean_change_pct:.1f}% ë³€í™”, í‘œì¤€í¸ì°¨ {std_change_pct:.1f}% ë³€í™”")
        
        validation_results['statistical_comparison'] = stat_comparison
        
        if not stat_comparison:
            print(f"   âœ… í° í†µê³„ì  ë³€í™” ì—†ìŒ")
        
        # 4.3 ìµœì¢… ê¶Œê³ ì‚¬í•­
        print(f"\nğŸ’¡ 4.3 ìµœì¢… ê¶Œê³ ì‚¬í•­:")
        
        total_score = 0
        factors = []
        
        # ì„±ëŠ¥ ì ìˆ˜
        if 'processed' in validation_results['model_performance'] and 'original' in validation_results['model_performance']:
            perf_improvement = (validation_results['model_performance']['processed']['r2_mean'] - 
                              validation_results['model_performance']['original']['r2_mean'])
            if perf_improvement > 0.05:
                total_score += 2
                factors.append("ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒ")
            elif perf_improvement > 0.01:
                total_score += 1
                factors.append("ì•½ê°„ì˜ ì„±ëŠ¥ í–¥ìƒ")
            elif perf_improvement > -0.01:
                total_score += 0
                factors.append("ì„±ëŠ¥ ìœ ì§€")
            else:
                total_score -= 1
                factors.append("ì„±ëŠ¥ ì €í•˜")
        
        # í†µê³„ì  ì•ˆì •ì„± ì ìˆ˜
        major_changes = sum(1 for changes in stat_comparison.values() 
                          if changes['mean_change_pct'] > 10 or changes['std_change_pct'] > 10)
        if major_changes == 0:
            total_score += 1
            factors.append("í†µê³„ì  ì•ˆì •ì„± ìœ ì§€")
        elif major_changes <= 2:
            total_score += 0
            factors.append("ì¼ë¶€ í†µê³„ì  ë³€í™”")
        else:
            total_score -= 1
            factors.append("ë§ì€ í†µê³„ì  ë³€í™”")
        
        # ë°ì´í„° ë³´ì¡´ ì ìˆ˜
        data_retention = processed_df.shape[0] / original_df.shape[0]
        if data_retention >= 0.95:
            total_score += 1
            factors.append("ë†’ì€ ë°ì´í„° ë³´ì¡´ìœ¨")
        elif data_retention >= 0.85:
            total_score += 0
            factors.append("ë³´í†µ ë°ì´í„° ë³´ì¡´ìœ¨")
        else:
            total_score -= 1
            factors.append("ë‚®ì€ ë°ì´í„° ë³´ì¡´ìœ¨")
        
        # ìµœì¢… ê²°ì •
        if total_score >= 2:
            recommendation = "ì „ì²˜ë¦¬ ê²°ê³¼ ì ìš© ê¶Œì¥ âœ…"
        elif total_score >= 0:
            recommendation = "ì¡°ê±´ë¶€ ì ìš© (ì¶”ê°€ ê²€í†  í•„ìš”) âš ï¸"
        else:
            recommendation = "ì ìš© ë¹„ê¶Œì¥ (ì¬ì‘ì—… í•„ìš”) âŒ"
        
        validation_results['recommendation'] = recommendation
        
        print(f"   ì¢…í•© ì ìˆ˜: {total_score}/4")
        print(f"   í‰ê°€ ìš”ì†Œ: {', '.join(factors)}")
        print(f"   ìµœì¢… ê¶Œê³ : {recommendation}")
        
        return validation_results
    
    def generate_pipeline_report(self, original_df, processed_df, execution_log, validation_results):
        """ì¢…í•© íŒŒì´í”„ë¼ì¸ ë³´ê³ ì„œ ìƒì„±"""
        print(f"\nğŸ“‹ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¢…í•© ë³´ê³ ì„œ")
        print("="*60)
        
        # ì „ì²´ ìš”ì•½
        print(f"\nğŸ“Š ì²˜ë¦¬ ìš”ì•½:")
        print(f"   ì›ë³¸ ë°ì´í„°: {original_df.shape[0]:,}í–‰ Ã— {original_df.shape[1]}ì—´")
        print(f"   ì²˜ë¦¬ëœ ë°ì´í„°: {processed_df.shape[0]:,}í–‰ Ã— {processed_df.shape[1]}ì—´")
        print(f"   í–‰ ë³€í™”: {processed_df.shape[0] - original_df.shape[0]:+,}ê°œ")
        print(f"   ì—´ ë³€í™”: {processed_df.shape[1] - original_df.shape[1]:+,}ê°œ")
        
        # ì‹¤í–‰ ë‹¨ê³„ ìš”ì•½
        print(f"\nâš™ï¸ ì‹¤í–‰ëœ ì²˜ë¦¬ ë‹¨ê³„:")
        step_counts = {}
        for log in execution_log:
            step_type = log.get('step', 'unknown')
            step_counts[step_type] = step_counts.get(step_type, 0) + 1
        
        for step_type, count in step_counts.items():
            print(f"   {step_type}: {count}íšŒ")
        
        # ì„±ëŠ¥ ê²°ê³¼
        if validation_results['model_performance']:
            print(f"\nğŸ“ˆ ì„±ëŠ¥ ê²€ì¦ ê²°ê³¼:")
            if 'original' in validation_results['model_performance']:
                orig_perf = validation_results['model_performance']['original']
                print(f"   ì›ë³¸ ëª¨ë¸ RÂ²: {orig_perf['r2_mean']:.3f} (ìƒ˜í”Œ: {orig_perf['sample_size']:,})")
            
            if 'processed' in validation_results['model_performance']:
                proc_perf = validation_results['model_performance']['processed']
                print(f"   ì²˜ë¦¬ëœ ëª¨ë¸ RÂ²: {proc_perf['r2_mean']:.3f} (ìƒ˜í”Œ: {proc_perf['sample_size']:,})")
                
                if 'original' in validation_results['model_performance']:
                    improvement = proc_perf['r2_mean'] - orig_perf['r2_mean']
                    print(f"   ì„±ëŠ¥ ë³€í™”: {improvement:+.3f}")
        
        # ìµœì¢… ê¶Œê³ ì‚¬í•­
        print(f"\nğŸ’¡ ìµœì¢… ê¶Œê³ ì‚¬í•­:")
        print(f"   {validation_results['recommendation']}")
        
        # íŒŒì´í”„ë¼ì¸ ë©”íƒ€ë°ì´í„°
        print(f"\nğŸ”§ íŒŒì´í”„ë¼ì¸ ì„¤ì •:")
        print(f"   ì•ˆì „ ìˆ˜ì¤€: {self.safety_level}")
        print(f"   ë„ë©”ì¸ ê·œì¹™: {'ì ìš©ë¨' if self.domain_expertise else 'ì—†ìŒ'}")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: ì‹œë®¬ë ˆì´ì…˜")
        
        return {
            'original_shape': original_df.shape,
            'processed_shape': processed_df.shape,
            'execution_summary': step_counts,
            'performance_results': validation_results['model_performance'],
            'final_recommendation': validation_results['recommendation']
        }

# í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë°ëª¨
print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë°ëª¨")
print("="*60)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ë¬¸ì œê°€ ìˆëŠ” House Prices ìŠ¤íƒ€ì¼ ë°ì´í„°)
np.random.seed(42)
n_samples = 800

demo_data = pd.DataFrame({
    'SalePrice': np.random.lognormal(12, 0.5, n_samples),
    'GrLivArea': np.random.normal(1500, 400, n_samples),
    'YearBuilt': np.random.randint(1920, 2023, n_samples),
    'TotalBsmtSF': np.random.normal(1000, 300, n_samples),
    'GarageArea': np.random.normal(500, 150, n_samples),
    'LotArea': np.random.exponential(9000, n_samples),
    'Neighborhood': np.random.choice(['A', 'B', 'C'], n_samples, p=[0.3, 0.5, 0.2])
})

# ì˜ë„ì ìœ¼ë¡œ ë¬¸ì œ ì¶”ê°€
# ê²°ì¸¡ì¹˜
demo_data.loc[np.random.choice(n_samples, 100, replace=False), 'GarageArea'] = np.nan
demo_data.loc[np.random.choice(n_samples, 80, replace=False), 'TotalBsmtSF'] = np.nan

# ì´ìƒì¹˜
demo_data.loc[np.random.choice(n_samples, 30, replace=False), 'SalePrice'] = demo_data['SalePrice'] * 5
demo_data.loc[np.random.choice(n_samples, 20, replace=False), 'GrLivArea'] = 10000

# ì¤‘ë³µ ë°ì´í„°
for i in range(25):
    demo_data.loc[n_samples + i] = demo_data.loc[i % 100]

# ì˜ëª»ëœ ì—°ë„
demo_data.loc[np.random.choice(len(demo_data), 15, replace=False), 'YearBuilt'] = 1800

print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {demo_data.shape[0]:,}í–‰ Ã— {demo_data.shape[1]}ì—´")

# í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
pipeline = HybridPreprocessingPipeline(safety_level='high')

# Phase 1: AI íƒìƒ‰
ai_insights = pipeline.phase1_ai_exploration(demo_data, 'SalePrice')

# Phase 2: ì¸ê°„ ê²€í† 
human_decisions = pipeline.phase2_human_review(demo_data, ai_insights)

# Phase 3: í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰
processed_data, execution_log, quality_check = pipeline.phase3_hybrid_execution(
    demo_data, ai_insights, human_decisions
)

# Phase 4: ì„±ëŠ¥ ê²€ì¦
validation_results = pipeline.phase4_performance_validation(
    demo_data, processed_data, 'SalePrice'
)

# ìµœì¢… ë³´ê³ ì„œ
final_report = pipeline.generate_pipeline_report(
    demo_data, processed_data, execution_log, validation_results
)

print(f"\nğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
print(f"ìµœì¢… ë°ì´í„°: {processed_data.shape[0]:,}í–‰ Ã— {processed_data.shape[1]}ì—´")
print(f"ê¶Œê³ ì‚¬í•­: {validation_results['recommendation']}")
```

**ğŸ” ì½”ë“œ í•´ì„¤:**
- `HybridPreprocessingPipeline` í´ë˜ìŠ¤ë¡œ ì¸ê°„-AI í˜‘ì—… ê¸°ë°˜ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ ì™„ì „ êµ¬í˜„
- 4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤ (AI íƒìƒ‰ â†’ ì¸ê°„ ê²€í†  â†’ í•˜ì´ë¸Œë¦¬ë“œ ì‹¤í–‰ â†’ ì„±ëŠ¥ ê²€ì¦)
- ì•ˆì „ ìˆ˜ì¤€ë³„ ìë™í™” ì •ë„ ì¡°ì ˆê³¼ ë„ë©”ì¸ ì§€ì‹ í†µí•© ë©”ì»¤ë‹ˆì¦˜
- ì „ì²´ ê³¼ì •ì˜ íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±ì„ ë³´ì¥í•˜ëŠ” ì¢…í•© íŒŒì´í”„ë¼ì¸

> **ğŸ“Š ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸:**  
> "Create a comprehensive hybrid preprocessing pipeline visualization showing: 1) A 4-phase workflow diagram (AI exploration â†’ Human review â†’ Hybrid execution â†’ Performance validation), 2) A decision tree for automation safety levels, 3) Before/after data quality comparison dashboard, 4) Performance validation metrics with model comparison charts, 5) A final recommendation system with traffic light indicators. Use modern data science styling with clear process flow and validation checkpoints."

---

## ğŸ“ ìš”ì•½ ë° í•µì‹¬ ì •ë¦¬

### ğŸ¯ Part 4 í•µì‹¬ ì„±ì·¨

ì´ë²ˆ Partì—ì„œëŠ” **AI ë„êµ¬ë¥¼ í™œìš©í•œ ìë™ ì „ì²˜ë¦¬ì˜ ì–‘ë©´ì„±**ì„ ê¹Šì´ ìˆê²Œ íƒêµ¬í–ˆìŠµë‹ˆë‹¤. 

**âœ… ì£¼ìš” í•™ìŠµ ì„±ê³¼:**

1. **AutoML ì „ì²˜ë¦¬ ë„êµ¬ ì™„ì „ ë§ˆìŠ¤í„°**
   - H2O, Google AutoML, DataRobot, AutoGluon, TPOT 5ê°œ ì£¼ìš” í”Œë«í¼ ë¹„êµ ë¶„ì„
   - ê° ë„êµ¬ì˜ ì¥ë‹¨ì ê³¼ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ëª…í™•íˆ êµ¬ë¶„
   - ì‹¤ë¬´ì  ì„ íƒ ê¸°ì¤€ê³¼ ë¹„ìš© íš¨ìœ¨ì„± í‰ê°€ ë°©ë²• ìŠµë“

2. **AI ê¸°ë°˜ íŠ¹ì„± ìƒì„± ì‹œìŠ¤í…œ êµ¬ì¶•**
   - ìˆ˜í•™ì  ì¡°í•©, í†µê³„ì  ë³€í™˜, ìƒí˜¸ì‘ìš© íŠ¹ì„± ë“± ë‹¤ì–‘í•œ ìë™ ìƒì„± ì „ëµ
   - ìƒí˜¸ì •ë³´ëŸ‰ê³¼ ìƒê´€ê³„ìˆ˜ë¥¼ í™œìš©í•œ ê°ê´€ì  íŠ¹ì„± í‰ê°€ ì‹œìŠ¤í…œ
   - 60ê°œ ì´ìƒ íŠ¹ì„± ìƒì„± í›„ ìƒìœ„ 30ê°œ ì„ ë³„í•˜ëŠ” ì§€ëŠ¥í˜• í•„í„°ë§

3. **ì§€ëŠ¥í˜• ë°ì´í„° í´ë¦¬ë‹ ì™„ì „ ì •ë³µ**
   - 5ì°¨ì› í’ˆì§ˆ í‰ê°€ (ì™„ì „ì„±, ì¼ê´€ì„±, ì •í™•ì„±, ìœ íš¨ì„±, ê³ ìœ ì„±)
   - Isolation Forest, DBSCAN, í†µê³„ì  ë°©ë²•ì„ í†µí•©í•œ ë‹¤ì¸µì  ì´ìƒ íƒì§€
   - 4ì°¨ì› ê²€ì¦ í”„ë ˆì„ì›Œí¬ì™€ A-D ë“±ê¸‰ ì‹ ë¢°ë„ í‰ê°€ ì‹œìŠ¤í…œ

4. **ìë™í™” í•¨ì • ë°©ì§€ ì „ë¬¸ê°€ ì—­ëŸ‰**
   - ë„ë©”ì¸ ë§¥ë½ ë¬´ì‹œ, ê³¼ì í•©, ìë™í™” í¸í–¥, ë¸”ë™ë°•ìŠ¤ ë¬¸ì œì˜ êµ¬ì²´ì  ì‚¬ë¡€ í•™ìŠµ
   - ê° í•¨ì •ë³„ íƒì§€ ë°©ë²•ê³¼ 8ë‹¨ê³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë°©ì§€ ê°€ì´ë“œë¼ì¸
   - ì‹¤ì¦ì  ì‹œì—°ì„ í†µí•œ í•¨ì •ì˜ ì‹¤ì œ ì˜í–¥ê³¼ í•´ê²°ì±… ì²´í—˜

5. **í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•**
   - ì¸ê°„ì˜ ì§€í˜œì™€ AIì˜ íš¨ìœ¨ì„±ì„ ìµœì  ì¡°í•©í•œ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸
   - ì•ˆì „ ìˆ˜ì¤€ë³„ ìë™í™” ì •ë„ ì¡°ì ˆê³¼ ë„ë©”ì¸ ì§€ì‹ í†µí•© ë©”ì»¤ë‹ˆì¦˜
   - íˆ¬ëª…ì„±ê³¼ ì¬í˜„ì„±ì„ ë³´ì¥í•˜ëŠ” ì™„ì „í•œ í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ

### ğŸŒŸ Part 4ì˜ í•µì‹¬ ë©”ì‹œì§€

> **"AIëŠ” ê°•ë ¥í•œ ì¡°ìˆ˜ì´ì§€ë§Œ, ì„ ì¥ì€ ì—¬ì „íˆ ì¸ê°„ì´ì–´ì•¼ í•©ë‹ˆë‹¤."**

AI ìë™ ì „ì²˜ë¦¬ëŠ” ë¶„ëª… í˜ì‹ ì ì¸ ë„êµ¬ì´ì§€ë§Œ, **ë§¹ëª©ì  ì‹ ë¢°ëŠ” ìœ„í—˜**í•©ë‹ˆë‹¤. ì§„ì •í•œ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ëŠ”:

- **AIì˜ í˜ì„ í™œìš©í•˜ë˜ ê·¸ í•œê³„ë¥¼ ì •í™•íˆ ì´í•´**í•˜ê³ 
- **ë„ë©”ì¸ ì§€ì‹ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì„ ì ˆëŒ€ ë†“ì¹˜ì§€ ì•Šìœ¼ë©°**
- **íˆ¬ëª…ì„±ê³¼ ê²€ì¦ì„ í†µí•´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ë³´ì¥**í•©ë‹ˆë‹¤

### ğŸ“Š ì‹¤ë¬´ ì ìš© ê°€ì´ë“œë¼ì¸

**ğŸ”¹ AutoML ë„êµ¬ ì„ íƒ ì‹œ:**
- ë°ì´í„° í¬ê¸°ì™€ ë³µì¡ë„ì— ë§ëŠ” í”Œë«í¼ ì„ íƒ
- ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì  ê³ ë ¤
- í•´ì„ ê°€ëŠ¥ì„± ìš”êµ¬ì‚¬í•­ ì‚¬ì „ í™•ì¸

**ğŸ”¹ AI íŠ¹ì„± ìƒì„± í™œìš© ì‹œ:**
- ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì‚¬ì „ ê²€í†  í•„ìˆ˜
- ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦ ê°•í™”
- íŠ¹ì„± ì„ íƒì„ í†µí•œ ì°¨ì› ì¶•ì†Œ ê³ ë ¤

**ğŸ”¹ ì§€ëŠ¥í˜• í´ë¦¬ë‹ ì ìš© ì‹œ:**
- 5ì°¨ì› í’ˆì§ˆ í‰ê°€ë¡œ ì¢…í•©ì  ì§„ë‹¨
- ë‹¤ì¸µì  ì´ìƒ íƒì§€ë¡œ ì •í™•ë„ í–¥ìƒ
- ì‹ ë¢°ë„ ì ìˆ˜ ê¸°ë°˜ ê²°ê³¼ í™œìš© ê²°ì •

**ğŸ”¹ í•¨ì • ë°©ì§€ë¥¼ ìœ„í•´:**
- 8ë‹¨ê³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì˜ë¬´ì  ì ìš©
- ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ì˜ ì •ê¸°ì  ê²€í† 
- í¸í–¥ì„± ëª¨ë‹ˆí„°ë§ ì²´ê³„ êµ¬ì¶•

---

## ğŸ® ì§ì ‘ í•´ë³´ê¸°

### â­â­ ì—°ìŠµë¬¸ì œ 1: AutoML ë„êµ¬ ë¹„êµ ë¶„ì„
**ëª©í‘œ:** ë‹¤ì–‘í•œ AutoML í”Œë«í¼ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³  ìƒí™©ë³„ ìµœì  ì„ íƒ ëŠ¥ë ¥ ë°°ì–‘

**ê³¼ì œ:**
ì£¼ì–´ì§„ 3ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ê°€ì¥ ì í•©í•œ AutoML í”Œë«í¼ì„ ì„ íƒí•˜ê³  ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.

```python
# ì‹œë‚˜ë¦¬ì˜¤ë³„ ìš”êµ¬ì‚¬í•­
scenarios = {
    'startup_mvp': {
        'budget': 'limited',
        'timeline': '2_weeks', 
        'data_size': 'small_10k_rows',
        'team_skill': 'beginner',
        'interpretability': 'medium'
    },
    'enterprise_finance': {
        'budget': 'high',
        'timeline': '3_months',
        'data_size': 'large_1m_rows', 
        'team_skill': 'expert',
        'interpretability': 'critical'
    },
    'research_project': {
        'budget': 'medium',
        'timeline': '6_months',
        'data_size': 'medium_100k_rows',
        'team_skill': 'advanced', 
        'interpretability': 'high'
    }
}

def recommend_automl_platform(scenario_requirements):
    """
    ì£¼ì–´ì§„ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” AutoML í”Œë«í¼ ì¶”ì²œ
    
    ê³ ë ¤í•´ì•¼ í•  í”Œë«í¼ë“¤:
    - H2O AutoML: ê°•ë ¥í•˜ì§€ë§Œ ë³µì¡
    - Google AutoML: ì‚¬ìš© ì‰½ì§€ë§Œ ë¹„ìŒˆ
    - DataRobot: ì—”í„°í”„ë¼ì´ì¦ˆê¸‰, ë§¤ìš° ë¹„ìŒˆ
    - AutoGluon: ë¬´ë£Œ, AWS ìµœì í™”
    - TPOT: ë¬´ë£Œ, ì—°êµ¬ ì¹œí™”ì 
    """
    # ì—¬ê¸°ì— ì¶”ì²œ ë¡œì§ êµ¬í˜„
    pass

# TODO: ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê³  ì„ íƒ ì´ìœ ë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

**íŒíŠ¸:** ë¹„ìš©, ì‚¬ìš© í¸ì˜ì„±, ì„±ëŠ¥, í•´ì„ ê°€ëŠ¥ì„±, í™•ì¥ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì„¸ìš”.

---

### â­â­â­ ì—°ìŠµë¬¸ì œ 2: AI íŠ¹ì„± ìƒì„± ì‹œìŠ¤í…œ êµ¬í˜„
**ëª©í‘œ:** ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ì§€ëŠ¥í˜• íŠ¹ì„± ìƒì„± ì‹œìŠ¤í…œ ì„¤ê³„ ë° êµ¬í˜„

**ê³¼ì œ:**
ì „ììƒê±°ë˜ ê³ ê° ë°ì´í„°ì— ëŒ€í•´ AI ê¸°ë°˜ íŠ¹ì„± ìƒì„± ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ê³ , ìƒì„±ëœ íŠ¹ì„±ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ í‰ê°€í•˜ì„¸ìš”.

```python
import pandas as pd
import numpy as np

# ì „ììƒê±°ë˜ ê³ ê° ë°ì´í„° (ì˜ˆì‹œ)
np.random.seed(42)
n_customers = 1000

ecommerce_data = pd.DataFrame({
    'customer_id': range(1, n_customers + 1),
    'age': np.random.normal(35, 12, n_customers),
    'income': np.random.lognormal(10, 0.8, n_customers),
    'total_purchases': np.random.poisson(15, n_customers),
    'avg_order_value': np.random.lognormal(4, 0.6, n_customers),
    'days_since_last_purchase': np.random.exponential(30, n_customers),
    'website_visits_per_month': np.random.poisson(8, n_customers),
    'mobile_app_usage_hours': np.random.exponential(2, n_customers),
    'customer_service_contacts': np.random.poisson(2, n_customers),
    'return_rate': np.random.beta(2, 8, n_customers),  # 0-1 ì‚¬ì´
    'product_categories_purchased': np.random.poisson(3, n_customers),
    'customer_lifetime_value': np.random.lognormal(6, 1, n_customers)  # íƒ€ê²Ÿ ë³€ìˆ˜
})

class EcommerceFeatureGenerator:
    """ì „ììƒê±°ë˜ íŠ¹í™” AI íŠ¹ì„± ìƒì„±ê¸°"""
    
    def __init__(self):
        self.domain_knowledge = {
            'customer_segments': ['new', 'regular', 'vip', 'at_risk'],
            'engagement_metrics': ['frequency', 'recency', 'monetary'],
            'behavior_patterns': ['browser', 'buyer', 'returner', 'complainer']
        }
    
    def generate_rfm_features(self, df):
        """RFM ë¶„ì„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
        # TODO: Recency, Frequency, Monetary íŠ¹ì„± ìƒì„±
        # Recency: ìµœê·¼ êµ¬ë§¤ì¼
        # Frequency: êµ¬ë§¤ ë¹ˆë„ 
        # Monetary: êµ¬ë§¤ ê¸ˆì•¡
        pass
    
    def generate_engagement_features(self, df):
        """ê³ ê° ì°¸ì—¬ë„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
        # TODO: ì›¹ì‚¬ì´íŠ¸/ì•± ì°¸ì—¬ë„ ì¢…í•© ì ìˆ˜
        # êµ¬ë§¤ ì „í™˜ìœ¨, ì„¸ì…˜ë‹¹ í˜ì´ì§€ë·° ë“±
        pass
    
    def generate_lifecycle_features(self, df):
        """ê³ ê° ìƒì• ì£¼ê¸° ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
        # TODO: ì‹ ê·œ/ì„±ì¥/ì„±ìˆ™/ì‡ í‡´ ë‹¨ê³„ ë¶„ë¥˜
        # ê³ ê° ë‚˜ì´, ì´íƒˆ ìœ„í—˜ë„ ë“±
        pass
    
    def generate_behavioral_features(self, df):
        """í–‰ë™ íŒ¨í„´ ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
        # TODO: ì‡¼í•‘ íŒ¨í„´, ì„ í˜¸ë„, ì¶©ì„±ë„ ì§€í‘œ
        pass
    
    def evaluate_feature_business_value(self, df, new_features, target_col):
        """ìƒì„±ëœ íŠ¹ì„±ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ í‰ê°€"""
        # TODO: ê° íŠ¹ì„±ì´ CLV ì˜ˆì¸¡ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
        # íŠ¹ì„±ë³„ ì¤‘ìš”ë„ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„ ì œê³µ
        pass

# TODO: EcommerceFeatureGeneratorë¥¼ ì™„ì„±í•˜ê³  ì‹¤í–‰í•´ë³´ì„¸ìš”
# ê° íŠ¹ì„±ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  ì˜ë¯¸ì™€ í™œìš© ë°©ì•ˆì„ ì„¤ëª…í•˜ì„¸ìš”
```

**í‰ê°€ ê¸°ì¤€:**
- ë„ë©”ì¸ ì§€ì‹ì˜ ì ì ˆí•œ í™œìš© (30%)
- íŠ¹ì„± ìƒì„±ì˜ ì°½ì˜ì„±ê³¼ ìœ ìš©ì„± (40%) 
- ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ í•´ì„ì˜ ì •í™•ì„± (30%)

---

### â­â­â­â­ ì—°ìŠµë¬¸ì œ 3: ìë™í™” í•¨ì • íƒì§€ ì‹œìŠ¤í…œ
**ëª©í‘œ:** ìë™í™” ì „ì²˜ë¦¬ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” í•¨ì •ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ íƒì§€í•˜ê³  í•´ê²°í•˜ëŠ” ì „ë¬¸ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•

**ê³¼ì œ:**
ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì—ì„œ ìë™í™” ì „ì²˜ë¦¬ì˜ 4ê°€ì§€ ì£¼ìš” í•¨ì •ì„ íƒì§€í•˜ê³  ê²½ê³ í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”.

```python
class AutomationPitfallDetector:
    """ìë™í™” ì „ì²˜ë¦¬ í•¨ì • íƒì§€ ë° ê²½ê³  ì‹œìŠ¤í…œ"""
    
    def __init__(self, sensitivity='medium'):
        self.sensitivity = sensitivity  # 'low', 'medium', 'high'
        self.detection_history = []
        self.warning_thresholds = self._set_thresholds()
    
    def _set_thresholds(self):
        """ë¯¼ê°ë„ë³„ ê²½ê³  ì„ê³„ê°’ ì„¤ì •"""
        thresholds = {
            'low': {'context_change': 0.3, 'overfitting_ratio': 0.2, 'bias_difference': 0.15},
            'medium': {'context_change': 0.2, 'overfitting_ratio': 0.15, 'bias_difference': 0.1}, 
            'high': {'context_change': 0.1, 'overfitting_ratio': 0.1, 'bias_difference': 0.05}
        }
        return thresholds[self.sensitivity]
    
    def detect_context_ignorance(self, original_df, processed_df, domain_rules=None):
        """ë„ë©”ì¸ ë§¥ë½ ë¬´ì‹œ í•¨ì • íƒì§€"""
        warnings = []
        
        # TODO: ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì²´í¬í•˜ì—¬ í•¨ì • íƒì§€
        # 1. ê²°ì¸¡ì¹˜ íŒ¨í„´ì˜ ì˜ë¯¸ ë¬´ì‹œ
        # 2. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜
        # 3. ë„ë©”ì¸ ìƒì‹ì— ì–´ê¸‹ë‚˜ëŠ” ë³€í™˜
        # 4. ì¤‘ìš”í•œ ë§¥ë½ ì •ë³´ ì†ì‹¤
        
        return warnings
    
    def detect_overfitting_artifacts(self, df, target_col, generated_features):
        """ê³¼ì í•© ë° í—ˆìœ„ íŒ¨í„´ í•¨ì • íƒì§€"""
        warnings = []
        
        # TODO: ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì²´í¬í•˜ì—¬ í•¨ì • íƒì§€
        # 1. íŠ¹ì„± ìˆ˜ vs ìƒ˜í”Œ ìˆ˜ ë¹„ìœ¨
        # 2. êµì°¨ ê²€ì¦ ì„±ëŠ¥ ì €í•˜
        # 3. íŠ¹ì„± ê°„ ë†’ì€ ìƒê´€ê´€ê³„ (ë‹¤ì¤‘ê³µì„ ì„±)
        # 4. ì˜ë¯¸ ì—†ëŠ” ìˆ˜í•™ì  ì¡°í•©ë“¤
        
        return warnings
    
    def detect_automation_bias(self, df, protected_attributes=None):
        """ìë™í™” í¸í–¥ í•¨ì • íƒì§€"""
        warnings = []
        
        # TODO: ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì²´í¬í•˜ì—¬ í•¨ì • íƒì§€
        # 1. ë³´í˜¸ ì†ì„±ì— ë”°ë¥¸ ì°¨ë³„ì  ì²˜ë¦¬
        # 2. ê³¼ê±° í¸í–¥ ë°ì´í„°ì˜ íŒ¨í„´ í•™ìŠµ
        # 3. ëŒ€í‘œì„± ë¶€ì¡±í•œ ê·¸ë£¹ì˜ ì†Œì™¸
        # 4. ê³µì •ì„± ì§€í‘œ ìœ„ë°˜
        
        return warnings
    
    def detect_black_box_problems(self, preprocessing_pipeline):
        """ë¸”ë™ë°•ìŠ¤ ë¬¸ì œ í•¨ì • íƒì§€"""
        warnings = []
        
        # TODO: ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ì²´í¬í•˜ì—¬ í•¨ì • íƒì§€
        # 1. ì„¤ëª… ë¶ˆê°€ëŠ¥í•œ ë³€í™˜ ê³¼ì •
        # 2. ì¬í˜„ì„± ë¶€ì¡±
        # 3. ë””ë²„ê¹… ì–´ë ¤ì›€
        # 4. ê·œì œ ìš”êµ¬ì‚¬í•­ ë¯¸ì¶©ì¡±
        
        return warnings
    
    def generate_comprehensive_report(self, detection_results):
        """ì¢…í•© í•¨ì • íƒì§€ ë³´ê³ ì„œ ìƒì„±"""
        # TODO: íƒì§€ëœ ëª¨ë“  í•¨ì •ë“¤ì„ ìš°ì„ ìˆœìœ„ë³„ë¡œ ì •ë¦¬
        # ê° í•¨ì •ë³„ í•´ê²° ë°©ì•ˆê³¼ ì˜ˆë°©ì±… ì œì‹œ
        pass
    
    def recommend_safety_measures(self, warnings):
        """ì•ˆì „ ì¡°ì¹˜ ê¶Œê³ ì‚¬í•­ ìƒì„±"""
        # TODO: íƒì§€ëœ í•¨ì •ì— ë”°ë¥¸ êµ¬ì²´ì  í•´ê²°ì±… ì œì‹œ
        pass

# TODO: ë‹¤ìŒ 3ê°€ì§€ ë°ì´í„°ì…‹ì— ëŒ€í•´ í•¨ì • íƒì§€ ì‹œìŠ¤í…œì„ ì ìš©í•˜ì„¸ìš”
test_datasets = {
    'medical_diagnosis': 'ì˜ë£Œ ì§„ë‹¨ ë°ì´í„° (í¸í–¥ ìœ„í—˜ ë†’ìŒ)',
    'financial_credit': 'ì‹ ìš© í‰ê°€ ë°ì´í„° (ê³µì •ì„± ì¤‘ìš”)',
    'marketing_campaign': 'ë§ˆì¼€íŒ… ìº í˜ì¸ ë°ì´í„° (ê³¼ì í•© ìœ„í—˜)'
}

# ê° ë°ì´í„°ì…‹ë³„ë¡œ ì–´ë–¤ í•¨ì •ì´ íƒì§€ë˜ëŠ”ì§€ ë¶„ì„í•˜ê³ 
# ì ì ˆí•œ í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”
```

**ë„ì „ ê³¼ì œ:**
- ì‹¤ì œ ë°ì´í„°ì…‹ì—ì„œ í•¨ì • ì‚¬ë¡€ë¥¼ ì°¾ì•„ ë¶„ì„
- í•¨ì •ë³„ ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ì„¤ê³„
- í•¨ì • ë°©ì§€ë¥¼ ìœ„í•œ ì˜ˆë°©ì  ê°€ì´ë“œë¼ì¸ ìˆ˜ë¦½

---

### â­â­â­â­â­ ì—°ìŠµë¬¸ì œ 4: ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ
**ëª©í‘œ:** ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ ì„¤ê³„ ë° êµ¬í˜„

**ê³¼ì œ:**
ê¸ˆìœµ ê¸°ê´€ì˜ ëŒ€ì¶œ ì‹¬ì‚¬ ì‹œìŠ¤í…œì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•˜ì„¸ìš”. ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤:

**ìš”êµ¬ì‚¬í•­:**
1. **ê·œì œ ì¤€ìˆ˜**: ê¸ˆìœµ ê·œì œ ìš”êµ¬ì‚¬í•­ ë§Œì¡±
2. **ê³µì •ì„±**: ì„±ë³„, ì¸ì¢… ë“±ì— ë”°ë¥¸ ì°¨ë³„ ë°©ì§€  
3. **íˆ¬ëª…ì„±**: ëª¨ë“  ì²˜ë¦¬ ê³¼ì • ì¶”ì  ê°€ëŠ¥
4. **ì„±ëŠ¥**: ê¸°ì¡´ ì‹œìŠ¤í…œ ëŒ€ë¹„ 20% ì´ìƒ ì„±ëŠ¥ í–¥ìƒ
5. **í™•ì¥ì„±**: ì¼ì¼ 10ë§Œ ê±´ ì²˜ë¦¬ ê°€ëŠ¥
6. **ì•ˆì •ì„±**: 99.9% ê°€ìš©ì„± ë³´ì¥

```python
class ProductionHybridPreprocessor:
    """í”„ë¡œë•ì…˜ê¸‰ í•˜ì´ë¸Œë¦¬ë“œ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config):
        self.config = config
        self.audit_log = []
        self.performance_metrics = {}
        self.regulatory_compliance = RegulatorySuite()
        self.fairness_monitor = FairnessMonitor()
        self.quality_control = QualityController()
    
    def process_loan_application(self, application_data):
        """ëŒ€ì¶œ ì‹ ì²­ ë°ì´í„° ì „ì²˜ë¦¬"""
        # TODO: ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
        pass
    
    def ensure_regulatory_compliance(self, data, processing_steps):
        """ê·œì œ ì¤€ìˆ˜ í™•ì¸"""
        # TODO: GDPR, ê³µì •ì‹ ìš©ë³´ê³ ë²• ë“± ì¤€ìˆ˜ ê²€ì¦
        pass
    
    def monitor_fairness(self, data, protected_attributes):
        """ê³µì •ì„± ëª¨ë‹ˆí„°ë§"""
        # TODO: ê·¸ë£¹ë³„ ì²˜ë¦¬ ê²°ê³¼ ê³µì •ì„± ê²€ì¦
        pass
    
    def audit_processing_pipeline(self):
        """ì²˜ë¦¬ ê³¼ì • ê°ì‚¬"""
        # TODO: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì¶”ì  ê°€ëŠ¥ì„± ë³´ì¥
        pass
    
    def performance_benchmark(self, test_data):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
        # TODO: ê¸°ì¡´ ì‹œìŠ¤í…œ ëŒ€ë¹„ ì„±ëŠ¥ ì¸¡ì •
        pass
    
    def stress_test(self, load_level):
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        # TODO: ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ ì„±ëŠ¥ ê²€ì¦
        pass

# TODO: ì™„ì „í•œ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ê³  ë‹¤ìŒì„ ì œì¶œí•˜ì„¸ìš”:
# 1. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨
# 2. í•µì‹¬ ì½”ë“œ êµ¬í˜„
# 3. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë° ê²°ê³¼
# 4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ
# 5. ê·œì œ ì¤€ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸
# 6. ìš´ì˜ ë§¤ë‰´ì–¼
```

**í‰ê°€ ê¸°ì¤€:**
- ì‹œìŠ¤í…œ ì™„ì„±ë„ (25%)
- ê·œì œ ì¤€ìˆ˜ ìˆ˜ì¤€ (20%)
- ì„±ëŠ¥ ê°œì„  ì •ë„ (20%)
- ì½”ë“œ í’ˆì§ˆ ë° ë¬¸ì„œí™” (15%)
- í˜ì‹ ì„± ë° ì°½ì˜ì„± (10%)
- ì‹¤ë¬´ ì ìš© ê°€ëŠ¥ì„± (10%)

---

## ğŸ¤” ìƒê°í•´ë³´ê¸°

### ğŸ’­ ì‹¬í™” ì§ˆë¬¸ë“¤

1. **AI ìë™í™”ì˜ ë¯¸ë˜**  
   "10ë…„ í›„ ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì™„ì „íˆ ìë™í™”ë  ê²ƒì´ë¼ê³  ìƒê°í•˜ì‹œë‚˜ìš”? ê·¸ë ‡ë‹¤ë©´ ë°ì´í„° ë¶„ì„ê°€ì˜ ì—­í• ì€ ì–´ë–»ê²Œ ë³€í™”í• ê¹Œìš”?"

2. **ìœ¤ë¦¬ì  ë”œë ˆë§ˆ**  
   "AI ì „ì²˜ë¦¬ ì‹œìŠ¤í…œì´ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ ì¼ë¶€ ê·¸ë£¹ì—ê²Œ ë¶ˆê³µì •í•œ ê²°ê³¼ë¥¼ ë‚³ëŠ”ë‹¤ë©´, ì„±ëŠ¥ê³¼ ê³µì •ì„± ì¤‘ ë¬´ì—‡ì„ ìš°ì„ í•´ì•¼ í• ê¹Œìš”?"

3. **ë„ë©”ì¸ ì§€ì‹ì˜ ê°€ì¹˜**  
   "AIê°€ ì ì  ë°œì „í•˜ë©´ì„œ ë„ë©”ì¸ ì „ë¬¸ê°€ì˜ ì§€ì‹ì´ ë¶ˆí•„ìš”í•´ì§ˆê¹Œìš”? ì•„ë‹ˆë©´ ì˜¤íˆë ¤ ë” ì¤‘ìš”í•´ì§ˆê¹Œìš”?"

4. **ì‹ ë¢°ì™€ ê²€ì¦**  
   "ì–´ëŠ ì •ë„ì˜ ê²€ì¦ì´ë©´ AI ì „ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì‹ ë¢°í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•˜ì‹œë‚˜ìš”? 100% ê²€ì¦ì€ ê°€ëŠ¥í• ê¹Œìš”?"

5. **ê·œì œì™€ í˜ì‹ **  
   "AI ì „ì²˜ë¦¬ì— ëŒ€í•œ ê°•í•œ ê·œì œê°€ í˜ì‹ ì„ ì €í•´í•  ìˆ˜ ìˆë‹¤ê³  ìƒê°í•˜ì‹œë‚˜ìš”? ì ì ˆí•œ ê· í˜•ì ì€ ì–´ë””ì¼ê¹Œìš”?"

### ğŸ”® ë‹¤ìŒ Part ì˜ˆê³ : "ì‹¤ì œ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"

ë‹¤ìŒ **Part 5**ì—ì„œëŠ” ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ **ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

**ğŸ¯ Part 5 ì£¼ìš” ë‚´ìš©:**
- **í™•ì¥ ê°€ëŠ¥í•œ ì „ì²˜ë¦¬ ì•„í‚¤í…ì²˜ ì„¤ê³„**
- **CI/CDë¥¼ í™œìš©í•œ ìë™í™”ëœ íŒŒì´í”„ë¼ì¸ ë°°í¬**  
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ í’ˆì§ˆ ê´€ë¦¬ ì‹œìŠ¤í…œ**
- **ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¶„ì‚° ì‹œìŠ¤í…œ**
- **House Prices ë°ì´í„°ë¥¼ í™œìš©í•œ ì™„ì „í•œ MLOps íŒŒì´í”„ë¼ì¸**

**ğŸ’¡ ë¯¸ë¦¬ ì¤€ë¹„í•  ê²ƒ:**
- Part 1-4ì—ì„œ í•™ìŠµí•œ ëª¨ë“  ì „ì²˜ë¦¬ ê¸°ë²• ë³µìŠµ
- Dockerì™€ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ê¸°ì´ˆ ì§€ì‹
- í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ë°ì´í„° ì²˜ë¦¬ ê²½í—˜ ìƒê¸°

> **"Part 4ì—ì„œ ë°°ìš´ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì´ Part 5ì—ì„œ ì‹¤ì œ ì‹œìŠ¤í…œìœ¼ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤. AIì˜ íš¨ìœ¨ì„±ê³¼ ì¸ê°„ì˜ ì§€í˜œë¥¼ ê²°í•©í•œ ì°¨ì„¸ëŒ€ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œìŠ¤í…œì„ í•¨ê»˜ ë§Œë“¤ì–´ë³´ì„¸ìš”!"**

---

**ğŸ‰ Part 4 ì™„ë£Œ!** 

ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ ì—¬ëŸ¬ë¶„ì€ **AI ìë™ ì „ì²˜ë¦¬ì˜ ëª…ì•”ì„ ëª¨ë‘ ì´í•´í•˜ê³ , ê·¸ í˜ì„ ì•ˆì „í•˜ê³  íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì „ë¬¸ê°€**ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. 

ë‹¤ìŒ Partì—ì„œëŠ” ì´ ëª¨ë“  ì§€ì‹ì„ ì‹¤ì œ ì‹œìŠ¤í…œìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ **ì§„ì •í•œ ë°ì´í„° ì „ì²˜ë¦¬ ë§ˆìŠ¤í„°**ì˜ ê²½ì§€ì— ë„ë‹¬í•´ë³´ê² ìŠµë‹ˆë‹¤! ğŸš€

