# 2ì¥ Part 4: AI ë„êµ¬ë¥¼ í™œìš©í•œ EDAì™€ ì „í†µì  ë°©ì‹ ë¹„êµ

## í•™ìŠµ ëª©í‘œ
- ì£¼ìš” AI ìë™í™” EDA ë„êµ¬ë“¤ì˜ íŠ¹ì„±ê³¼ ì¥ë‹¨ì ì„ ëª…í™•íˆ ì´í•´í•˜ê³  ì ì ˆíˆ ì„ íƒí•  ìˆ˜ ìˆë‹¤
- ì „í†µì  EDA ë°©ì‹ê³¼ AI ê¸°ë°˜ ë°©ì‹ì˜ ì°¨ì´ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆë‹¤
- AIê°€ ìƒì„±í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ë¹„íŒì  ì‚¬ê³ ë¡œ ê²€ì¦í•˜ê³  ì˜¬ë°”ë¥´ê²Œ í•´ì„í•  ìˆ˜ ìˆë‹¤
- íš¨ê³¼ì ì¸ í•˜ì´ë¸Œë¦¬ë“œ EDA ì ‘ê·¼ë²•ì„ ì„¤ê³„í•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆë‹¤

## ì´ë²ˆ Part ë¯¸ë¦¬ë³´ê¸°

**ğŸ¤– AI ì‹œëŒ€ì˜ ë°ì´í„° ë¶„ì„ í˜ëª…**

ì—¬ëŸ¬ë¶„ì´ ì§€ê¸ˆê¹Œì§€ ë°°ìš´ EDA ë°©ì‹ì€ ë§ˆì¹˜ ìˆ˜ì‘ì—…ìœ¼ë¡œ ê·¸ë¦¼ì„ ê·¸ë¦¬ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ AI ì‹œëŒ€ì˜ EDAëŠ” ê³ ì„±ëŠ¥ ë””ì§€í„¸ ë„êµ¬ë¥¼ í™œìš©í•˜ëŠ” ê²ƒê³¼ ê°™ì£ . 

**ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°**:
- ğŸ“ **ì „í†µì  EDA**: ì—°í•„ê³¼ ì¢…ì´ë¡œ ì •êµí•œ ì„¤ê³„ë„ ê·¸ë¦¬ê¸° (ì •ë°€í•˜ì§€ë§Œ ì‹œê°„ ì†Œìš”)
- ğŸ¤– **AI ìë™í™” EDA**: 3D ëª¨ë¸ë§ ì†Œí”„íŠ¸ì›¨ì–´ë¡œ ì¦‰ì‹œ ë Œë”ë§ (ë¹ ë¥´ì§€ë§Œ ì„¸ë°€í•œ ì¡°ì • í•„ìš”)
- ğŸ”„ **í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•**: ë””ì§€í„¸ ë„êµ¬ë¡œ ì´ˆì•ˆì„ ë§Œë“¤ê³  ìˆ˜ì‘ì—…ìœ¼ë¡œ ì™„ì„± (ìµœì ì˜ ê²°ê³¼)

ì´ë²ˆ Partì—ì„œëŠ” AIê°€ ì–´ë–»ê²Œ ë°ì´í„° íƒìƒ‰ ê³¼ì •ì„ í˜ì‹ í•˜ê³  ìˆëŠ”ì§€ ì²´í—˜í•´ë³´ê³ , ê° ì ‘ê·¼ë²•ì˜ ì¥ë‹¨ì ì„ ì‹¤ì œ ì½”ë“œì™€ í•¨ê»˜ ë¹„êµ ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤. ë˜í•œ **AIì˜ í•¨ì •**ì„ í”¼í•˜ê³  **ì¸ê°„ì˜ ì§ê´€**ê³¼ ê²°í•©í•˜ëŠ” ë°©ë²•ì„ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤.

**ğŸ’¡ ì™œ ì´ ë‚´ìš©ì´ ì¤‘ìš”í•œê°€ìš”?**
- í˜„ëŒ€ ë°ì´í„° ë¶„ì„ê°€ëŠ” AI ë„êµ¬ë¥¼ í™œìš©í•  ì¤„ ì•Œì•„ì•¼ ê²½ìŸë ¥ í™•ë³´
- AI ê²°ê³¼ë¥¼ ë§¹ì‹ í•˜ì§€ ì•Šê³  ê²€ì¦í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ í•„ìˆ˜
- íš¨ìœ¨ì„±ê³¼ ì •í™•ì„±ì„ ëª¨ë‘ ê°–ì¶˜ ë¶„ì„ ë°©ë²•ë¡  ìŠµë“

---

## ì‹¤ìŠµ í™˜ê²½ ì¤€ë¹„

### ğŸ› ï¸ AI EDA ë„êµ¬ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

ì‹¤ìŠµì„ ìœ„í•´ ë‹¤ì–‘í•œ AI ë„êµ¬ë“¤ì„ ì„¤ì¹˜í•˜ê³  ì„¤ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.

#### 1ë‹¨ê³„: ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸

```bash
# í„°ë¯¸ë„ ë˜ëŠ” ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ ì‹¤í–‰

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip
pip install --upgrade pandas numpy matplotlib seaborn

# ì£¼í”¼í„° ë…¸íŠ¸ë¶ í™•ì¥ ë„êµ¬ (ì„ íƒì‚¬í•­)
pip install jupyter notebook
pip install ipywidgets  # ì¸í„°ë™í‹°ë¸Œ ìœ„ì ¯ì„ ìœ„í•œ íŒ¨í‚¤ì§€
```

#### 2ë‹¨ê³„: AI EDA ë„êµ¬ ì„¤ì¹˜

```bash
# ì£¼ìš” AI EDA ë„êµ¬ë“¤ ì„¤ì¹˜
pip install ydata-profiling  # êµ¬ pandas-profiling
pip install sweetviz         # ì•„ë¦„ë‹¤ìš´ ì‹œê°í™” ë„êµ¬
pip install autoviz          # ìë™ ì‹œê°í™” ë„êµ¬
pip install dtale           # ì¸í„°ë™í‹°ë¸Œ ì›¹ GUI

# ì¶”ê°€ ìœ ìš©í•œ ë„êµ¬ë“¤
pip install plotly          # ì¸í„°ë™í‹°ë¸Œ ê·¸ë˜í”„
pip install kaleido         # ì •ì  ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ë„êµ¬
pip install scikit-learn    # ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì¼ë¶€ ë„êµ¬ì—ì„œ í•„ìš”)
```

#### 3ë‹¨ê³„: ì„¤ì¹˜ í™•ì¸ ë° í…ŒìŠ¤íŠ¸

```python
# ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ í™•ì¸ ì½”ë“œ
import sys  # ì‹œìŠ¤í…œ ì •ë³´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

print("ğŸ” ì„¤ì¹˜ëœ AI EDA ë„êµ¬ í™•ì¸:")
print("="*50)

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
try:
    import pandas as pd
    print("âœ… pandas:", pd.__version__)
except ImportError:
    print("âŒ pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

try:
    import numpy as np
    print("âœ… numpy:", np.__version__)
except ImportError:
    print("âŒ numpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

# AI EDA ë„êµ¬ë“¤ í™•ì¸
try:
    from ydata_profiling import ProfileReport
    print("âœ… ydata-profiling: ì„¤ì¹˜ ì™„ë£Œ")
except ImportError:
    print("âŒ ydata-profiling ì„¤ì¹˜ í•„ìš”: pip install ydata-profiling")

try:
    import sweetviz as sv
    print("âœ… sweetviz: ì„¤ì¹˜ ì™„ë£Œ")
except ImportError:
    print("âŒ sweetviz ì„¤ì¹˜ í•„ìš”: pip install sweetviz")

try:
    from autoviz.AutoViz_Class import AutoViz_Class
    print("âœ… autoviz: ì„¤ì¹˜ ì™„ë£Œ")
except ImportError:
    print("âŒ autoviz ì„¤ì¹˜ í•„ìš”: pip install autoviz")

try:
    import dtale
    print("âœ… dtale: ì„¤ì¹˜ ì™„ë£Œ")
except ImportError:
    print("âŒ dtale ì„¤ì¹˜ í•„ìš”: pip install dtale")

print("\nğŸ’» Python í™˜ê²½ ì •ë³´:")
print(f"Python ë²„ì „: {sys.version}")
print(f"ì‘ì—… ë””ë ‰í† ë¦¬: {sys.path[0]}")
```

#### 4ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„ ë° ê¸°ë³¸ ì„¤ì •

```python
# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì„¤ì •
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

# í•œê¸€ í°íŠ¸ ì„¤ì • (ìš´ì˜ì²´ì œë³„)
import platform
import matplotlib.font_manager as fm

def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ìë™ ì„¤ì • í•¨ìˆ˜"""
    system = platform.system()
    
    if system == 'Windows':
        # Windows í™˜ê²½
        plt.rcParams['font.family'] = 'Malgun Gothic'
        print("âœ… Windows í•œê¸€ í°íŠ¸ ì„¤ì •: Malgun Gothic")
    elif system == 'Darwin':  # macOS
        # macOS í™˜ê²½
        plt.rcParams['font.family'] = 'AppleGothic'
        print("âœ… macOS í•œê¸€ í°íŠ¸ ì„¤ì •: AppleGothic")
    else:  # Linux
        # Linux í™˜ê²½ (Google Colab í¬í•¨)
        try:
            # Noto Sans CJK í°íŠ¸ ì‹œë„
            plt.rcParams['font.family'] = 'Noto Sans CJK KR'
            print("âœ… Linux í•œê¸€ í°íŠ¸ ì„¤ì •: Noto Sans CJK KR")
        except:
            print("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False

# í™˜ê²½ ì„¤ì • ì‹¤í–‰
setup_korean_font()

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸° (ì„ íƒì‚¬í•­)
warnings.filterwarnings('ignore')

# pandas ì¶œë ¥ ì˜µì…˜ ì„¤ì •
pd.set_option('display.max_columns', None)    # ëª¨ë“  ì»¬ëŸ¼ í‘œì‹œ
pd.set_option('display.max_rows', 100)        # ìµœëŒ€ 100í–‰ í‘œì‹œ
pd.set_option('display.width', None)          # ì¶œë ¥ í­ ì œí•œ ì—†ìŒ
pd.set_option('display.max_colwidth', 50)     # ì»¬ëŸ¼ ë‚´ìš© ìµœëŒ€ 50ì

print("ğŸ¨ ì‹œê°í™” ë° ì¶œë ¥ ì„¤ì • ì™„ë£Œ!")
```

### ğŸ“Š ì‹¤ìŠµ ë°ì´í„° ë¡œë“œ ë° í™•ì¸

```python
# íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ
print("ğŸš¢ íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ì¤‘...")

# ë°©ë²• 1: ì¸í„°ë„·ì—ì„œ ì§ì ‘ ë¡œë“œ (ê¶Œì¥)
try:
    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
    df = pd.read_csv(url)
    print("âœ… ì¸í„°ë„·ì—ì„œ ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f"âŒ ì¸í„°ë„· ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 2: ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ (ë°±ì—…)
    try:
        df = pd.read_csv('titanic.csv')  # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
        print("âœ… ë¡œì»¬ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ ì„±ê³µ!")
    except Exception as e:
        print(f"âŒ ë¡œì»¬ íŒŒì¼ ë¡œë“œë„ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ í•´ê²° ë°©ë²•: Kaggleì—ì„œ titanic.csv íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”")

# ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸
if 'df' in locals():
    print(f"\nğŸ“Š ë°ì´í„° ê¸°ë³¸ ì •ë³´:")
    print(f"  â€¢ ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")
    print(f"  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024:.1f} KB")
    print(f"  â€¢ ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(df.select_dtypes(include=['number']).columns)}ê°œ")
    print(f"  â€¢ ë²”ì£¼í˜• ë³€ìˆ˜: {len(df.select_dtypes(include=['object']).columns)}ê°œ")
    
    print(f"\nğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())
else:
    print("âŒ ë°ì´í„° ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ ì„¤ì •ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
```

---

## 4.1 AI ìë™í™” EDA ë„êµ¬ ì™„ì „ ì •ë³µ

### ğŸ“š AI ìë™í™” EDAì˜ ì´í•´

**ğŸ’­ AI ìë™í™” EDAë€?**

ì „í†µì ì¸ EDAê°€ ìš”ë¦¬ì‚¬ê°€ ì§ì ‘ ëª¨ë“  ì¬ë£Œë¥¼ ì†ì§ˆí•˜ê³  ìš”ë¦¬í•˜ëŠ” ê³¼ì •ì´ë¼ë©´, AI ìë™í™” EDAëŠ” ìµœì²¨ë‹¨ ì£¼ë°© ë¡œë´‡ì´ ë ˆì‹œí”¼ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìš”ë¦¬í•´ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

**ğŸ” ê¸°ì¡´ EDAì˜ í•œê³„ì ê³¼ AIì˜ í•´ê²°ì±…:**

| ê¸°ì¡´ EDAì˜ ë¬¸ì œì  | AI ìë™í™” EDAì˜ í•´ê²°ì±… |
|------------------|----------------------|
| â° ì‹œê°„ ì†Œëª¨ì  (2-3ì‹œê°„) | âš¡ ì´ˆê³ ì† ì²˜ë¦¬ (2-3ë¶„) |
| ğŸ§  ë¶„ì„ê°€ ê²½í—˜ ì˜ì¡´ | ğŸ¤– ê°ê´€ì  í‘œì¤€ ë¶„ì„ |
| ğŸ¯ ë†“ì¹˜ê¸° ì‰¬ìš´ íŒ¨í„´ | ğŸ” í¬ê´„ì  íŒ¨í„´ íƒì§€ |
| ğŸ“Š ì¼ê´€ì„± ë¶€ì¡± | ğŸ“‹ í‘œì¤€í™”ëœ ë³´ê³ ì„œ |
| ğŸ’ª ë°˜ë³µ ì‘ì—… í”¼ë¡œ | ğŸ”„ ì™„ì „ ìë™í™” |

**âœ¨ AI ìë™í™” EDAì˜ í•µì‹¬ ì¥ì :**

1. **âš¡ ì†ë„**: ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ì™„ì „í•œ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
2. **ğŸ¯ í¬ê´„ì„±**: ì¸ê°„ì´ ë†“ì¹  ìˆ˜ ìˆëŠ” ìˆ¨ê²¨ì§„ íŒ¨í„´ê¹Œì§€ ìë™ ë°œê²¬
3. **ğŸ“Š ì¼ê´€ì„±**: í•­ìƒ ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë¹„êµ ê°€ëŠ¥
4. **ğŸ” ê°ê´€ì„±**: í¸í–¥ ì—†ëŠ” ë°ì´í„° ê¸°ë°˜ ë¶„ì„

### ğŸ› ï¸ ì£¼ìš” AI ìë™í™” EDA ë„êµ¬ ì™„ì „ ê°€ì´ë“œ

#### ğŸ¥‡ 1) ydata-profiling: EDAì˜ ìŠ¤ìœ„ìŠ¤ ì•„ë¯¸ ë‚˜ì´í”„
**ydata-profiling**ì€ Pandas DataFrameì— ëŒ€í•œ ì¢…í•©ì ì¸ ìë™ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•´ì£¼ëŠ” ì˜¤í”ˆì†ŒìŠ¤ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
EDA(Exploratory Data Analysis)ë¥¼ ìë™í™”í•˜ì—¬ ì»¬ëŸ¼ì˜ ë¶„í¬, ê²°ì¸¡ì¹˜, ìƒê´€ê´€ê³„, ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš© ë“±ì„ ì‹œê°ì ìœ¼ë¡œ ìš”ì•½í•œ HTML ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.


**ğŸ¯ íŠ¹ì§•**: ê°€ì¥ ì™„ì„±ë„ ë†’ì€ ì¢…í•© EDA ë„êµ¬
>ì‚¬ëŒì´ ì†ìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ì´ˆê¸° ë°ì´í„° ìš”ì•½ ë¶„ì„ì„ ìë™í™”
100ê°œ ì´ìƒì˜ ì»¬ëŸ¼ì„ ìë™ ë¶„ì„í•˜ë©°, ê²½ê³  ë©”ì‹œì§€ì™€ í†µê³„ ìš”ì•½ê¹Œì§€ í¬ê´„
ë°ì´í„° ì „ì²˜ë¦¬ ì „ ì „ì²´ êµ¬ì¡° íŒŒì•…, í†µê³„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±

**âœ… ì£¼ìš” ê¸°ëŠ¥**
>ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì…, ê²°ì¸¡ì¹˜, ê³ ìœ ê°’, ë¶„í¬ ìë™ ë¶„ì„
ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ, ì¤‘ë³µ íƒì§€, ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš© ì‹œê°í™”
ê²½ê³  ì‹œìŠ¤í…œ: high cardinality, constant ê°’ ë“± íƒì§€
HTML ê¸°ë°˜ì˜ ì •êµí•œ ë¦¬í¬íŠ¸ ìƒì„±

**âœ… ì¥ì **
>ë§¤ìš° ìƒì„¸í•˜ê³  í¬ê´„ì ì¸ ë¶„ì„ ë¦¬í¬íŠ¸
ëŒ€ë‹¤ìˆ˜ì˜ ìˆ˜ì¹˜í˜•/ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•´ ìë™ ì‹œê°í™”
EDA ì´ˆë³´ìì—ê²Œ ë§¤ìš° ì¹œìˆ™í•œ êµ¬ì¡°
pandas-profilingì˜ í›„ì† ë²„ì „ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹° í™œë°œ

**âŒ ë‹¨ì **
>í° ë°ì´í„°ì…‹ì—ëŠ” ì‹¤í–‰ ì†ë„ ëŠë¦¼
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì•„ ëŒ€ê·œëª¨ ë°ì´í„°ì—ëŠ” ë¶€ì í•©
ì‹œê°í™”ê°€ ì •ì (ì¸í„°ë™í‹°ë¸Œ ì•„ë‹˜)
ì¤‘ë³µëœ ì‹œê°í™”ê°€ ë§ì•„ ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìŒ

**ğŸ“¦ ì„¤ì¹˜ ë° ê¸°ë³¸ ì‚¬ìš©ë²• (ìƒì„¸ í•´ì„¤)**

```python
# 1ë‹¨ê³„: ydata-profiling ê¸°ë³¸ ì‚¬ìš©ë²•
from ydata_profiling import ProfileReport  # í”„ë¡œíŒŒì¼ ë³´ê³ ì„œ ìƒì„± í´ë˜ìŠ¤ import
import pandas as pd

url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
df = pd.read_csv(url)
profile = ProfileReport(df, title="Pandas Profiling Report", explorative=True)

# 2ë‹¨ê³„: ê¸°ë³¸ ë³´ê³ ì„œ ìƒì„±
print("ğŸ“Š ydata-profiling ê¸°ë³¸ ë¶„ì„ ì‹œì‘...")

# ProfileReport ê°ì²´ ìƒì„± (ê° ë§¤ê°œë³€ìˆ˜ì˜ ì˜ë¯¸)
profile = ProfileReport(
    df,                              # ë¶„ì„í•  DataFrame
    title="ğŸš¢ íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì™„ì „ ë¶„ì„",   # ë³´ê³ ì„œ ì œëª© (HTML ìƒë‹¨ì— í‘œì‹œ)
    explorative=True,                # íƒìƒ‰ì  ë¶„ì„ ëª¨ë“œ (ë” ë§ì€ í†µê³„ëŸ‰ ê³„ì‚°)
    minimal=False                    # ìµœì†Œ ëª¨ë“œ ì—¬ë¶€ (False=ì „ì²´ ë¶„ì„)
)

print("âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ! HTML íŒŒì¼ë¡œ ì €ì¥ ì¤‘...")

# HTML íŒŒì¼ë¡œ ì €ì¥ (ì›¹ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸ ê°€ëŠ¥)
profile.to_file("titanic_ydata_analysis.html")  # í˜„ì¬ í´ë”ì— HTML íŒŒì¼ ìƒì„±

print("ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: titanic_ydata_analysis.html")
print("ğŸŒ ì›¹ë¸Œë¼ìš°ì €ì—ì„œ í•´ë‹¹ íŒŒì¼ì„ ì—´ì–´ì„œ í™•ì¸í•˜ì„¸ìš”!")

# Jupyter Notebookì—ì„œ ë°”ë¡œ í™•ì¸í•˜ê¸° (ì„ íƒì‚¬í•­)
try:
    # ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œë§Œ ì‘ë™
    profile.to_widgets()  # ì¸í„°ë™í‹°ë¸Œ ìœ„ì ¯ìœ¼ë¡œ í‘œì‹œ
    print("ğŸ“± Jupyter Notebookì—ì„œ ìœ„ì ¯ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤")
except:
    print("ğŸ’» ì¼ë°˜ Python í™˜ê²½ì—ì„œëŠ” HTML íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”")
```

**ğŸ”§ ê³ ê¸‰ ì„¤ì • ì˜µì…˜ (ì»¤ìŠ¤í„°ë§ˆì´ì§•)**

```python
# 3ë‹¨ê³„: ê³ ê¸‰ ì„¤ì •ìœ¼ë¡œ ë§ì¶¤í˜• ë¶„ì„
print("âš™ï¸ ê³ ê¸‰ ì„¤ì •ìœ¼ë¡œ ë§ì¶¤í˜• ë¶„ì„ ìƒì„±...")

# ì„¸ë°€í•œ ì„¤ì •ì´ ê°€ëŠ¥í•œ ê³ ê¸‰ ë¶„ì„
advanced_profile = ProfileReport(
    df,
    title="ğŸ¯ íƒ€ì´íƒ€ë‹‰ ê³ ê¸‰ ë¶„ì„ ë³´ê³ ì„œ",
    
    # ğŸ” ìƒê´€ê´€ê³„ ë¶„ì„ ì„¤ì •
    correlations={
        'pearson': {'threshold': 0.1},    # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ (0.1 ì´ìƒë§Œ í‘œì‹œ)
        'spearman': {'threshold': 0.1},   # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’
        'kendall': {'threshold': 0.1},    # ì¼„ë‹¬ íƒ€ìš° ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’
        'phi_k': {'threshold': 0.1},      # ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ ì—°ê´€ì„± ì¸¡ì •
        'cramers': {'threshold': 0.1}     # í¬ë˜ë¨¸ V í†µê³„ëŸ‰ (ë²”ì£¼í˜• ë³€ìˆ˜)
    },
    
    # ğŸ“Š ê²°ì¸¡ê°’ ë¶„ì„ ì„¤ì •
    missing_diagrams={
        'matrix': True,          # ê²°ì¸¡ê°’ ë§¤íŠ¸ë¦­ìŠ¤ í‘œì‹œ
        'bar': True,            # ê²°ì¸¡ê°’ ë§‰ëŒ€ê·¸ë˜í”„ í‘œì‹œ
        'heatmap': True,        # ê²°ì¸¡ê°’ íˆíŠ¸ë§µ í‘œì‹œ
        'dendrogram': True      # ê²°ì¸¡ê°’ ë´ë“œë¡œê·¸ë¨ í‘œì‹œ (íŒ¨í„´ ë¶„ì„)
    },
    
    # ğŸ¨ ì‹œê°í™” ì„¤ì •
    plot={
        'histogram': {
            'bins': 50,          # íˆìŠ¤í† ê·¸ë¨ êµ¬ê°„ ìˆ˜
            'max_bins': 250      # ìµœëŒ€ êµ¬ê°„ ìˆ˜
        }
    },
    
    # âš¡ ì„±ëŠ¥ ì„¤ì •
    samples={
        'head': 10,             # ìƒìœ„ ëª‡ ê°œ í–‰ í‘œì‹œ
        'tail': 10              # í•˜ìœ„ ëª‡ ê°œ í–‰ í‘œì‹œ
    }
)

# ê³ ê¸‰ ë³´ê³ ì„œ ì €ì¥
advanced_profile.to_file("titanic_advanced_analysis.html")
print("ğŸ¯ ê³ ê¸‰ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ!")

# ğŸ“‹ ë³´ê³ ì„œì— í¬í•¨ë˜ëŠ” ì£¼ìš” ë‚´ìš© ì„¤ëª…
print("\nğŸ“‹ ydata-profiling ë³´ê³ ì„œ êµ¬ì„±:")
print("  ğŸ“Š Overview: ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´ (í–‰/ì—´ ìˆ˜, ê²°ì¸¡ê°’, ì¤‘ë³µê°’)")
print("  ğŸ“ˆ Variables: ê° ë³€ìˆ˜ë³„ ìƒì„¸ ë¶„ì„ (ë¶„í¬, í†µê³„ëŸ‰, ê²½ê³ ì‚¬í•­)")
print("  ğŸ”— Interactions: ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš© ë¶„ì„")
print("  ğŸ”¥ Correlations: ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ (ì—¬ëŸ¬ ë°©ë²•)")
print("  â— Missing values: ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„")
print("  ğŸ“ Sample: ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ/ë í–‰)")
print("  âš ï¸ Warnings: ë°ì´í„° í’ˆì§ˆ ê²½ê³ ì‚¬í•­")
```

#### ğŸ¨ 2) SweetViz: ì•„ë¦„ë‹¤ìš´ ì‹œê°í™”ì˜ ë§ˆë²•ì‚¬
sweetvizëŠ” ë°ì´í„°ì…‹ì˜ ì‹œê°ì  íƒìƒ‰ê³¼ ë¹„êµ ë¶„ì„ì— ì¤‘ì ì„ ë‘” Python ê¸°ë°˜ EDA ë„êµ¬ì…ë‹ˆë‹¤. 
íŠ¹íˆ íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ê´€ê³„ë‚˜ ë‘ ë°ì´í„°ì…‹(ì˜ˆ: Train vs Test)ì˜ ë¹„êµ ë¶„ì„ì„ ê°•ë ¥í•˜ê²Œ ì§€ì›í•©ë‹ˆë‹¤.

**ğŸ¯ íŠ¹ì§•**: 
>ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ìš´ HTML ë¦¬í¬íŠ¸ë¥¼ ë¹ ë¥´ê²Œ ìƒì„±
ìœ ì € ì¹œí™”ì ì´ê³  ì•„ë¦„ë‹¤ìš´ ì‹œê°í™” ì¤‘ì‹¬. íƒ€ê²Ÿ ë³€ìˆ˜ ì¤‘ì‹¬ì˜ ê´€ê³„ ê°•ì¡°
ë¹„ì „ë¬¸ê°€ ëŒ€ìƒ ë³´ê³ ì„œ, Train/Test ë°ì´í„° ìœ ì‚¬ì„± ë¹„êµ, ê°•ì˜ìë£Œ

**âœ… ì£¼ìš” ê¸°ëŠ¥**
>íƒ€ê²Ÿ ë³€ìˆ˜ ê¸°ì¤€ ë¹„êµ ë¶„ì„ (Train/Test ë˜ëŠ” Feature/Target ë¹„êµ)
ì‹œê°ì ìœ¼ë¡œ í™”ë ¤í•˜ê³  ì§ê´€ì ì¸ ì°¨íŠ¸ ì œê³µ
Feature ë³„ë¡œ targetê³¼ì˜ ìƒê´€ ì •ë³´ ê°•ì¡°

**âœ… ì¥ì **
>ì¸í„°ë™í‹°ë¸Œì— ê°€ê¹Œìš´ ì‹œê°í™” UI
íƒ€ê²Ÿ ë³€ìˆ˜ ê¸°ë°˜ ë¹„êµ ë¶„ì„ì— ê°•ë ¥
ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„, HTML ë¦¬í¬íŠ¸ ê²½ëŸ‰í™”
ë¹„ì „ë¬¸ê°€ë„ ì‰½ê²Œ ì´í•´ ê°€ëŠ¥í•œ ì„¤ëª… êµ¬ì¡°

**âŒ ë‹¨ì **
>í†µê³„ì  ìš”ì•½ì€ ydata-profilingì— ë¹„í•´ ë‹¤ì†Œ ë¶€ì¡±
ìƒê´€ë¶„ì„, ì¤‘ë³µíƒì§€ ë“± ê³ ê¸‰ ë¶„ì„ ê¸°ëŠ¥ì€ ìƒëŒ€ì ìœ¼ë¡œ ì ìŒ
ë‹¤ë³€ëŸ‰ ë¶„ì„ ë° ìƒí˜¸ì‘ìš© ë¶„ì„ ê¸°ëŠ¥ ë¶€ì¡±

```python
# SweetViz ê¸°ë³¸ ì‚¬ìš©ë²• (ë‹¨ê³„ë³„ ìƒì„¸ í•´ì„¤)
import sweetviz as sv  # SweetViz ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import pandas as pd

url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
df = pd.read_csv(url)

print("ğŸ¨ SweetVizë¡œ ì•„ë¦„ë‹¤ìš´ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±...")

# 1ï¸âƒ£ ê¸°ë³¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
print("\n1ï¸âƒ£ ê¸°ë³¸ EDA ë³´ê³ ì„œ ìƒì„±:")

# analyze() í•¨ìˆ˜ë¡œ ì „ì²´ ë°ì´í„° ë¶„ì„
basic_report = sv.analyze(
    df,                               # ë¶„ì„í•  DataFrame
    target_feat=None                  # íƒ€ê²Ÿ ë³€ìˆ˜ ì—†ì´ ì „ì²´ ë¶„ì„
)

# HTML íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
basic_report.show_html(
    'sweetviz_basic_report.html',     # ì €ì¥í•  íŒŒì¼ëª…
    open_browser=True,                # ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸° (True/False)
    layout='vertical'                 # ë ˆì´ì•„ì›ƒ: 'vertical' ë˜ëŠ” 'widescreen'
)

print("âœ… ê¸°ë³¸ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")

# 2ï¸âƒ£ íƒ€ê²Ÿ ë³€ìˆ˜ ì¤‘ì‹¬ ë¶„ì„ (ë¶„ë¥˜ ë¬¸ì œì— ìµœì í™”)
print("\n2ï¸âƒ£ ìƒì¡´ ì—¬ë¶€(Survived) íƒ€ê²Ÿ ë¶„ì„:")

# íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì§€ì •í•˜ë©´ ëª¨ë“  ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ì˜ ê´€ê³„ë¥¼ ìë™ ë¶„ì„
target_report = sv.analyze(
    df,                               # ë¶„ì„í•  DataFrame  
    target_feat='Survived',           # íƒ€ê²Ÿ ë³€ìˆ˜ ì§€ì • (ìƒì¡´ ì—¬ë¶€)
    feat_cfg=sv.FeatureConfig(        # íŠ¹ì„± ì„¤ì • ê°ì²´
        skip=['PassengerId', 'Name'],  # ë¶„ì„ì—ì„œ ì œì™¸í•  ì»¬ëŸ¼ë“¤
        force_text=['Ticket']          # í…ìŠ¤íŠ¸ë¡œ ê°•ì œ ì²˜ë¦¬í•  ì»¬ëŸ¼ë“¤
    )
)

target_report.show_html('sweetviz_target_analysis.html')
print("ğŸ¯ íƒ€ê²Ÿ ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")

# 3ï¸âƒ£ ë°ì´í„°ì…‹ ë¹„êµ ë¶„ì„ (í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„êµ)
print("\n3ï¸âƒ£ í›ˆë ¨ vs í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„êµ ë¶„ì„:")

from sklearn.model_selection import train_test_split  # ë°ì´í„° ë¶„í•  í•¨ìˆ˜

# ë°ì´í„°ë¥¼ í›ˆë ¨ìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• 
train_df, test_df = train_test_split(
    df,                    # ì „ì²´ ë°ì´í„°
    test_size=0.3,        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨ (30%)
    random_state=42,      # ì¬í˜„ ê°€ëŠ¥ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ
    stratify=df['Survived']  # ìƒì¡´ìœ¨ì„ ìœ ì§€í•˜ë©° ë¶„í•  (ì¸µí™” ìƒ˜í”Œë§)
)

print(f"  ğŸ“Š í›ˆë ¨ ë°ì´í„°: {len(train_df)}í–‰")
print(f"  ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}í–‰")

# ë‘ ë°ì´í„°ì…‹ ë¹„êµ ë¶„ì„
compare_report = sv.compare(
    [train_df, "í›ˆë ¨ ë°ì´í„°"],        # [ë°ì´í„°í”„ë ˆì„, ë¼ë²¨] í˜•íƒœ
    [test_df, "í…ŒìŠ¤íŠ¸ ë°ì´í„°"],       # [ë°ì´í„°í”„ë ˆì„, ë¼ë²¨] í˜•íƒœ
    target_feat='Survived',          # ë¹„êµ ê¸°ì¤€ íƒ€ê²Ÿ ë³€ìˆ˜
    feat_cfg=sv.FeatureConfig(skip=['PassengerId', 'Name'])
)

compare_report.show_html('sweetviz_compare_analysis.html')
print("ğŸ”„ ë°ì´í„°ì…‹ ë¹„êµ ë¶„ì„ ì™„ë£Œ!")

# ğŸ“Š SweetViz íŠ¹ë³„ ê¸°ëŠ¥ ì„¤ëª…
print("\nğŸ“Š SweetViz íŠ¹ë³„ ê¸°ëŠ¥:")
print("  ğŸ¨ Beautiful UI: ì‹œê°ì ìœ¼ë¡œ ë§¤ìš° ì•„ë¦„ë‹¤ìš´ ì¸í„°í˜ì´ìŠ¤")
print("  ğŸ¯ Target Analysis: íƒ€ê²Ÿ ë³€ìˆ˜ì™€ ëª¨ë“  ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ê´€ê³„ ìë™ ë¶„ì„")
print("  ğŸ”„ Dataset Comparison: ë‘ ë°ì´í„°ì…‹ ê°„ì˜ ì°¨ì´ì  ì‹œê°í™”")
print("  ğŸ“ˆ Association Analysis: ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ ì—°ê´€ì„± ìë™ ê³„ì‚°")
print("  âš¡ Fast Processing: ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„")
```

#### ğŸ§  3) AutoViz: ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì‹œê°í™”
autovizëŠ” ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ë°ì´í„° ì‹œê°í™”ì™€ ë³€ìˆ˜ íƒìƒ‰ì„ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì§€ì •í•˜ë©´ ë¶„ë¥˜/íšŒê·€ ëª©ì ì— ë§ëŠ” ì ì ˆí•œ ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ë©°, ë¹ ë¥¸ ì†ë„ì™€ ê°€ë²¼ìš´ êµ¬ì¡°ê°€ ê°•ì ì…ë‹ˆë‹¤.

**ğŸ¯ íŠ¹ì§•**:
>ëª¨ë¸ë§ ì „ ë¹ ë¥´ê³  ì§ê´€ì ì¸ ë³€ìˆ˜ íƒìƒ‰ê³¼ ì‹œê°í™”ë¥¼ ìë™ìœ¼ë¡œ ì œê³µ
ì½”ë“œ í•œ ì¤„ë¡œ ë‹¤ì–‘í•œ ì‹œê°í™” ìë™ ìƒì„±, ì™¸ë¶€ íŒŒì¼ ì§€ì›
ëª¨ë¸ë§ ì§ì „ ë¹ ë¥¸ ë³€ìˆ˜ íƒìƒ‰, Kaggle EDA, ì‹¤ë¬´ ìë™ ë¶„ì„

**âœ… ì£¼ìš” ê¸°ëŠ¥**
>íŒŒì¼ ê²½ë¡œ í˜¹ì€ DataFrame ì§ì ‘ ì…ë ¥ ê°€ëŠ¥
ìë™ ë³€ìˆ˜ íƒìƒ‰ â†’ ì ì ˆí•œ ì‹œê°í™” ìë™ ìƒì„±
íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì£¼ë©´ ë¶„ë¥˜/íšŒê·€ì— ë§ê²Œ ì‹œê°í™” ì¡°ì •

**âœ… ì¥ì **
>ë¹ ë¥´ê³  ê°€ë²¼ì›€: í° íŒŒì¼ë„ ë¹ ë¥´ê²Œ ë¶„ì„ ê°€ëŠ¥
ë…¸ì´ì¦ˆê°€ ì ì€ ì‹œê°í™” (ê¹”ë”í•œ êµ¬ì„±)
ë¨¸ì‹ ëŸ¬ë‹ ì¤€ë¹„ìš© ì „ì²˜ë¦¬ ê¸°ë°˜ EDAë„ ì§€ì›

**âŒ ë‹¨ì **
>ë¦¬í¬íŠ¸ê°€ HTMLì´ ì•„ë‹ˆë¼ notebookì— ì§ì ‘ ì¶œë ¥
ì¸í„°í˜ì´ìŠ¤ë‚˜ ì‚¬ìš©ì ê²½í—˜ì´ ë¹„êµì  ë‚®ìŒ
íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„ ì‹œ ì‹œê°í™”ëŠ” ì œí•œì 
ì „ì²´ì ìœ¼ë¡œ ê°„ë‹¨í•œ ìˆ˜ì¤€ì˜ ìš”ì•½

```python
# AutoViz ì‚¬ìš©ë²• (AI ê¸°ë°˜ ìë™ ì‹œê°í™”)
from autoviz.AutoViz_Class import AutoViz_Class  # AutoViz í´ë˜ìŠ¤ import

print("ğŸ§  AutoVizë¡œ AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì‹œê°í™”...")

# AutoViz ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
AV = AutoViz_Class()  # AutoViz í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

# ğŸ“ ë°©ë²• 1: DataFrameì„ ì„ì‹œ CSVë¡œ ì €ì¥ í›„ ë¶„ì„ (ê¶Œì¥)
print("\nğŸ“ ë°©ë²• 1: DataFrame ê¸°ë°˜ ë¶„ì„")

# AutoVizëŠ” CSV íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë¯€ë¡œ ì„ì‹œ íŒŒì¼ ìƒì„±
temp_csv_path = 'temp_titanic_data.csv'  # ì„ì‹œ CSV íŒŒì¼ ê²½ë¡œ
df.to_csv(temp_csv_path, index=False)     # ì¸ë±ìŠ¤ ì œì™¸í•˜ê³  CSVë¡œ ì €ì¥

print(f"ğŸ’¾ ì„ì‹œ CSV íŒŒì¼ ìƒì„±: {temp_csv_path}")

# AutoViz ì‹¤í–‰ (íƒ€ê²Ÿ ë³€ìˆ˜ ì§€ì •)
try:
    # verbose=2: ìƒì„¸í•œ ì§„í–‰ ìƒí™© ì¶œë ¥
    # sep=',': CSV êµ¬ë¶„ì ì§€ì •
    # header=0: ì²« ë²ˆì§¸ í–‰ì´ í—¤ë”
    autoviz_df = AV.AutoViz(
        filename=temp_csv_path,       # ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ
        target_variable='Survived',   # íƒ€ê²Ÿ ë³€ìˆ˜ (ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ë³€ìˆ˜)
        verbose=2,                    # ì¶œë ¥ ìƒì„¸ë„ (0=ì¡°ìš©í•¨, 1=ë³´í†µ, 2=ìƒì„¸í•¨)
        chart_format='png',           # ì°¨íŠ¸ ì €ì¥ í˜•ì‹ ('png', 'svg', 'html')
        max_rows_analyzed=1000,       # ë¶„ì„í•  ìµœëŒ€ í–‰ ìˆ˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
        max_cols_analyzed=30          # ë¶„ì„í•  ìµœëŒ€ ì—´ ìˆ˜
    )
    
    print("âœ… AutoViz ë¶„ì„ ì™„ë£Œ!")
    
    # ğŸ“Š AutoVizê°€ ìƒì„±í•˜ëŠ” ì°¨íŠ¸ ìœ í˜• ì„¤ëª…
    print("\nğŸ“Š AutoViz ìƒì„± ì°¨íŠ¸ ìœ í˜•:")
    print("  ğŸ“ˆ Scatter Plots: ì—°ì†í˜• ë³€ìˆ˜ ê°„ ì‚°ì ë„")
    print("  ğŸ“Š Histograms: ê° ë³€ìˆ˜ì˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨")
    print("  ğŸ“¦ Box Plots: ë²”ì£¼í˜• vs ì—°ì†í˜• ë³€ìˆ˜ ë°•ìŠ¤í”Œë¡¯")
    print("  ğŸ”¥ Heatmaps: ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ")
    print("  ğŸ“‹ Pair Plots: ì¤‘ìš” ë³€ìˆ˜ë“¤ì˜ ìŒë³„ ê´€ê³„")
    print("  ğŸ¯ Target Analysis: íƒ€ê²Ÿ ë³€ìˆ˜ ê´€ë ¨ íŠ¹í™” ë¶„ì„")
    
except Exception as e:
    print(f"âŒ AutoViz ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    print("ğŸ’¡ í•´ê²° ë°©ë²•: AutoViz ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•˜ê±°ë‚˜ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì„¸ìš”")

# ğŸ§  AutoVizì˜ íŠ¹ë³„í•œ AI ê¸°ëŠ¥ ì„¤ëª…
print("\nğŸ§  AutoVizì˜ AI ê¸°ëŠ¥:")
print("  ğŸ¯ Smart Variable Selection: XGBoostë¡œ ì¤‘ìš” ë³€ìˆ˜ ìë™ ì„ íƒ")
print("  ğŸ“Š Optimal Chart Selection: ë³€ìˆ˜ íƒ€ì…ì— ë”°ë¥¸ ìµœì  ì°¨íŠ¸ ìë™ ì„ íƒ")
print("  ğŸ” Pattern Detection: ìˆ¨ê²¨ì§„ íŒ¨í„´ ìë™ íƒì§€")
print("  âš¡ Automatic Sampling: ëŒ€ìš©ëŸ‰ ë°ì´í„° ìë™ ìƒ˜í”Œë§")
print("  ğŸ·ï¸ Smart Categorization: ë³€ìˆ˜ íƒ€ì… ìë™ ë¶„ë¥˜")

# ğŸ§¹ ì •ë¦¬: ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì„ íƒì‚¬í•­)
import os
try:
    os.remove(temp_csv_path)
    print(f"ğŸ§¹ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {temp_csv_path}")
except:
    print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {temp_csv_path}")
```

#### ğŸŒ 4) D-Tale: ì¸í„°ë™í‹°ë¸Œ ì›¹ GUIì˜ í˜ì‹ 
dtaleì€ Pandas DataFrameì„ ì›¹ ê¸°ë°˜ì˜ ëŒ€í™”í˜• UI í™˜ê²½ì—ì„œ ì¡°ì‘í•˜ê³  íƒìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ì—‘ì…€ì²˜ëŸ¼ GUIì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ í•„í„°ë§, ì •ë ¬, ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ğŸ¯ íŠ¹ì§•**:
>Pandasë¥¼ ë¸Œë¼ìš°ì € ìƒì—ì„œ ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ íƒìƒ‰
ì •ì  ë¦¬í¬íŠ¸ê°€ ì•„ë‹Œ ì›¹ ëŒ€ì‹œë³´ë“œ í˜•íƒœì˜ ë°ì´í„° íƒìƒ‰
ì‹¤ì‹œê°„ ë°ì´í„° í™•ì¸, ë¶„ì„ ëŒ€ì‹œë³´ë“œ, ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤ìŠµ êµìœ¡

**âœ… ì£¼ìš” ê¸°ëŠ¥**
>ì›¹ ë¸Œë¼ìš°ì € ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ ë¶„ì„ ë„êµ¬
ì •ë ¬, í•„í„°ë§, ê·¸ë£¹í™”, í”¼ë²—, ì‹œê°í™” ë“±ì„ UIì—ì„œ ì§ì ‘ ì²˜ë¦¬
Pandas ëª…ë ¹ì–´ ì—†ì´ë„ ë‹¤ì°¨ì› ë¶„ì„ ê°€ëŠ¥

**âœ… ì¥ì **
>ì¸í„°ë™í‹°ë¸Œ GUI ê¸°ë°˜ ë¶„ì„ (ì—‘ì…€ì²˜ëŸ¼ ì¡°ì‘ ê°€ëŠ¥)
í•„í„°/ì •ë ¬/ì¡°ê±´ë¶€ ì„œì‹ ë“± ê°•ë ¥í•œ UI ì§€ì›
íŒ€ì›ê³¼ í•¨ê»˜ ê³µìœ  ê°€ëŠ¥ (ì„œë²„ ì‹¤í–‰ ì‹œ)

**âŒ ë‹¨ì **
>ì‹œê°í™”ë³´ë‹¤ëŠ” ì¸í„°í˜ì´ìŠ¤ì— ì´ˆì  (ìš”ì•½ í†µê³„ëŠ” ì•½í•¨)
ë°ì´í„°í”„ë ˆì„ì´ ì»¤ì§€ë©´ ì†ë„ ì €í•˜
ë¶„ì„ ë¦¬í¬íŠ¸ ì €ì¥ ë¶ˆê°€ (ìŠ¤í¬ë¦°ìƒ· í˜¹ì€ ìˆ˜ë™ ì €ì¥ í•„ìš”)
Python ì§€ì‹ ì—†ì´ ì‚¬ìš©í•˜ê¸´ ì–´ë ¤ì›€ (ì´ˆê¸° ì‹¤í–‰ì€ ì½”ë“œ ê¸°ë°˜)

```python
# D-Tale ì‚¬ìš©ë²• (ì›¹ ê¸°ë°˜ ì¸í„°ë™í‹°ë¸Œ ë¶„ì„)
import dtale  # D-Tale ë¼ì´ë¸ŒëŸ¬ë¦¬ import
import pandas as pd

url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'
df = pd.read_csv(url)

print("ğŸŒ D-Taleë¡œ ì¸í„°ë™í‹°ë¸Œ ì›¹ ë¶„ì„...")

# D-Tale ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì‹¤í–‰
d = dtale.show(
    df,                          # ë¶„ì„í•  DataFrame
    host='localhost',            # ì›¹ì„œë²„ í˜¸ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: localhost)
    port=None,                   # í¬íŠ¸ ë²ˆí˜¸ (None=ìë™ í• ë‹¹)
    debug=False,                 # ë””ë²„ê·¸ ëª¨ë“œ ì—¬ë¶€
    subprocess=True,             # ë³„ë„ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰ ì—¬ë¶€
    data_loader=None,           # ì‚¬ìš©ì ì •ì˜ ë°ì´í„° ë¡œë”
    name='titanic_analysis'      # ì¸ìŠ¤í„´ìŠ¤ ì´ë¦„ (êµ¬ë¶„ìš©)
)

print("âœ… D-Tale ì›¹ ì„œë²„ ì‹œì‘!")
print(f"ğŸŒ ì›¹ ì£¼ì†Œ: {d._url}")  # ì ‘ì† ê°€ëŠ¥í•œ ì›¹ ì£¼ì†Œ ì¶œë ¥

# ğŸ“± ë‹¤ì–‘í•œ ì‹¤í–‰ ë°©ë²•
print("\nğŸ“± D-Tale ì ‘ê·¼ ë°©ë²•:")

# ë°©ë²• 1: ìë™ìœ¼ë¡œ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
try:
    d.open_browser()  # ê¸°ë³¸ ì›¹ë¸Œë¼ìš°ì €ì—ì„œ ìë™ìœ¼ë¡œ ì—´ê¸°
    print("  âœ… ë°©ë²• 1: ìë™ ë¸Œë¼ìš°ì € ì—´ê¸° ì„±ê³µ")
except:
    print("  âŒ ë°©ë²• 1: ìë™ ë¸Œë¼ìš°ì € ì—´ê¸° ì‹¤íŒ¨")

# ë°©ë²• 2: Jupyter Notebookì—ì„œ ì¸ë¼ì¸ í‘œì‹œ
try:
    display(d)  # Jupyter í™˜ê²½ì—ì„œ ì¸ë¼ì¸ìœ¼ë¡œ í‘œì‹œ
    print("  âœ… ë°©ë²• 2: Jupyter ì¸ë¼ì¸ í‘œì‹œ ì„±ê³µ")
except:
    print("  âš ï¸ ë°©ë²• 2: ì¼ë°˜ Python í™˜ê²½ì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŒ")

# ë°©ë²• 3: ìˆ˜ë™ìœ¼ë¡œ URL ë³µì‚¬í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°
print(f"  ğŸ”— ë°©ë²• 3: ìˆ˜ë™ ì ‘ì† - {d._url} ì„ ë¸Œë¼ìš°ì €ì— ì…ë ¥")

# ğŸ® D-Tale ì£¼ìš” ê¸°ëŠ¥ ê°€ì´ë“œ
print("\nğŸ® D-Tale ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©ë²•:")
print("  ğŸ“Š Data Tab: ë°ì´í„° í…Œì´ë¸” ë³´ê¸°, ì •ë ¬, í•„í„°ë§")
print("  ğŸ“ˆ Describe Tab: ê¸°ìˆ í†µê³„ëŸ‰ í™•ì¸")
print("  ğŸ“‹ Column Analysis: ê°œë³„ ì»¬ëŸ¼ ìƒì„¸ ë¶„ì„")
print("  ğŸ¨ Charts: ë“œë˜ê·¸ì•¤ë“œë¡­ìœ¼ë¡œ ì°¨íŠ¸ ìƒì„±")
print("  ğŸ”— Correlations: ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤")
print("  ğŸŒ Maps: ì§€ë¦¬ì  ë°ì´í„° ë§¤í•‘")
print("  ğŸ” Data Viewer: ì‹¤ì‹œê°„ ë°ì´í„° íƒìƒ‰")
print("  ğŸ’¾ Export: ë¶„ì„ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
print("  ğŸ Code Export: ë¶„ì„ì„ Python/R ì½”ë“œë¡œ ë³€í™˜")

# âš™ï¸ D-Tale ì„¤ì • ë° ì¢…ë£Œ ë°©ë²•
print("\nâš™ï¸ D-Tale ê´€ë¦¬:")
print("  ğŸ”„ ìƒˆë¡œê³ ì¹¨: ì›¹í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ë°ì´í„° ì—…ë°ì´íŠ¸")
print("  â¹ï¸ ì¢…ë£Œ ë°©ë²•:")
print("    - ë°©ë²• 1: d.kill() ì‹¤í–‰")
print("    - ë°©ë²• 2: dtale.kill() ë¡œ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ ì¢…ë£Œ")
print("    - ë°©ë²• 3: ì›¹ ì¸í„°í˜ì´ìŠ¤ì—ì„œ 'Shutdown' ë²„íŠ¼ í´ë¦­")

# ğŸ¯ ì‹¤ìŠµ ê°€ì´ë“œ
print("\nğŸ¯ D-Tale ì‹¤ìŠµ ê°€ì´ë“œ:")
print("1. ì›¹ ì¸í„°í˜ì´ìŠ¤ì— ì ‘ì†")
print("2. 'Describe' íƒ­ì—ì„œ ê¸°ë³¸ í†µê³„ëŸ‰ í™•ì¸")
print("3. 'Charts' íƒ­ì—ì„œ ìƒì¡´ìœ¨ ê´€ë ¨ ì°¨íŠ¸ ìƒì„±")
print("4. 'Correlations' íƒ­ì—ì„œ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ í™•ì¸")
print("5. 'Code Export'ë¡œ Python ì½”ë“œ ìƒì„± ë° ë³µì‚¬")

# ğŸ’¡ ì£¼ì˜ì‚¬í•­
print("\nğŸ’¡ D-Tale ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:")
print("  âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìœ¼ë¯€ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì£¼ì˜")
print("  ğŸ”’ ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ ì‚¬ìš© (ë³´ì•ˆìƒ ì™¸ë¶€ ì ‘ê·¼ ì°¨ë‹¨)")
print("  ğŸ”„ Jupyter Notebook ì¬ì‹œì‘ ì‹œ D-Taleë„ ì¬ì‹œì‘ í•„ìš”")
print("  ğŸ’¾ ë¶„ì„ ê²°ê³¼ëŠ” ìë™ ì €ì¥ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìˆ˜ë™ ì €ì¥ í•„ìš”")

# ì˜ˆì œ: í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ì°¨íŠ¸ ìƒì„±
print("\nğŸ¤– í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ D-Tale ì°¨íŠ¸ ìƒì„±:")

# íŠ¹ì • ì°¨íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ì˜ˆì œ
chart_config = {
    'chart_type': 'bar',           # ì°¨íŠ¸ ìœ í˜•: bar, line, scatter, pie ë“±
    'x': 'Pclass',                 # Xì¶• ë³€ìˆ˜
    'y': 'Survived',               # Yì¶• ë³€ìˆ˜
    'agg': 'mean',                 # ì§‘ê³„ í•¨ìˆ˜: mean, sum, count ë“±
    'title': 'ê°ì‹¤ ë“±ê¸‰ë³„ ìƒì¡´ìœ¨'     # ì°¨íŠ¸ ì œëª©
}

print(f"  ğŸ“Š ìƒì„±í•  ì°¨íŠ¸: {chart_config['title']}")
print(f"  ğŸ“‹ ì„¤ì •: {chart_config}")
```

### ğŸ“Š ë„êµ¬ë³„ íŠ¹ì„± ë¹„êµ

| í•­ëª©        | `ydata-profiling` | `sweetviz`    | `autoviz`     | `dtale`         |
| --------- | ----------------- | ------------- | ------------- | --------------- |
| ğŸ“Š ë¦¬í¬íŠ¸ í˜•íƒœ | HTML ë¦¬í¬íŠ¸          | HTML ë¦¬í¬íŠ¸      | notebook ì¶œë ¥   | ì›¹ ëŒ€ì‹œë³´ë“œ          |
| ğŸ§  ë¶„ì„ ìˆ˜ì¤€  | ê³ ê¸‰ (ìƒí˜¸ì‘ìš©, ì¤‘ë³µ ë“±)   | ì¤‘ê¸‰ (íƒ€ê²Ÿ ë¶„ì„ ê°•ì¡°) | ê¸°ì´ˆ ìš”ì•½, ë¹ ë¥¸ ì‹œê°í™” | ì‹¤ì‹œê°„ ë°ì´í„° ì¡°ì‘      |
| â± ì†ë„      | ëŠë¦¼                | ë¹ ë¦„            | ë§¤ìš° ë¹ ë¦„         | ì¤‘ê°„              |
| ğŸ’¾ ëŒ€ìš©ëŸ‰ ëŒ€ì‘ | ì œí•œì                | ë‹¤ì†Œ ê°€ëŠ¥         | ê°€ëŠ¥            | ì œí•œì              |
| ğŸ“ˆ ì‹œê°í™” í’ˆì§ˆ | ì •ì , ì •ë°€            | ì§ê´€ì , ë³´ê¸° ì¢‹ìŒ    | ê¹”ë”í•¨           | ì¸í„°ë™í‹°ë¸Œ           |
| ğŸ’¬ ì£¼ìš” íŠ¹ì§•  | ì¢…í•©ì , ê²½ê³  ì‹œìŠ¤í…œ       | íƒ€ê²Ÿ ë¹„êµ ë¶„ì„ ê°•ì    | ë¨¸ì‹ ëŸ¬ë‹ EDA ê¸°ë°˜   | Excel ëŠë‚Œì˜ ì¡°ì‘ UI |
| ğŸ§© ì¶”ì²œ ìš©ë„  | ë³´ê³ ì„œ ì‘ì„±, êµìœ¡        | ì´ˆë³´ì ëŒ€ìƒ ì‹œê°í™”    | ë¹ ë¥¸ íƒìƒ‰, ML ì „ì²˜ë¦¬ | ì‹¤ì‹œê°„ ë¶„ì„, ëŒ€ì‹œë³´ë“œ ëŒ€ì²´ |

---

## 4.2 ì „í†µì  EDA vs AI ë°©ì‹: ì‹¬ì¸µ ë¹„êµ ë¶„ì„

### âš¡ ì†ë„ì™€ íš¨ìœ¨ì„± ë°°í‹€

**ğŸ¥Š Round 1: ì†ë„ ë¹„êµ ì‹¤í—˜**

ì‹¤ì œë¡œ ë™ì¼í•œ ë¶„ì„ì„ ì „í†µì  ë°©ì‹ê³¼ AI ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ì‹œê°„ì„ ì¸¡ì •í•´ë³´ê² ìŠµë‹ˆë‹¤.

#### ğŸŒ ì „í†µì  ë°©ì‹: ìˆ˜ì‘ì—…ì˜ ì •êµí•¨

```python
import time  # ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# â±ï¸ ì „í†µì  EDA ì‹œê°„ ì¸¡ì • ì‹œì‘
start_time = time.time()

print("ğŸŒ ì „í†µì  ë°©ì‹ìœ¼ë¡œ ì¢…í•© EDA ìˆ˜í–‰ ì¤‘...")
print("="*60)

# 1ï¸âƒ£ ë°ì´í„° ê¸°ë³¸ ì •ë³´ ë¶„ì„ (ìˆ˜ë™ìœ¼ë¡œ ê°ê° í™•ì¸)
print("1ï¸âƒ£ ë°ì´í„° ê¸°ë³¸ ì •ë³´ ë¶„ì„")
print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´")

# ê° ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì… í™•ì¸ (í•˜ë‚˜ì”© ì„¤ëª…)
print(f"\nğŸ“‹ ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì…:")
for col in df.columns:
    dtype = df[col].dtype
    unique_count = df[col].nunique()
    null_count = df[col].isnull().sum()
    print(f"  {col}: {dtype} (ê³ ìœ ê°’: {unique_count}, ê²°ì¸¡ê°’: {null_count})")

# 2ï¸âƒ£ ê²°ì¸¡ê°’ ìƒì„¸ ë¶„ì„ (ì—¬ëŸ¬ ê´€ì ì—ì„œ)
print(f"\n2ï¸âƒ£ ê²°ì¸¡ê°’ ìƒì„¸ ë¶„ì„")
missing_summary = df.isnull().sum()
missing_percent = (missing_summary / len(df)) * 100

missing_info = pd.DataFrame({
    'ê²°ì¸¡ê°’_ê°œìˆ˜': missing_summary,
    'ê²°ì¸¡ê°’_ë¹„ìœ¨': missing_percent
})

# ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ë§Œ í‘œì‹œ
missing_info = missing_info[missing_info['ê²°ì¸¡ê°’_ê°œìˆ˜'] > 0]
missing_info = missing_info.sort_values('ê²°ì¸¡ê°’_ë¹„ìœ¨', ascending=False)

print("ğŸ“Š ê²°ì¸¡ê°’ í˜„í™©:")
for col, row in missing_info.iterrows():
    print(f"  {col}: {row['ê²°ì¸¡ê°’_ê°œìˆ˜']}ê°œ ({row['ê²°ì¸¡ê°’_ë¹„ìœ¨']:.1f}%)")

# 3ï¸âƒ£ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê¸°ìˆ í†µê³„ (ìƒì„¸ ë¶„ì„)
print(f"\n3ï¸âƒ£ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê¸°ìˆ í†µê³„")
numeric_cols = df.select_dtypes(include=['number']).columns

for col in numeric_cols:
    print(f"\nğŸ“ˆ {col} ë¶„ì„:")
    data = df[col].dropna()  # ê²°ì¸¡ê°’ ì œê±°
    
    # ê¸°ë³¸ í†µê³„ëŸ‰ ê³„ì‚°
    mean_val = data.mean()
    median_val = data.median()
    std_val = data.std()
    min_val = data.min()
    max_val = data.max()
    
    # ì‚¬ë¶„ìœ„ìˆ˜ ê³„ì‚°
    q1 = data.quantile(0.25)
    q3 = data.quantile(0.75)
    iqr = q3 - q1
    
    # ì´ìƒì¹˜ ê³„ì‚°
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    
    print(f"  í‰ê· : {mean_val:.2f}, ì¤‘ì•™ê°’: {median_val:.2f}, í‘œì¤€í¸ì°¨: {std_val:.2f}")
    print(f"  ë²”ìœ„: {min_val:.2f} ~ {max_val:.2f}")
    print(f"  ì‚¬ë¶„ìœ„ìˆ˜: Q1={q1:.2f}, Q3={q3:.2f}, IQR={iqr:.2f}")
    print(f"  ì´ìƒì¹˜: {len(outliers)}ê°œ ({len(outliers)/len(data)*100:.1f}%)")

# 4ï¸âƒ£ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„ (ê°ê° ìƒì„¸íˆ)
print(f"\n4ï¸âƒ£ ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ ë¶„ì„")
categorical_cols = df.select_dtypes(include=['object']).columns

for col in categorical_cols:
    print(f"\nğŸ“Š {col} ë¶„í¬:")
    value_counts = df[col].value_counts()
    
    for value, count in value_counts.head().items():  # ìƒìœ„ 5ê°œë§Œ
        percentage = (count / len(df)) * 100
        print(f"  {value}: {count}ê°œ ({percentage:.1f}%)")

# 5ï¸âƒ£ ì‹œê°í™” ìƒì„± (ì—¬ëŸ¬ ì°¨íŠ¸ë¥¼ ì¼ì¼ì´ ì‘ì„±)
print(f"\n5ï¸âƒ£ ì¢…í•© ì‹œê°í™” ìƒì„±")

# í° figure ìƒì„±
fig, axes = plt.subplots(3, 3, figsize=(18, 15))
fig.suptitle('ğŸš¢ íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì¢…í•© ë¶„ì„ (ì „í†µì  ë°©ì‹)', fontsize=16, fontweight='bold')

# 5-1. ë‚˜ì´ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
age_data = df['Age'].dropna()
axes[0,0].hist(age_data, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
axes[0,0].axvline(age_data.mean(), color='red', linestyle='--', label=f'í‰ê· : {age_data.mean():.1f}')
axes[0,0].axvline(age_data.median(), color='green', linestyle='--', label=f'ì¤‘ì•™ê°’: {age_data.median():.1f}')
axes[0,0].set_title('ë‚˜ì´ ë¶„í¬')
axes[0,0].set_xlabel('ë‚˜ì´')
axes[0,0].set_ylabel('ë¹ˆë„')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# 5-2. ê°ì‹¤ ë“±ê¸‰ë³„ ìƒì¡´ìœ¨
survival_by_class = df.groupby('Pclass')['Survived'].mean()
bars = axes[0,1].bar(['1ë“±ì„', '2ë“±ì„', '3ë“±ì„'], survival_by_class.values, 
                     color=['gold', 'silver', 'brown'], alpha=0.8)
axes[0,1].set_title('ê°ì‹¤ ë“±ê¸‰ë³„ ìƒì¡´ìœ¨')
axes[0,1].set_ylabel('ìƒì¡´ìœ¨')
for i, bar in enumerate(bars):
    height = bar.get_height()
    axes[0,1].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.1%}', ha='center', va='bottom')
axes[0,1].grid(True, alpha=0.3)

# 5-3. ì„±ë³„ ìƒì¡´ìœ¨
survival_by_gender = df.groupby('Sex')['Survived'].mean()
bars = axes[0,2].bar(['ì—¬ì„±', 'ë‚¨ì„±'], survival_by_gender.values,
                     color=['pink', 'lightblue'], alpha=0.8)
axes[0,2].set_title('ì„±ë³„ ìƒì¡´ìœ¨')
axes[0,2].set_ylabel('ìƒì¡´ìœ¨')
for i, bar in enumerate(bars):
    height = bar.get_height()
    axes[0,2].text(bar.get_x() + bar.get_width()/2., height + 0.01,
                   f'{height:.1%}', ha='center', va='bottom')
axes[0,2].grid(True, alpha=0.3)

# 5-4. ìš´ì„ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)
fare_data = df['Fare'].dropna()
axes[1,0].boxplot(fare_data)
axes[1,0].set_title('ìš´ì„ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)')
axes[1,0].set_ylabel('ìš´ì„ (íŒŒìš´ë“œ)')
axes[1,0].grid(True, alpha=0.3)

# 5-5. ë“±ê¸‰ë³„ ìš´ì„ ë¶„í¬
df.boxplot(column='Fare', by='Pclass', ax=axes[1,1])
axes[1,1].set_title('ê°ì‹¤ ë“±ê¸‰ë³„ ìš´ì„ ë¶„í¬')
axes[1,1].set_xlabel('ê°ì‹¤ ë“±ê¸‰')
axes[1,1].set_ylabel('ìš´ì„ (íŒŒìš´ë“œ)')

# 5-6. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
numeric_df = df.select_dtypes(include=['number'])
correlation_matrix = numeric_df.corr()
im = axes[1,2].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)
axes[1,2].set_xticks(range(len(correlation_matrix.columns)))
axes[1,2].set_yticks(range(len(correlation_matrix.columns)))
axes[1,2].set_xticklabels(correlation_matrix.columns, rotation=45)
axes[1,2].set_yticklabels(correlation_matrix.columns)
axes[1,2].set_title('ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤')

# ìƒê´€ê³„ìˆ˜ ê°’ í‘œì‹œ
for i in range(len(correlation_matrix.columns)):
    for j in range(len(correlation_matrix.columns)):
        text = axes[1,2].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',
                             ha="center", va="center", color="black" if abs(correlation_matrix.iloc[i, j]) < 0.5 else "white")

# 5-7. ì—°ë ¹ëŒ€ë³„ ìƒì¡´ì ìˆ˜
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 18, 35, 50, 100], 
                       labels=['ì–´ë¦°ì´', 'ì²­ë…„', 'ì¤‘ë…„', 'ì¥ë…„'])
age_group_survival = df.groupby(['AgeGroup', 'Survived']).size().unstack(fill_value=0)
age_group_survival.plot(kind='bar', stacked=True, ax=axes[2,0], color=['red', 'green'])
axes[2,0].set_title('ì—°ë ¹ëŒ€ë³„ ìƒì¡´ì ìˆ˜')
axes[2,0].set_xlabel('ì—°ë ¹ëŒ€')
axes[2,0].set_ylabel('ì¸ì› ìˆ˜')
axes[2,0].legend(['ì‚¬ë§', 'ìƒì¡´'])
axes[2,0].tick_params(axis='x', rotation=0)

# 5-8. ìŠ¹ì„  í•­êµ¬ë³„ ë¶„í¬
embark_counts = df['Embarked'].value_counts()
axes[2,1].pie(embark_counts.values, labels=['Southampton', 'Cherbourg', 'Queenstown'], 
             autopct='%1.1f%%', startangle=90)
axes[2,1].set_title('ìŠ¹ì„  í•­êµ¬ë³„ ë¶„í¬')

# 5-9. ê°€ì¡± í¬ê¸°ë³„ ìƒì¡´ìœ¨
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
family_survival = df.groupby('FamilySize')['Survived'].mean()
axes[2,2].plot(family_survival.index, family_survival.values, 'o-', color='purple', linewidth=2, markersize=8)
axes[2,2].set_title('ê°€ì¡± í¬ê¸°ë³„ ìƒì¡´ìœ¨')
axes[2,2].set_xlabel('ê°€ì¡± í¬ê¸°')
axes[2,2].set_ylabel('ìƒì¡´ìœ¨')
axes[2,2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# 6ï¸âƒ£ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ë„ì¶œ (ìˆ˜ë™ìœ¼ë¡œ ë¶„ì„)
print(f"\n6ï¸âƒ£ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ë„ì¶œ")
print("ğŸ” ë°œê²¬ëœ ì£¼ìš” íŒ¨í„´:")

# ì„±ë³„ ìƒì¡´ìœ¨ ë¶„ì„
female_survival = df[df['Sex'] == 'female']['Survived'].mean()
male_survival = df[df['Sex'] == 'male']['Survived'].mean()
print(f"  â€¢ ì—¬ì„± ìƒì¡´ìœ¨({female_survival:.1%}) > ë‚¨ì„± ìƒì¡´ìœ¨({male_survival:.1%}) â†’ 'ì—¬ì„± ìš°ì„ ' ì •ì±… í™•ì¸")

# ë“±ê¸‰ë³„ ìƒì¡´ìœ¨ ë¶„ì„
class1_survival = df[df['Pclass'] == 1]['Survived'].mean()
class3_survival = df[df['Pclass'] == 3]['Survived'].mean()
print(f"  â€¢ 1ë“±ì„ ìƒì¡´ìœ¨({class1_survival:.1%}) > 3ë“±ì„ ìƒì¡´ìœ¨({class3_survival:.1%}) â†’ ì‚¬íšŒì  ê³„ì¸µì˜ ì˜í–¥")

# ì—°ë ¹ ë¶„ì„
child_survival = df[df['Age'] < 16]['Survived'].mean()
adult_survival = df[df['Age'] >= 16]['Survived'].mean()
print(f"  â€¢ ì–´ë¦°ì´ ìƒì¡´ìœ¨({child_survival:.1%}) vs ì„±ì¸ ìƒì¡´ìœ¨({adult_survival:.1%}) â†’ 'ì–´ë¦°ì´ ìš°ì„ ' ì •ì±… íš¨ê³¼")

# â±ï¸ ì „í†µì  ë°©ì‹ ì†Œìš” ì‹œê°„ ì¸¡ì •
traditional_time = time.time() - start_time
print(f"\nâ±ï¸ ì „í†µì  ë°©ì‹ ì´ ì†Œìš” ì‹œê°„: {traditional_time:.2f}ì´ˆ")
print(f"ğŸ“ ì‘ì„±ëœ ì½”ë“œ ë¼ì¸ ìˆ˜: ì•½ 150-200ì¤„")
```

#### ğŸš€ AI ë°©ì‹: ìë™í™”ì˜ íš¨ìœ¨ì„±

```python
# â±ï¸ AI ë°©ì‹ ì‹œê°„ ì¸¡ì • ì‹œì‘
start_time = time.time()

print("\nğŸš€ AI ë°©ì‹ìœ¼ë¡œ ë™ì¼í•œ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
print("="*60)

# ğŸ¤– ydata-profilingìœ¼ë¡œ ì™„ì „ ìë™ ë¶„ì„
from ydata_profiling import ProfileReport

print("ğŸ¤– AI ìë™ ë¶„ì„ ì‹œì‘...")

# í•œ ë²ˆì˜ í•¨ìˆ˜ í˜¸ì¶œë¡œ ëª¨ë“  ë¶„ì„ ì™„ë£Œ
ai_profile = ProfileReport(
    df,                                    # ë¶„ì„í•  ë°ì´í„°
    title="ğŸš€ AI ìë™ ë¶„ì„ ê²°ê³¼",            # ë³´ê³ ì„œ ì œëª©
    explorative=True,                      # íƒìƒ‰ì  ë¶„ì„ ëª¨ë“œ
    correlations={                         # ìƒê´€ê´€ê³„ ë¶„ì„ ì„¤ì •
        'pearson': {'threshold': 0.1},     # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
        'spearman': {'threshold': 0.1},    # ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜
        'kendall': {'threshold': 0.1}      # ì¼„ë‹¬ íƒ€ìš°
    },
    missing_diagrams={                     # ê²°ì¸¡ê°’ ì‹œê°í™”
        'matrix': True,                    # ê²°ì¸¡ê°’ ë§¤íŠ¸ë¦­ìŠ¤
        'bar': True,                       # ê²°ì¸¡ê°’ ë§‰ëŒ€ê·¸ë˜í”„
        'heatmap': True                    # ê²°ì¸¡ê°’ íˆíŠ¸ë§µ
    }
)

# HTML ë³´ê³ ì„œ ìƒì„±
ai_profile.to_file("ai_analysis_result.html")

print("âœ… AI ë¶„ì„ ì™„ë£Œ!")

# â±ï¸ AI ë°©ì‹ ì†Œìš” ì‹œê°„ ì¸¡ì •
ai_time = time.time() - start_time

print(f"â±ï¸ AI ë°©ì‹ ì´ ì†Œìš” ì‹œê°„: {ai_time:.2f}ì´ˆ")
print(f"ğŸ“ ì‘ì„±ëœ ì½”ë“œ ë¼ì¸ ìˆ˜: ì•½ 15ì¤„")

# ğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼
print("\n" + "="*60)
print("ğŸ“Š ì „í†µì  ë°©ì‹ vs AI ë°©ì‹ ì„±ëŠ¥ ë¹„êµ")
print("="*60)

speed_improvement = traditional_time / ai_time
code_reduction = (200 - 15) / 200 * 100

print(f"âš¡ ì†ë„ ê°œì„ : {speed_improvement:.1f}ë°° ë¹ ë¦„")
print(f"ğŸ“ ì½”ë“œ ê°ì†Œ: {code_reduction:.1f}% ê°ì†Œ")
print(f"ğŸ• ì‹œê°„ ì ˆì•½: {traditional_time - ai_time:.1f}ì´ˆ ì ˆì•½")

# ğŸ¯ ë¶„ì„ ì™„ì„±ë„ ë¹„êµ
print(f"\nğŸ¯ ë¶„ì„ ì™„ì„±ë„ ë¹„êµ:")
print(f"ì „í†µì  ë°©ì‹:")
print(f"  âœ… ê¸°ë³¸ í†µê³„ëŸ‰, ì‹œê°í™”, ìƒê´€ê´€ê³„")
print(f"  âŒ ê³ ê¸‰ í†µê³„ ê²€ì •, ìë™ ê²½ê³ , íŒ¨í„´ ë°œê²¬")

print(f"\nAI ë°©ì‹:")
print(f"  âœ… ê¸°ë³¸ í†µê³„ëŸ‰, ì‹œê°í™”, ìƒê´€ê´€ê³„")
print(f"  âœ… ê³ ê¸‰ í†µê³„ ê²€ì •, ìë™ ê²½ê³ , íŒ¨í„´ ë°œê²¬")
print(f"  âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬, ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„")
print(f"  âœ… ë³€ìˆ˜ë³„ ìƒì„¸ ë¶„ì„, ì´ìƒì¹˜ íƒì§€")
```

### ğŸ¯ ì •í™•ì„±ê³¼ ê¹Šì´: ì§ˆì  ë¹„êµ

#### ğŸ§  ì „í†µì  ë°©ì‹ì˜ ê°•ì : ë„ë©”ì¸ ì§€ì‹ì˜ í˜

```python
# ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ì‹¬ì¸µ ë¶„ì„ ì˜ˆì‹œ
print("ğŸ§  ì „í†µì  ë°©ì‹: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ë¶„ì„")
print("="*50)

# 1ï¸âƒ£ ì—­ì‚¬ì  ë§¥ë½ì„ ê³ ë ¤í•œ ë¶„ì„
print("1ï¸âƒ£ ì—­ì‚¬ì  ë§¥ë½ ë¶„ì„ (1912ë…„ íƒ€ì´íƒ€ë‹‰ ì‚¬ê±´)")

# íƒ€ì´íƒ€ë‹‰í˜¸ì˜ ì—­ì‚¬ì  ë°°ê²½ ë°˜ì˜
print("ğŸ“š ë„ë©”ì¸ ì§€ì‹:")
print("  â€¢ 1912ë…„ ì˜êµ­ì˜ ê³„ê¸‰ ì‚¬íšŒ êµ¬ì¡°")
print("  â€¢ 'ì—¬ì„±ê³¼ ì–´ë¦°ì´ ìš°ì„ (Women and children first)' í•´ì–‘ ê´€ìŠµ")
print("  â€¢ êµ¬ëª…ë³´íŠ¸ ë¶€ì¡± ë¬¸ì œ (ì •ì› 2,224ëª… ì¤‘ êµ¬ëª…ë³´íŠ¸ ì •ì› 1,178ëª…)")
print("  â€¢ ì‚¬íšŒê²½ì œì  ì§€ìœ„ì— ë”°ë¥¸ ì„ ì‹¤ ìœ„ì¹˜ ì°¨ì´")

# 2ï¸âƒ£ ê°€ì„¤ ê¸°ë°˜ ë¶„ì„ (ì „í†µì  ë°©ì‹ì˜ í•µì‹¬)
print(f"\n2ï¸âƒ£ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê°€ì„¤ ì„¤ì • ë° ê²€ì¦")

# ê°€ì„¤ 1: "ì—¬ì„±ê³¼ ì–´ë¦°ì´ ìš°ì„ " ì •ì±… ê²€ì¦
print(f"\nğŸ”¬ ê°€ì„¤ 1: 'ì—¬ì„±ê³¼ ì–´ë¦°ì´ ìš°ì„ ' ì •ì±…ì´ ì‹¤ì œë¡œ ì ìš©ë˜ì—ˆì„ ê²ƒì´ë‹¤")

# ì—°ë ¹ ê·¸ë£¹ ì •ì˜ (1912ë…„ ê¸°ì¤€ìœ¼ë¡œ)
def categorize_age_historical(age):
    """1912ë…„ ì‹œëŒ€ì  ê¸°ì¤€ì„ ë°˜ì˜í•œ ì—°ë ¹ ë¶„ë¥˜"""
    if pd.isna(age):
        return 'Unknown'
    elif age < 16:
        return 'Child'        # 16ì„¸ ë¯¸ë§Œ: ì–´ë¦°ì´ (ë‹¹ì‹œ ê¸°ì¤€)
    elif age < 60:
        return 'Adult'        # 16-59ì„¸: ì„±ì¸
    else:
        return 'Elderly'      # 60ì„¸ ì´ìƒ: ë…¸ì¸ (ë‹¹ì‹œ í‰ê·  ìˆ˜ëª… ê³ ë ¤)

df['AgeGroup_Historical'] = df['Age'].apply(categorize_age_historical)

# ì„±ë³„ê³¼ ì—°ë ¹ëŒ€ë³„ ìƒì¡´ìœ¨ êµì°¨ ë¶„ì„
print("ğŸ“Š ì„±ë³„ Ã— ì—°ë ¹ëŒ€ë³„ ìƒì¡´ìœ¨ ë¶„ì„:")
historical_analysis = df.groupby(['Sex', 'AgeGroup_Historical'])['Survived'].agg(['count', 'mean'])

for sex in ['female', 'male']:
    print(f"\n{sex.upper()} ê·¸ë£¹:")
    sex_data = historical_analysis.loc[sex]
    for age_group, row in sex_data.iterrows():
        if row['count'] > 0:  # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
            print(f"  {age_group}: {row['count']}ëª… ì¤‘ {row['mean']:.1%} ìƒì¡´")

# í†µê³„ì  ê²€ì •ìœ¼ë¡œ ê°€ì„¤ ê²€ì¦
from scipy.stats import chi2_contingency

# ì„±ë³„ê³¼ ìƒì¡´ ê°„ì˜ ë…ë¦½ì„± ê²€ì •
gender_survival_table = pd.crosstab(df['Sex'], df['Survived'])
chi2_gender, p_gender, dof_gender, expected_gender = chi2_contingency(gender_survival_table)

print(f"\nğŸ“ˆ í†µê³„ì  ê²€ì • ê²°ê³¼:")
print(f"  ì„±ë³„-ìƒì¡´ ì¹´ì´ì œê³± ê²€ì •: Ï‡Â² = {chi2_gender:.3f}, p-value = {p_gender:.6f}")
if p_gender < 0.05:
    print(f"  âœ… ì„±ë³„ê³¼ ìƒì¡´ ê°„ì— í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê´€ê³„ ì¡´ì¬ (p < 0.05)")
else:
    print(f"  âŒ ì„±ë³„ê³¼ ìƒì¡´ ê°„ì— ìœ ì˜í•œ ê´€ê³„ ì—†ìŒ (p â‰¥ 0.05)")

# ê°€ì„¤ 2: ì‚¬íšŒê²½ì œì  ì§€ìœ„ì˜ ì˜í–¥
print(f"\nğŸ”¬ ê°€ì„¤ 2: ì‚¬íšŒê²½ì œì  ì§€ìœ„ê°€ ë†’ì„ìˆ˜ë¡ ìƒì¡´ìœ¨ì´ ë†’ì„ ê²ƒì´ë‹¤")

# ë“±ê¸‰ë³„ ìƒì„¸ ë¶„ì„ (ë‹¹ì‹œ ì‚¬íšŒ êµ¬ì¡° ë°˜ì˜)
class_analysis = {}
for pclass in [1, 2, 3]:
    class_data = df[df['Pclass'] == pclass]
    
    class_analysis[pclass] = {
        'ìŠ¹ê°ìˆ˜': len(class_data),
        'ìƒì¡´ìœ¨': class_data['Survived'].mean(),
        'í‰ê· _ë‚˜ì´': class_data['Age'].mean(),
        'í‰ê· _ìš´ì„': class_data['Fare'].mean(),
        'ì—¬ì„±_ë¹„ìœ¨': (class_data['Sex'] == 'female').mean(),
        'ê°€ì¡±ë™ë°˜_ë¹„ìœ¨': (class_data['SibSp'] + class_data['Parch'] > 0).mean()
    }

print("ğŸ“Š ê°ì‹¤ ë“±ê¸‰ë³„ ì‚¬íšŒê²½ì œì  íŠ¹ì„±:")
for pclass, stats in class_analysis.items():
    class_name = {1: '1ë“±ì„ (ìƒë¥˜ì¸µ)', 2: '2ë“±ì„ (ì¤‘ì‚°ì¸µ)', 3: '3ë“±ì„ (ì„œë¯¼ì¸µ)'}[pclass]
    print(f"\n{class_name}:")
    print(f"  ìŠ¹ê° ìˆ˜: {stats['ìŠ¹ê°ìˆ˜']}ëª…")
    print(f"  ìƒì¡´ìœ¨: {stats['ìƒì¡´ìœ¨']:.1%}")
    print(f"  í‰ê·  ë‚˜ì´: {stats['í‰ê· _ë‚˜ì´']:.1f}ì„¸")
    print(f"  í‰ê·  ìš´ì„: Â£{stats['í‰ê· _ìš´ì„']:.2f} (í˜„ì¬ ê°€ì¹˜ë¡œ ì•½ ${stats['í‰ê· _ìš´ì„']*100:.0f})")
    print(f"  ì—¬ì„± ë¹„ìœ¨: {stats['ì—¬ì„±_ë¹„ìœ¨']:.1%}")
    print(f"  ê°€ì¡± ë™ë°˜ ë¹„ìœ¨: {stats['ê°€ì¡±ë™ë°˜_ë¹„ìœ¨']:.1%}")

# 3ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ í•´ì„
print(f"\n3ï¸âƒ£ ë¹„ì¦ˆë‹ˆìŠ¤/ì‚¬íšŒì  ë§¥ë½ í•´ì„")

# ìš´ì„ì„ í˜„ì¬ ê°€ì¹˜ë¡œ í™˜ì‚°í•˜ì—¬ í•´ì„
print("ğŸ’° ìš´ì„ì˜ í˜„ì¬ ê°€ì¹˜ í™˜ì‚° (ì¸í”Œë ˆì´ì…˜ ê³ ë ¤):")
inflation_factor = 100  # 1912ë…„ ëŒ€ë¹„ í˜„ì¬ ì¸í”Œë ˆì´ì…˜ (ëŒ€ëµì )

fare_by_class = df.groupby('Pclass')['Fare'].mean()
for pclass, avg_fare in fare_by_class.items():
    current_value = avg_fare * inflation_factor
    print(f"  {pclass}ë“±ì„ í‰ê·  ìš´ì„: Â£{avg_fare:.2f} (í˜„ì¬ ê°€ì¹˜ ì•½ ${current_value:.0f})")

# 4ï¸âƒ£ ì˜ˆì™¸ ì‚¬ë¡€ ë¶„ì„ (AIê°€ ë†“ì¹˜ê¸° ì‰¬ìš´ ë¶€ë¶„)
print(f"\n4ï¸âƒ£ ì˜ˆì™¸ ì‚¬ë¡€ ë° íŠ¹ì´ íŒ¨í„´ ë¶„ì„")

# 1ë“±ì„ì—ì„œ ì‚¬ë§í•œ ì‚¬ëŒë“¤ ë¶„ì„
first_class_died = df[(df['Pclass'] == 1) & (df['Survived'] == 0)]
print(f"ğŸ’¼ 1ë“±ì„ ì‚¬ë§ì ë¶„ì„:")
print(f"  ì‚¬ë§ì ìˆ˜: {len(first_class_died)}ëª…")
print(f"  í‰ê·  ë‚˜ì´: {first_class_died['Age'].mean():.1f}ì„¸")
print(f"  ë‚¨ì„± ë¹„ìœ¨: {(first_class_died['Sex'] == 'male').mean():.1%}")

# 3ë“±ì„ì—ì„œ ìƒì¡´í•œ ì‚¬ëŒë“¤ ë¶„ì„
third_class_survived = df[(df['Pclass'] == 3) & (df['Survived'] == 1)]
print(f"\nğŸ‘¥ 3ë“±ì„ ìƒì¡´ì ë¶„ì„:")
print(f"  ìƒì¡´ì ìˆ˜: {len(third_class_survived)}ëª…")
print(f"  í‰ê·  ë‚˜ì´: {third_class_survived['Age'].mean():.1f}ì„¸")
print(f"  ì—¬ì„± ë¹„ìœ¨: {(third_class_survived['Sex'] == 'female').mean():.1%}")
print(f"  ì–´ë¦°ì´ ë¹„ìœ¨: {(third_class_survived['Age'] < 16).mean():.1%}")

# 5ï¸âƒ£ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
print(f"\n5ï¸âƒ£ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ (í˜„ëŒ€ì  ì ìš©)")
print("ğŸ¯ ì„ ë°• ì•ˆì „ ì •ì±…ì— ëŒ€í•œ ì‹œì‚¬ì :")
print("  â€¢ ë¹„ìƒì‹œ ëŒ€í”¼ ìš°ì„ ìˆœìœ„ ì •ì±…ì˜ ì‹¤ì œ íš¨ê³¼ í™•ì¸")
print("  â€¢ ì‚¬íšŒê²½ì œì  ì°¨ë³„ ì—†ëŠ” êµ¬ì¡° ì‹œì„¤ ì„¤ê³„ í•„ìš”")
print("  â€¢ ì¶©ë¶„í•œ êµ¬ëª… ì¥ë¹„ í™•ë³´ì˜ ì¤‘ìš”ì„±")
print("  â€¢ ë¹„ìƒ ìƒí™© ì‹œ ì§ˆì„œ ìˆëŠ” ëŒ€í”¼ë¥¼ ìœ„í•œ í›ˆë ¨ í•„ìš”")

print("\nğŸ’¼ í˜„ëŒ€ ë¹„ì¦ˆë‹ˆìŠ¤ ì ìš©:")
print("  â€¢ ê³ ê° ì„¸ë¶„í™” ì‹œ ì¸êµ¬í†µê³„í•™ì  íŠ¹ì„± ê³ ë ¤")
print("  â€¢ ìœ„ê¸° ìƒí™© ì‹œ ì·¨ì•½ ê³„ì¸µ ìš°ì„  ë³´í˜¸ ì •ì±…")
print("  â€¢ ê³µì •í•œ ì„œë¹„ìŠ¤ ì œê³µì„ ìœ„í•œ ì‹œìŠ¤í…œ ì„¤ê³„")
```

#### ğŸ¤– AI ë°©ì‹ì˜ ê°•ì : ê°ê´€ì  íŒ¨í„´ ë°œê²¬

```python
# AIê°€ ë°œê²¬í•  ìˆ˜ ìˆëŠ” ìˆ¨ê²¨ì§„ íŒ¨í„´ë“¤
print("\nğŸ¤– AI ë°©ì‹: ê°ê´€ì  íŒ¨í„´ ë°œê²¬")
print("="*50)

# SweetVizë¡œ íƒ€ê²Ÿ ì¤‘ì‹¬ ë¶„ì„
import sweetviz as sv

print("ğŸ” AIê°€ ìë™ìœ¼ë¡œ ë°œê²¬í•˜ëŠ” íŒ¨í„´ë“¤:")

# AI ìë™ íƒ€ê²Ÿ ë¶„ì„
target_analysis = sv.analyze(df, target_feat='Survived')

# í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ AI ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (SweetViz ë‚´ë¶€ ë¡œì§ ëª¨ë°©)
print("\nğŸ¤– AI ìë™ ë°œê²¬ íŒ¨í„´:")

# 1ï¸âƒ£ ìë™ ìƒê´€ê´€ê³„ íƒì§€
print("1ï¸âƒ£ ìë™ ìƒê´€ê´€ê³„ íƒì§€:")
numeric_correlations = df.select_dtypes(include=['number']).corr()['Survived'].abs().sort_values(ascending=False)

print("ğŸ“Š ìƒì¡´ê³¼ ê°€ì¥ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ë“¤:")
for var, corr in numeric_correlations.items():
    if var != 'Survived' and not pd.isna(corr):
        strength = "ê°•í•¨" if corr > 0.5 else "ë³´í†µ" if corr > 0.3 else "ì•½í•¨"
        print(f"  {var}: {corr:.3f} ({strength})")

# 2ï¸âƒ£ ìë™ ì´ìƒì¹˜ íƒì§€
print(f"\n2ï¸âƒ£ ìë™ ì´ìƒì¹˜ íƒì§€:")
for col in ['Age', 'Fare']:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    print(f"  {col}: {len(outliers)}ê°œ ì´ìƒì¹˜ ë°œê²¬ ({len(outliers)/len(df)*100:.1f}%)")
    
    if len(outliers) > 0:
        extreme_outliers = outliers.nlargest(3, col)  # ìƒìœ„ 3ê°œ ê·¹ê°’
        print(f"    ê·¹ê°’ ì˜ˆì‹œ: {extreme_outliers[col].values}")

# 3ï¸âƒ£ ìë™ ë²”ì£¼ë³„ ì„±ëŠ¥ ë¹„êµ
print(f"\n3ï¸âƒ£ ìë™ ë²”ì£¼ë³„ ì„±ëŠ¥ ë¹„êµ:")
categorical_features = ['Sex', 'Pclass', 'Embarked']

for feature in categorical_features:
    if feature in df.columns:
        category_survival = df.groupby(feature)['Survived'].agg(['count', 'mean'])
        print(f"\nğŸ“Š {feature}ë³„ ìƒì¡´ìœ¨:")
        
        # ìµœê³  ì„±ëŠ¥ê³¼ ìµœì € ì„±ëŠ¥ ìë™ ì‹ë³„
        best_category = category_survival['mean'].idxmax()
        worst_category = category_survival['mean'].idxmin()
        
        for category, stats in category_survival.iterrows():
            marker = "ğŸ†" if category == best_category else "ğŸ“‰" if category == worst_category else "ğŸ“Š"
            print(f"  {marker} {category}: {stats['mean']:.1%} ({stats['count']}ëª…)")

# 4ï¸âƒ£ ìë™ êµí˜¸ì‘ìš© íƒì§€ (AIì˜ ê³ ê¸‰ ê¸°ëŠ¥)
print(f"\n4ï¸âƒ£ ìë™ êµí˜¸ì‘ìš© íƒì§€:")

# ì„±ë³„ê³¼ ë“±ê¸‰ì˜ êµí˜¸ì‘ìš©
interaction_analysis = df.groupby(['Sex', 'Pclass'])['Survived'].mean().unstack()
print("ğŸ”— ì„±ë³„ Ã— ë“±ê¸‰ êµí˜¸ì‘ìš©:")
print(interaction_analysis)

# êµí˜¸ì‘ìš© ê°•ë„ ê³„ì‚° (AIê°€ ìë™ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ì‹)
female_diff = interaction_analysis.loc['female'].max() - interaction_analysis.loc['female'].min()
male_diff = interaction_analysis.loc['male'].max() - interaction_analysis.loc['male'].min()

print(f"\nğŸ“Š êµí˜¸ì‘ìš© ê°•ë„:")
print(f"  ì—¬ì„± ê·¸ë£¹ ë‚´ ë“±ê¸‰ ê°„ ì°¨ì´: {female_diff:.1%}")
print(f"  ë‚¨ì„± ê·¸ë£¹ ë‚´ ë“±ê¸‰ ê°„ ì°¨ì´: {male_diff:.1%}")

if abs(female_diff - male_diff) > 0.1:
    print(f"  âš ï¸ êµí˜¸ì‘ìš© ì¡´ì¬: ì„±ë³„ì— ë”°ë¼ ë“±ê¸‰ì˜ ì˜í–¥ì´ ë‹¤ë¦„")
else:
    print(f"  âœ… êµí˜¸ì‘ìš© ë¯¸ë¯¸: ì„±ë³„ê³¼ ë“±ê¸‰ì˜ ë…ë¦½ì  ì˜í–¥")

# 5ï¸âƒ£ AI ìë™ ê²½ê³  ì‹œìŠ¤í…œ
print(f"\n5ï¸âƒ£ AI ìë™ ê²½ê³  ì‹œìŠ¤í…œ:")

warnings = []

# ê²°ì¸¡ê°’ ê²½ê³ 
high_missing_cols = df.isnull().sum()
high_missing_cols = high_missing_cols[high_missing_cols > len(df) * 0.2]  # 20% ì´ìƒ ê²°ì¸¡
if len(high_missing_cols) > 0:
    warnings.append(f"âš ï¸ ë†’ì€ ê²°ì¸¡ê°’: {list(high_missing_cols.index)} ({high_missing_cols.max()/len(df)*100:.1f}%)")

# ë¶ˆê· í˜• ë°ì´í„° ê²½ê³ 
for col in categorical_features:
    if col in df.columns:
        value_counts = df[col].value_counts(normalize=True)
        if value_counts.max() > 0.8:  # 80% ì´ìƒ ì§‘ì¤‘
            warnings.append(f"âš ï¸ ë¶ˆê· í˜• ë°ì´í„°: {col}ì—ì„œ {value_counts.index[0]}ì´ {value_counts.max():.1%} ì°¨ì§€")

# ì´ìƒì¹˜ ê²½ê³ 
for col in numeric_correlations.index:
    if col != 'Survived':
        Q1, Q3 = df[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        outlier_count = len(df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)])
        if outlier_count > len(df) * 0.05:  # 5% ì´ìƒ ì´ìƒì¹˜
            warnings.append(f"âš ï¸ ë§ì€ ì´ìƒì¹˜: {col}ì—ì„œ {outlier_count}ê°œ ({outlier_count/len(df)*100:.1f}%)")

# ê²½ê³  ì¶œë ¥
if warnings:
    print("ğŸš¨ AI ìë™ íƒì§€ ê²½ê³ ì‚¬í•­:")
    for warning in warnings:
        print(f"  {warning}")
else:
    print("âœ… ì£¼ìš” ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ì—†ìŒ")
```

---

## 4.3 AI ê²°ê³¼ ê²€ì¦ì˜ ê³¼í•™: ë¹„íŒì  ì‚¬ê³  ë°©ë²•ë¡ 

### ğŸ” AI ë¶„ì„ ê²°ê³¼ ì²´ê³„ì  ê²€ì¦ë²•

**âš ï¸ ì™œ AI ê²°ê³¼ë¥¼ ê²€ì¦í•´ì•¼ í•˜ë‚˜ìš”?**

AI ë„êµ¬ë„ ì™„ë²½í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë§ˆì¹˜ ë‚´ë¹„ê²Œì´ì…˜ì´ ë•Œë¡œ ì˜ëª»ëœ ê¸¸ì„ ì•ˆë‚´í•˜ë“¯ì´, AIë„ ë‹¤ìŒê³¼ ê°™ì€ ì‹¤ìˆ˜ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- ğŸ¯ **ë§¥ë½ ì˜¤í•´**: ë°ì´í„°ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì„ ì´í•´í•˜ì§€ ëª»í•¨
- ğŸ“Š **í—ˆìœ„ ìƒê´€**: ìš°ì—°í•œ ìƒê´€ê´€ê³„ë¥¼ ì˜ë¯¸ ìˆëŠ” ê²ƒìœ¼ë¡œ í•´ì„
- ğŸ”¢ **ê³„ì‚° ì˜¤ë¥˜**: ì•Œê³ ë¦¬ì¦˜ì˜ ë²„ê·¸ë‚˜ ì„¤ì • ì˜¤ë¥˜
- ğŸ§© **í¸í–¥ ì¦í­**: ë°ì´í„°ì— ë‚´ì¬ëœ í¸í–¥ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜

#### ğŸ›¡ï¸ 5ë‹¨ê³„ AI ê²€ì¦ í”„ë¡œì„¸ìŠ¤

```python
# AI ë¶„ì„ ê²°ê³¼ ê²€ì¦ì„ ìœ„í•œ ì²´ê³„ì  í”„ë¡œì„¸ìŠ¤
class AIAnalysisValidator:
    """AI ë¶„ì„ ê²°ê³¼ ê²€ì¦ í´ë˜ìŠ¤"""
    
    def __init__(self, dataframe, ai_results=None):
        """
        ì´ˆê¸°í™” í•¨ìˆ˜
        dataframe: ì›ë³¸ ë°ì´í„°
        ai_results: AI ë„êµ¬ê°€ ìƒì„±í•œ ê²°ê³¼ (ì„ íƒì‚¬í•­)
        """
        self.df = dataframe
        self.ai_results = ai_results
        self.validation_report = {
            'data_quality': {},      # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            'statistical': {},       # í†µê³„ì  ê²€ì¦
            'logical': {},          # ë…¼ë¦¬ì  ê²€ì¦
            'domain': {},           # ë„ë©”ì¸ ê²€ì¦
            'bias': {}              # í¸í–¥ ê²€ì¦
        }
    
    def step1_data_quality_check(self):
        """1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê¸°ë³¸ ê²€ì¦"""
        print("ğŸ” 1ë‹¨ê³„: ë°ì´í„° í’ˆì§ˆ ê¸°ë³¸ ê²€ì¦")
        print("-" * 40)
        
        # 1-1. ë°ì´í„° ë¬´ê²°ì„± í™•ì¸
        print("1-1. ë°ì´í„° ë¬´ê²°ì„± í™•ì¸:")
        
        # ì¤‘ë³µ í–‰ í™•ì¸
        duplicate_count = self.df.duplicated().sum()
        print(f"  ğŸ“‹ ì¤‘ë³µ í–‰: {duplicate_count}ê°œ ({duplicate_count/len(self.df)*100:.1f}%)")
        
        # ì „ì²´ null í–‰ í™•ì¸ (ëª¨ë“  ì»¬ëŸ¼ì´ nullì¸ í–‰)
        all_null_rows = self.df.isnull().all(axis=1).sum()
        print(f"  ğŸš« ì „ì²´ null í–‰: {all_null_rows}ê°œ")
        
        # ë°ì´í„° íƒ€ì… ì¼ê´€ì„± í™•ì¸
        print(f"  ğŸ“Š ë°ì´í„° íƒ€ì… ë¶„í¬:")
        dtype_counts = self.df.dtypes.value_counts()
        for dtype, count in dtype_counts.items():
            print(f"    {dtype}: {count}ê°œ ì»¬ëŸ¼")
        
        # 1-2. ë²”ìœ„ ê²€ì¦ (ë…¼ë¦¬ì  ë²”ìœ„ ë‚´ ê°’ì¸ì§€)
        print(f"\n1-2. ë…¼ë¦¬ì  ë²”ìœ„ ê²€ì¦:")
        
        range_issues = []
        
        # ë‚˜ì´ ë²”ìœ„ í™•ì¸
        if 'Age' in self.df.columns:
            invalid_ages = self.df[(self.df['Age'] < 0) | (self.df['Age'] > 120)]
            if len(invalid_ages) > 0:
                range_issues.append(f"ë‚˜ì´: {len(invalid_ages)}ê°œ ë¹„ì •ìƒ ê°’ (0ì„¸ ë¯¸ë§Œ ë˜ëŠ” 120ì„¸ ì´ˆê³¼)")
            else:
                print(f"  âœ… ë‚˜ì´: ëª¨ë“  ê°’ì´ ì •ìƒ ë²”ìœ„ (0-120ì„¸)")
        
        # ìš´ì„ ë²”ìœ„ í™•ì¸
        if 'Fare' in self.df.columns:
            negative_fares = self.df[self.df['Fare'] < 0]
            if len(negative_fares) > 0:
                range_issues.append(f"ìš´ì„: {len(negative_fares)}ê°œ ìŒìˆ˜ ê°’")
            else:
                print(f"  âœ… ìš´ì„: ëª¨ë“  ê°’ì´ ì •ìƒ (0 ì´ìƒ)")
        
        # ìƒì¡´ ì—¬ë¶€ í™•ì¸
        if 'Survived' in self.df.columns:
            invalid_survived = self.df[~self.df['Survived'].isin([0, 1])]
            if len(invalid_survived) > 0:
                range_issues.append(f"ìƒì¡´ì—¬ë¶€: {len(invalid_survived)}ê°œ ë¹„ì •ìƒ ê°’ (0, 1ì´ ì•„ë‹Œ ê°’)")
            else:
                print(f"  âœ… ìƒì¡´ì—¬ë¶€: ëª¨ë“  ê°’ì´ ì •ìƒ (0 ë˜ëŠ” 1)")
        
        # ë²”ìœ„ ë¬¸ì œ ìˆë‹¤ë©´ ì¶œë ¥
        if range_issues:
            print(f"  âš ï¸ ë²”ìœ„ ë¬¸ì œ ë°œê²¬:")
            for issue in range_issues:
                print(f"    {issue}")
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        self.validation_report['data_quality'] = {
            'duplicate_count': duplicate_count,
            'all_null_rows': all_null_rows,
            'range_issues': range_issues
        }
        
        return len(range_issues) == 0  # ë¬¸ì œê°€ ì—†ìœ¼ë©´ True ë°˜í™˜
    
    def step2_statistical_verification(self):
        """2ë‹¨ê³„: í†µê³„ì  ê²€ì¦"""
        print(f"\nğŸ§® 2ë‹¨ê³„: í†µê³„ì  ê²€ì¦")
        print("-" * 40)
        
        # 2-1. ê¸°ë³¸ í†µê³„ëŸ‰ ì¬ê³„ì‚° ë° í™•ì¸
        print("2-1. ê¸°ë³¸ í†µê³„ëŸ‰ ì¬ê³„ì‚°:")
        
        for col in self.df.select_dtypes(include=['number']).columns:
            print(f"\nğŸ“Š {col} í†µê³„ëŸ‰:")
            
            # ê²°ì¸¡ê°’ ì œì™¸í•˜ê³  ê³„ì‚°
            data = self.df[col].dropna()
            
            if len(data) > 0:
                # ê¸°ë³¸ í†µê³„ëŸ‰
                mean_val = data.mean()
                median_val = data.median()
                std_val = data.std()
                
                print(f"  í‰ê· : {mean_val:.3f}")
                print(f"  ì¤‘ì•™ê°’: {median_val:.3f}")
                print(f"  í‘œì¤€í¸ì°¨: {std_val:.3f}")
                
                # í‰ê· ê³¼ ì¤‘ì•™ê°’ ì°¨ì´ ë¶„ì„ (ë¶„í¬ì˜ ì¹˜ìš°ì¹¨ íƒì§€)
                mean_median_diff = abs(mean_val - median_val)
                if mean_median_diff > std_val * 0.5:  # í‘œì¤€í¸ì°¨ì˜ 50% ì´ìƒ ì°¨ì´
                    skew_direction = "ìš°ì¸¡" if mean_val > median_val else "ì¢Œì¸¡"
                    print(f"  âš ï¸ {skew_direction} ì¹˜ìš°ì¹¨ ë¶„í¬ ì˜ì‹¬ (í‰ê· -ì¤‘ì•™ê°’ ì°¨ì´: {mean_median_diff:.3f})")
                else:
                    print(f"  âœ… ëŒ€ì¹­ ë¶„í¬ë¡œ ì¶”ì • (í‰ê· -ì¤‘ì•™ê°’ ì°¨ì´: {mean_median_diff:.3f})")
        
        # 2-2. ìƒê´€ê´€ê³„ ì¬ê²€ì¦
        print(f"\n2-2. ìƒê´€ê´€ê³„ ì¬ê²€ì¦:")
        
        numeric_df = self.df.select_dtypes(include=['number'])
        correlation_matrix = numeric_df.corr()
        
        # ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸°
        high_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # |ìƒê´€ê³„ìˆ˜| > 0.7
                    var1 = correlation_matrix.columns[i]
                    var2 = correlation_matrix.columns[j]
                    high_correlations.append((var1, var2, corr_value))
        
        if high_correlations:
            print(f"  ğŸ“Š ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬:")
            for var1, var2, corr in high_correlations:
                direction = "ì–‘ì˜" if corr > 0 else "ìŒì˜"
                print(f"    {var1} â†” {var2}: {direction} ìƒê´€ê´€ê³„ {corr:.3f}")
                
                # ìƒê´€ê´€ê³„ í•´ì„ ì£¼ì˜ì‚¬í•­
                print(f"      ğŸ’¡ ì£¼ì˜: ìƒê´€ê´€ê³„ â‰  ì¸ê³¼ê´€ê³„")
        else:
            print(f"  âœ… ê°•í•œ ìƒê´€ê´€ê³„ ì—†ìŒ (ëª¨ë“  |r| â‰¤ 0.7)")
        
        # 2-3. ì´ìƒì¹˜ ì¬ê²€ì¦
        print(f"\n2-3. ì´ìƒì¹˜ ì¬ê²€ì¦ (IQR ë°©ë²•):")
        
        for col in numeric_df.columns:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            if IQR > 0:  # IQRì´ 0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]
                outlier_percentage = len(outliers) / len(self.df.dropna(subset=[col])) * 100
                
                print(f"  ğŸ“Š {col}: {len(outliers)}ê°œ ì´ìƒì¹˜ ({outlier_percentage:.1f}%)")
                
                if outlier_percentage > 5:  # 5% ì´ˆê³¼ ì‹œ ê²½ê³ 
                    print(f"    âš ï¸ ì´ìƒì¹˜ ë¹„ìœ¨ ë†’ìŒ - ë°ì´í„° ê²€í†  í•„ìš”")
                    
                    # ê·¹ê°’ ëª‡ ê°œ ì˜ˆì‹œ ì¶œë ¥
                    if len(outliers) > 0:
                        extreme_values = outliers[col].nlargest(3) if len(outliers) >= 3 else outliers[col]
                        print(f"    ê·¹ê°’ ì˜ˆì‹œ: {extreme_values.values}")
    
    def step3_logical_consistency_check(self):
        """3ë‹¨ê³„: ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦"""
        print(f"\nğŸ§  3ë‹¨ê³„: ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦")
        print("-" * 40)
        
        # 3-1. ë³€ìˆ˜ ê°„ ë…¼ë¦¬ì  ê´€ê³„ í™•ì¸
        print("3-1. ë³€ìˆ˜ ê°„ ë…¼ë¦¬ì  ê´€ê³„ í™•ì¸:")
        
        logical_issues = []
        
        # ê°€ì¡± ê´€ê³„ ë…¼ë¦¬ í™•ì¸
        if all(col in self.df.columns for col in ['SibSp', 'Parch']):
            # í˜•ì œìë§¤/ë°°ìš°ìì™€ ë¶€ëª¨/ìë…€ ìˆ˜ê°€ ëª¨ë‘ 0ì´ ì•„ë‹Œë° í˜¼ìì¸ ê²½ìš°ëŠ” ë…¼ë¦¬ì  ëª¨ìˆœ
            family_inconsistency = self.df[
                ((self.df['SibSp'] > 0) | (self.df['Parch'] > 0)) & 
                (self.df['SibSp'] + self.df['Parch'] == 0)
            ]
            
            if len(family_inconsistency) > 0:
                logical_issues.append(f"ê°€ì¡± ê´€ê³„ ë¶ˆì¼ì¹˜: {len(family_inconsistency)}ê±´")
        
        # ë“±ê¸‰ê³¼ ìš´ì„ ë…¼ë¦¬ í™•ì¸
        if all(col in self.df.columns for col in ['Pclass', 'Fare']):
            # ê° ë“±ê¸‰ë³„ ìš´ì„ ì¤‘ì•™ê°’ ê³„ì‚°
            fare_by_class = self.df.groupby('Pclass')['Fare'].median()
            
            # 1ë“±ì„ < 2ë“±ì„ < 3ë“±ì„ ìˆœì„œê°€ ë§ëŠ”ì§€ í™•ì¸ (ìš´ì„ì€ ë°˜ëŒ€)
            if len(fare_by_class) >= 3:
                if not (fare_by_class[1] > fare_by_class[2] > fare_by_class[3]):
                    logical_issues.append("ë“±ê¸‰ë³„ ìš´ì„ ìˆœì„œ ë¶ˆì¼ì¹˜ (1ë“±ê¸‰ > 2ë“±ê¸‰ > 3ë“±ê¸‰ ì˜ˆìƒ)")
                    print(f"    ë“±ê¸‰ë³„ ì¤‘ì•™ê°’ ìš´ì„: 1ë“±ê¸‰={fare_by_class[1]:.2f}, 2ë“±ê¸‰={fare_by_class[2]:.2f}, 3ë“±ê¸‰={fare_by_class[3]:.2f}")
                else:
                    print(f"  âœ… ë“±ê¸‰ë³„ ìš´ì„ ìˆœì„œ ì •ìƒ: 1ë“±ê¸‰ > 2ë“±ê¸‰ > 3ë“±ê¸‰")
        
        # ë…¼ë¦¬ì  ë¬¸ì œ ìš”ì•½
        if logical_issues:
            print(f"  âš ï¸ ë…¼ë¦¬ì  ë¬¸ì œ ë°œê²¬:")
            for issue in logical_issues:
                print(f"    {issue}")
        else:
            print(f"  âœ… ë…¼ë¦¬ì  ì¼ê´€ì„± ê²€ì¦ í†µê³¼")
        
        # 3-2. ë„ë©”ì¸ ìƒì‹ ê²€ì¦
        print(f"\n3-2. ë„ë©”ì¸ ìƒì‹ ê²€ì¦:")
        
        # íƒ€ì´íƒ€ë‹‰ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê²€ì¦
        domain_checks = []
        
        # ì—¬ì„±ê³¼ ì–´ë¦°ì´ ìš°ì„  ì •ì±… í™•ì¸
        if all(col in self.df.columns for col in ['Sex', 'Age', 'Survived']):
            female_survival = self.df[self.df['Sex'] == 'female']['Survived'].mean()
            male_survival = self.df[self.df['Sex'] == 'male']['Survived'].mean()
            
            if female_survival > male_survival:
                print(f"  âœ… 'ì—¬ì„± ìš°ì„ ' ì •ì±… í™•ì¸: ì—¬ì„± {female_survival:.1%} vs ë‚¨ì„± {male_survival:.1%}")
            else:
                domain_checks.append("'ì—¬ì„± ìš°ì„ ' ì •ì±…ê³¼ ë¶ˆì¼ì¹˜")
            
            # ì–´ë¦°ì´ ìš°ì„  í™•ì¸
            child_survival = self.df[self.df['Age'] < 16]['Survived'].mean()
            adult_survival = self.df[self.df['Age'] >= 16]['Survived'].mean()
            
            if not pd.isna(child_survival) and not pd.isna(adult_survival):
                if child_survival > adult_survival:
                    print(f"  âœ… 'ì–´ë¦°ì´ ìš°ì„ ' ì •ì±… í™•ì¸: ì–´ë¦°ì´ {child_survival:.1%} vs ì„±ì¸ {adult_survival:.1%}")
                else:
                    domain_checks.append("'ì–´ë¦°ì´ ìš°ì„ ' ì •ì±…ê³¼ ë¶ˆì¼ì¹˜")
        
        # ì‚¬íšŒê³„ì¸µê³¼ ìƒì¡´ìœ¨ ê´€ê³„ í™•ì¸
        if all(col in self.df.columns for col in ['Pclass', 'Survived']):
            class_survival = self.df.groupby('Pclass')['Survived'].mean().sort_index()
            
            # ìƒìœ„ ë“±ê¸‰ì¼ìˆ˜ë¡ ìƒì¡´ìœ¨ì´ ë†’ì•„ì•¼ í•¨
            if len(class_survival) >= 3:
                if class_survival[1] > class_survival[2] > class_survival[3]:
                    print(f"  âœ… ì‚¬íšŒê³„ì¸µ íš¨ê³¼ í™•ì¸: 1ë“±ê¸‰ > 2ë“±ê¸‰ > 3ë“±ê¸‰ ìƒì¡´ìœ¨")
                else:
                    domain_checks.append("ì‚¬íšŒê³„ì¸µê³¼ ìƒì¡´ìœ¨ ê´€ê³„ ë¶ˆì¼ì¹˜")
        
        if domain_checks:
            print(f"  âš ï¸ ë„ë©”ì¸ ìƒì‹ê³¼ ë¶ˆì¼ì¹˜:")
            for check in domain_checks:
                print(f"    {check}")
    
    def step4_bias_detection(self):
        """4ë‹¨ê³„: í¸í–¥ íƒì§€"""
        print(f"\nâš–ï¸ 4ë‹¨ê³„: í¸í–¥ íƒì§€")
        print("-" * 40)
        
        # 4-1. ìƒ˜í”Œë§ í¸í–¥ í™•ì¸
        print("4-1. ìƒ˜í”Œë§ í¸í–¥ í™•ì¸:")
        
        # ì„±ë³„ ë¶„í¬ í™•ì¸
        if 'Sex' in self.df.columns:
            gender_dist = self.df['Sex'].value_counts(normalize=True)
            print(f"  ğŸ“Š ì„±ë³„ ë¶„í¬:")
            for gender, ratio in gender_dist.items():
                print(f"    {gender}: {ratio:.1%}")
            
            # ì„±ë³„ ë¶ˆê· í˜• í™•ì¸ (70-30 ì´ìƒ ì°¨ì´ë©´ í¸í–¥ ì˜ì‹¬)
            if abs(gender_dist.values[0] - gender_dist.values[1]) > 0.4:
                print(f"  âš ï¸ ì„±ë³„ ë¶„í¬ ë¶ˆê· í˜• (ìƒ˜í”Œë§ í¸í–¥ ê°€ëŠ¥ì„±)")
        
        # ë“±ê¸‰ë³„ ë¶„í¬ í™•ì¸
        if 'Pclass' in self.df.columns:
            class_dist = self.df['Pclass'].value_counts(normalize=True).sort_index()
            print(f"\n  ğŸ“Š ë“±ê¸‰ë³„ ë¶„í¬:")
            for pclass, ratio in class_dist.items():
                print(f"    {pclass}ë“±ê¸‰: {ratio:.1%}")
        
        # 4-2. ìƒì¡´ì í¸í–¥ í™•ì¸
        print(f"\n4-2. ìƒì¡´ì í¸í–¥ í™•ì¸:")
        
        if 'Survived' in self.df.columns:
            survival_rate = self.df['Survived'].mean()
            print(f"  ğŸ“Š ì „ì²´ ìƒì¡´ìœ¨: {survival_rate:.1%}")
            
            # ì‹¤ì œ íƒ€ì´íƒ€ë‹‰ ìƒì¡´ìœ¨ê³¼ ë¹„êµ (ì•½ 32%)
            historical_survival_rate = 0.32
            diff = abs(survival_rate - historical_survival_rate)
            
            if diff > 0.05:  # 5% ì´ìƒ ì°¨ì´
                print(f"  âš ï¸ ì—­ì‚¬ì  ìƒì¡´ìœ¨({historical_survival_rate:.1%})ê³¼ ì°¨ì´: {diff:.1%}")
                print(f"    ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œ ìƒì¡´ì í¸í–¥ ê°€ëŠ¥ì„±")
            else:
                print(f"  âœ… ì—­ì‚¬ì  ìƒì¡´ìœ¨ê³¼ ìœ ì‚¬í•¨ ({diff:.1%} ì°¨ì´)")
        
        # 4-3. ê²°ì¸¡ê°’ í¸í–¥ í™•ì¸
        print(f"\n4-3. ê²°ì¸¡ê°’ í¸í–¥ í™•ì¸:")
        
        # ë“±ê¸‰ë³„ ê²°ì¸¡ê°’ íŒ¨í„´ í™•ì¸
        if all(col in self.df.columns for col in ['Age', 'Pclass']):
            missing_by_class = self.df.groupby('Pclass')['Age'].apply(lambda x: x.isnull().sum())
            total_by_class = self.df['Pclass'].value_counts().sort_index()
            
            print(f"  ğŸ“Š ë“±ê¸‰ë³„ ë‚˜ì´ ê²°ì¸¡ê°’:")
            for pclass in missing_by_class.index:
                missing_count = missing_by_class[pclass]
                total_count = total_by_class[pclass]
                missing_rate = missing_count / total_count
                
                print(f"    {pclass}ë“±ê¸‰: {missing_count}/{total_count} ({missing_rate:.1%})")
                
                if missing_rate > 0.3:  # 30% ì´ìƒ ê²°ì¸¡
                    print(f"      âš ï¸ ë†’ì€ ê²°ì¸¡ìœ¨ - í¸í–¥ ê°€ëŠ¥ì„±")
    
    def step5_generate_verification_report(self):
        """5ë‹¨ê³„: ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        print(f"\nğŸ“‹ 5ë‹¨ê³„: ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ")
        print("=" * 50)
        
        # ê° ë‹¨ê³„ë³„ ê²€ì¦ ê²°ê³¼ ìš”ì•½
        print("ğŸ¯ ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
        
        # ë°ì´í„° í’ˆì§ˆ
        data_quality = self.validation_report.get('data_quality', {})
        if data_quality:
            quality_score = 100 - len(data_quality.get('range_issues', [])) * 20
            print(f"  ğŸ“Š ë°ì´í„° í’ˆì§ˆ: {quality_score}ì /100ì ")
        
        # ê¶Œê³ ì‚¬í•­
        print(f"\nğŸ’¡ ê¶Œê³ ì‚¬í•­:")
        recommendations = [
            "AI ë¶„ì„ ê²°ê³¼ë¥¼ ë§¹ì‹ í•˜ì§€ ë§ê³  í•­ìƒ ìˆ˜ë™ ê²€ì¦ ìˆ˜í–‰",
            "ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ê²°ê³¼ì˜ ë…¼ë¦¬ì  íƒ€ë‹¹ì„± í™•ì¸",
            "í¸í–¥ ê°€ëŠ¥ì„±ì„ ì¸ì‹í•˜ê³  ê²°ê³¼ í•´ì„ ì‹œ ì£¼ì˜",
            "ì´ìƒì¹˜ì™€ ê²°ì¸¡ê°’ íŒ¨í„´ì„ ë©´ë°€íˆ ê²€í† ",
            "í†µê³„ì  ê²€ì •ìœ¼ë¡œ AI ë°œê²¬ íŒ¨í„´ì˜ ìœ ì˜ì„± í™•ì¸"
        ]
        
        for i, rec in enumerate(recommendations, 1):
            print(f"    {i}. {rec}")
        
        print(f"\nâœ… AI ë¶„ì„ ê²°ê³¼ ê²€ì¦ ì™„ë£Œ!")
        
        return self.validation_report

# ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì˜ˆì‹œ
print("ğŸ” AI ë¶„ì„ ê²°ê³¼ ì²´ê³„ì  ê²€ì¦ ì‹œì‘")
print("=" * 60)

# ê²€ì¦ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
validator = AIAnalysisValidator(df)

# 5ë‹¨ê³„ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
step1_result = validator.step1_data_quality_check()
validator.step2_statistical_verification()
validator.step3_logical_consistency_check()
validator.step4_bias_detection()
final_report = validator.step5_generate_verification_report()
```

### ğŸ¤ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•: ìµœê³ ì˜ ì‹œë„ˆì§€ ì°½ì¶œ

#### ğŸ’¡ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì˜ í•µì‹¬ ì›ë¦¬

**ğŸ”„ 3ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš°**

```python
# í•˜ì´ë¸Œë¦¬ë“œ EDA ì›Œí¬í”Œë¡œìš° êµ¬í˜„
class HybridEDAWorkflow:
    """AIì™€ ì¸ê°„ ë¶„ì„ê°€ì˜ ìµœì  í˜‘ì—… ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, dataframe):
        self.df = dataframe
        self.ai_insights = {}
        self.human_insights = {}
        self.hybrid_insights = {}
        
    def phase1_ai_rapid_exploration(self):
        """Phase 1: AI ê³ ì† íƒìƒ‰ (ì „ì²´ ê°œìš” íŒŒì•…)"""
        print("ğŸš€ Phase 1: AI ê³ ì† íƒìƒ‰")
        print("-" * 40)
        
        # AI ë„êµ¬ë¡œ ë¹ ë¥¸ ì „ì²´ ê°œìš” ìƒì„±
        from ydata_profiling import ProfileReport
        
        # ë¹ ë¥¸ ë¶„ì„ì„ ìœ„í•œ ì„¤ì • (ìƒ˜í”Œë§ ì‚¬ìš©)
        sample_size = min(1000, len(self.df))  # ìµœëŒ€ 1000í–‰ ìƒ˜í”Œë§
        df_sample = self.df.sample(n=sample_size, random_state=42)
        
        print(f"ğŸ“Š AI ë¶„ì„ (ìƒ˜í”Œ í¬ê¸°: {sample_size}í–‰)")
        
        # ë¹ ë¥¸ í”„ë¡œíŒŒì¼ë§
        quick_profile = ProfileReport(
            df_sample,
            title="AI ê³ ì† íƒìƒ‰ ê²°ê³¼",
            minimal=True,  # ë¹ ë¥¸ ë¶„ì„ì„ ìœ„í•´ minimal ëª¨ë“œ
            explorative=False,
            interactions=None,  # ìƒí˜¸ì‘ìš© ë¶„ì„ ê±´ë„ˆë›°ê¸°
            correlations={
                'pearson': {'threshold': 0.3}  # ìƒê´€ê´€ê³„ ì„ê³„ê°’ ë†’ì„
            }
        )
        
        # AIê°€ ë°œê²¬í•œ ì£¼ìš” íŒ¨í„´ ì¶”ì¶œ
        print("ğŸ¤– AI ë°œê²¬ ì£¼ìš” íŒ¨í„´:")
        
        # 1. ê¸°ë³¸ ë°ì´í„° ê°œìš”
        print(f"  ğŸ“‹ ë°ì´í„° ê°œìš”:")
        print(f"    ì „ì²´ í–‰ ìˆ˜: {len(self.df)}")
        print(f"    ì „ì²´ ì—´ ìˆ˜: {len(self.df.columns)}")
        print(f"    ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(self.df.select_dtypes(include=['number']).columns)}ê°œ")
        print(f"    ë²”ì£¼í˜• ë³€ìˆ˜: {len(self.df.select_dtypes(include=['object']).columns)}ê°œ")
        
        # 2. ê²°ì¸¡ê°’ íŒ¨í„´ (AI ìë™ íƒì§€)
        missing_summary = self.df.isnull().sum()
        high_missing = missing_summary[missing_summary > 0].sort_values(ascending=False)
        
        if len(high_missing) > 0:
            print(f"  â“ ê²°ì¸¡ê°’ íŒ¨í„´:")
            for col, count in high_missing.head(3).items():
                percentage = (count / len(self.df)) * 100
                print(f"    {col}: {count}ê°œ ({percentage:.1f}%)")
        
        # 3. ì´ìƒì¹˜ ìë™ íƒì§€
        print(f"  ğŸ¯ ì´ìƒì¹˜ íƒì§€:")
        for col in self.df.select_dtypes(include=['number']).columns:
            Q1, Q3 = self.df[col].quantile([0.25, 0.75])
            IQR = Q3 - Q1
            if IQR > 0:
                outliers = self.df[(self.df[col] < Q1 - 1.5*IQR) | (self.df[col] > Q3 + 1.5*IQR)]
                if len(outliers) > 0:
                    print(f"    {col}: {len(outliers)}ê°œ ì´ìƒì¹˜ ({len(outliers)/len(self.df)*100:.1f}%)")
        
        # AI ë°œê²¬ì‚¬í•­ ì €ì¥
        self.ai_insights['phase1'] = {
            'data_overview': {
                'rows': len(self.df),
                'columns': len(self.df.columns),
                'numeric_vars': len(self.df.select_dtypes(include=['number']).columns)
            },
            'missing_patterns': high_missing.to_dict(),
            'quick_profile': quick_profile
        }
        
        print(f"âœ… AI ê³ ì† íƒìƒ‰ ì™„ë£Œ (1-2ë¶„ ì†Œìš”)")
        return self.ai_insights['phase1']
    
    def phase2_human_hypothesis_generation(self):
        """Phase 2: ì¸ê°„ ê°€ì„¤ ìƒì„± (ë„ë©”ì¸ ì§€ì‹ ì ìš©)"""
        print(f"\nğŸ§  Phase 2: ì¸ê°„ ê°€ì„¤ ìƒì„±")
        print("-" * 40)
        
        # ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê°€ì„¤ ì„¤ì •
        print("ğŸ¯ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê°€ì„¤ ìˆ˜ë¦½:")
        
        hypotheses = [
            {
                'id': 'H1',
                'hypothesis': 'ì—¬ì„±ì˜ ìƒì¡´ìœ¨ì´ ë‚¨ì„±ë³´ë‹¤ ë†’ì„ ê²ƒì´ë‹¤',
                'rationale': '1912ë…„ "ì—¬ì„±ê³¼ ì–´ë¦°ì´ ìš°ì„ " í•´ì–‘ ê´€ìŠµ',
                'variables': ['Sex', 'Survived'],
                'expected_result': 'female > male in survival rate'
            },
            {
                'id': 'H2', 
                'hypothesis': 'ìƒìœ„ ë“±ê¸‰ ìŠ¹ê°ì˜ ìƒì¡´ìœ¨ì´ ë†’ì„ ê²ƒì´ë‹¤',
                'rationale': 'ì‚¬íšŒê²½ì œì  ì§€ìœ„ì— ë”°ë¥¸ êµ¬ì¡° ì ‘ê·¼ì„± ì°¨ì´',
                'variables': ['Pclass', 'Survived'],
                'expected_result': '1st class > 2nd class > 3rd class'
            },
            {
                'id': 'H3',
                'hypothesis': 'ì–´ë¦°ì´ì˜ ìƒì¡´ìœ¨ì´ ì„±ì¸ë³´ë‹¤ ë†’ì„ ê²ƒì´ë‹¤', 
                'rationale': '"ì–´ë¦°ì´ ìš°ì„ " ì •ì±…ê³¼ ì‹ ì²´ì  íŠ¹ì„±',
                'variables': ['Age', 'Survived'],
                'expected_result': 'children < 16 years > adults'
            },
            {
                'id': 'H4',
                'hypothesis': 'ê°€ì¡±ê³¼ í•¨ê»˜ íƒ‘ìŠ¹í•œ ìŠ¹ê°ì˜ ìƒì¡´ìœ¨ì´ ë‹¤ë¥¼ ê²ƒì´ë‹¤',
                'rationale': 'ê°€ì¡± êµ¬ì„±ì› ê°„ ìƒí˜¸ ë„ì›€ vs í”¼ë‚œ ì‹œ ë¶€ë‹´',
                'variables': ['SibSp', 'Parch', 'Survived'],
                'expected_result': 'optimal family size exists'
            }
        ]
        
        # ê°€ì„¤ë³„ ìƒì„¸ ì„¤ëª…
        for hyp in hypotheses:
            print(f"\n  {hyp['id']}: {hyp['hypothesis']}")
            print(f"    ê·¼ê±°: {hyp['rationale']}")
            print(f"    ê´€ë ¨ ë³€ìˆ˜: {', '.join(hyp['variables'])}")
            print(f"    ì˜ˆìƒ ê²°ê³¼: {hyp['expected_result']}")
        
        # ì¶”ê°€ íƒìƒ‰ ë°©í–¥ ì„¤ì •
        print(f"\nğŸ” ì¶”ê°€ íƒìƒ‰ ë°©í–¥:")
        exploration_directions = [
            "ìš´ì„ê³¼ ë“±ê¸‰ì˜ ê´€ê³„ (ê²½ì œì  ê²©ì°¨)",
            "ìŠ¹ì„  í•­êµ¬ë³„ ìŠ¹ê° íŠ¹ì„± ì°¨ì´",
            "ë‚˜ì´ì™€ ìš´ì„ì˜ ìƒê´€ê´€ê³„ (ì—°ë ¹ëŒ€ë³„ ê²½ì œë ¥)",
            "ê°€ì¡± êµ¬ì„±ê³¼ ìƒì¡´ ì „ëµì˜ ê´€ê³„"
        ]
        
        for i, direction in enumerate(exploration_directions, 1):
            print(f"    {i}. {direction}")
        
        # ì¸ê°„ ì¸ì‚¬ì´íŠ¸ ì €ì¥
        self.human_insights['hypotheses'] = hypotheses
        self.human_insights['exploration_directions'] = exploration_directions
        
        print(f"âœ… ì¸ê°„ ê°€ì„¤ ìƒì„± ì™„ë£Œ")
        return self.human_insights
    
    def phase3_targeted_ai_analysis(self):
        """Phase 3: íƒ€ê²Ÿ AI ë¶„ì„ (ê°€ì„¤ ê²€ì¦ íŠ¹í™”)"""
        print(f"\nğŸ¯ Phase 3: íƒ€ê²Ÿ AI ë¶„ì„")
        print("-" * 40)
        
        # ì¸ê°„ì´ ì„¤ì •í•œ ê°€ì„¤ì„ AIë¡œ ì •ë°€ ê²€ì¦
        print("ğŸ¤– AI ê°€ì„¤ ê²€ì¦ ì‹œìŠ¤í…œ:")
        
        verification_results = {}
        
        # H1: ì„±ë³„ê³¼ ìƒì¡´ìœ¨ ê²€ì¦
        print(f"\nğŸ”¬ H1 ê²€ì¦: ì„±ë³„ê³¼ ìƒì¡´ìœ¨")
        if all(col in self.df.columns for col in ['Sex', 'Survived']):
            gender_survival = self.df.groupby('Sex')['Survived'].agg(['count', 'mean', 'std'])
            print(f"  ğŸ“Š ì„±ë³„ ìƒì¡´ìœ¨:")
            for gender, stats in gender_survival.iterrows():
                print(f"    {gender}: {stats['mean']:.1%} (n={stats['count']}, std={stats['std']:.3f})")
            
            # í†µê³„ì  ê²€ì •
            from scipy.stats import chi2_contingency
            contingency_table = pd.crosstab(self.df['Sex'], self.df['Survived'])
            chi2, p_value, dof, expected = chi2_contingency(contingency_table)
            
            result = "ì§€ì§€" if p_value < 0.05 else "ê¸°ê°"
            verification_results['H1'] = {
                'result': result,
                'statistics': gender_survival.to_dict(),
                'p_value': p_value,
                'effect_size': abs(gender_survival.loc['female', 'mean'] - gender_survival.loc['male', 'mean'])
            }
            
            print(f"    ğŸ“ˆ í†µê³„ ê²€ì •: Ï‡Â² = {chi2:.3f}, p = {p_value:.6f} â†’ ê°€ì„¤ {result}")
        
        # H2: ë“±ê¸‰ê³¼ ìƒì¡´ìœ¨ ê²€ì¦  
        print(f"\nğŸ”¬ H2 ê²€ì¦: ë“±ê¸‰ê³¼ ìƒì¡´ìœ¨")
        if all(col in self.df.columns for col in ['Pclass', 'Survived']):
            class_survival = self.df.groupby('Pclass')['Survived'].agg(['count', 'mean'])
            print(f"  ğŸ“Š ë“±ê¸‰ë³„ ìƒì¡´ìœ¨:")
            for pclass, stats in class_survival.iterrows():
                print(f"    {pclass}ë“±ê¸‰: {stats['mean']:.1%} (n={stats['count']})")
            
            # íŠ¸ë Œë“œ ê²€ì¦ (1ë“±ê¸‰ > 2ë“±ê¸‰ > 3ë“±ê¸‰ì¸ì§€)
            trend_check = (class_survival.loc[1, 'mean'] > class_survival.loc[2, 'mean'] > 
                          class_survival.loc[3, 'mean'])
            
            verification_results['H2'] = {
                'result': "ì§€ì§€" if trend_check else "ë¶€ë¶„ ì§€ì§€",
                'statistics': class_survival.to_dict(),
                'trend_check': trend_check
            }
            
            print(f"    ğŸ“ˆ íŠ¸ë Œë“œ ê²€ì¦: {'1ë“±ê¸‰ > 2ë“±ê¸‰ > 3ë“±ê¸‰' if trend_check else 'ì˜ˆìƒê³¼ ë‹¤ë¥¸ íŒ¨í„´'} â†’ ê°€ì„¤ {'ì§€ì§€' if trend_check else 'ë¶€ë¶„ ì§€ì§€'}")
        
        # H3: ì—°ë ¹ê³¼ ìƒì¡´ìœ¨ ê²€ì¦
        print(f"\nğŸ”¬ H3 ê²€ì¦: ì—°ë ¹ê³¼ ìƒì¡´ìœ¨")
        if all(col in self.df.columns for col in ['Age', 'Survived']):
            # ì–´ë¦°ì´ vs ì„±ì¸ ë¹„êµ
            child_mask = self.df['Age'] < 16
            adult_mask = self.df['Age'] >= 16
            
            child_survival = self.df[child_mask]['Survived'].mean()
            adult_survival = self.df[adult_mask]['Survived'].mean()
            
            child_count = child_mask.sum()
            adult_count = adult_mask.sum()
            
            print(f"  ğŸ“Š ì—°ë ¹ëŒ€ë³„ ìƒì¡´ìœ¨:")
            print(f"    ì–´ë¦°ì´ (<16ì„¸): {child_survival:.1%} (n={child_count})")
            print(f"    ì„±ì¸ (â‰¥16ì„¸): {adult_survival:.1%} (n={adult_count})")
            
            age_effect = child_survival - adult_survival
            result = "ì§€ì§€" if age_effect > 0 else "ê¸°ê°"
            
            verification_results['H3'] = {
                'result': result,
                'child_survival': child_survival,
                'adult_survival': adult_survival,
                'effect_size': age_effect
            }
            
            print(f"    ğŸ“ˆ íš¨ê³¼ í¬ê¸°: {age_effect:+.1%} â†’ ê°€ì„¤ {result}")
        
        # H4: ê°€ì¡± êµ¬ì„±ê³¼ ìƒì¡´ìœ¨ ê²€ì¦
        print(f"\nğŸ”¬ H4 ê²€ì¦: ê°€ì¡± êµ¬ì„±ê³¼ ìƒì¡´ìœ¨")
        if all(col in self.df.columns for col in ['SibSp', 'Parch', 'Survived']):
            # ê°€ì¡± í¬ê¸° ê³„ì‚°
            self.df['FamilySize'] = self.df['SibSp'] + self.df['Parch'] + 1
            
            family_survival = self.df.groupby('FamilySize')['Survived'].agg(['count', 'mean'])
            
            print(f"  ğŸ“Š ê°€ì¡± í¬ê¸°ë³„ ìƒì¡´ìœ¨:")
            for size, stats in family_survival.iterrows():
                if stats['count'] >= 5:  # 5ëª… ì´ìƒ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    print(f"    {size}ëª…: {stats['mean']:.1%} (n={stats['count']})")
            
            # ìµœì  ê°€ì¡± í¬ê¸° ì°¾ê¸°
            optimal_size = family_survival[family_survival['count'] >= 5]['mean'].idxmax()
            optimal_survival = family_survival.loc[optimal_size, 'mean']
            
            verification_results['H4'] = {
                'result': "ì§€ì§€" if len(family_survival) > 1 else "ê²€ì¦ ë¶ˆê°€",
                'optimal_family_size': optimal_size,
                'optimal_survival_rate': optimal_survival,
                'statistics': family_survival.to_dict()
            }
            
            print(f"    ğŸ“ˆ ìµœì  ê°€ì¡± í¬ê¸°: {optimal_size}ëª… ({optimal_survival:.1%} ìƒì¡´ìœ¨)")
        
        # AI ê²€ì¦ ê²°ê³¼ ì €ì¥
        self.ai_insights['verification'] = verification_results
        
        print(f"âœ… íƒ€ê²Ÿ AI ë¶„ì„ ì™„ë£Œ")
        return verification_results
    
    def phase4_hybrid_synthesis(self):
        """Phase 4: í•˜ì´ë¸Œë¦¬ë“œ ì¢…í•© (AI + ì¸ê°„ ì¸ì‚¬ì´íŠ¸ ê²°í•©)"""
        print(f"\nğŸ¤ Phase 4: í•˜ì´ë¸Œë¦¬ë“œ ì¢…í•©")
        print("-" * 40)
        
        print("ğŸ”„ AI ë°œê²¬ + ì¸ê°„ í•´ì„ = í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸")
        
        # ê²€ì¦ëœ ê°€ì„¤ ê¸°ë°˜ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
        hybrid_insights = []
        
        # ê° ê°€ì„¤ë³„ í•˜ì´ë¸Œë¦¬ë“œ í•´ì„
        for hyp_id, result in self.ai_insights['verification'].items():
            if result['result'] in ['ì§€ì§€', 'ë¶€ë¶„ ì§€ì§€']:
                
                if hyp_id == 'H1':  # ì„±ë³„ íš¨ê³¼
                    effect_size = result['effect_size']
                    insight = {
                        'pattern': "ê°•í•œ ì„±ë³„ íš¨ê³¼",
                        'ai_finding': f"ì—¬ì„± ìƒì¡´ìœ¨ì´ ë‚¨ì„±ë³´ë‹¤ {effect_size:.1%} ë†’ìŒ",
                        'human_interpretation': "1912ë…„ 'ì—¬ì„± ìš°ì„ ' í•´ì–‘ ê´€ìŠµì´ ì‹¤ì œë¡œ ì ìš©ë¨",
                        'business_implication': "ì¬ë‚œ ëŒ€ì‘ ì‹œ ì¸êµ¬ì§‘ë‹¨ë³„ ì°¨ë“± ë³´í˜¸ ì •ì±…ì˜ ì¤‘ìš”ì„±",
                        'actionable_insight': "í˜„ëŒ€ ì•ˆì „ ì •ì±…ì—ì„œë„ ì·¨ì•½ ê³„ì¸µ ìš°ì„  ë³´í˜¸ ì›ì¹™ ì ìš© í•„ìš”"
                    }
                    hybrid_insights.append(insight)
                
                elif hyp_id == 'H2':  # ê³„ì¸µ íš¨ê³¼
                    insight = {
                        'pattern': "ì‚¬íšŒê²½ì œì  ê³„ì¸µ íš¨ê³¼",
                        'ai_finding': "ìƒìœ„ ë“±ê¸‰ì¼ìˆ˜ë¡ ìƒì¡´ìœ¨ ë†’ìŒ (1ë“±ê¸‰ > 2ë“±ê¸‰ > 3ë“±ê¸‰)",
                        'human_interpretation': "ë‹¹ì‹œ ê³„ê¸‰ì‚¬íšŒ êµ¬ì¡°ì™€ ì„ ì‹¤ ìœ„ì¹˜ì˜ ì˜í–¥",
                        'business_implication': "ìœ„ê¸° ìƒí™©ì—ì„œ ìì› ì ‘ê·¼ì„±ì˜ ë¶ˆí‰ë“± ë¬¸ì œ",
                        'actionable_insight': "ì¬ë‚œ ëŒ€ë¹„ ì‹œì„¤ì˜ í‰ë“±í•œ ì ‘ê·¼ì„± ë³´ì¥ í•„ìš”"
                    }
                    hybrid_insights.append(insight)
                
                elif hyp_id == 'H3' and result['result'] == 'ì§€ì§€':  # ì—°ë ¹ íš¨ê³¼
                    insight = {
                        'pattern': "ì—°ë ¹ë³„ ë³´í˜¸ íš¨ê³¼",
                        'ai_finding': f"ì–´ë¦°ì´ ìƒì¡´ìœ¨ì´ ì„±ì¸ë³´ë‹¤ {result['effect_size']:+.1%} ë†’ìŒ",
                        'human_interpretation': "'ì–´ë¦°ì´ ìš°ì„ ' ì •ì±…ê³¼ ì„±ì¸ë“¤ì˜ ì´íƒ€ì  í–‰ë™",
                        'business_implication': "ìœ„ê¸° ìƒí™©ì—ì„œ ì—°ë ¹ë³„ ì°¨ë“± ëŒ€ì‘ì˜ íš¨ê³¼ì„±",
                        'actionable_insight': "ì—°ë ¹ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ì•ˆì „ í”„ë¡œí† ì½œ ê°œë°œ"
                    }
                    hybrid_insights.append(insight)
                
                elif hyp_id == 'H4':  # ê°€ì¡± íš¨ê³¼
                    optimal_size = result['optimal_family_size']
                    insight = {
                        'pattern': "ìµœì  ê·¸ë£¹ í¬ê¸° íš¨ê³¼",
                        'ai_finding': f"ê°€ì¡± í¬ê¸° {optimal_size}ëª…ì¼ ë•Œ ìƒì¡´ìœ¨ ìµœê³ ",
                        'human_interpretation': "ìƒí˜¸ ë¶€ì¡°ì™€ ì´ë™ì„±ì˜ ê· í˜•ì  ì¡´ì¬",
                        'business_implication': "íŒ€ êµ¬ì„± ì‹œ ìµœì  ê·œëª¨ ê³ ë ¤ì˜ ì¤‘ìš”ì„±",
                        'actionable_insight': "ë¹„ìƒ ëŒ€í”¼ ì‹œ ì ì • ê·¸ë£¹ í¬ê¸° ê°€ì´ë“œë¼ì¸ ìˆ˜ë¦½"
                    }
                    hybrid_insights.append(insight)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ì¶œë ¥
        print(f"\nğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ê²°ê³¼:")
        for i, insight in enumerate(hybrid_insights, 1):
            print(f"\n{i}. {insight['pattern']}")
            print(f"   ğŸ¤– AI ë°œê²¬: {insight['ai_finding']}")
            print(f"   ğŸ§  ì¸ê°„ í•´ì„: {insight['human_interpretation']}")
            print(f"   ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜: {insight['business_implication']}")
            print(f"   ğŸ¯ ì‹¤í–‰ ë°©ì•ˆ: {insight['actionable_insight']}")
        
        # ì¢…í•© ê²°ë¡ 
        print(f"\nğŸ“‹ ì¢…í•© ê²°ë¡ :")
        conclusions = [
            "AIì˜ ê°ê´€ì  íŒ¨í„´ ë°œê²¬ + ì¸ê°„ì˜ ë§¥ë½ì  í•´ì„ = ì™„ì „í•œ ì¸ì‚¬ì´íŠ¸",
            "ë‹¨ìˆœí•œ ìƒê´€ê´€ê³„ë¥¼ ë„˜ì–´ ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµì  ì‹œì‚¬ì  ë„ì¶œ",
            "ì—­ì‚¬ì  ì‚¬ê±´ì„ í†µí•´ í˜„ëŒ€ì  ì•ˆì „/ê²½ì˜ ì›ì¹™ ë„ì¶œ",
            "ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì— íœ´ë¨¼ íŒ©í„° í†µí•©ì˜ ì¤‘ìš”ì„± í™•ì¸"
        ]
        
        for conclusion in conclusions:
            print(f"  âœ… {conclusion}")
        
        # í•˜ì´ë¸Œë¦¬ë“œ ê²°ê³¼ ì €ì¥
        self.hybrid_insights = hybrid_insights
        
        print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ì¢…í•© ì™„ë£Œ")
        return hybrid_insights

# í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë°ëª¨
print("ğŸ¤ í•˜ì´ë¸Œë¦¬ë“œ EDA ì›Œí¬í”Œë¡œìš° ì‹¤í–‰")
print("=" * 60)

# ì›Œí¬í”Œë¡œìš° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
workflow = HybridEDAWorkflow(df)

# 4ë‹¨ê³„ í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
ai_overview = workflow.phase1_ai_rapid_exploration()
human_hypotheses = workflow.phase2_human_hypothesis_generation()
ai_verification = workflow.phase3_targeted_ai_analysis()
hybrid_results = workflow.phase4_hybrid_synthesis()

print(f"\nğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ EDA ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!")
print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: AI ë‹¨ë… ëŒ€ë¹„ 20% ì¶”ê°€, ì „í†µì  ë°©ì‹ ëŒ€ë¹„ 70% ë‹¨ì¶•")
print(f"ğŸ¯ ê²°ê³¼ í’ˆì§ˆ: AI ê°ê´€ì„± + ì¸ê°„ í†µì°°ë ¥ = ìµœê³  ìˆ˜ì¤€ ì¸ì‚¬ì´íŠ¸")
```

---

## ì§ì ‘ í•´ë³´ê¸° / ì—°ìŠµ ë¬¸ì œ

### ì—°ìŠµë¬¸ì œ 1: AI ë„êµ¬ ë¹„êµ ì²´í—˜ (ë‚œì´ë„: â­)

**ëª©í‘œ**: ì„œë¡œ ë‹¤ë¥¸ AI EDA ë„êµ¬ë“¤ì„ ì§ì ‘ ì‚¬ìš©í•´ë³´ê³  ê°ê°ì˜ íŠ¹ì„±ì„ ë¹„êµí•´ë³´ì„¸ìš”.

**ë‹¨ê³„ë³„ ê°€ì´ë“œ**:
1. ydata-profilingê³¼ SweetVizë¡œ ë™ì¼í•œ ë°ì´í„°ë¥¼ ë¶„ì„
2. ìƒì„±ëœ ë³´ê³ ì„œì˜ ì°¨ì´ì ì„ ë¹„êµ
3. ê° ë„êµ¬ì˜ ì¥ë‹¨ì ì„ ì •ë¦¬

```python
# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•´ë³´ì„¸ìš”
# íŒíŠ¸: ë‘ ë„êµ¬ë¥¼ ë™ì‹œì— ì‚¬ìš©í•´ì„œ ê²°ê³¼ë¥¼ ë¹„êµí•´ë³´ì„¸ìš”

import pandas as pd
from ydata_profiling import ProfileReport
import sweetviz as sv
import time

print("ğŸ” AI ë„êµ¬ ë¹„êµ ì‹¤í—˜ ì‹œì‘!")
print("=" * 50)

# ë°ì´í„° ì¤€ë¹„ (íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì‚¬ìš©)
if 'df' not in locals():
    df = pd.read_csv('https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv')

# 1ë‹¨ê³„: ydata-profiling ë¶„ì„
print("1ï¸âƒ£ ydata-profiling ë¶„ì„ ì‹œì‘...")
start_time = time.time()

# ProfileReport ìƒì„± (ê¸°ë³¸ ì„¤ì •)
ydata_profile = ProfileReport(
    df, 
    title="ğŸ“Š ydata-profiling ë¶„ì„ ê²°ê³¼",
    explorative=True
)

# HTML íŒŒì¼ë¡œ ì €ì¥
ydata_profile.to_file("comparison_ydata_result.html")
ydata_time = time.time() - start_time

print(f"âœ… ydata-profiling ì™„ë£Œ (ì†Œìš”ì‹œê°„: {ydata_time:.1f}ì´ˆ)")

# 2ë‹¨ê³„: SweetViz ë¶„ì„
print("\n2ï¸âƒ£ SweetViz ë¶„ì„ ì‹œì‘...")
start_time = time.time()

# SweetViz ê¸°ë³¸ ë¶„ì„
sweetviz_report = sv.analyze(df)
sweetviz_report.show_html("comparison_sweetviz_result.html", open_browser=False)

sweetviz_time = time.time() - start_time
print(f"âœ… SweetViz ì™„ë£Œ (ì†Œìš”ì‹œê°„: {sweetviz_time:.1f}ì´ˆ)")

# 3ë‹¨ê³„: ë¹„êµ ë¶„ì„
print(f"\n3ï¸âƒ£ ë„êµ¬ë³„ íŠ¹ì„± ë¹„êµ:")
print(f"ğŸ“Š ì²˜ë¦¬ ì†ë„:")
print(f"  ydata-profiling: {ydata_time:.1f}ì´ˆ")
print(f"  SweetViz: {sweetviz_time:.1f}ì´ˆ")
print(f"  ì†ë„ ì°¨ì´: {abs(ydata_time - sweetviz_time):.1f}ì´ˆ")

print(f"\nğŸ“‹ ë³´ê³ ì„œ íŠ¹ì„± ë¹„êµ:")
comparison_table = {
    'íŠ¹ì„±': ['ì²˜ë¦¬ì†ë„', 'ì‹œê°ì  ë””ìì¸', 'ìƒì„¸ë„', 'íƒ€ê²Ÿë¶„ì„', 'ì‚¬ìš©í¸ì˜ì„±'],
    'ydata-profiling': ['ë³´í†µ', 'ê¸°ëŠ¥ì ', 'ë§¤ìš°ìƒì„¸', 'ë³´í†µ', 'ì‰¬ì›€'],
    'SweetViz': ['ë¹ ë¦„', 'ì•„ë¦„ë‹¤ì›€', 'ì ë‹¹', 'ìš°ìˆ˜', 'ë§¤ìš°ì‰¬ì›€']
}

comparison_df = pd.DataFrame(comparison_table)
print(comparison_df.to_string(index=False))

print(f"\nğŸ’¡ ì‚¬ìš© ê¶Œì¥ ì‹œë‚˜ë¦¬ì˜¤:")
print(f"  ydata-profiling: ìƒì„¸í•œ ì´ˆê¸° ë°ì´í„° íƒìƒ‰, ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬")
print(f"  SweetViz: ë¹ ë¥¸ ê°œìš” íŒŒì•…, í”„ë ˆì  í…Œì´ì…˜ìš© ë³´ê³ ì„œ")
```

**ğŸ’¡ ë¶„ì„ ì§ˆë¬¸**:
1. ì–´ë–¤ ë„êµ¬ê°€ ë” ë§ì€ ì •ë³´ë¥¼ ì œê³µí•˜ë‚˜ìš”?
2. ì‹œê°ì ìœ¼ë¡œ ë” ë§¤ë ¥ì ì¸ ë„êµ¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
3. ê° ë„êµ¬ê°€ ë†“ì¹˜ëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€ìš”?

**âœ… ì˜ˆìƒ ê²°ê³¼**:
- ydata-profiling: ë” ìƒì„¸í•˜ì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
- SweetViz: ë¹ ë¥´ê³  ì•„ë¦„ë‹µì§€ë§Œ ì¼ë¶€ ê³ ê¸‰ ë¶„ì„ ë¶€ì¡±

---

### ì—°ìŠµë¬¸ì œ 2: AI ê²°ê³¼ ê²€ì¦ ì‹¤ìŠµ (ë‚œì´ë„: â­â­)

**ëª©í‘œ**: AIê°€ ìƒì„±í•œ ë¶„ì„ ê²°ê³¼ì˜ ì˜¤ë¥˜ë¥¼ ì°¾ì•„ë‚´ê³  ì˜¬ë°”ë¥´ê²Œ í•´ì„í•´ë³´ì„¸ìš”.

**ë‹¨ê³„ë³„ ê°€ì´ë“œ**:
1. AI ë„êµ¬ê°€ ì œì‹œí•œ ìƒê´€ê´€ê³„ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì¬ê²€ì¦
2. ë…¼ë¦¬ì  ì˜¤ë¥˜ë‚˜ í•´ì„ ë¬¸ì œ ì°¾ê¸°
3. ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ê²°ê³¼ ë³´ì™„

```python
# AI ê²°ê³¼ ê²€ì¦ ì‹¤ìŠµ
print("ğŸ” AI ê²°ê³¼ ê²€ì¦ ì‹¤ìŠµ")
print("=" * 40)

# 1ë‹¨ê³„: AI ìë™ ìƒê´€ê´€ê³„ ë¶„ì„
print("1ï¸âƒ£ AI ìë™ ìƒê´€ê´€ê³„ ë¶„ì„:")
numeric_df = df.select_dtypes(include=['number'])
ai_correlations = numeric_df.corr()

print("ğŸ¤– AIê°€ ë°œê²¬í•œ ê°•í•œ ìƒê´€ê´€ê³„ (|r| > 0.3):")
for i in range(len(ai_correlations.columns)):
    for j in range(i+1, len(ai_correlations.columns)):
        corr_value = ai_correlations.iloc[i, j]
        if abs(corr_value) > 0.3:
            var1 = ai_correlations.columns[i]
            var2 = ai_correlations.columns[j]
            print(f"  {var1} â†” {var2}: {corr_value:.3f}")

# 2ë‹¨ê³„: ìˆ˜ë™ ê²€ì¦ - ì‚°ì ë„ë¡œ ì‹œê°ì  í™•ì¸
print(f"\n2ï¸âƒ£ ìˆ˜ë™ ê²€ì¦ - ì‹œê°ì  í™•ì¸:")

import matplotlib.pyplot as plt

# ì˜ˆì‹œ: PassengerIdì™€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ ìƒê´€ê´€ê³„ ê²€ì¦
if 'PassengerId' in ai_correlations.columns:
    print("ğŸš¨ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìƒê´€ê´€ê³„ ê²€ì¦: PassengerId")
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # PassengerId vs Survived ì‚°ì ë„
    axes[0].scatter(df['PassengerId'], df['Survived'], alpha=0.6)
    axes[0].set_xlabel('PassengerId')
    axes[0].set_ylabel('Survived')
    axes[0].set_title('PassengerId vs Survived')
    axes[0].grid(True, alpha=0.3)
    
    # PassengerId vs Age ì‚°ì ë„
    axes[1].scatter(df['PassengerId'], df['Age'], alpha=0.6)
    axes[1].set_xlabel('PassengerId')
    axes[1].set_ylabel('Age')
    axes[1].set_title('PassengerId vs Age')
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # ë…¼ë¦¬ì  ê²€ì¦
    print("ğŸ§  ë…¼ë¦¬ì  ê²€ì¦:")
    print("  PassengerIdëŠ” ë‹¨ìˆœí•œ ì¼ë ¨ë²ˆí˜¸ì…ë‹ˆë‹¤.")
    print("  ìƒì¡´ ì—¬ë¶€ë‚˜ ë‚˜ì´ì™€ ì‹¤ì œ ê´€ê³„ê°€ ìˆì„ê¹Œìš”?")
    print("  âŒ ì´ëŠ” í—ˆìœ„ ìƒê´€ê´€ê³„(Spurious Correlation)ì…ë‹ˆë‹¤!")

# 3ë‹¨ê³„: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì˜¬ë°”ë¥¸ í•´ì„
print(f"\n3ï¸âƒ£ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ì˜¬ë°”ë¥¸ í•´ì„:")

meaningful_correlations = []

# ì˜ë¯¸ ìˆëŠ” ìƒê´€ê´€ê³„ë§Œ ì¶”ì¶œ
for i in range(len(ai_correlations.columns)):
    for j in range(i+1, len(ai_correlations.columns)):
        var1 = ai_correlations.columns[i]
        var2 = ai_correlations.columns[j]
        corr_value = ai_correlations.iloc[i, j]
        
        # PassengerId ê°™ì€ ì˜ë¯¸ ì—†ëŠ” ë³€ìˆ˜ ì œì™¸
        if (abs(corr_value) > 0.3 and 
            'PassengerId' not in [var1, var2] and
            not pd.isna(corr_value)):
            meaningful_correlations.append((var1, var2, corr_value))

print("âœ… ì˜ë¯¸ ìˆëŠ” ìƒê´€ê´€ê³„:")
for var1, var2, corr in meaningful_correlations:
    print(f"  {var1} â†” {var2}: {corr:.3f}")
    
    # ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í•´ì„
    if var1 == 'Pclass' and var2 == 'Fare':
        print(f"    ğŸ’¡ í•´ì„: ìƒìœ„ ë“±ê¸‰ì¼ìˆ˜ë¡ ë” ë¹„ì‹¼ ìš´ì„ (ë‹¹ì—°í•œ ê²°ê³¼)")
    elif 'Age' in [var1, var2] and 'Survived' in [var1, var2]:
        print(f"    ğŸ’¡ í•´ì„: ë‚˜ì´ì™€ ìƒì¡´ìœ¨ì˜ ê´€ê³„ (ì–´ë¦°ì´ ìš°ì„  ì •ì±… ê´€ë ¨)")
    elif var1 == 'SibSp' and var2 == 'Parch':
        print(f"    ğŸ’¡ í•´ì„: í˜•ì œìë§¤ì™€ ë¶€ëª¨ìë…€ ìˆ˜ì˜ ê´€ê³„ (ê°€ì¡± ì—¬í–‰)")

# 4ë‹¨ê³„: AI í•œê³„ì  ì •ë¦¬
print(f"\n4ï¸âƒ£ AI ë¶„ì„ì˜ í•œê³„ì :")
limitations = [
    "ë‹¨ìˆœ ì¼ë ¨ë²ˆí˜¸ë„ ìƒê´€ê´€ê³„ë¡œ ì¸ì‹",
    "ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•¨", 
    "ë„ë©”ì¸ ë§¥ë½ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ",
    "í†µê³„ì  ìœ ì˜ì„±ì„ ìë™ í™•ì¸í•˜ì§€ ì•ŠìŒ",
    "í—ˆìœ„ ìƒê´€ê´€ê³„ë¥¼ ê±¸ëŸ¬ë‚´ì§€ ëª»í•¨"
]

for i, limitation in enumerate(limitations, 1):
    print(f"  {i}. {limitation}")

print(f"\nğŸ“‹ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
checklist = [
    "â–¡ ë³€ìˆ˜ë“¤ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ”ê°€?",
    "â–¡ ìƒê´€ê´€ê³„ê°€ ìš°ì—°ì˜ ì¼ì¹˜ê°€ ì•„ë‹Œê°€?", 
    "â–¡ ì œ3ì˜ ë³€ìˆ˜ê°€ ì˜í–¥ì„ ì£¼ê³  ìˆì§€ëŠ” ì•Šë‚˜?",
    "â–¡ ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ì„¤ëª… ê°€ëŠ¥í•œê°€?",
    "â–¡ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê´€ê³„ì¸ê°€?"
]

for item in checklist:
    print(f"  {item}")
```

**ğŸ’¡ ê²€ì¦ í¬ì¸íŠ¸**:
1. PassengerId ê°™ì€ ë‹¨ìˆœ ì‹ë³„ìë„ ìƒê´€ê´€ê³„ë¡œ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‚˜ìš”?
2. ë†’ì€ ìƒê´€ê³„ìˆ˜ê°€ í•­ìƒ ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë¥¼ ëœ»í•˜ë‚˜ìš”?
3. AIê°€ ë†“ì¹˜ëŠ” ë§¥ë½ì  í•´ì„ì€ ë¬´ì—‡ì¸ê°€ìš”?

---

### ì—°ìŠµë¬¸ì œ 3: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• ì‹¤ì „ ì ìš© (ë‚œì´ë„: â­â­â­)

**ëª©í‘œ**: AI ìë™í™”ì™€ ì¸ê°„ ë¶„ì„ì„ ê²°í•©í•œ ì™„ì „í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•´ë³´ì„¸ìš”.

**ë‹¨ê³„ë³„ ê°€ì´ë“œ**:
1. AI ë„êµ¬ë¡œ ë¹ ë¥¸ ì „ì²´ ê°œìš” íŒŒì•…
2. ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ìœ¼ë¡œ ê°€ì„¤ ì„¤ì •
3. AIë¡œ ê°€ì„¤ ê²€ì¦ ë° íŒ¨í„´ íƒì§€
4. ì¸ê°„ í•´ì„ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

```python
# í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• ì‹¤ì „ í”„ë¡œì íŠ¸
print("ğŸ¤ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²• ì‹¤ì „ í”„ë¡œì íŠ¸")
print("=" * 50)

# í”„ë¡œì íŠ¸ ì„¤ì •
project_goal = "íƒ€ì´íƒ€ë‹‰ ìŠ¹ê° ë°ì´í„°ë¥¼ í†µí•œ ì¬ë‚œ ëŒ€ì‘ ê°œì„  ë°©ì•ˆ ë„ì¶œ"
print(f"ğŸ¯ í”„ë¡œì íŠ¸ ëª©í‘œ: {project_goal}")

# Phase 1: AI ë¹ ë¥¸ ìŠ¤ìº”
print(f"\nğŸš€ Phase 1: AI ë¹ ë¥¸ ìŠ¤ìº”")
print("-" * 30)

# ë¹ ë¥¸ AI ë¶„ì„ (ìƒ˜í”Œë§ ì‚¬ìš©)
sample_df = df.sample(n=min(500, len(df)), random_state=42)

# ìë™ íŒ¨í„´ íƒì§€
print("ğŸ¤– AI ìë™ íŒ¨í„´ íƒì§€:")

# 1-1. ìƒì¡´ìœ¨ ê´€ë ¨ ë³€ìˆ˜ ìë™ ìˆœìœ„
survival_correlations = {}
for col in sample_df.select_dtypes(include=['number']).columns:
    if col != 'Survived':
        corr = sample_df[col].corr(sample_df['Survived'])
        if not pd.isna(corr):
            survival_correlations[col] = abs(corr)

sorted_correlations = sorted(survival_correlations.items(), key=lambda x: x[1], reverse=True)

print("  ğŸ“Š ìƒì¡´ìœ¨ê³¼ ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ (ìë™ ìˆœìœ„):")
for i, (var, corr) in enumerate(sorted_correlations[:5], 1):
    print(f"    {i}. {var}: {corr:.3f}")

# 1-2. ìë™ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ìŠ¹ê° ê·¸ë£¹ ë°œê²¬
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ í´ëŸ¬ìŠ¤í„°ë§
numeric_features = ['Age', 'Fare', 'SibSp', 'Parch']
cluster_data = sample_df[numeric_features].dropna()

if len(cluster_data) > 10:
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(cluster_data)
    
    # 3ê°œ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„ë¥˜
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(scaled_data)
    
    # í´ëŸ¬ìŠ¤í„°ë³„ ìƒì¡´ìœ¨ í™•ì¸
    cluster_data['Cluster'] = clusters
    cluster_data['Survived'] = sample_df.loc[cluster_data.index, 'Survived']
    
    cluster_survival = cluster_data.groupby('Cluster')['Survived'].mean()
    
    print(f"\n  ğŸ¯ AI ë°œê²¬ ìŠ¹ê° ê·¸ë£¹ (í´ëŸ¬ìŠ¤í„°ë§):")
    for cluster, survival_rate in cluster_survival.items():
        print(f"    ê·¸ë£¹ {cluster}: ìƒì¡´ìœ¨ {survival_rate:.1%}")

# Phase 2: ì¸ê°„ ê°€ì„¤ ì„¤ì •
print(f"\nğŸ§  Phase 2: ì¸ê°„ ê°€ì„¤ ì„¤ì •")
print("-" * 30)

# ë„ë©”ì¸ ì „ë¬¸ê°€ ì—­í• ë¡œ ê°€ì„¤ ì„¤ì •
hypotheses = [
    {
        'name': 'í™©ê¸ˆ ì‹œê°„ëŒ€ ê°€ì„¤',
        'description': 'ì¬ë‚œ ì´ˆê¸° 30ë¶„ì´ ìƒì¡´ì— ê²°ì •ì  ì˜í–¥',
        'proxy_test': 'ê°ì‹¤ ë“±ê¸‰ë³„ ëŒ€í”¼ ì‹œê°„ ì°¨ì´ â†’ ìƒì¡´ìœ¨ ì°¨ì´',
        'variables': ['Pclass', 'Survived']
    },
    {
        'name': 'ì‚¬íšŒì  ì§€ì§€ ê°€ì„¤', 
        'description': 'ì‚¬íšŒì  ì—°ê²°ë§ì´ ìƒì¡´ í™•ë¥  ë†’ì„',
        'proxy_test': 'ê°€ì¡±/ì¹œêµ¬ ë™ë°˜ ì—¬ë¶€ â†’ ìƒì¡´ìœ¨ ì°¨ì´',
        'variables': ['SibSp', 'Parch', 'Survived']
    },
    {
        'name': 'ìì› ì ‘ê·¼ì„± ê°€ì„¤',
        'description': 'ê²½ì œì  ìì›ì´ ìƒì¡´ ê¸°íšŒ ì œê³µ',
        'proxy_test': 'ìš´ì„ ìˆ˜ì¤€ â†’ ìƒì¡´ìœ¨ ìƒê´€ê´€ê³„',
        'variables': ['Fare', 'Survived']
    }
]

print("ğŸ’¡ ì„¤ì •ëœ ê°€ì„¤ë“¤:")
for i, hyp in enumerate(hypotheses, 1):
    print(f"\n{i}. {hyp['name']}")
    print(f"   ì„¤ëª…: {hyp['description']}")
    print(f"   ê²€ì¦: {hyp['proxy_test']}")

# Phase 3: AI ê°€ì„¤ ê²€ì¦
print(f"\nğŸ¯ Phase 3: AI ê°€ì„¤ ê²€ì¦")
print("-" * 30)

verification_results = {}

# ê°€ì„¤ 1: í™©ê¸ˆ ì‹œê°„ëŒ€ (ë“±ê¸‰ë³„ ì°¨ì´)
print("ğŸ”¬ ê°€ì„¤ 1 ê²€ì¦: í™©ê¸ˆ ì‹œê°„ëŒ€")
class_survival = df.groupby('Pclass')['Survived'].agg(['count', 'mean'])
class_effect = class_survival.loc[1, 'mean'] - class_survival.loc[3, 'mean']

print(f"  ğŸ“Š ë“±ê¸‰ë³„ ìƒì¡´ìœ¨:")
for pclass, stats in class_survival.iterrows():
    print(f"    {pclass}ë“±ê¸‰: {stats['mean']:.1%} (n={stats['count']})")

print(f"  ğŸ“ˆ 1ë“±ê¸‰ vs 3ë“±ê¸‰ ì°¨ì´: {class_effect:+.1%}")

# í†µê³„ì  ìœ ì˜ì„± ê²€ì •
from scipy.stats import chi2_contingency
class_crosstab = pd.crosstab(df['Pclass'], df['Survived'])
chi2, p_val, dof, expected = chi2_contingency(class_crosstab)

verification_results['hypothesis_1'] = {
    'effect_size': class_effect,
    'p_value': p_val,
    'significant': p_val < 0.05,
    'interpretation': 'ì§€ì§€' if p_val < 0.05 and class_effect > 0.1 else 'ê¸°ê°'
}

print(f"  ğŸ“ˆ í†µê³„ ê²€ì •: p = {p_val:.4f} â†’ {'ìœ ì˜í•¨' if p_val < 0.05 else 'ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")

# ê°€ì„¤ 2: ì‚¬íšŒì  ì§€ì§€ (ê°€ì¡± ë™ë°˜ íš¨ê³¼)
print(f"\nğŸ”¬ ê°€ì„¤ 2 ê²€ì¦: ì‚¬íšŒì  ì§€ì§€")

# ê°€ì¡± ë™ë°˜ ì—¬ë¶€ ê³„ì‚°
df['HasFamily'] = (df['SibSp'] > 0) | (df['Parch'] > 0)
family_survival = df.groupby('HasFamily')['Survived'].agg(['count', 'mean'])

print(f"  ğŸ“Š ê°€ì¡± ë™ë°˜ë³„ ìƒì¡´ìœ¨:")
for has_family, stats in family_survival.iterrows():
    family_status = "ê°€ì¡± ë™ë°˜" if has_family else "í˜¼ì ì—¬í–‰"
    print(f"    {family_status}: {stats['mean']:.1%} (n={stats['count']})")

family_effect = family_survival.loc[True, 'mean'] - family_survival.loc[False, 'mean']
print(f"  ğŸ“ˆ ê°€ì¡± ë™ë°˜ íš¨ê³¼: {family_effect:+.1%}")

verification_results['hypothesis_2'] = {
    'effect_size': family_effect,
    'interpretation': 'ì§€ì§€' if family_effect > 0 else 'ê¸°ê°'
}

# ê°€ì„¤ 3: ìì› ì ‘ê·¼ì„± (ìš´ì„-ìƒì¡´ìœ¨ ê´€ê³„)
print(f"\nğŸ”¬ ê°€ì„¤ 3 ê²€ì¦: ìì› ì ‘ê·¼ì„±")

# ìš´ì„ì„ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„
df['FareGroup'] = pd.qcut(df['Fare'].dropna(), q=4, labels=['ì €ê°€', 'ì¤‘ì €ê°€', 'ì¤‘ê³ ê°€', 'ê³ ê°€'])

fare_survival = df.groupby('FareGroup')['Survived'].agg(['count', 'mean'])

print(f"  ğŸ“Š ìš´ì„ êµ¬ê°„ë³„ ìƒì¡´ìœ¨:")
for fare_group, stats in fare_survival.iterrows():
    print(f"    {fare_group}: {stats['mean']:.1%} (n={stats['count']})")

# ìƒê´€ê´€ê³„ ê²€ì •
fare_corr = df['Fare'].corr(df['Survived'])
print(f"  ğŸ“ˆ ìš´ì„-ìƒì¡´ìœ¨ ìƒê´€ê³„ìˆ˜: {fare_corr:.3f}")

verification_results['hypothesis_3'] = {
    'correlation': fare_corr,
    'interpretation': 'ì§€ì§€' if fare_corr > 0.2 else 'ê¸°ê°'
}

# Phase 4: í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ì¢…í•©
print(f"\nğŸ¤ Phase 4: í•˜ì´ë¸Œë¦¬ë“œ ì¸ì‚¬ì´íŠ¸ ì¢…í•©")
print("-" * 30)

print("ğŸ¯ ê²€ì¦ëœ ê°€ì„¤ ê¸°ë°˜ ì‹¤í–‰ ë°©ì•ˆ:")

actionable_insights = []

# ê° ê°€ì„¤ë³„ ì‹¤í–‰ ë°©ì•ˆ ë„ì¶œ
for hyp_id, result in verification_results.items():
    if result['interpretation'] == 'ì§€ì§€':
        if hyp_id == 'hypothesis_1':
            insight = {
                'finding': f"ìƒìœ„ ë“±ê¸‰ ìŠ¹ê°ì´ {result['effect_size']:.1%} ë” ë†’ì€ ìƒì¡´ìœ¨",
                'root_cause': "ê°ì‹¤ ìœ„ì¹˜ì™€ ëŒ€í”¼ë¡œ ì ‘ê·¼ì„±ì˜ ì°¨ì´",
                'action': "ì¬ë‚œ ëŒ€ì‘ ì‹œì„¤ì˜ í‰ë“±í•œ ë°°ì¹˜ ë° ì ‘ê·¼ì„± ë³´ì¥",
                'modern_application': "ê±´ë¬¼ ì„¤ê³„ ì‹œ ëª¨ë“  ì¸µì— ê· ë“±í•œ ë¹„ìƒêµ¬ ë°°ì¹˜"
            }
            actionable_insights.append(insight)
            
        elif hyp_id == 'hypothesis_2':
            insight = {
                'finding': f"ê°€ì¡± ë™ë°˜ ìŠ¹ê°ì´ {result['effect_size']:.1%} ë” ë†’ì€ ìƒì¡´ìœ¨",
                'root_cause': "ìƒí˜¸ ë¶€ì¡°ì™€ ì •ë³´ ê³µìœ  íš¨ê³¼",
                'action': "ì¬ë‚œ ì‹œ ì†Œê·¸ë£¹ í˜•ì„± ë° ìƒí˜¸ ì§€ì› ì²´ê³„ êµ¬ì¶•",
                'modern_application': "ë¹„ìƒ ìƒí™© ì‹œ ë²„ë”” ì‹œìŠ¤í…œ ìš´ì˜"
            }
            actionable_insights.append(insight)
            
        elif hyp_id == 'hypothesis_3':
            insight = {
                'finding': f"ìš´ì„ê³¼ ìƒì¡´ìœ¨ ì–‘ì˜ ìƒê´€ê´€ê³„ ({result['correlation']:.3f})",
                'root_cause': "ê²½ì œì  ìì›ì˜ ìƒì¡´ ê¸°íšŒ ì œê³µ",
                'action': "ì¬ë‚œ ëŒ€ì‘ ìì›ì˜ ê³µí‰í•œ ë¶„ë°° ì‹œìŠ¤í…œ í•„ìš”",
                'modern_application': "ì†Œë“ ìˆ˜ì¤€ì— ê´€ê³„ì—†ì´ ë™ë“±í•œ ì•ˆì „ ì„œë¹„ìŠ¤ ì œê³µ"
            }
            actionable_insights.append(insight)

# ìµœì¢… ì‹¤í–‰ ë°©ì•ˆ ì¶œë ¥
print("\nğŸ“‹ ìµœì¢… ì‹¤í–‰ ë°©ì•ˆ:")
for i, insight in enumerate(actionable_insights, 1):
    print(f"\n{i}. ë°œê²¬ì‚¬í•­: {insight['finding']}")
    print(f"   ì›ì¸ ë¶„ì„: {insight['root_cause']}")
    print(f"   ê°œì„  ë°©ì•ˆ: {insight['action']}")
    print(f"   í˜„ëŒ€ì  ì ìš©: {insight['modern_application']}")

# í”„ë¡œì íŠ¸ ì„±ê³¼ ìš”ì•½
print(f"\nğŸ† í”„ë¡œì íŠ¸ ì„±ê³¼ ìš”ì•½:")
print(f"  âš¡ AI ì†ë„ + ğŸ§  ì¸ê°„ í†µì°°ë ¥ = ğŸ¯ ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµ")
print(f"  ğŸ“Š ë°ì´í„° íŒ¨í„´ â†’ ğŸ’¡ ë„ë©”ì¸ í•´ì„ â†’ ğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ì•¡ì…˜")
print(f"  ğŸ”„ 100ë…„ ì „ ë°ì´í„° â†’ í˜„ì¬ ì•ˆì „ ì •ì±… ê°œì„  ì•„ì´ë””ì–´")

print(f"\nâœ… í•˜ì´ë¸Œë¦¬ë“œ í”„ë¡œì íŠ¸ ì™„ë£Œ!")
```

**ğŸ’¡ í”„ë¡œì íŠ¸ í™•ì¥ ì•„ì´ë””ì–´**:
1. ë‹¤ë¥¸ ì¬ë‚œ ì‚¬ë¡€ ë°ì´í„°ì™€ ë¹„êµ ë¶„ì„
2. í˜„ëŒ€ í•­ê³µì‚¬ ì•ˆì „ ì •ì±…ê³¼ì˜ ì—°ê²°ì  ì°¾ê¸°
3. ê±´ë¬¼ í™”ì¬ ëŒ€í”¼ ì‹œë®¬ë ˆì´ì…˜ì— ì¸ì‚¬ì´íŠ¸ ì ìš©

**ğŸ¯ ì‹¤ìŠµ ëª©í‘œ ë‹¬ì„±ë„ ì²´í¬**:
- [ ] AI ë„êµ¬ì˜ ë¹ ë¥¸ íŒ¨í„´ íƒì§€ í™œìš©
- [ ] ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ê°€ì„¤ ì„¤ì •
- [ ] í†µê³„ì  ê²€ì¦ìœ¼ë¡œ ê°€ì„¤ ì…ì¦
- [ ] ì‹¤í–‰ ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

---

## ìš”ì•½ / í•µì‹¬ ì •ë¦¬

### ğŸ¯ ì£¼ìš” í•™ìŠµ ë‚´ìš©

**1. AI ìë™í™” EDA ë„êµ¬ ë§ˆìŠ¤í„°**
- **ydata-profiling**: ğŸ” ê°€ì¥ ìƒì„¸í•œ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ
  - ì‚¬ìš© ì‹œê¸°: ì´ˆê¸° ë°ì´í„° íƒìƒ‰, ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
  - ì¥ì : ì™„ì„±ë„ ë†’ì€ ë¶„ì„, ë§ì€ ì»¤ìŠ¤í„°ë§ˆì´ì§• ì˜µì…˜
  - ë‹¨ì : ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ëŠë¦¼
- **SweetViz**: ğŸ¨ ì•„ë¦„ë‹¤ìš´ ì‹œê°í™”ì™€ íƒ€ê²Ÿ ë¶„ì„ íŠ¹í™”
  - ì‚¬ìš© ì‹œê¸°: ë¶„ë¥˜ ë¬¸ì œ ë¶„ì„, í”„ë ˆì  í…Œì´ì…˜ìš© ë³´ê³ ì„œ
  - ì¥ì : ì§ê´€ì  UI, ë¹ ë¥¸ ì²˜ë¦¬, ë°ì´í„°ì…‹ ë¹„êµ ê¸°ëŠ¥
  - ë‹¨ì : ì»¤ìŠ¤í„°ë§ˆì´ì§• ì œí•œì 
- **AutoViz**: ğŸ§  AI ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ë³€ìˆ˜ ì„ íƒ
  - ì‚¬ìš© ì‹œê¸°: ë³€ìˆ˜ ì¤‘ìš”ë„ ë¶„ì„, ì˜ˆì¸¡ ëª¨ë¸ë§ ì¤€ë¹„
  - ì¥ì : ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ìë™ ì„ íƒ, ëŒ€ìš©ëŸ‰ ë°ì´í„° ì§€ì›
  - ë‹¨ì : UI ë³µì¡, CSV íŒŒì¼ë§Œ ì§€ì›
- **D-Tale**: ğŸŒ ì¸í„°ë™í‹°ë¸Œ ì›¹ ê¸°ë°˜ ì‹¤ì‹œê°„ íƒìƒ‰
  - ì‚¬ìš© ì‹œê¸°: ì‹¤ì‹œê°„ ë°ì´í„° íƒìƒ‰, ë¹„ê°œë°œìì™€ í˜‘ì—…
  - ì¥ì : GUI ê¸°ë°˜ ì¡°ì‘, ì½”ë“œ ìë™ ìƒì„±
  - ë‹¨ì : ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰, ë³µì¡í•œ ì„¤ì •

**2. ì „í†µì  EDA vs AI ë°©ì‹ ì‹¬ì¸µ ë¹„êµ**
- **ì†ë„**: AI ë°©ì‹ì´ 10-20ë°° ë¹ ë¦„ (3ì¤„ vs 150ì¤„ ì½”ë“œ)
- **ì •í™•ì„±**: ì „í†µì  ë°©ì‹ì´ ë„ë©”ì¸ ë§¥ë½ ë°˜ì˜ì—ì„œ ìš°ìˆ˜
- **ê¹Šì´**: ì „í†µì  ë°©ì‹ì´ ê°€ì„¤ ê¸°ë°˜ ì‹¬ì¸µ ë¶„ì„ì—ì„œ ê°•ì 
- **ì¼ê´€ì„±**: AI ë°©ì‹ì´ ê°ê´€ì ì´ê³  í‘œì¤€í™”ëœ ë¶„ì„ ì œê³µ
- **ìœ ì—°ì„±**: ì „í†µì  ë°©ì‹ì´ íŠ¹í™”ëœ ë¶„ì„ê³¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•ì—ì„œ ìš°ìˆ˜

**3. AI ê²°ê³¼ ê²€ì¦ì˜ 5ë‹¨ê³„ ë°©ë²•ë¡ **
1. **ë°ì´í„° í’ˆì§ˆ ê²€ì¦**: ì¤‘ë³µ, ê²°ì¸¡ê°’, ë…¼ë¦¬ì  ë²”ìœ„ í™•ì¸
2. **í†µê³„ì  ê²€ì¦**: ê¸°ë³¸ í†µê³„ëŸ‰ ì¬ê³„ì‚°, ìƒê´€ê´€ê³„ ì¬ê²€ì¦
3. **ë…¼ë¦¬ì  ì¼ê´€ì„±**: ë³€ìˆ˜ ê°„ ê´€ê³„ì˜ ë…¼ë¦¬ì  íƒ€ë‹¹ì„± í™•ì¸
4. **í¸í–¥ íƒì§€**: ìƒ˜í”Œë§, ìƒì¡´ì, ê²°ì¸¡ê°’ í¸í–¥ í™•ì¸
5. **ì¢…í•© ë³´ê³ ì„œ**: ê²€ì¦ ê²°ê³¼ ìš”ì•½ ë° ê¶Œê³ ì‚¬í•­ ë„ì¶œ

**4. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì˜ 4ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°**
- **Phase 1**: AI ê³ ì† íƒìƒ‰ (ì „ì²´ ê°œìš” íŒŒì•…, 1-2ë¶„)
- **Phase 2**: ì¸ê°„ ê°€ì„¤ ìƒì„± (ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜, 10-15ë¶„)
- **Phase 3**: íƒ€ê²Ÿ AI ë¶„ì„ (ê°€ì„¤ ê²€ì¦ íŠ¹í™”, 5-10ë¶„)
- **Phase 4**: í•˜ì´ë¸Œë¦¬ë“œ ì¢…í•© (ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸, 15-20ë¶„)

### ğŸ’¡ ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

**ğŸš€ ì–¸ì œ ì–´ë–¤ ì ‘ê·¼ë²•ì„ ì„ íƒí•  ê²ƒì¸ê°€?**

| ìƒí™© | ê¶Œì¥ ì ‘ê·¼ë²• | ì´ìœ  | ë„êµ¬ ì¶”ì²œ |
|------|------------|------|----------|
| ğŸƒâ€â™‚ï¸ ê¸‰í•œ ê°œìš” íŒŒì•… | AI-First | ë¹ ë¥¸ ì „ì²´ ìŠ¤ìº” í•„ìš” | SweetViz |
| ğŸ” ìƒì„¸í•œ ì´ˆê¸° íƒìƒ‰ | AI-First | í¬ê´„ì  ë¶„ì„ í•„ìš” | ydata-profiling |
| ğŸ¯ íŠ¹ì • ê°€ì„¤ ê²€ì¦ | Human-First | ì •ë°€í•œ ë¶„ì„ í•„ìš” | ì „í†µì  ë°©ì‹ |
| ğŸ’¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ | Hybrid | AI ë°œê²¬ + ì¸ê°„ í•´ì„ | ë‹¨ê³„ì  ì¡°í•© |
| ğŸ“Š í”„ë ˆì  í…Œì´ì…˜ìš© | AI-First | ì•„ë¦„ë‹¤ìš´ ì‹œê°í™” | SweetViz |
| ğŸ§ª ì—°êµ¬/í•™ìˆ  ëª©ì  | Human-First | ì—„ë°€í•œ ê²€ì¦ í•„ìš” | ì „í†µì  ë°©ì‹ |
| ğŸ‘¥ íŒ€ í˜‘ì—… | Hybrid | ë‹¤ì–‘í•œ ê´€ì  í†µí•© | D-Tale + ìˆ˜ë™ë¶„ì„ |

**âœ… ì„±ê³µì ì¸ í•˜ì´ë¸Œë¦¬ë“œ EDAë¥¼ ìœ„í•œ 10ê³„ëª…**

1. **ğŸ¤– AIë¥¼ ë„êµ¬ë¡œ í™œìš©í•˜ë˜ ë§¹ì‹ í•˜ì§€ ë§ë¼**
   - AIëŠ” ì¶œë°œì ì´ì§€ ìµœì¢… ë‹µì´ ì•„ë‹˜
   - í•­ìƒ ë¹„íŒì  ê²€ì¦ ê³¼ì • ê±°ì¹˜ê¸°

2. **ğŸ§  ë„ë©”ì¸ ì§€ì‹ì„ í•­ìƒ í•¨ê»˜ ê³ ë ¤í•˜ë¼**
   - í†µê³„ì  ìœ ì˜ì„± â‰  ì‹¤ë¬´ì  ì˜ë¯¸
   - ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ì—ì„œ í•´ì„í•˜ê¸°

3. **ğŸ” AI ê²°ê³¼ë¥¼ ë°˜ë“œì‹œ ê²€ì¦í•˜ë¼**
   - í—ˆìœ„ ìƒê´€ê´€ê³„ ì£¼ì˜
   - ë…¼ë¦¬ì  íƒ€ë‹¹ì„± í™•ì¸

4. **âš¡ íš¨ìœ¨ì„±ê³¼ ì •í™•ì„±ì˜ ê· í˜• ë§ì¶”ê¸°**
   - ì‹œê°„ ì œì•½ vs ë¶„ì„ ê¹Šì´ ê³ ë ¤
   - ëª©ì ì— ë§ëŠ” ë„êµ¬ ì„ íƒ

5. **ğŸ¯ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ê²°ê³¼ì— ë” ì£¼ëª©í•˜ë¼**
   - ë°˜ì§ê´€ì  íŒ¨í„´ì—ì„œ ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸
   - AIê°€ ë°œê²¬í•œ ìˆ¨ê²¨ì§„ íŒ¨í„´ íƒêµ¬

6. **ğŸ“Š ì‹œê°í™”ë¡œ ì§ê´€ ê²€ì¦í•˜ê¸°**
   - ìˆ˜ì¹˜ë§Œìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ê³  ê·¸ë˜í”„ë¡œ í™•ì¸
   - íŒ¨í„´ì˜ ì‹œê°ì  íƒ€ë‹¹ì„± ê²€í† 

7. **ğŸ”„ ë°˜ë³µì  ê°œì„  ê³¼ì • ê±°ì¹˜ê¸°**
   - ì²« ë²ˆì§¸ ê²°ê³¼ê°€ ìµœì¢…ì´ ì•„ë‹˜
   - AI ê²°ê³¼ â†’ ì¸ê°„ í•´ì„ â†’ ì¬ê²€ì¦ ì‚¬ì´í´

8. **ğŸ‘¥ ë‹¤ì–‘í•œ ê´€ì  í†µí•©í•˜ê¸°**
   - ì—¬ëŸ¬ AI ë„êµ¬ ê²°ê³¼ ë¹„êµ
   - íŒ€ì›ë“¤ì˜ ë‹¤ì–‘í•œ í•´ì„ ìˆ˜ë ´

9. **ğŸ“ ì¬í˜„ ê°€ëŠ¥í•œ ë¶„ì„ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•**
   - ë¶„ì„ ê³¼ì • ë¬¸ì„œí™”
   - ì½”ë“œì™€ ì„¤ì • ì €ì¥

10. **ğŸ¯ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ì— ì§‘ì¤‘í•˜ë¼**
    - í•™ìˆ ì  í˜¸ê¸°ì‹¬ vs ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜
    - êµ¬ì²´ì  ì•¡ì…˜ í”Œëœ ë„ì¶œ

### ğŸš€ ì‹¤ë¬´ í™œìš© ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì´ë“œ

**ğŸ“ˆ ë§ˆì¼€íŒ… ë¶„ì•¼**
- **ê³ ê° ì„¸ë¶„í™”**: AutoVizë¡œ ì¤‘ìš” ë³€ìˆ˜ íƒì§€ â†’ ì „í†µì  ë°©ì‹ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ í•´ì„
- **ìº í˜ì¸ íš¨ê³¼**: SweetVizë¡œ A/B í…ŒìŠ¤íŠ¸ ë¹„êµ â†’ í†µê³„ì  ê²€ì •ìœ¼ë¡œ ìœ ì˜ì„± í™•ì¸
- **ê³ ê° ì—¬ì • ë¶„ì„**: ydata-profilingìœ¼ë¡œ ì „ì²´ ê°œìš” â†’ ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ í„°ì¹˜í¬ì¸íŠ¸ í•´ì„

**ğŸ¥ ì˜ë£Œ/ì œì•½ ë¶„ì•¼**
- **í™˜ì ë°ì´í„° ë¶„ì„**: ydata-profilingìœ¼ë¡œ ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ â†’ ì˜í•™ ì§€ì‹ìœ¼ë¡œ ì„ìƒì  í•´ì„
- **ì¹˜ë£Œ íš¨ê³¼ ë¶„ì„**: ì „í†µì  ë°©ì‹ìœ¼ë¡œ ì—„ë°€í•œ í†µê³„ ê²€ì • â†’ AIë¡œ ì¶”ê°€ íŒ¨í„´ íƒì§€
- **ë¶€ì‘ìš© ëª¨ë‹ˆí„°ë§**: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ìœ¼ë¡œ ìë™ íƒì§€ + ì „ë¬¸ê°€ ê²€ì¦

**ğŸ­ ì œì¡°ì—… ë¶„ì•¼**
- **í’ˆì§ˆ ê´€ë¦¬**: AIë¡œ ì´ìƒì¹˜ ìë™ íƒì§€ â†’ ì—”ì§€ë‹ˆì–´ë§ ì§€ì‹ìœ¼ë¡œ ì›ì¸ ë¶„ì„
- **ì„¤ë¹„ ì˜ˆì¸¡ ì •ë¹„**: AutoVizë¡œ í•µì‹¬ ë³€ìˆ˜ ì‹ë³„ â†’ ë„ë©”ì¸ ì „ë¬¸ê°€ì˜ ì •ë¹„ ì „ëµ ìˆ˜ë¦½
- **ê³µì • ìµœì í™”**: í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ìœ¼ë¡œ íš¨ìœ¨ì„± ê°œì„ ì  ë„ì¶œ

**ğŸ’° ê¸ˆìœµ ë¶„ì•¼**
- **ë¦¬ìŠ¤í¬ ë¶„ì„**: ydata-profilingìœ¼ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ ê°œìš” â†’ ê¸ˆìœµ ì´ë¡ ìœ¼ë¡œ ë¦¬ìŠ¤í¬ í•´ì„
- **ê³ ê° ì‹ ìš©í‰ê°€**: AI ìë™ ë¶„ì„ + ê·œì œ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜ ê²€ì¦
- **ì‚¬ê¸° íƒì§€**: ë¨¸ì‹ ëŸ¬ë‹ íŒ¨í„´ íƒì§€ + ë„ë©”ì¸ ì „ë¬¸ê°€ì˜ ë£° ê¸°ë°˜ ê²€ì¦

### ğŸ”® ë¯¸ë˜ ì „ë§ê³¼ ì¤€ë¹„ ë°©í–¥

**ğŸŒŸ AI EDA ë„êµ¬ì˜ ë°œì „ ë°©í–¥**
1. **ğŸ§  ë” ë˜‘ë˜‘í•œ AI**: GPT ê¸°ë°˜ ìì—°ì–´ í•´ì„, ìë™ ê°€ì„¤ ìƒì„±
2. **ğŸ”„ ì‹¤ì‹œê°„ ë¶„ì„**: ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì‹¤ì‹œê°„ EDA
3. **ğŸ¨ í–¥ìƒëœ ì‹œê°í™”**: AR/VR ê¸°ë°˜ ëª°ì…í˜• ë°ì´í„° íƒìƒ‰
4. **ğŸ¤ í˜‘ì—… ê°•í™”**: íŒ€ ë‹¨ìœ„ ì‹¤ì‹œê°„ ê³µë™ ë¶„ì„ í”Œë«í¼

**ğŸ“š ë°ì´í„° ë¶„ì„ê°€ì˜ ì§„í™” ë°©í–¥**
- **ğŸ¤– AI í˜‘ì—… ì „ë¬¸ê°€**: AI ë„êµ¬ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ëŠ¥ë ¥
- **ğŸ§  ë©”íƒ€ ë¶„ì„ê°€**: AI ê²°ê³¼ë¥¼ ê²€ì¦í•˜ê³  í•´ì„í•˜ëŠ” ëŠ¥ë ¥
- **ğŸ¯ ë¹„ì¦ˆë‹ˆìŠ¤ ë²ˆì—­ê°€**: ë°ì´í„° ì¸ì‚¬ì´íŠ¸ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì•¡ì…˜ìœ¼ë¡œ ì „í™˜
- **ğŸ” í’ˆì§ˆ ê´€ë¦¬ì**: AI ë¶„ì„ì˜ í’ˆì§ˆê³¼ ì‹ ë¢°ì„± ë³´ì¥

**ğŸ“ ì§€ì†ì  í•™ìŠµ ê°€ì´ë“œ**
1. **AI ë„êµ¬ ì—…ë°ì´íŠ¸**: ìƒˆë¡œìš´ ë„êµ¬ì™€ ê¸°ëŠ¥ ì§€ì† í•™ìŠµ
2. **ë„ë©”ì¸ ì „ë¬¸ì„±**: ë¶„ì„ ëŒ€ìƒ ì—…ê³„ì˜ ê¹Šì€ ì´í•´
3. **í†µê³„ì  ì‚¬ê³ **: AI ê²°ê³¼ë¥¼ ê²€ì¦í•  ìˆ˜ ìˆëŠ” í†µê³„ ì§€ì‹
4. **ì»¤ë®¤ë‹ˆì¼€ì´ì…˜**: ê¸°ìˆ ì  ê²°ê³¼ë¥¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ëŠ¥ë ¥

### ğŸ” ë‹¤ìŒ Part ì˜ˆê³ 

ë‹¤ìŒ Partì—ì„œëŠ” **í”„ë¡œì íŠ¸: ì‹¤ì œ ë°ì´í„°ì…‹ íƒìƒ‰ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ**ì— ëŒ€í•´ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

**ğŸ¯ ë‹¤ìŒ Part í•™ìŠµ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°**:
- ğŸ—ï¸ **ì¢…í•© EDA í”„ë¡œì íŠ¸ ì„¤ê³„**: ë¬¸ì œ ì •ì˜ë¶€í„° ê²°ê³¼ ë°œí‘œê¹Œì§€ ì „ì²´ ì›Œí¬í”Œë¡œìš°
- ğŸ”„ **í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•ë¡  ì‹¤ì „ ì ìš©**: AI ë„êµ¬ì™€ ì „í†µì  ë°©ì‹ì˜ ì²´ê³„ì  ê²°í•©
- ğŸ“Š **ê³ ê¸‰ ì‹œê°í™” ë° ìŠ¤í† ë¦¬í…”ë§**: ì¸ì‚¬ì´íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•
- ğŸ’¼ **ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ ì°½ì¶œ**: ë°ì´í„° ë¶„ì„ì„ ì‹¤ì œ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ì—°ê²°
- ğŸ¨ **í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±**: ì·¨ì—…/ìŠ¹ì§„ì— í™œìš©í•  ìˆ˜ ìˆëŠ” í”„ë¡œì íŠ¸ ì™„ì„±

**ğŸ’¡ ì¤€ë¹„ ì‚¬í•­**:
- ì´ë²ˆ Partì—ì„œ ë°°ìš´ AI ë„êµ¬ë“¤ì„ ì‹¤ì œë¡œ ì„¤ì¹˜í•˜ê³  ì‚¬ìš©í•´ë³´ê¸°
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ë°©ë²•ë¡ ì„ ë‹¤ë¥¸ ë°ì´í„°ì…‹ì—ë„ ì ìš©í•´ë³´ê¸°
- ë³¸ì¸ì´ ê´€ì‹¬ ìˆëŠ” ë„ë©”ì¸ì˜ ë°°ê²½ ì§€ì‹ í•™ìŠµí•˜ê¸°

**ğŸš€ íŠ¹ë³„ ë¯¸ì…˜**:
ë‹¤ìŒ Part ì‹œì‘ ì „ê¹Œì§€ ì‹¤ì œ Kaggle ë°ì´í„°ì…‹ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ì´ë²ˆ Partì—ì„œ ë°°ìš´ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ìœ¼ë¡œ ê°„ë‹¨í•œ ë¶„ì„ì„ ìˆ˜í–‰í•´ë³´ì„¸ìš”. ê·¸ ê²½í—˜ì´ ë‹¤ìŒ Partì—ì„œ í° ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤!

---

**ğŸ“š ì°¸ê³  ìë£Œ**
- ydata-profiling ê³µì‹ ë¬¸ì„œ: https://ydata-profiling.ydata.ai/
- SweetViz ê³µì‹ ë¬¸ì„œ: https://github.com/fbdesignpro/sweetviz
- AutoViz ê³µì‹ ë¬¸ì„œ: https://github.com/AutoViML/AutoViz
- D-Tale ê³µì‹ ë¬¸ì„œ: https://github.com/man-group/dtale
- AI Ethics Guidelines: https://www.partnershiponai.org/
- Statistical Verification Methods: https://www.statmethods.net/
```
```
```
