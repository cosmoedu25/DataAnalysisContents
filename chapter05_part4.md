# 5ì¥ Part 4: ëª¨ë¸ í‰ê°€ì™€ ê²€ì¦ ë°©ë²•

## í•™ìŠµ ëª©í‘œ
ì´ë²ˆ íŒŒíŠ¸ë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• ì˜ ì¤‘ìš”ì„±ì„ ì´í•´í•˜ê³  ì˜¬ë°”ë¥´ê²Œ ì ìš©í•  ìˆ˜ ìˆë‹¤
- K-Fold, Stratified K-Fold ë“± êµì°¨ ê²€ì¦ ê¸°ë²•ì„ êµ¬í˜„í•˜ê³  í™œìš©í•  ìˆ˜ ìˆë‹¤
- í•™ìŠµ ê³¡ì„ ì„ í†µí•´ ê³¼ì í•©ê³¼ ê³¼ì†Œì í•©ì„ ì§„ë‹¨í•˜ê³  í•´ê²°í•  ìˆ˜ ìˆë‹¤
- í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ë¥¼ ì´í•´í•˜ê³  ëª¨ë¸ ë³µì¡ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤
- ê²€ì¦ ê³¡ì„ ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤
- ì²´ê³„ì ì¸ ëª¨ë¸ ì„ íƒ í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤

## ì´ë²ˆ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°
ì¢‹ì€ ëª¨ë¸ì´ë€ ë¬´ì—‡ì¼ê¹Œìš”? ë‹¨ìˆœíˆ í›ˆë ¨ ë°ì´í„°ì—ì„œ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì´ëŠ” ëª¨ë¸ì¼ê¹Œìš”? ì•„ë‹™ë‹ˆë‹¤! **ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œë„ ì˜ ì‘ë™í•˜ëŠ” ëª¨ë¸**ì´ ì§„ì§œ ì¢‹ì€ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë²ˆ íŒŒíŠ¸ì—ì„œëŠ” ëª¨ë¸ì„ ì˜¬ë°”ë¥´ê²Œ í‰ê°€í•˜ê³  ê²€ì¦í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì‹œí—˜ ë¬¸ì œë¥¼ ë¯¸ë¦¬ ë³´ê³  ê³µë¶€í•˜ëŠ” ê²ƒê³¼ ì‹¤ì œ ì‹œí—˜ì˜ ì°¨ì´ì²˜ëŸ¼, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë„ ì œëŒ€ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤!

---

## 5.4.1 ì™œ ëª¨ë¸ í‰ê°€ê°€ ì¤‘ìš”í•œê°€?

### ì‹œí—˜ ê³µë¶€ì˜ ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import (train_test_split, KFold, StratifiedKFold, 
                                   cross_val_score, learning_curve, validation_curve)
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.datasets import load_iris, load_wine, fetch_california_housing
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ëª¨ë¸ í‰ê°€ì˜ ì¤‘ìš”ì„± ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# ì™¼ìª½: ì˜ëª»ëœ í‰ê°€ (ë¬¸ì œì§‘ë§Œ í’€ê¸°)
study_methods = ['ë¬¸ì œì§‘ A', 'ë¬¸ì œì§‘ A', 'ë¬¸ì œì§‘ A', 'ì‹¤ì œ ì‹œí—˜']
scores_wrong = [95, 98, 100, 65]
colors_wrong = ['green', 'green', 'green', 'red']

ax1.bar(study_methods, scores_wrong, color=colors_wrong, alpha=0.7, edgecolor='black')
ax1.set_ylim(0, 110)
ax1.set_ylabel('ì ìˆ˜', fontsize=12)
ax1.set_title('ì˜ëª»ëœ í•™ìŠµ: ê°™ì€ ë¬¸ì œë§Œ ë°˜ë³µ', fontsize=14, fontweight='bold')
ax1.axhline(y=80, color='gray', linestyle='--', alpha=0.5, label='ëª©í‘œ ì ìˆ˜')

# ì ìˆ˜ í‘œì‹œ
for i, score in enumerate(scores_wrong):
    ax1.text(i, score + 2, str(score), ha='center', fontsize=12, fontweight='bold')

# ì˜¤ë¥¸ìª½: ì˜¬ë°”ë¥¸ í‰ê°€ (ë‹¤ì–‘í•œ ë¬¸ì œ í’€ê¸°)
study_methods_right = ['ë¬¸ì œì§‘ A', 'ë¬¸ì œì§‘ B', 'ëª¨ì˜ê³ ì‚¬', 'ì‹¤ì œ ì‹œí—˜']
scores_right = [85, 82, 80, 78]
colors_right = ['blue', 'blue', 'blue', 'green']

ax2.bar(study_methods_right, scores_right, color=colors_right, alpha=0.7, edgecolor='black')
ax2.set_ylim(0, 110)
ax2.set_ylabel('ì ìˆ˜', fontsize=12)
ax2.set_title('ì˜¬ë°”ë¥¸ í•™ìŠµ: ë‹¤ì–‘í•œ ë¬¸ì œ ì—°ìŠµ', fontsize=14, fontweight='bold')
ax2.axhline(y=80, color='gray', linestyle='--', alpha=0.5, label='ëª©í‘œ ì ìˆ˜')

# ì ìˆ˜ í‘œì‹œ
for i, score in enumerate(scores_right):
    ax2.text(i, score + 2, str(score), ha='center', fontsize=12, fontweight='bold')

plt.tight_layout()
plt.show()

print("ğŸ’¡ í•µì‹¬ êµí›ˆ:")
print("   â€¢ ê°™ì€ ë¬¸ì œë§Œ ë°˜ë³µí•˜ë©´ ì‹¤ì „ì—ì„œ ì‹¤íŒ¨í•©ë‹ˆë‹¤")
print("   â€¢ ë‹¤ì–‘í•œ ë¬¸ì œë¡œ ì—°ìŠµí•´ì•¼ ì‹¤ë ¥ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤")
print("   â€¢ ë¨¸ì‹ ëŸ¬ë‹ë„ ë§ˆì°¬ê°€ì§€! ë‹¤ì–‘í•œ ë°ì´í„°ë¡œ ê²€ì¦í•´ì•¼ í•©ë‹ˆë‹¤")
```

### ë°ì´í„° ë¶„í• ì˜ ì›ì¹™

```python
# ë°ì´í„° ë¶„í•  ë¹„ìœ¨ ì‹œê°í™”
fig, ax = plt.subplots(figsize=(10, 4))

# ë°ì´í„° ë¶„í•  ë¹„ìœ¨
sections = [0.6, 0.2, 0.2]
labels = ['í›ˆë ¨ ë°ì´í„° (60%)', 'ê²€ì¦ ë°ì´í„° (20%)', 'í…ŒìŠ¤íŠ¸ ë°ì´í„° (20%)']
colors = ['#3498db', '#e74c3c', '#2ecc71']
explode = (0.05, 0.05, 0.05)

# ë§‰ëŒ€ ê·¸ë˜í”„ë¡œ í‘œí˜„
left = 0
for i, (section, label, color) in enumerate(zip(sections, labels, colors)):
    ax.barh(0, section, left=left, height=0.5, color=color, 
            edgecolor='black', linewidth=2)
    ax.text(left + section/2, 0, label, ha='center', va='center', 
            fontsize=12, fontweight='bold', color='white')
    left += section

ax.set_xlim(0, 1)
ax.set_ylim(-0.5, 0.5)
ax.axis('off')
ax.set_title('ì˜¬ë°”ë¥¸ ë°ì´í„° ë¶„í•  ë¹„ìœ¨', fontsize=16, fontweight='bold', pad=20)

# ì„¤ëª… ì¶”ê°€
descriptions = [
    "ëª¨ë¸ í•™ìŠµìš©\níŒ¨í„´ì„ ì°¾ëŠ”ë‹¤",
    "í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹\nëª¨ë¸ ì„ íƒ",
    "ìµœì¢… í‰ê°€ìš©\nì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠëŠ”ë‹¤!"
]

for i, (section, desc) in enumerate(zip(sections, descriptions)):
    ax.text(sum(sections[:i]) + section/2, -0.35, desc, 
           ha='center', va='top', fontsize=10, style='italic')

plt.tight_layout()
plt.show()

# ë°ì´í„° ë¶„í•  ì˜ˆì‹œ
print("\n=== ì‹¤ì œ ë°ì´í„° ë¶„í•  ì˜ˆì‹œ ===")
X = np.random.randn(1000, 5)
y = np.random.randint(0, 2, 1000)

# 1ì°¨ ë¶„í• : í›ˆë ¨+ê²€ì¦ vs í…ŒìŠ¤íŠ¸
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2ì°¨ ë¶„í• : í›ˆë ¨ vs ê²€ì¦
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

print(f"ì „ì²´ ë°ì´í„°: {len(X)}ê°œ")
print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ ({len(X_train)/len(X)*100:.0f}%)")
print(f"ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ ({len(X_val)/len(X)*100:.0f}%)")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ ({len(X_test)/len(X)*100:.0f}%)")
```

## 5.4.2 êµì°¨ ê²€ì¦ (Cross-Validation)

### K-Fold êµì°¨ ê²€ì¦ì˜ ì›ë¦¬

ë‹¨ í•œ ë²ˆì˜ ê²€ì¦ìœ¼ë¡œëŠ” ìš´ì´ ì¢‹ê±°ë‚˜ ë‚˜ì  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **ì—¬ëŸ¬ ë²ˆ ê²€ì¦**í•´ì„œ ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# K-Fold êµì°¨ ê²€ì¦ ì‹œê°í™”
fig, axes = plt.subplots(5, 1, figsize=(10, 8))
n_samples = 100
n_folds = 5

# ë°ì´í„° ì¸ë±ìŠ¤
indices = np.arange(n_samples)
kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):
    ax = axes[fold]
    
    # ì „ì²´ ë°ì´í„°ë¥¼ íšŒìƒ‰ìœ¼ë¡œ
    ax.barh(0, n_samples, height=0.5, color='lightgray', edgecolor='black')
    
    # í›ˆë ¨ ë°ì´í„°ëŠ” íŒŒë€ìƒ‰
    for idx in train_idx:
        ax.barh(0, 1, left=idx, height=0.5, color='#3498db', edgecolor='none')
    
    # ê²€ì¦ ë°ì´í„°ëŠ” ë¹¨ê°„ìƒ‰
    for idx in val_idx:
        ax.barh(0, 1, left=idx, height=0.5, color='#e74c3c', edgecolor='none')
    
    ax.set_xlim(0, n_samples)
    ax.set_ylim(-0.5, 0.5)
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f'Fold {fold + 1}', fontsize=12, loc='left')
    
    # ë²”ë¡€ ì¶”ê°€ (ì²« ë²ˆì§¸ foldì—ë§Œ)
    if fold == 0:
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='#3498db', label='í›ˆë ¨ ë°ì´í„°'),
                         Patch(facecolor='#e74c3c', label='ê²€ì¦ ë°ì´í„°')]
        ax.legend(handles=legend_elements, loc='upper right')

plt.suptitle('5-Fold êµì°¨ ê²€ì¦: ë°ì´í„°ë¥¼ 5ë²ˆ ë‹¤ë¥´ê²Œ ë‚˜ëˆ„ì–´ ê²€ì¦', 
             fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

print("ğŸ’¡ K-Fold êµì°¨ ê²€ì¦ì˜ ì¥ì :")
print("   â€¢ ëª¨ë“  ë°ì´í„°ê°€ í›ˆë ¨ê³¼ ê²€ì¦ì— ì‚¬ìš©ë¨")
print("   â€¢ 5ë²ˆì˜ í‰ê°€ë¡œ ë” ì•ˆì •ì ì¸ ì„±ëŠ¥ ì¸¡ì •")
print("   â€¢ ë°ì´í„°ê°€ ì ì„ ë•Œ íŠ¹íˆ ìœ ìš©!")
```

### êµì°¨ ê²€ì¦ ì‹¤ìŠµ

```python
# Iris ë°ì´í„°ë¡œ êµì°¨ ê²€ì¦ ì‹¤ìŠµ
iris = load_iris()
X, y = iris.data, iris.target

# ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Decision Tree': DecisionTreeClassifier(max_depth=3, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=10, random_state=42)
}

# êµì°¨ ê²€ì¦ ìˆ˜í–‰
cv_results = []
for name, model in models.items():
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    cv_results.append({
        'Model': name,
        'Scores': scores,
        'Mean': scores.mean(),
        'Std': scores.std()
    })
    
    print(f"\n{name}:")
    print(f"   ê° Fold ì ìˆ˜: {scores.round(3)}")
    print(f"   í‰ê·  ì ìˆ˜: {scores.mean():.3f} (Â±{scores.std():.3f})")

# ê²°ê³¼ ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# ë°•ìŠ¤í”Œë¡¯
all_scores = [result['Scores'] for result in cv_results]
model_names = [result['Model'] for result in cv_results]

bp = ax1.boxplot(all_scores, labels=model_names, patch_artist=True)
for patch, color in zip(bp['boxes'], ['#3498db', '#e74c3c', '#2ecc71']):
    patch.set_facecolor(color)
    patch.set_alpha(0.7)

ax1.set_ylabel('ì •í™•ë„', fontsize=12)
ax1.set_title('ëª¨ë¸ë³„ êµì°¨ ê²€ì¦ ì ìˆ˜ ë¶„í¬', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')

# í‰ê· ê³¼ í‘œì¤€í¸ì°¨
means = [result['Mean'] for result in cv_results]
stds = [result['Std'] for result in cv_results]

x = np.arange(len(model_names))
ax2.bar(x, means, yerr=stds, capsize=10, alpha=0.7, 
        color=['#3498db', '#e74c3c', '#2ecc71'], edgecolor='black')
ax2.set_xticks(x)
ax2.set_xticklabels(model_names)
ax2.set_ylabel('í‰ê·  ì •í™•ë„', fontsize=12)
ax2.set_title('ëª¨ë¸ë³„ í‰ê·  ì„±ëŠ¥ê³¼ í‘œì¤€í¸ì°¨', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# ì ìˆ˜ í‘œì‹œ
for i, (mean, std) in enumerate(zip(means, stds)):
    ax2.text(i, mean + std + 0.01, f'{mean:.3f}', ha='center', fontsize=10)

plt.tight_layout()
plt.show()
```

### Stratified K-Fold: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°

```python
# ë¶ˆê· í˜• ë°ì´í„° ìƒì„±
np.random.seed(42)
n_samples = 1000
n_class_0 = 900  # 90%
n_class_1 = 100  # 10%

X_imbalanced = np.vstack([
    np.random.randn(n_class_0, 2),
    np.random.randn(n_class_1, 2) + 2
])
y_imbalanced = np.array([0] * n_class_0 + [1] * n_class_1)

# ì¼ë°˜ K-Fold vs Stratified K-Fold ë¹„êµ
fig, axes = plt.subplots(2, 5, figsize=(15, 6))

# ì¼ë°˜ K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
for fold, (train_idx, val_idx) in enumerate(kf.split(X_imbalanced)):
    ax = axes[0, fold]
    
    # ê²€ì¦ ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬
    val_y = y_imbalanced[val_idx]
    class_counts = np.bincount(val_y)
    
    ax.bar(['Class 0', 'Class 1'], class_counts, color=['blue', 'red'], alpha=0.7)
    ax.set_title(f'Fold {fold + 1}')
    ax.set_ylim(0, 200)
    
    # í´ë˜ìŠ¤ 1 ë¹„ìœ¨ í‘œì‹œ
    class_1_ratio = class_counts[1] / len(val_y) * 100
    ax.text(0.5, 0.9, f'Class 1: {class_1_ratio:.1f}%', 
           transform=ax.transAxes, ha='center')

axes[0, 0].set_ylabel('ì¼ë°˜ K-Fold\n(ë¶ˆê· í˜•!)', fontsize=12, fontweight='bold')

# Stratified K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for fold, (train_idx, val_idx) in enumerate(skf.split(X_imbalanced, y_imbalanced)):
    ax = axes[1, fold]
    
    # ê²€ì¦ ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬
    val_y = y_imbalanced[val_idx]
    class_counts = np.bincount(val_y)
    
    ax.bar(['Class 0', 'Class 1'], class_counts, color=['blue', 'red'], alpha=0.7)
    ax.set_title(f'Fold {fold + 1}')
    ax.set_ylim(0, 200)
    
    # í´ë˜ìŠ¤ 1 ë¹„ìœ¨ í‘œì‹œ
    class_1_ratio = class_counts[1] / len(val_y) * 100
    ax.text(0.5, 0.9, f'Class 1: {class_1_ratio:.1f}%', 
           transform=ax.transAxes, ha='center')

axes[1, 0].set_ylabel('Stratified K-Fold\n(ê· í˜• ìœ ì§€!)', fontsize=12, fontweight='bold')

plt.suptitle('í´ë˜ìŠ¤ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œì˜ êµì°¨ ê²€ì¦', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print("\nğŸ’¡ Stratified K-Foldì˜ ì¤‘ìš”ì„±:")
print("   â€¢ ì›ë³¸ ë°ì´í„°ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ê° Foldì—ì„œ ìœ ì§€")
print("   â€¢ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ í•„ìˆ˜ì !")
print("   â€¢ ë” ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€ ê²°ê³¼")
```

## 5.4.3 ê³¼ì í•©ê³¼ ê³¼ì†Œì í•© ì§„ë‹¨

### í•™ìŠµ ê³¡ì„ ìœ¼ë¡œ ë¬¸ì œ ì§„ë‹¨í•˜ê¸°

ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•˜ë©´ **ê³¼ì í•©**, ë„ˆë¬´ ë‹¨ìˆœí•˜ë©´ **ê³¼ì†Œì í•©**ì´ ë°œìƒí•©ë‹ˆë‹¤:

```python
# ê³¼ì í•©, ì ì ˆí•œ ì í•©, ê³¼ì†Œì í•© ì‹œë®¬ë ˆì´ì…˜
np.random.seed(42)
n_samples = 100
X_sim = np.sort(np.random.uniform(0, 5, n_samples))
y_true = 2 * np.sin(X_sim) + X_sim
y_sim = y_true + np.random.normal(0, 0.5, n_samples)

# ì„¸ ê°€ì§€ ëª¨ë¸ ì¤€ë¹„
X_sim_reshape = X_sim.reshape(-1, 1)

# 1. ê³¼ì†Œì í•©: 1ì°¨ ë‹¤í•­ì‹
poly1 = PolynomialFeatures(degree=1)
X_poly1 = poly1.fit_transform(X_sim_reshape)
model_under = Ridge(alpha=0)
model_under.fit(X_poly1, y_sim)

# 2. ì ì ˆí•œ ì í•©: 3ì°¨ ë‹¤í•­ì‹
poly3 = PolynomialFeatures(degree=3)
X_poly3 = poly3.fit_transform(X_sim_reshape)
model_good = Ridge(alpha=0.1)
model_good.fit(X_poly3, y_sim)

# 3. ê³¼ì í•©: 15ì°¨ ë‹¤í•­ì‹
poly15 = PolynomialFeatures(degree=15)
X_poly15 = poly15.fit_transform(X_sim_reshape)
model_over = Ridge(alpha=0)
model_over.fit(X_poly15, y_sim)

# ì˜ˆì¸¡ì„ ìœ„í•œ ì„¸ë°€í•œ X ê°’
X_plot = np.linspace(0, 5, 300).reshape(-1, 1)

# ì‹œê°í™”
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

models = [
    (model_under, poly1, 'ê³¼ì†Œì í•©\n(ë„ˆë¬´ ë‹¨ìˆœ)', 'red'),
    (model_good, poly3, 'ì ì ˆí•œ ì í•©\n(ê· í˜•)', 'green'),
    (model_over, poly15, 'ê³¼ì í•©\n(ë„ˆë¬´ ë³µì¡)', 'purple')
]

for ax, (model, poly, title, color) in zip(axes, models):
    # ì˜ˆì¸¡
    X_plot_poly = poly.transform(X_plot)
    y_pred = model.predict(X_plot_poly)
    
    # í›ˆë ¨ ë°ì´í„° ì˜ˆì¸¡
    X_train_poly = poly.transform(X_sim_reshape)
    y_train_pred = model.predict(X_train_poly)
    train_error = mean_squared_error(y_sim, y_train_pred)
    
    # í”Œë¡¯
    ax.scatter(X_sim, y_sim, alpha=0.5, s=30, label='ë°ì´í„°')
    ax.plot(X_sim, y_true, 'g--', linewidth=2, alpha=0.7, label='ì‹¤ì œ í•¨ìˆ˜')
    ax.plot(X_plot, y_pred, color=color, linewidth=2, label='ëª¨ë¸ ì˜ˆì¸¡')
    
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel('X')
    ax.set_ylabel('y')
    ax.legend()
    ax.set_ylim(-5, 10)
    
    # MSE í‘œì‹œ
    ax.text(0.05, 0.95, f'í›ˆë ¨ MSE: {train_error:.2f}', 
           transform=ax.transAxes, bbox=dict(boxstyle='round', 
           facecolor='yellow', alpha=0.7))

plt.tight_layout()
plt.show()

print("\nğŸ” ì§„ë‹¨ ê²°ê³¼:")
print("   â€¢ ê³¼ì†Œì í•©: í›ˆë ¨ ì˜¤ì°¨ë„ í¬ê³ , í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ë„ í¼")
print("   â€¢ ì ì ˆí•œ ì í•©: í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ê°€ ëª¨ë‘ ì ë‹¹í•¨")
print("   â€¢ ê³¼ì í•©: í›ˆë ¨ ì˜¤ì°¨ëŠ” ì‘ì§€ë§Œ, í…ŒìŠ¤íŠ¸ ì˜¤ì°¨ê°€ í¼")
```

### í•™ìŠµ ê³¡ì„  (Learning Curves)

í•™ìŠµ ê³¡ì„ ì€ **ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”**ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤:

```python
# í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸° í•¨ìˆ˜
def plot_learning_curves(model, X, y, title):
    train_sizes, train_scores, val_scores = learning_curve(
        model, X, y, cv=5, n_jobs=-1,
        train_sizes=np.linspace(0.1, 1.0, 10),
        scoring='neg_mean_squared_error'
    )
    
    # MSEë¥¼ ì–‘ìˆ˜ë¡œ ë³€í™˜
    train_scores = -train_scores
    val_scores = -val_scores
    
    # í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
    train_mean = train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    plt.figure(figsize=(10, 6))
    
    # í‰ê·  ê³¡ì„ 
    plt.plot(train_sizes, train_mean, 'o-', color='blue', 
            label='í›ˆë ¨ ì˜¤ì°¨', linewidth=2, markersize=8)
    plt.plot(train_sizes, val_mean, 'o-', color='red', 
            label='ê²€ì¦ ì˜¤ì°¨', linewidth=2, markersize=8)
    
    # ì‹ ë¢°êµ¬ê°„
    plt.fill_between(train_sizes, train_mean - train_std, 
                    train_mean + train_std, alpha=0.2, color='blue')
    plt.fill_between(train_sizes, val_mean - val_std, 
                    val_mean + val_std, alpha=0.2, color='red')
    
    plt.xlabel('í›ˆë ¨ ë°ì´í„° í¬ê¸°', fontsize=12)
    plt.ylabel('í‰ê·  ì œê³± ì˜¤ì°¨ (MSE)', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(loc='upper right')
    plt.grid(True, alpha=0.3)
    
    return train_mean, val_mean

# ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ë°ì´í„°ë¡œ í•™ìŠµ ê³¡ì„  ë¶„ì„
california = fetch_california_housing()
X_cal, y_cal = california.data[:1000], california.target[:1000]  # ì¼ë¶€ë§Œ ì‚¬ìš©

# í‘œì¤€í™”
scaler = StandardScaler()
X_cal_scaled = scaler.fit_transform(X_cal)

# ì„¸ ê°€ì§€ ë³µì¡ë„ì˜ ëª¨ë¸
models_complexity = [
    ('ê³¼ì†Œì í•© ëª¨ë¸ (ì„ í˜•)', Ridge(alpha=100)),
    ('ì ì ˆí•œ ëª¨ë¸', Ridge(alpha=1)),
    ('ê³¼ì í•© ëª¨ë¸ (ë³µì¡)', Ridge(alpha=0.001))
]

fig, axes = plt.subplots(1, 3, figsize=(18, 5))

for ax, (name, model) in zip(axes, models_complexity):
    plt.sca(ax)
    train_mean, val_mean = plot_learning_curves(model, X_cal_scaled, y_cal, name)
    
    # ì§„ë‹¨ ê²°ê³¼ ì¶”ê°€
    gap = val_mean[-1] - train_mean[-1]
    if gap > 0.5:
        diagnosis = "ê³¼ì í•© ì§•í›„"
        color = 'red'
    elif train_mean[-1] > 0.8:
        diagnosis = "ê³¼ì†Œì í•© ì§•í›„"
        color = 'orange'
    else:
        diagnosis = "ì ì ˆí•œ ì í•©"
        color = 'green'
    
    ax.text(0.5, 0.95, diagnosis, transform=ax.transAxes, 
           ha='center', va='top', fontsize=12, fontweight='bold',
           bbox=dict(boxstyle='round', facecolor=color, alpha=0.3))

plt.tight_layout()
plt.show()

print("\nğŸ“Š í•™ìŠµ ê³¡ì„  í•´ì„ë²•:")
print("   1. í›ˆë ¨/ê²€ì¦ ì˜¤ì°¨ê°€ ëª¨ë‘ ë†’ìŒ â†’ ê³¼ì†Œì í•© (ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœ)")
print("   2. í›ˆë ¨ ì˜¤ì°¨ëŠ” ë‚®ì€ë° ê²€ì¦ ì˜¤ì°¨ê°€ ë†’ìŒ â†’ ê³¼ì í•© (ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡)")
print("   3. ë‘˜ ë‹¤ ë‚®ê³  ì°¨ì´ê°€ ì‘ìŒ â†’ ì ì ˆí•œ ì í•©")
print("   4. ë°ì´í„°ë¥¼ ëŠ˜ë ¤ë„ ê°œì„ ì´ ì—†ìœ¼ë©´ â†’ ëª¨ë¸ ë³µì¡ë„ë¥¼ ë†’ì—¬ì•¼ í•¨")
```

## 5.4.4 í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„

### í™œì˜ê¸° ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°

í¸í–¥(Bias)ê³¼ ë¶„ì‚°(Variance)ì˜ ê´€ê³„ë¥¼ í™œì˜ê¸°ì— ë¹„ìœ í•´ë´…ì‹œë‹¤:

```python
# í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# ê³¼ë… ì¤‘ì‹¬
center = np.array([0, 0])

# 4ê°€ì§€ ê²½ìš°ì˜ í™”ì‚´ ìœ„ì¹˜ ìƒì„±
np.random.seed(42)

# 1. ë‚®ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚° (ì´ìƒì )
low_bias_low_var = np.random.normal(center, 0.3, (20, 2))

# 2. ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚°
high_bias_low_var = np.random.normal(center + [2, 2], 0.3, (20, 2))

# 3. ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚°
low_bias_high_var = np.random.normal(center, 1.5, (20, 2))

# 4. ë†’ì€ í¸í–¥, ë†’ì€ ë¶„ì‚° (ìµœì•…)
high_bias_high_var = np.random.normal(center + [2, 2], 1.5, (20, 2))

cases = [
    (low_bias_low_var, "ë‚®ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚°\n(ì´ìƒì !)", 'green'),
    (high_bias_low_var, "ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚°\n(ì¼ê´€ë˜ê²Œ ë¹—ë‚˜ê°)", 'orange'),
    (low_bias_high_var, "ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚°\n(ë¶ˆì•ˆì •í•¨)", 'blue'),
    (high_bias_high_var, "ë†’ì€ í¸í–¥, ë†’ì€ ë¶„ì‚°\n(ìµœì•…ì˜ ê²½ìš°)", 'red')
]

for ax, (points, title, color) in zip(axes.flat, cases):
    # ê³¼ë… ê·¸ë¦¬ê¸°
    for radius in [3, 2, 1]:
        circle = plt.Circle(center, radius, fill=False, 
                          edgecolor='gray', linewidth=1)
        ax.add_patch(circle)
    
    # ì¤‘ì‹¬ì 
    ax.plot(0, 0, 'ko', markersize=10)
    
    # í™”ì‚´ ìœ„ì¹˜
    ax.scatter(points[:, 0], points[:, 1], s=100, alpha=0.7, 
              color=color, edgecolor='black')
    
    # í‰ê·  ìœ„ì¹˜
    mean_point = points.mean(axis=0)
    ax.plot(mean_point[0], mean_point[1], 'r*', markersize=20, 
           label='í‰ê·  ìœ„ì¹˜')
    
    ax.set_xlim(-4, 4)
    ax.set_ylim(-4, 4)
    ax.set_aspect('equal')
    ax.set_title(title, fontsize=12, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
    ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)

plt.tight_layout()
plt.show()

# ëª¨ë¸ ë³µì¡ë„ì— ë”°ë¥¸ í¸í–¥-ë¶„ì‚° ë³€í™”
complexity = np.linspace(0, 10, 100)
bias_squared = 10 / (complexity + 1)**2
variance = complexity**1.5 / 10
total_error = bias_squared + variance

plt.figure(figsize=(10, 6))
plt.plot(complexity, bias_squared, 'b-', linewidth=2, label='í¸í–¥Â²')
plt.plot(complexity, variance, 'r-', linewidth=2, label='ë¶„ì‚°')
plt.plot(complexity, total_error, 'g-', linewidth=3, label='ì´ ì˜¤ì°¨')

# ìµœì ì  í‘œì‹œ
optimal_idx = np.argmin(total_error)
plt.plot(complexity[optimal_idx], total_error[optimal_idx], 'go', 
        markersize=15, label='ìµœì  ë³µì¡ë„')
plt.axvline(x=complexity[optimal_idx], color='gray', linestyle='--', alpha=0.5)

plt.xlabel('ëª¨ë¸ ë³µì¡ë„', fontsize=12)
plt.ylabel('ì˜¤ì°¨', fontsize=12)
plt.title('í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„', fontsize=14, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# ì˜ì—­ë³„ ì„¤ëª… ì¶”ê°€
plt.text(1, 8, 'ê³¼ì†Œì í•©\n(ë†’ì€ í¸í–¥)', fontsize=11, ha='center', 
        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
plt.text(8, 8, 'ê³¼ì í•©\n(ë†’ì€ ë¶„ì‚°)', fontsize=11, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
plt.text(complexity[optimal_idx], -1, 'ìµœì ì ', fontsize=11, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))

plt.tight_layout()
plt.show()

print("\nğŸ’¡ í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ í•µì‹¬:")
print("   â€¢ í¸í–¥(Bias): ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì‹¤ì œê°’ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ëŠ”ê°€")
print("   â€¢ ë¶„ì‚°(Variance): ë‹¤ë¥¸ ë°ì´í„°ì—ì„œ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ë‹¬ë¼ì§€ëŠ”ê°€")
print("   â€¢ ë‹¨ìˆœí•œ ëª¨ë¸: ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚°")
print("   â€¢ ë³µì¡í•œ ëª¨ë¸: ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚°")
print("   â€¢ ëª©í‘œ: ë‘˜ ì‚¬ì´ì˜ ìµœì  ê· í˜•ì  ì°¾ê¸°!")
```

## 5.4.5 í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

### ê²€ì¦ ê³¡ì„ ìœ¼ë¡œ ìµœì ê°’ ì°¾ê¸°

ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì¡°ì •í•´ë´…ì‹œë‹¤:

```python
# ê²€ì¦ ê³¡ì„  ì˜ˆì‹œ: Ridge íšŒê·€ì˜ alpha íŒŒë¼ë¯¸í„°
param_range = np.logspace(-3, 3, 20)
train_scores, val_scores = validation_curve(
    Ridge(), X_cal_scaled, y_cal, 
    param_name='alpha', param_range=param_range,
    cv=5, scoring='neg_mean_squared_error'
)

# MSEë¥¼ ì–‘ìˆ˜ë¡œ ë³€í™˜í•˜ê³  RMSEë¡œ ê³„ì‚°
train_rmse = np.sqrt(-train_scores)
val_rmse = np.sqrt(-val_scores)

# í‰ê· ê³¼ í‘œì¤€í¸ì°¨
train_mean = train_rmse.mean(axis=1)
train_std = train_rmse.std(axis=1)
val_mean = val_rmse.mean(axis=1)
val_std = val_rmse.std(axis=1)

plt.figure(figsize=(10, 6))

# í‰ê·  ê³¡ì„ 
plt.semilogx(param_range, train_mean, 'b-', linewidth=2, 
            label='í›ˆë ¨ RMSE', marker='o', markersize=8)
plt.semilogx(param_range, val_mean, 'r-', linewidth=2, 
            label='ê²€ì¦ RMSE', marker='o', markersize=8)

# ì‹ ë¢°êµ¬ê°„
plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, 
                alpha=0.2, color='blue')
plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, 
                alpha=0.2, color='red')

# ìµœì ê°’ í‘œì‹œ
best_idx = np.argmin(val_mean)
best_alpha = param_range[best_idx]
plt.axvline(x=best_alpha, color='green', linestyle='--', linewidth=2)
plt.plot(best_alpha, val_mean[best_idx], 'go', markersize=15)

plt.xlabel('ì •ê·œí™” ê°•ë„ (alpha)', fontsize=12)
plt.ylabel('RMSE', fontsize=12)
plt.title('ê²€ì¦ ê³¡ì„ : Ridge íšŒê·€ì˜ ìµœì  Alpha ì°¾ê¸°', fontsize=14, fontweight='bold')
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)

# ì˜ì—­ë³„ ì„¤ëª…
plt.text(0.0001, val_mean[0] + 0.1, 'ê³¼ì í•©\n(alpha ë„ˆë¬´ ì‘ìŒ)', 
        fontsize=10, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
plt.text(100, val_mean[-1] + 0.1, 'ê³¼ì†Œì í•©\n(alpha ë„ˆë¬´ í¼)', 
        fontsize=10, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))
plt.text(best_alpha, val_mean[best_idx] - 0.15, f'ìµœì : Î±={best_alpha:.3f}', 
        fontsize=10, ha='center',
        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))

plt.tight_layout()
plt.show()

print(f"\nğŸ¯ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
print(f"   â€¢ ìµœì  alpha ê°’: {best_alpha:.3f}")
print(f"   â€¢ ê²€ì¦ RMSE: {val_mean[best_idx]:.3f}")
```

### ì—¬ëŸ¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì‹œ íŠœë‹

```python
# Decision Treeì˜ ë‘ ê°€ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
from sklearn.model_selection import GridSearchCV

# Wine ë°ì´í„°ì…‹ ì‚¬ìš©
wine = load_wine()
X_wine, y_wine = wine.data, wine.target

# íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
param_grid = {
    'max_depth': [2, 3, 4, 5, 6],
    'min_samples_split': [2, 5, 10, 20, 30]
}

# GridSearchCV
dt = DecisionTreeClassifier(random_state=42)
grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
grid_search.fit(X_wine, y_wine)

# ê²°ê³¼ë¥¼ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
results = grid_search.cv_results_
scores = results['mean_test_score'].reshape(5, 5)

plt.figure(figsize=(10, 8))
sns.heatmap(scores, annot=True, fmt='.3f', cmap='YlOrRd',
            xticklabels=param_grid['min_samples_split'],
            yticklabels=param_grid['max_depth'])
plt.xlabel('min_samples_split', fontsize=12)
plt.ylabel('max_depth', fontsize=12)
plt.title('Decision Tree í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„œì¹˜', fontsize=14, fontweight='bold')

# ìµœì  íŒŒë¼ë¯¸í„° í‘œì‹œ
best_params = grid_search.best_params_
best_i = param_grid['max_depth'].index(best_params['max_depth'])
best_j = param_grid['min_samples_split'].index(best_params['min_samples_split'])
plt.plot(best_j + 0.5, best_i + 0.5, 'ws', markersize=20, 
        markeredgecolor='black', markeredgewidth=3)

plt.tight_layout()
plt.show()

print(f"\nğŸ† ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼:")
print(f"   â€¢ ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"   â€¢ ìµœê³  ì ìˆ˜: {grid_search.best_score_:.3f}")
print(f"   â€¢ ì´ {len(param_grid['max_depth']) * len(param_grid['min_samples_split'])}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸")
```

## 5.4.6 ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€

ì´ì œ ë°°ìš´ ëª¨ë“  ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì²´ê³„ì ì¸ ëª¨ë¸ í‰ê°€ë¥¼ ìˆ˜í–‰í•´ë´…ì‹œë‹¤:

```python
# í”„ë¡œì íŠ¸: Wine í’ˆì§ˆ ë¶„ë¥˜ ëª¨ë¸ ê°œë°œ ë° í‰ê°€
print("=== ğŸ· Wine í’ˆì§ˆ ë¶„ë¥˜ í”„ë¡œì íŠ¸ ===\n")
print("ëª©í‘œ: ì™€ì¸ì˜ í™”í•™ì  íŠ¹ì„±ìœ¼ë¡œ í’ˆì§ˆ ë“±ê¸‰ì„ ì˜ˆì¸¡")
print("ê³¼ì œ: ìµœì ì˜ ëª¨ë¸ì„ ì²´ê³„ì ìœ¼ë¡œ ì„ íƒí•˜ê¸°\n")

# 1ë‹¨ê³„: ë°ì´í„° ì¤€ë¹„
print("1ï¸âƒ£ ë°ì´í„° ì¤€ë¹„ ë° íƒìƒ‰")
print("-" * 50)

X_train, X_test, y_train, y_test = train_test_split(
    X_wine, y_wine, test_size=0.2, random_state=42, stratify=y_wine
)

print(f"ì „ì²´ ë°ì´í„°: {len(X_wine)}ê°œ")
print(f"í›ˆë ¨ ë°ì´í„°: {len(X_train)}ê°œ")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê°œ (ìµœì¢… í‰ê°€ìš©, ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ!)")
print(f"í´ë˜ìŠ¤ ë¶„í¬: {np.bincount(y_wine)}")

# 2ë‹¨ê³„: ê¸°ë³¸ ëª¨ë¸ ë¹„êµ (êµì°¨ ê²€ì¦)
print("\n2ï¸âƒ£ ê¸°ë³¸ ëª¨ë¸ ë¹„êµ (5-Fold êµì°¨ ê²€ì¦)")
print("-" * 50)

models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

cv_results = []
for name, model in models.items():
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_results.append({
        'Model': name,
        'Mean CV Score': scores.mean(),
        'Std': scores.std(),
        'Scores': scores
    })
    print(f"{name}: {scores.mean():.3f} (Â±{scores.std():.3f})")

# 3ë‹¨ê³„: ìµœê³  ëª¨ë¸ì˜ í•™ìŠµ ê³¡ì„  ë¶„ì„
print("\n3ï¸âƒ£ Random Forest í•™ìŠµ ê³¡ì„  ë¶„ì„")
print("-" * 50)

best_model = RandomForestClassifier(n_estimators=100, random_state=42)
train_sizes, train_scores, val_scores = learning_curve(
    best_model, X_train, y_train, cv=5,
    train_sizes=np.linspace(0.1, 1.0, 10),
    scoring='accuracy', n_jobs=-1
)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', 
        color='blue', label='í›ˆë ¨ ì •í™•ë„', linewidth=2)
plt.plot(train_sizes, val_scores.mean(axis=1), 'o-', 
        color='red', label='ê²€ì¦ ì •í™•ë„', linewidth=2)
plt.fill_between(train_sizes, 
                train_scores.mean(axis=1) - train_scores.std(axis=1),
                train_scores.mean(axis=1) + train_scores.std(axis=1), 
                alpha=0.2, color='blue')
plt.fill_between(train_sizes, 
                val_scores.mean(axis=1) - val_scores.std(axis=1),
                val_scores.mean(axis=1) + val_scores.std(axis=1), 
                alpha=0.2, color='red')
plt.xlabel('í›ˆë ¨ ë°ì´í„° í¬ê¸°', fontsize=12)
plt.ylabel('ì •í™•ë„', fontsize=12)
plt.title('Random Forest í•™ìŠµ ê³¡ì„ ', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# 4ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
print("\n4ï¸âƒ£ Random Forest í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
print("-" * 50)

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid, cv=5, scoring='accuracy', n_jobs=-1
)
grid_search.fit(X_train, y_train)

print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœê³  CV ì ìˆ˜: {grid_search.best_score_:.3f}")

# 5ë‹¨ê³„: ìµœì¢… ëª¨ë¸ í‰ê°€
print("\n5ï¸âƒ£ ìµœì¢… ëª¨ë¸ í‰ê°€ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸)")
print("-" * 50)

final_model = grid_search.best_estimator_
test_score = final_model.score(X_test, y_test)
train_score = final_model.score(X_train, y_train)

print(f"í›ˆë ¨ ì •í™•ë„: {train_score:.3f}")
print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.3f}")
print(f"ê³¼ì í•© ì •ë„: {train_score - test_score:.3f}")

if train_score - test_score < 0.05:
    print("âœ… ê³¼ì í•©ì´ ê±°ì˜ ì—†ëŠ” ì¢‹ì€ ëª¨ë¸ì…ë‹ˆë‹¤!")
else:
    print("âš ï¸ ì•½ê°„ì˜ ê³¼ì í•©ì´ ìˆìŠµë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")

# 6ë‹¨ê³„: ëª¨ë¸ í‰ê°€ ìš”ì•½ ë¦¬í¬íŠ¸
print("\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¢…í•© ë¦¬í¬íŠ¸")
print("=" * 50)

report = pd.DataFrame({
    'Metric': ['Initial CV Score', 'Optimized CV Score', 'Final Test Score', 
               'Overfitting Gap', 'Total Evaluation Time'],
    'Value': [
        f"{cv_results[2]['Mean CV Score']:.3f}",
        f"{grid_search.best_score_:.3f}",
        f"{test_score:.3f}",
        f"{train_score - test_score:.3f}",
        "~2ë¶„"
    ]
})
print(report.to_string(index=False))

print("\nğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸:")
print("   1. Random Forestê°€ ê°€ì¥ ì¢‹ì€ ê¸°ë³¸ ì„±ëŠ¥ì„ ë³´ì„")
print("   2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ ")
print("   3. ê³¼ì í•©ì´ ì ì–´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ìŒ")
print("   4. ìµœì¢… ëª¨ë¸ì€ ì‹¤ì œ ì„œë¹„ìŠ¤ì— ë°°í¬ ê°€ëŠ¥í•œ ìˆ˜ì¤€")
```

## ğŸ¯ ì§ì ‘ í•´ë³´ê¸°

### ì—°ìŠµ ë¬¸ì œ 1: êµì°¨ ê²€ì¦ êµ¬í˜„
```python
# ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ 3-Fold êµì°¨ ê²€ì¦ì„ ìˆ˜ë™ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì„¸ìš”
X_practice = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])
y_practice = np.array([0, 1, 0, 1, 0, 1])

# TODO: KFoldë¥¼ ì‚¬ìš©í•˜ì—¬ 3-Fold êµì°¨ ê²€ì¦ ìˆ˜í–‰
# 1. KFold ê°ì²´ ìƒì„± (n_splits=3)
# 2. ê° foldì—ì„œ í›ˆë ¨/ê²€ì¦ ì¸ë±ìŠ¤ ì¶œë ¥
# 3. ê° foldì˜ ë°ì´í„° í¬ê¸° í™•ì¸

# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

### ì—°ìŠµ ë¬¸ì œ 2: í•™ìŠµ ê³¡ì„  í•´ì„
```python
# ë‹¤ìŒ í•™ìŠµ ê³¡ì„ ì„ ë³´ê³  ì§„ë‹¨í•´ë³´ì„¸ìš”
train_scores = [0.6, 0.7, 0.8, 0.85, 0.9, 0.92, 0.93, 0.94]
val_scores = [0.58, 0.65, 0.7, 0.72, 0.73, 0.73, 0.73, 0.73]
data_sizes = [50, 100, 200, 400, 600, 800, 1000, 1200]

# TODO: 
# 1. í•™ìŠµ ê³¡ì„ ì„ ê·¸ë ¤ë³´ì„¸ìš”
# 2. ê³¼ì í•©ì¸ì§€ ê³¼ì†Œì í•©ì¸ì§€ ì§„ë‹¨í•˜ì„¸ìš”
# 3. ê°œì„  ë°©ë²•ì„ ì œì•ˆí•˜ì„¸ìš”

# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

### ì—°ìŠµ ë¬¸ì œ 3: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```python
# Decision Treeì˜ max_depthë¥¼ ê²€ì¦ ê³¡ì„ ìœ¼ë¡œ ì°¾ì•„ë³´ì„¸ìš”
from sklearn.datasets import make_classification

# ê°€ìƒ ë°ì´í„° ìƒì„±
X_tune, y_tune = make_classification(n_samples=200, n_features=10, 
                                    n_informative=5, random_state=42)

# TODO:
# 1. max_depthë¥¼ 1ë¶€í„° 20ê¹Œì§€ ë³€í™”ì‹œí‚¤ë©° ê²€ì¦ ê³¡ì„  ê·¸ë¦¬ê¸°
# 2. ìµœì ì˜ max_depth ì°¾ê¸°
# 3. ê³¼ì í•©ì´ ì‹œì‘ë˜ëŠ” ì§€ì  íŒŒì•…

# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

## ğŸ“š í•µì‹¬ ì •ë¦¬

### âœ… ëª¨ë¸ í‰ê°€ì˜ í•µì‹¬ ì›ì¹™
1. **ë°ì´í„° ë¶„í• **
   - í›ˆë ¨(60%), ê²€ì¦(20%), í…ŒìŠ¤íŠ¸(20%)
   - í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” ìµœì¢… í‰ê°€ê¹Œì§€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€

2. **êµì°¨ ê²€ì¦**
   - K-Fold: ë°ì´í„°ë¥¼ Kê°œë¡œ ë‚˜ëˆ„ì–´ Kë²ˆ í‰ê°€
   - Stratified K-Fold: í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ (ë¶ˆê· í˜• ë°ì´í„° í•„ìˆ˜)
   - ë” ì•ˆì •ì ì´ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì¶”ì •

3. **ê³¼ì í•©/ê³¼ì†Œì í•© ì§„ë‹¨**
   - í•™ìŠµ ê³¡ì„ ìœ¼ë¡œ ë¬¸ì œ íŒŒì•…
   - ê³¼ì†Œì í•©: í›ˆë ¨/ê²€ì¦ ì˜¤ì°¨ ëª¨ë‘ ë†’ìŒ
   - ê³¼ì í•©: í›ˆë ¨ ì˜¤ì°¨ëŠ” ë‚®ê³  ê²€ì¦ ì˜¤ì°¨ëŠ” ë†’ìŒ

4. **í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„**
   - í¸í–¥: ì˜ˆì¸¡ì˜ ì •í™•ë„
   - ë¶„ì‚°: ì˜ˆì¸¡ì˜ ì¼ê´€ì„±
   - ìµœì ì : ë‘˜ ì‚¬ì´ì˜ ê· í˜•

5. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
   - ê²€ì¦ ê³¡ì„ : ë‹¨ì¼ íŒŒë¼ë¯¸í„° ìµœì í™”
   - ê·¸ë¦¬ë“œ ì„œì¹˜: ì—¬ëŸ¬ íŒŒë¼ë¯¸í„° ë™ì‹œ ìµœì í™”
   - êµì°¨ ê²€ì¦ê³¼ í•¨ê»˜ ì‚¬ìš©

### ğŸ’¡ ì‹¤ë¬´ íŒ
- í•­ìƒ êµì°¨ ê²€ì¦ìœ¼ë¡œ ëª¨ë¸ í‰ê°€
- í•™ìŠµ ê³¡ì„ ì„ ê·¸ë ¤ ë¬¸ì œ ì§„ë‹¨
- ë‹¨ìˆœí•œ ëª¨ë¸ë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ë³µì¡ë„ ì¦ê°€
- í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ëŠ” ë§ˆì§€ë§‰ì— ë‹¨ í•œ ë²ˆë§Œ ì‚¬ìš©

---

## ğŸš€ ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ”

**í”„ë¡œì íŠ¸: ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ ë° í‰ê°€**ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤:
- ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œ ì •ì˜
- ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ í‰ê°€ê¹Œì§€ ì „ì²´ ì›Œí¬í”Œë¡œìš°
- ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ë° ìµœì  ëª¨ë¸ ì„ íƒ
- ê²°ê³¼ í•´ì„ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ëª¨ë“  ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ **ì™„ì „í•œ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸**ë¥¼ ìˆ˜í–‰í•´ë´…ì‹œë‹¤!
