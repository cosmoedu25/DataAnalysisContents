# 6ì¥ Part 3: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
## ëª¨ë¸ ì„±ëŠ¥ì˜ ìˆ¨ê²¨ì§„ ì ì¬ë ¥ ê¹¨ìš°ê¸°

### í•™ìŠµ ëª©í‘œ
ì´ë²ˆ íŒŒíŠ¸ë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ëŠ¥ë ¥ì„ ê°–ê²Œ ë©ë‹ˆë‹¤:
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ê°œë…ê³¼ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ê·¸ë¦¬ë“œ ì„œì¹˜, ëœë¤ ì„œì¹˜, ë² ì´ì§€ì•ˆ ìµœì í™”ì˜ ì›ë¦¬ì™€ ì¥ë‹¨ì ì„ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- êµì°¨ ê²€ì¦ì„ í†µí•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í‰ê°€ ë°©ë²•ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³  êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì‹¤ì œ ë°ì´í„°ì…‹ì— ìµœì í™” ê¸°ë²•ì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

---

### 6.12 í•˜ì´í¼íŒŒë¼ë¯¸í„°ë€? - ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢Œìš°í•˜ëŠ” ë¹„ë°€ ì„¤ì •

#### ğŸ›ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° vs íŒŒë¼ë¯¸í„°

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë¼ë””ì˜¤ì— ë¹„ìœ í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¼ë””ì˜¤ì—ì„œ ì¢‹ì€ ìŒì§ˆë¡œ ë°©ì†¡ì„ ë“£ê¸° ìœ„í•´ì„œëŠ”:
- **ì£¼íŒŒìˆ˜ ì¡°ì •**: ì˜¬ë°”ë¥¸ ë°©ì†¡êµ­ì„ ì°¾ê¸° (í•˜ì´í¼íŒŒë¼ë¯¸í„°)
- **ë³¼ë¥¨, ë² ì´ìŠ¤, íŠ¸ë ˆë¸”**: ìŒì§ˆì„ ì„¸ë°€í•˜ê²Œ ì¡°ì • (í•˜ì´í¼íŒŒë¼ë¯¸í„°)
- **ë‚´ë¶€ íšŒë¡œì˜ ì‹ í˜¸ ì¦í­**: ë¼ë””ì˜¤ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬ (íŒŒë¼ë¯¸í„°)

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**ëŠ” ìš°ë¦¬ê°€ ì§ì ‘ ì„¤ì •í•´ì•¼ í•˜ëŠ” "ì„¤ì •ê°’"ì´ê³ , **íŒŒë¼ë¯¸í„°**ëŠ” ëª¨ë¸ì´ í•™ìŠµì„ í†µí•´ ìë™ìœ¼ë¡œ ì°¾ëŠ” "í•™ìŠµëœ ê°’"ì…ë‹ˆë‹¤.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, load_digits
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import cross_val_score, validation_curve
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# ì‹œê°í™”ë¥¼ ìœ„í•œ í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥ ì‹œì—°ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
print("ğŸ›ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥ ë¶„ì„")
print("=" * 40)

# ë¶„ë¥˜ ë¬¸ì œìš© ë°ì´í„° ìƒì„±
X, y = make_classification(
    n_samples=1000,      # 1000ê°œ ìƒ˜í”Œ
    n_features=20,       # 20ê°œ íŠ¹ì„±
    n_informative=10,    # ìœ ìš©í•œ íŠ¹ì„± 10ê°œ
    n_redundant=10,      # ì¤‘ë³µ íŠ¹ì„± 10ê°œ
    n_clusters_per_class=1,
    random_state=42
)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ ìƒ˜í”Œ")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ìƒ˜í”Œ")
print(f"íŠ¹ì„± ìˆ˜: {X_train.shape[1]}ê°œ")

# ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜, ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì„±ëŠ¥ ì°¨ì´ ë¹„êµ
rf_default = RandomForestClassifier(random_state=42)
rf_tuned = RandomForestClassifier(
    n_estimators=200,    # íŠ¸ë¦¬ ê°œìˆ˜ë¥¼ 100ê°œì—ì„œ 200ê°œë¡œ ì¦ê°€
    max_depth=10,        # ìµœëŒ€ ê¹Šì´ ì œí•œ
    min_samples_split=5, # ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    min_samples_leaf=2,  # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    random_state=42
)

# ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ ë¹„êµ
models = {
    'ê¸°ë³¸ ì„¤ì •': rf_default,
    'íŠœë‹ëœ ì„¤ì •': rf_tuned
}

for name, model in models.items():
    model.fit(X_train, y_train)
    
    # í›ˆë ¨ ì„±ëŠ¥
    train_score = model.score(X_train, y_train)
    # í…ŒìŠ¤íŠ¸ ì„±ëŠ¥
    test_score = model.score(X_test, y_test)
    
    print(f"\n{name}:")
    print(f"  í›ˆë ¨ ì •í™•ë„: {train_score:.4f}")
    print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_score:.4f}")
    print(f"  ê³¼ì í•© ì •ë„: {train_score - test_score:.4f}")
```

**ì™œ ì´ ë¹„êµê°€ ì¤‘ìš”í•œê°€?**
- **ê¸°ë³¸ ì„¤ì •**: ëŒ€ë¶€ë¶„ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ì œê³µí•˜ëŠ” ê¸°ë³¸ê°’
- **íŠœë‹ëœ ì„¤ì •**: ë°ì´í„°ì™€ ë¬¸ì œì— ë§ê²Œ ì¡°ì •ëœ ê°’
- ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ë¼ë„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì— ë”°ë¼ ì„±ëŠ¥ì´ í¬ê²Œ ë‹¬ë¼ì§‘ë‹ˆë‹¤

#### ğŸ“Š ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ë³„ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
# ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ë³„ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì†Œê°œ
print("\nğŸ“Š ì•Œê³ ë¦¬ì¦˜ë³„ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°")
print("=" * 50)

hyperparameter_guide = {
    'Random Forest': {
        'n_estimators': 'íŠ¸ë¦¬ì˜ ê°œìˆ˜ (ë§ì„ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ, ê³„ì‚° ì‹œê°„ ì¦ê°€)',
        'max_depth': 'íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (ê¹Šì„ìˆ˜ë¡ ë³µì¡, ê³¼ì í•© ìœ„í—˜)',
        'min_samples_split': 'ë…¸ë“œ ë¶„í•  ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë‹¨ìˆœ)',
        'min_samples_leaf': 'ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë‹¨ìˆœ)',
        'max_features': 'ê° ë¶„í• ì—ì„œ ê³ ë ¤í•  íŠ¹ì„± ìˆ˜ (ì ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±)'
    },
    
    'SVM': {
        'C': 'ì˜¤ë¥˜ í—ˆìš© ì •ë„ (ë†’ì„ìˆ˜ë¡ ë³µì¡, ê³¼ì í•© ìœ„í—˜)',
        'kernel': 'ì»¤ë„ í•¨ìˆ˜ ì¢…ë¥˜ (linear, rbf, poly)',
        'gamma': 'RBF ì»¤ë„ì˜ ì˜í–¥ ë²”ìœ„ (ë†’ì„ìˆ˜ë¡ ë³µì¡)',
        'degree': 'ë‹¤í•­ì‹ ì»¤ë„ì˜ ì°¨ìˆ˜ (polynomial ì»¤ë„ ì‚¬ìš©ì‹œ)'
    },
    
    'Logistic Regression': {
        'C': 'ì •ê·œí™” ê°•ë„ì˜ ì—­ìˆ˜ (ë†’ì„ìˆ˜ë¡ ë³µì¡)',
        'penalty': 'ì •ê·œí™” ë°©ë²• (l1, l2, elasticnet)',
        'solver': 'ìµœì í™” ì•Œê³ ë¦¬ì¦˜ (liblinear, lbfgs, newton-cg)',
        'max_iter': 'ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ìˆ˜ë ´ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°˜ë³µ)'
    }
}

for algorithm, params in hyperparameter_guide.items():
    print(f"\nğŸ”§ {algorithm}:")
    for param, description in params.items():
        print(f"  â€¢ {param}: {description}")
```

**í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì˜ ì¼ë°˜ì  ì›ì¹™**
- **ë³µì¡ë„ ì¡°ì ˆ**: ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•˜ê±°ë‚˜ ë³µì¡í•˜ì§€ ì•Šë„ë¡ ê· í˜• ìœ ì§€
- **ê³¼ì í•© ë°©ì§€**: í›ˆë ¨ ë°ì´í„°ì—ë§Œ ì˜ ë§ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- **ê³„ì‚° íš¨ìœ¨ì„±**: ì„±ëŠ¥ê³¼ ê³„ì‚° ì‹œê°„ì˜ ì ì ˆí•œ íŠ¸ë ˆì´ë“œì˜¤í”„

---

### 6.13 ê·¸ë¦¬ë“œ ì„œì¹˜ - ì²´ê³„ì ì¸ ì „ìˆ˜ ì¡°ì‚¬

#### ğŸ” ê·¸ë¦¬ë“œ ì„œì¹˜ì˜ ì›ë¦¬

ê·¸ë¦¬ë“œ ì„œì¹˜ëŠ” ë§ˆì¹˜ ë³´ë¬¼ì°¾ê¸°ì—ì„œ ê²©ì ë¬´ëŠ¬ë¡œ ë•…ì„ íŒŒë³´ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ëª¨ë“  ê°€ëŠ¥í•œ ì¡°í•©ì„ ì²´ê³„ì ìœ¼ë¡œ ì‹œë„í•´ì„œ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ” ì¡°í•©ì„ ì°¾ìŠµë‹ˆë‹¤.

```python
# ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤ìŠµ
print("ğŸ” ê·¸ë¦¬ë“œ ì„œì¹˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
print("=" * 50)

# Random Forestë¥¼ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²©ì ì •ì˜
param_grid_rf = {
    'n_estimators': [50, 100, 200],        # íŠ¸ë¦¬ ê°œìˆ˜
    'max_depth': [5, 10, None],            # ìµœëŒ€ ê¹Šì´
    'min_samples_split': [2, 5, 10],       # ë¶„í•  ìµœì†Œ ìƒ˜í”Œ
    'min_samples_leaf': [1, 2, 4]          # ë¦¬í”„ ìµœì†Œ ìƒ˜í”Œ
}

print("ğŸ¯ íƒìƒ‰í•  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©:")
total_combinations = 1
for param, values in param_grid_rf.items():
    print(f"  {param}: {values}")
    total_combinations *= len(values)

print(f"\nì´ ì¡°í•© ìˆ˜: {total_combinations}ê°œ")
print(f"5-fold CV ì‚¬ìš©ì‹œ ì´ í›ˆë ¨ íšŸìˆ˜: {total_combinations * 5}íšŒ")

# ê·¸ë¦¬ë“œ ì„œì¹˜ ìˆ˜í–‰
print("\nâ° ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰ ì¤‘...")
grid_search_rf = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid_rf,
    cv=5,                    # 5-fold êµì°¨ ê²€ì¦
    scoring='accuracy',      # ì •í™•ë„ë¡œ í‰ê°€
    n_jobs=-1,              # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
    verbose=0               # ì§„í–‰ ìƒí™© ì¶œë ¥ ì•ˆí•¨
)

# ì‹¤ì œ ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹¤í–‰
grid_search_rf.fit(X_train, y_train)

print("âœ… ê·¸ë¦¬ë“œ ì„œì¹˜ ì™„ë£Œ!")
print(f"\nğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
for param, value in grid_search_rf.best_params_.items():
    print(f"  {param}: {value}")

print(f"\nğŸ“Š ìµœê³  êµì°¨ ê²€ì¦ ì ìˆ˜: {grid_search_rf.best_score_:.4f}")

# ìµœì  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
best_rf = grid_search_rf.best_estimator_
test_score = best_rf.score(X_test, y_test)
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì ìˆ˜: {test_score:.4f}")
```

**ê·¸ë¦¬ë“œ ì„œì¹˜ì˜ ì¥ì ê³¼ ë‹¨ì **
- **ì¥ì **: ëª¨ë“  ì¡°í•©ì„ ì‹œë„í•˜ë¯€ë¡œ ëˆ„ë½ ì—†ì´ ìµœì í•´ ë°œê²¬
- **ë‹¨ì **: í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê³„ì‚° ì‹œê°„ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€

#### ğŸ“ˆ ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ì‹œê°í™”

```python
# ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ìƒì„¸ ë¶„ì„
print("\nğŸ“ˆ ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ ìƒì„¸ ë¶„ì„")
print("=" * 40)

# ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ì„
import pandas as pd

results_df = pd.DataFrame(grid_search_rf.cv_results_)

# ìƒìœ„ 10ê°œ ê²°ê³¼ í™•ì¸
top_10_results = results_df.nlargest(10, 'mean_test_score')[
    ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']
]

print("ğŸ… ìƒìœ„ 10ê°œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©:")
for idx, row in top_10_results.iterrows():
    params = row['params']
    score = row['mean_test_score']
    std = row['std_test_score']
    rank = row['rank_test_score']
    
    print(f"\n{rank}ë“± - ì ìˆ˜: {score:.4f} (Â±{std:.4f})")
    for param, value in params.items():
        print(f"      {param}: {value}")

# í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.ravel()

# ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥ ë¶„ì„
hyperparams = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']

for i, param in enumerate(hyperparams):
    # í•´ë‹¹ íŒŒë¼ë¯¸í„°ë³„ í‰ê·  ì„±ëŠ¥ ê³„ì‚°
    param_performance = results_df.groupby(f'param_{param}')['mean_test_score'].mean().sort_index()
    
    ax = axes[i]
    param_performance.plot(kind='bar', ax=ax, color='skyblue', alpha=0.7)
    ax.set_title(f'{param}ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”')
    ax.set_xlabel(param)
    ax.set_ylabel('í‰ê·  ì •í™•ë„')
    ax.tick_params(axis='x', rotation=45)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ìµœê³  ì„±ëŠ¥ê³¼ ê¸°ë³¸ ì„¤ì • ë¹„êµ
default_rf = RandomForestClassifier(random_state=42)
default_rf.fit(X_train, y_train)
default_score = default_rf.score(X_test, y_test)

print(f"\nğŸ”„ ì„±ëŠ¥ ê°œì„  ë¹„êµ:")
print(f"  ê¸°ë³¸ ì„¤ì • í…ŒìŠ¤íŠ¸ ì ìˆ˜: {default_score:.4f}")
print(f"  ìµœì  ì„¤ì • í…ŒìŠ¤íŠ¸ ì ìˆ˜: {test_score:.4f}")
print(f"  ì„±ëŠ¥ í–¥ìƒ: {test_score - default_score:.4f} ({(test_score - default_score)*100:.2f}%p)")
```

**ê·¸ë¦¬ë“œ ì„œì¹˜ ê²°ê³¼ í•´ì„**
- ê° í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„
- ìƒìœ„ ì¡°í•©ë“¤ì„ ë¹„êµí•˜ì—¬ ì•ˆì •ì ì¸ íŒ¨í„´ ë°œê²¬
- ì„±ëŠ¥ í–¥ìƒ ì •ë„ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •

---

### 6.14 ëœë¤ ì„œì¹˜ - íš¨ìœ¨ì ì¸ í™•ë¥ ì  íƒìƒ‰

#### ğŸ² ëœë¤ ì„œì¹˜ì˜ í˜ì‹ ì  ì•„ì´ë””ì–´

ëœë¤ ì„œì¹˜ëŠ” ëª¨ë“  ì¡°í•©ì„ ì‹œë„í•˜ëŠ” ëŒ€ì‹ , ë¬´ì‘ìœ„ë¡œ ì¡°í•©ì„ ì„ íƒí•´ì„œ ì‹œë„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë†€ëê²Œë„ ë§ì€ ê²½ìš°ì— ê·¸ë¦¬ë“œ ì„œì¹˜ë³´ë‹¤ íš¨ìœ¨ì ì…ë‹ˆë‹¤!

```python
# ëœë¤ ì„œì¹˜ êµ¬í˜„
print("ğŸ² ëœë¤ ì„œì¹˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”")
print("=" * 50)

# ëœë¤ ì„œì¹˜ë¥¼ ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¶„í¬ ì •ì˜
from scipy.stats import randint, uniform

param_dist_rf = {
    'n_estimators': randint(50, 300),           # 50~299 ì‚¬ì´ì˜ ì •ìˆ˜
    'max_depth': [5, 10, 15, 20, None],         # ì´ì‚°ì  ì„ íƒ
    'min_samples_split': randint(2, 20),        # 2~19 ì‚¬ì´ì˜ ì •ìˆ˜
    'min_samples_leaf': randint(1, 10),         # 1~9 ì‚¬ì´ì˜ ì •ìˆ˜
    'max_features': uniform(0.1, 0.9)           # 0.1~1.0 ì‚¬ì´ì˜ ì‹¤ìˆ˜
}

print("ğŸ¯ ëœë¤ ì„œì¹˜ íŒŒë¼ë¯¸í„° ë¶„í¬:")
for param, dist in param_dist_rf.items():
    if hasattr(dist, 'rvs'):  # ì—°ì† ë¶„í¬ì¸ ê²½ìš°
        print(f"  {param}: {type(dist).__name__} ë¶„í¬")
    else:  # ì´ì‚° ë¶„í¬ì¸ ê²½ìš°
        print(f"  {param}: {dist}")

# ëœë¤ ì„œì¹˜ ìˆ˜í–‰ (ê·¸ë¦¬ë“œ ì„œì¹˜ì™€ ë™ì¼í•œ ì‹œê°„ ì˜ˆì‚°ìœ¼ë¡œ)
n_iter = 100  # 100ë²ˆì˜ ë¬´ì‘ìœ„ ì¡°í•© ì‹œë„

print(f"\nğŸ” {n_iter}ë²ˆì˜ ë¬´ì‘ìœ„ ì¡°í•© íƒìƒ‰ ì¤‘...")
random_search_rf = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_dist_rf,
    n_iter=n_iter,          # ì‹œë„í•  ì¡°í•© ìˆ˜
    cv=5,                   # 5-fold êµì°¨ ê²€ì¦
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=0
)

random_search_rf.fit(X_train, y_train)

print("âœ… ëœë¤ ì„œì¹˜ ì™„ë£Œ!")
print(f"\nğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
for param, value in random_search_rf.best_params_.items():
    print(f"  {param}: {value}")

print(f"\nğŸ“Š ìµœê³  êµì°¨ ê²€ì¦ ì ìˆ˜: {random_search_rf.best_score_:.4f}")

# ìµœì  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
best_rf_random = random_search_rf.best_estimator_
test_score_random = best_rf_random.score(X_test, y_test)
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì ìˆ˜: {test_score_random:.4f}")
```

**ëœë¤ ì„œì¹˜ì˜ í•µì‹¬ ì¥ì **
- **íš¨ìœ¨ì„±**: ì œí•œëœ ì‹œê°„ ë‚´ì—ì„œ ë” ë„“ì€ ê³µê°„ íƒìƒ‰
- **ì—°ì†ê°’ ì²˜ë¦¬**: ì‹¤ìˆ˜í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„°ë„ ìì—°ìŠ¤ëŸ½ê²Œ íƒìƒ‰
- **í™•ì¥ì„±**: í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì¦ê°€í•´ë„ ê³„ì‚° ì‹œê°„ ì„ í˜• ì¦ê°€

#### âš¡ ê·¸ë¦¬ë“œ ì„œì¹˜ vs ëœë¤ ì„œì¹˜ ì„±ëŠ¥ ë¹„êµ

```python
# ê·¸ë¦¬ë“œ ì„œì¹˜ì™€ ëœë¤ ì„œì¹˜ íš¨ìœ¨ì„± ë¹„êµ
print("\nâš¡ ê·¸ë¦¬ë“œ ì„œì¹˜ vs ëœë¤ ì„œì¹˜ ë¹„êµ")
print("=" * 50)

# ì‹œê°„ë‹¹ ì„±ëŠ¥ ë¹„êµ (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)
import time

# ì‘ì€ ê·¸ë¦¬ë“œë¡œ ì‹œê°„ ì¸¡ì •
small_param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [5, 10],
    'min_samples_split': [2, 5]
}

# ê·¸ë¦¬ë“œ ì„œì¹˜ ì‹œê°„ ì¸¡ì •
start_time = time.time()
small_grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    small_param_grid,
    cv=3,
    n_jobs=-1
)
small_grid_search.fit(X_train[:500], y_train[:500])  # ë°ì´í„° ì¼ë¶€ë§Œ ì‚¬ìš©
grid_time = time.time() - start_time

# ë™ì¼í•œ ì¡°í•© ìˆ˜ë§Œí¼ ëœë¤ ì„œì¹˜
n_combinations = len(small_param_grid['n_estimators']) * len(small_param_grid['max_depth']) * len(small_param_grid['min_samples_split'])

start_time = time.time()
small_random_search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    {
        'n_estimators': randint(50, 101),
        'max_depth': [5, 10],
        'min_samples_split': randint(2, 6)
    },
    n_iter=n_combinations,
    cv=3,
    n_jobs=-1,
    random_state=42
)
small_random_search.fit(X_train[:500], y_train[:500])
random_time = time.time() - start_time

print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„ ë¹„êµ ({n_combinations}ê°œ ì¡°í•©):")
print(f"  ê·¸ë¦¬ë“œ ì„œì¹˜: {grid_time:.2f}ì´ˆ")
print(f"  ëœë¤ ì„œì¹˜: {random_time:.2f}ì´ˆ")

print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ:")
print(f"  ê·¸ë¦¬ë“œ ì„œì¹˜ ìµœê³  ì ìˆ˜: {grid_search_rf.best_score_:.4f}")
print(f"  ëœë¤ ì„œì¹˜ ìµœê³  ì ìˆ˜: {random_search_rf.best_score_:.4f}")

# íƒìƒ‰ íš¨ìœ¨ì„± ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# 1. ì‹œê°„ ë¹„êµ
methods = ['ê·¸ë¦¬ë“œ ì„œì¹˜', 'ëœë¤ ì„œì¹˜']
times = [grid_time, random_time]
scores = [grid_search_rf.best_score_, random_search_rf.best_score_]

bars1 = ax1.bar(methods, times, color=['lightblue', 'lightgreen'], alpha=0.7)
ax1.set_ylabel('ì‹¤í–‰ ì‹œê°„ (ì´ˆ)')
ax1.set_title('ì‹¤í–‰ ì‹œê°„ ë¹„êµ')
for bar, time_val in zip(bars1, times):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height,
             f'{time_val:.2f}s', ha='center', va='bottom')

# 2. ì„±ëŠ¥ ë¹„êµ
bars2 = ax2.bar(methods, scores, color=['lightblue', 'lightgreen'], alpha=0.7)
ax2.set_ylabel('êµì°¨ ê²€ì¦ ì ìˆ˜')
ax2.set_title('ì„±ëŠ¥ ë¹„êµ')
ax2.set_ylim(min(scores) - 0.01, max(scores) + 0.01)
for bar, score in zip(bars2, scores):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
             f'{score:.4f}', ha='center', va='bottom')

plt.tight_layout()
plt.show()
```

**ì–¸ì œ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í• ê¹Œ?**
- **ê·¸ë¦¬ë“œ ì„œì¹˜**: í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì ê³ , ì²´ê³„ì  íƒìƒ‰ì´ ì¤‘ìš”í•œ ê²½ìš°
- **ëœë¤ ì„œì¹˜**: í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë§ê±°ë‚˜, ì‹œê°„ ì œì•½ì´ ìˆëŠ” ê²½ìš°

---

### 6.15 ë² ì´ì§€ì•ˆ ìµœì í™” - AIê°€ ìµœì í™”í•˜ëŠ” ìµœì í™”

#### ğŸ§  ë² ì´ì§€ì•ˆ ìµœì í™”ì˜ ì§€ëŠ¥ì  ì ‘ê·¼

ë² ì´ì§€ì•ˆ ìµœì í™”ëŠ” ë§ˆì¹˜ ìˆ™ë ¨ëœ íƒí—˜ê°€ê°€ ì§€ë„ë¥¼ ê·¸ë ¤ê°€ë©° ë³´ë¬¼ì„ ì°¾ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ì „ íƒìƒ‰ ê²°ê³¼ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ìŒì— íƒìƒ‰í•  ê°€ì¥ ìœ ë§í•œ ì§€ì ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.

```python
# ë² ì´ì§€ì•ˆ ìµœì í™” (scikit-optimize ì‚¬ìš©)
# ì„¤ì¹˜: pip install scikit-optimize
try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer, Categorical
    from skopt.utils import use_named_args
    from skopt import dump, load
    
    print("ğŸ§  ë² ì´ì§€ì•ˆ ìµœì í™” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    print("=" * 50)
    
    # ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ê²€ìƒ‰ ê³µê°„ ì •ì˜
    search_space = [
        Integer(50, 300, name='n_estimators'),
        Integer(3, 20, name='max_depth'),
        Integer(2, 20, name='min_samples_split'),
        Integer(1, 10, name='min_samples_leaf'),
        Real(0.1, 1.0, name='max_features')
    ]
    
    # ëª©ì  í•¨ìˆ˜ ì •ì˜ (ìµœì†Œí™”í•  í•¨ìˆ˜ - ë”°ë¼ì„œ ìŒì˜ ì •í™•ë„ ë°˜í™˜)
    @use_named_args(search_space)
    def objective(**params):
        # RandomForest ëª¨ë¸ ìƒì„±
        model = RandomForestClassifier(
            n_estimators=params['n_estimators'],
            max_depth=params['max_depth'],
            min_samples_split=params['min_samples_split'],
            min_samples_leaf=params['min_samples_leaf'],
            max_features=params['max_features'],
            random_state=42
        )
        
        # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
        
        # ë² ì´ì§€ì•ˆ ìµœì í™”ëŠ” ìµœì†Œí™” ë¬¸ì œì´ë¯€ë¡œ ìŒìˆ˜ ë°˜í™˜
        return -np.mean(scores)
    
    print("ğŸ” ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰ ì¤‘...")
    print("  ì´ì „ ê²°ê³¼ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ìŒ íƒìƒ‰ ì§€ì ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")
    
    # ë² ì´ì§€ì•ˆ ìµœì í™” ì‹¤í–‰
    result = gp_minimize(
        func=objective,
        dimensions=search_space,
        n_calls=50,              # 50ë²ˆì˜ í•¨ìˆ˜ í˜¸ì¶œ
        random_state=42,
        acq_func='EI'           # Expected Improvement íšë“ í•¨ìˆ˜
    )
    
    print("âœ… ë² ì´ì§€ì•ˆ ìµœì í™” ì™„ë£Œ!")
    
    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥
    param_names = ['n_estimators', 'max_depth', 'min_samples_split', 
                   'min_samples_leaf', 'max_features']
    
    print(f"\nğŸ† ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°:")
    best_params_bayes = {}
    for i, param_name in enumerate(param_names):
        best_value = result.x[i]
        best_params_bayes[param_name] = best_value
        print(f"  {param_name}: {best_value}")
    
    print(f"\nğŸ“Š ìµœê³  êµì°¨ ê²€ì¦ ì ìˆ˜: {-result.fun:.4f}")
    
    # ìµœì  ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ í‰ê°€
    best_rf_bayes = RandomForestClassifier(**best_params_bayes, random_state=42)
    best_rf_bayes.fit(X_train, y_train)
    test_score_bayes = best_rf_bayes.score(X_test, y_test)
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì ìˆ˜: {test_score_bayes:.4f}")
    
    bayes_available = True
    
except ImportError:
    print("ğŸ§  ë² ì´ì§€ì•ˆ ìµœì í™” (scikit-optimize ë¯¸ì„¤ì¹˜)")
    print("=" * 50)
    print("scikit-optimizeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´: pip install scikit-optimize")
    print("\nëŒ€ì‹  ê°„ë‹¨í•œ ë² ì´ì§€ì•ˆ ìµœì í™” ê°œë…ì„ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤...")
    
    # ë² ì´ì§€ì•ˆ ìµœì í™” ê°œë… ì‹œë®¬ë ˆì´ì…˜
    best_params_bayes = {
        'n_estimators': 150,
        'max_depth': 12,
        'min_samples_split': 4,
        'min_samples_leaf': 2,
        'max_features': 0.7
    }
    
    best_rf_bayes = RandomForestClassifier(**best_params_bayes, random_state=42)
    best_rf_bayes.fit(X_train, y_train)
    test_score_bayes = best_rf_bayes.score(X_test, y_test)
    
    print(f"ì‹œë®¬ë ˆì´ì…˜ëœ ë² ì´ì§€ì•ˆ ìµœì í™” ê²°ê³¼:")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì ìˆ˜: {test_score_bayes:.4f}")
    
    bayes_available = False
```

**ë² ì´ì§€ì•ˆ ìµœì í™”ì˜ í•µì‹¬ ê°œë…**
- **ëŒ€ë¦¬ ëª¨ë¸**: ì‹¤ì œ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•˜ëŠ” ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤ ëª¨ë¸
- **íšë“ í•¨ìˆ˜**: ë‹¤ìŒì— íƒìƒ‰í•  ì§€ì ì„ ì„ íƒí•˜ëŠ” ì „ëµ
- **íƒìƒ‰-í™œìš© ê· í˜•**: ìƒˆë¡œìš´ ì˜ì—­ íƒìƒ‰ê³¼ ì¢‹ì€ ì˜ì—­ ì§‘ì¤‘ íƒìƒ‰ì˜ ê· í˜•

---

### 6.16 ëª¨ë“  ìµœì í™” ë°©ë²• ì¢…í•© ë¹„êµ

#### ğŸ“Š ì„¸ ê°€ì§€ ìµœì í™” ë°©ë²•ì˜ ì„±ëŠ¥ê³¼ íš¨ìœ¨ì„± ë¹„êµ

```python
# ëª¨ë“  ìµœì í™” ë°©ë²• ì¢…í•© ë¹„êµ
print("ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë°©ë²• ì¢…í•© ë¹„êµ")
print("=" * 60)

# ê²°ê³¼ ì •ë¦¬
optimization_results = {
    'ê·¸ë¦¬ë“œ ì„œì¹˜': {
        'best_score': grid_search_rf.best_score_,
        'test_score': test_score,
        'n_evaluations': total_combinations * 5,  # CV fold ìˆ˜ ê³±í•˜ê¸°
        'method_type': 'ì „ìˆ˜ ì¡°ì‚¬'
    },
    'ëœë¤ ì„œì¹˜': {
        'best_score': random_search_rf.best_score_,
        'test_score': test_score_random,
        'n_evaluations': n_iter * 5,
        'method_type': 'í™•ë¥ ì  ìƒ˜í”Œë§'
    }
}

if bayes_available:
    optimization_results['ë² ì´ì§€ì•ˆ ìµœì í™”'] = {
        'best_score': -result.fun,
        'test_score': test_score_bayes,
        'n_evaluations': 50,
        'method_type': 'ì§€ëŠ¥ì  íƒìƒ‰'
    }
else:
    optimization_results['ë² ì´ì§€ì•ˆ ìµœì í™”'] = {
        'best_score': 0.85,  # ì‹œë®¬ë ˆì´ì…˜ ê°’
        'test_score': test_score_bayes,
        'n_evaluations': 50,
        'method_type': 'ì§€ëŠ¥ì  íƒìƒ‰'
    }

# ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
print("ğŸ† ìµœì í™” ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ:")
print(f"{'ë°©ë²•':<15} {'CV ì ìˆ˜':<10} {'í…ŒìŠ¤íŠ¸ ì ìˆ˜':<12} {'í‰ê°€ íšŸìˆ˜':<10} {'íŠ¹ì§•'}")
print("-" * 65)

for method, results in optimization_results.items():
    print(f"{method:<15} {results['best_score']:<10.4f} {results['test_score']:<12.4f} "
          f"{results['n_evaluations']:<10} {results['method_type']}")

# íš¨ìœ¨ì„± ë¶„ì„ (ì ìˆ˜ ëŒ€ë¹„ í‰ê°€ íšŸìˆ˜)
print(f"\nğŸ“ˆ íš¨ìœ¨ì„± ë¶„ì„ (ë†’ì€ ì ìˆ˜ë¥¼ ì ì€ í‰ê°€ë¡œ):")
for method, results in optimization_results.items():
    efficiency = results['test_score'] / results['n_evaluations'] * 1000  # 1000ë°° ìŠ¤ì¼€ì¼ë§
    print(f"  {method}: {efficiency:.2f} (ì ìˆ˜/í‰ê°€íšŸìˆ˜ Ã— 1000)")

# ì‹œê°í™”
methods = list(optimization_results.keys())
cv_scores = [results['best_score'] for results in optimization_results.values()]
test_scores = [results['test_score'] for results in optimization_results.values()]
n_evals = [results['n_evaluations'] for results in optimization_results.values()]

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. CV ì ìˆ˜ ë¹„êµ
ax1 = axes[0, 0]
bars1 = ax1.bar(methods, cv_scores, color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)
ax1.set_ylabel('êµì°¨ ê²€ì¦ ì ìˆ˜')
ax1.set_title('êµì°¨ ê²€ì¦ ì„±ëŠ¥ ë¹„êµ')
ax1.set_ylim(min(cv_scores) - 0.01, max(cv_scores) + 0.01)
for bar, score in zip(bars1, cv_scores):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,
             f'{score:.4f}', ha='center', va='bottom')

# 2. í…ŒìŠ¤íŠ¸ ì ìˆ˜ ë¹„êµ
ax2 = axes[0, 1]
bars2 = ax2.bar(methods, test_scores, color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)
ax2.set_ylabel('í…ŒìŠ¤íŠ¸ ì ìˆ˜')
ax2.set_title('í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¹„êµ')
ax2.set_ylim(min(test_scores) - 0.01, max(test_scores) + 0.01)
for bar, score in zip(bars2, test_scores):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.001,
             f'{score:.4f}', ha='center', va='bottom')

# 3. í‰ê°€ íšŸìˆ˜ ë¹„êµ
ax3 = axes[1, 0]
bars3 = ax3.bar(methods, n_evals, color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)
ax3.set_ylabel('ì´ í‰ê°€ íšŸìˆ˜')
ax3.set_title('ê³„ì‚° ë¹„ìš© ë¹„êµ')
ax3.set_yscale('log')  # ë¡œê·¸ ìŠ¤ì¼€ì¼ ì‚¬ìš©
for bar, n_eval in zip(bars3, n_evals):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height * 1.1,
             f'{n_eval}', ha='center', va='bottom')

# 4. íš¨ìœ¨ì„± ì‚°ì ë„
ax4 = axes[1, 1]
colors = ['blue', 'green', 'red']
for i, (method, color) in enumerate(zip(methods, colors)):
    ax4.scatter(n_evals[i], test_scores[i], c=color, s=200, alpha=0.7, label=method)
    ax4.annotate(method, (n_evals[i], test_scores[i]), 
                xytext=(5, 5), textcoords='offset points')

ax4.set_xlabel('í‰ê°€ íšŸìˆ˜')
ax4.set_ylabel('í…ŒìŠ¤íŠ¸ ì ìˆ˜')
ax4.set_title('íš¨ìœ¨ì„± ë¶„ì„ (ì™¼ìª½ ìœ„ê°€ ì¢‹ìŒ)')
ax4.set_xscale('log')
ax4.grid(True, alpha=0.3)
ax4.legend()

plt.tight_layout()
plt.show()
```

#### ğŸ¯ ìµœì í™” ë°©ë²• ì„ íƒ ê°€ì´ë“œë¼ì¸

```python
# ì‹¤ë¬´ì—ì„œì˜ ìµœì í™” ë°©ë²• ì„ íƒ ê°€ì´ë“œ
print("\nğŸ¯ ì‹¤ë¬´ ì ìš© ê°€ì´ë“œë¼ì¸")
print("=" * 40)

selection_guide = {
    "í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°œìˆ˜": {
        "ì ìŒ (â‰¤3ê°œ)": "ê·¸ë¦¬ë“œ ì„œì¹˜",
        "ë³´í†µ (4-6ê°œ)": "ëœë¤ ì„œì¹˜ ë˜ëŠ” ë² ì´ì§€ì•ˆ ìµœì í™”",
        "ë§ìŒ (â‰¥7ê°œ)": "ë² ì´ì§€ì•ˆ ìµœì í™”"
    },
    
    "ì‹œê°„ ì œì•½": {
        "ì¶©ë¶„í•œ ì‹œê°„": "ê·¸ë¦¬ë“œ ì„œì¹˜",
        "ì œí•œëœ ì‹œê°„": "ëœë¤ ì„œì¹˜",
        "ë§¤ìš° ì œí•œì ": "ë² ì´ì§€ì•ˆ ìµœì í™”"
    },
    
    "ì •í™•ë„ ìš”êµ¬ì‚¬í•­": {
        "ìµœê³  ì„±ëŠ¥ í•„ìš”": "ë² ì´ì§€ì•ˆ ìµœì í™”",
        "í•©ë¦¬ì  ì„±ëŠ¥": "ëœë¤ ì„œì¹˜",
        "ê¸°ë³¸ì  íŠœë‹": "ê·¸ë¦¬ë“œ ì„œì¹˜"
    },
    
    "íƒìƒ‰ ê³µê°„": {
        "ì´ì‚°ì  ê°’ë§Œ": "ê·¸ë¦¬ë“œ ì„œì¹˜",
        "ì—°ì†ì  ê°’ í¬í•¨": "ëœë¤ ì„œì¹˜ ë˜ëŠ” ë² ì´ì§€ì•ˆ ìµœì í™”",
        "ë³µì¡í•œ ìƒí˜¸ì‘ìš©": "ë² ì´ì§€ì•ˆ ìµœì í™”"
    }
}

for category, guidelines in selection_guide.items():
    print(f"\nğŸ“‹ {category}ë³„ ì¶”ì²œ ë°©ë²•:")
    for condition, method in guidelines.items():
        print(f"  â€¢ {condition}: {method}")

# ì‹¤ë¬´ íŒ
print(f"\nğŸ’¡ ì‹¤ë¬´ ì ìš© íŒ:")
tips = [
    "ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¨¼ì € ì‹¤í—˜í•˜ì—¬ ëŒ€ëµì ì¸ ë²”ìœ„ íŒŒì•…",
    "ëœë¤ ì„œì¹˜ë¡œ ë„“ì€ ë²”ìœ„ íƒìƒ‰ í›„ ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ì •ë°€ ì¡°ì •",
    "ë² ì´ì§€ì•ˆ ìµœì í™”ëŠ” ë¹„ìš©ì´ ë†’ì€ ëª¨ë¸(ë”¥ëŸ¬ë‹ ë“±)ì— íŠ¹íˆ ìœ ìš©",
    "êµì°¨ ê²€ì¦ í´ë“œ ìˆ˜ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°œìˆ˜ì˜ ê· í˜• ê³ ë ¤",
    "ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•´ validation setì„ ë³„ë„ë¡œ ìœ ì§€",
    "early stoppingê³¼ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ê³„ì‚° ì‹œê°„ ì ˆì•½"
]

for i, tip in enumerate(tips, 1):
    print(f"  {i}. {tip}")
```

**ìµœì í™” ë°©ë²•ë³„ íŠ¹ì„± ìš”ì•½**
- **ê·¸ë¦¬ë“œ ì„œì¹˜**: ì™„ì „í•˜ì§€ë§Œ ë¹„íš¨ìœ¨ì 
- **ëœë¤ ì„œì¹˜**: ê· í˜• ì¡íŒ íš¨ìœ¨ì„±
- **ë² ì´ì§€ì•ˆ ìµœì í™”**: ê°€ì¥ ì§€ëŠ¥ì ì´ì§€ë§Œ ë³µì¡í•¨

---

### 6.17 ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í‰ê°€ì™€ êµì°¨ ê²€ì¦

#### ğŸ”„ êµì°¨ ê²€ì¦ì˜ ì¤‘ìš”ì„±

í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ **ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í‰ê°€**ì…ë‹ˆë‹¤. ë§ˆì¹˜ ì‹œí—˜ì„ ì—¬ëŸ¬ ë²ˆ ë´ì„œ ì‹¤ë ¥ì„ ì •í™•íˆ ì¸¡ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
# êµì°¨ ê²€ì¦ ì „ëµì˜ ì¤‘ìš”ì„± ì‹œì—°
print("ğŸ”„ êµì°¨ ê²€ì¦ ì „ëµ ë¹„êµ")
print("=" * 40)

from sklearn.model_selection import cross_validate, StratifiedKFold, LeaveOneOut
from sklearn.model_selection import learning_curve, validation_curve

# ë‹¤ì–‘í•œ êµì°¨ ê²€ì¦ ë°©ë²• ë¹„êµ
cv_strategies = {
    '5-Fold CV': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    '10-Fold CV': StratifiedKFold(n_splits=10, shuffle=True, random_state=42),
    'Leave-One-Out': LeaveOneOut()
}

# ë™ì¼í•œ ëª¨ë¸ë¡œ ë‹¤ë¥¸ CV ì „ëµ ë¹„êµ
test_model = RandomForestClassifier(n_estimators=100, random_state=42)

print("ğŸ“Š êµì°¨ ê²€ì¦ ì „ëµë³„ ê²°ê³¼:")
print(f"{'ì „ëµ':<15} {'í‰ê·  ì ìˆ˜':<12} {'í‘œì¤€í¸ì°¨':<12} {'95% ì‹ ë¢°êµ¬ê°„'}")
print("-" * 60)

cv_results = {}
for name, cv_strategy in cv_strategies.items():
    if name == 'Leave-One-Out' and len(X_train) > 200:
        # LOOëŠ” ê³„ì‚° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ìŠ¤í‚µ
        continue
    
    # êµì°¨ ê²€ì¦ ìˆ˜í–‰
    scores = cross_val_score(test_model, X_train, y_train, cv=cv_strategy, scoring='accuracy')
    
    mean_score = np.mean(scores)
    std_score = np.std(scores)
    
    # 95% ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
    confidence_interval = 1.96 * std_score / np.sqrt(len(scores))
    
    cv_results[name] = {
        'scores': scores,
        'mean': mean_score,
        'std': std_score,
        'ci': confidence_interval
    }
    
    print(f"{name:<15} {mean_score:<12.4f} {std_score:<12.4f} "
          f"[{mean_score-confidence_interval:.4f}, {mean_score+confidence_interval:.4f}]")

# êµì°¨ ê²€ì¦ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 8))

# ìƒìê·¸ë¦¼ìœ¼ë¡œ ë¶„í¬ ë¹„êµ
cv_names = list(cv_results.keys())
cv_scores = [cv_results[name]['scores'] for name in cv_names]

plt.subplot(2, 2, 1)
plt.boxplot(cv_scores, labels=cv_names)
plt.title('êµì°¨ ê²€ì¦ ì ìˆ˜ ë¶„í¬')
plt.ylabel('ì •í™•ë„')
plt.xticks(rotation=45)

# í‰ê· ê³¼ ì‹ ë¢°êµ¬ê°„ ì‹œê°í™”
plt.subplot(2, 2, 2)
means = [cv_results[name]['mean'] for name in cv_names]
cis = [cv_results[name]['ci'] for name in cv_names]

x_pos = range(len(cv_names))
plt.errorbar(x_pos, means, yerr=cis, fmt='o', capsize=5, capthick=2)
plt.xticks(x_pos, cv_names, rotation=45)
plt.title('í‰ê·  ì ìˆ˜ì™€ 95% ì‹ ë¢°êµ¬ê°„')
plt.ylabel('ì •í™•ë„')

plt.tight_layout()
plt.show()

print(f"\nğŸ’¡ êµì°¨ ê²€ì¦ ì„ íƒ ê°€ì´ë“œ:")
print(f"  â€¢ 5-Fold CV: ì¼ë°˜ì ì¸ ì„ íƒ, ê³„ì‚° íš¨ìœ¨ì„±ê³¼ ì‹ ë¢°ì„±ì˜ ê· í˜•")
print(f"  â€¢ 10-Fold CV: ë” ì •í™•í•œ ì¶”ì •, ê³„ì‚° ì‹œê°„ ì¦ê°€")
print(f"  â€¢ Leave-One-Out: ê°€ì¥ ì •í™•í•˜ì§€ë§Œ ë§¤ìš° ëŠë¦¼, ì‘ì€ ë°ì´í„°ì…‹ìš©")
```

**êµì°¨ ê²€ì¦ ì „ëµ ì„ íƒì˜ í•µì‹¬**
- **ë°ì´í„° í¬ê¸°**: ì‘ì€ ë°ì´í„°ì…‹ì¼ìˆ˜ë¡ ë” ë§ì€ í´ë“œ í•„ìš”
- **ê³„ì‚° ì‹œê°„**: í´ë“œ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì‹œê°„ ì¦ê°€
- **ë¶„í¬ ë³´ì¡´**: StratifiedKFoldë¡œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€

#### ğŸ“ˆ í•™ìŠµ ê³¡ì„ ê³¼ ê²€ì¦ ê³¡ì„  ë¶„ì„

```python
# í•™ìŠµ ê³¡ì„ ìœ¼ë¡œ ëª¨ë¸ ì§„ë‹¨
print("\nğŸ“ˆ í•™ìŠµ ê³¡ì„ ê³¼ ê²€ì¦ ê³¡ì„  ë¶„ì„")
print("=" * 40)

# 1. í•™ìŠµ ê³¡ì„  - ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
train_sizes = np.linspace(0.1, 1.0, 10)
train_sizes_abs, train_scores, val_scores = learning_curve(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X_train, y_train,
    train_sizes=train_sizes,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
val_mean = np.mean(val_scores, axis=1)
val_std = np.std(val_scores, axis=1)

# 2. ê²€ì¦ ê³¡ì„  - í•˜ì´í¼íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ ì„±ëŠ¥ ë³€í™”
param_range = [10, 50, 100, 200, 300]
train_scores_val, val_scores_val = validation_curve(
    RandomForestClassifier(random_state=42),
    X_train, y_train,
    param_name='n_estimators',
    param_range=param_range,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# í•™ìŠµ ê³¡ì„ 
ax1.plot(train_sizes_abs, train_mean, 'o-', color='blue', label='í›ˆë ¨ ì ìˆ˜')
ax1.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')
ax1.plot(train_sizes_abs, val_mean, 'o-', color='red', label='ê²€ì¦ ì ìˆ˜')
ax1.fill_between(train_sizes_abs, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')
ax1.set_xlabel('í›ˆë ¨ ë°ì´í„° í¬ê¸°')
ax1.set_ylabel('ì •í™•ë„')
ax1.set_title('í•™ìŠµ ê³¡ì„ ')
ax1.legend()
ax1.grid(True, alpha=0.3)

# ê²€ì¦ ê³¡ì„ 
train_mean_val = np.mean(train_scores_val, axis=1)
train_std_val = np.std(train_scores_val, axis=1)
val_mean_val = np.mean(val_scores_val, axis=1)
val_std_val = np.std(val_scores_val, axis=1)

ax2.plot(param_range, train_mean_val, 'o-', color='blue', label='í›ˆë ¨ ì ìˆ˜')
ax2.fill_between(param_range, train_mean_val - train_std_val, train_mean_val + train_std_val, alpha=0.1, color='blue')
ax2.plot(param_range, val_mean_val, 'o-', color='red', label='ê²€ì¦ ì ìˆ˜')
ax2.fill_between(param_range, val_mean_val - val_std_val, val_mean_val + val_std_val, alpha=0.1, color='red')
ax2.set_xlabel('n_estimators')
ax2.set_ylabel('ì •í™•ë„')
ax2.set_title('ê²€ì¦ ê³¡ì„  (n_estimators)')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# ì§„ë‹¨ ê²°ê³¼ í•´ì„
print(f"ğŸ“Š ëª¨ë¸ ì§„ë‹¨ ê²°ê³¼:")
train_val_gap = train_mean[-1] - val_mean[-1]
print(f"  ìµœì¢… í›ˆë ¨-ê²€ì¦ ì ìˆ˜ ì°¨ì´: {train_val_gap:.4f}")

if train_val_gap > 0.05:
    print(f"  â†’ ê³¼ì í•© ì˜ì‹¬: ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ ë˜ëŠ” ì •ê·œí™” í•„ìš”")
elif train_val_gap < 0.01:
    print(f"  â†’ ê³¼ì†Œì í•© ì˜ì‹¬: ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ í•„ìš”")
else:
    print(f"  â†’ ì ì ˆí•œ ê· í˜•: ëª¨ë¸ì´ ì˜ ì¡°ì •ë¨")

# ìµœì  n_estimators ì¶”ì²œ
optimal_idx = np.argmax(val_mean_val)
optimal_n_estimators = param_range[optimal_idx]
print(f"  ì¶”ì²œ n_estimators: {optimal_n_estimators}")
```

**ê³¡ì„  ë¶„ì„ì„ í†µí•œ ì¸ì‚¬ì´íŠ¸**
- **í•™ìŠµ ê³¡ì„ **: ë” ë§ì€ ë°ì´í„°ê°€ ë„ì›€ì´ ë˜ëŠ”ì§€ íŒë‹¨
- **ê²€ì¦ ê³¡ì„ **: í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ìµœì ê°’ ì‹œê°ì  í™•ì¸
- **ê³¼ì í•©/ê³¼ì†Œì í•©**: í›ˆë ¨-ê²€ì¦ ì ìˆ˜ ì°¨ì´ë¡œ ì§„ë‹¨

---

### 6.18 ìë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìŠ¤í…œ

#### ğŸ¤– ì™„ì „ ìë™í™” íŠœë‹ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

ì‹¤ë¬´ì—ì„œëŠ” ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì„ ë™ì‹œì— ë¹„êµí•˜ê³  ìµœì í™”í•˜ëŠ” ìë™í™” ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤.

```python
# ìë™í™”ëœ ë©€í‹° ì•Œê³ ë¦¬ì¦˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìŠ¤í…œ
print("ğŸ¤– ìë™í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìŠ¤í…œ")
print("=" * 50)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
import warnings
warnings.filterwarnings('ignore')

class AutoMLTuner:
    def __init__(self, cv_folds=5, scoring='accuracy', n_jobs=-1):
        self.cv_folds = cv_folds
        self.scoring = scoring
        self.n_jobs = n_jobs
        
        # ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ ì •ì˜
        self.algorithms = {
            'RandomForest': {
                'estimator': RandomForestClassifier(random_state=42),
                'param_space': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [5, 10, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4]
                }
            },
            
            'GradientBoosting': {
                'estimator': GradientBoostingClassifier(random_state=42),
                'param_space': {
                    'n_estimators': [50, 100, 200],
                    'learning_rate': [0.01, 0.1, 0.2],
                    'max_depth': [3, 5, 7],
                    'subsample': [0.8, 0.9, 1.0]
                }
            },
            
            'SVM': {
                'estimator': SVC(random_state=42),
                'param_space': {
                    'C': [0.1, 1, 10, 100],
                    'kernel': ['rbf', 'linear'],
                    'gamma': ['scale', 'auto', 0.001, 0.01]
                }
            },
            
            'KNN': {
                'estimator': KNeighborsClassifier(),
                'param_space': {
                    'n_neighbors': [3, 5, 7, 9, 11],
                    'weights': ['uniform', 'distance'],
                    'metric': ['euclidean', 'manhattan']
                }
            }
        }
        
        self.results = {}
    
    def tune_all_algorithms(self, X_train, y_train, method='grid'):
        """ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìˆ˜í–‰"""
        print(f"ğŸ”§ {method.upper()} ë°©ë²•ìœ¼ë¡œ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ íŠœë‹ ì¤‘...")
        
        for algo_name, algo_config in self.algorithms.items():
            print(f"\n  ğŸ“Š {algo_name} íŠœë‹ ì¤‘...")
            
            estimator = algo_config['estimator']
            param_space = algo_config['param_space']
            
            if method == 'grid':
                searcher = GridSearchCV(
                    estimator=estimator,
                    param_grid=param_space,
                    cv=self.cv_folds,
                    scoring=self.scoring,
                    n_jobs=self.n_jobs
                )
            elif method == 'random':
                searcher = RandomizedSearchCV(
                    estimator=estimator,
                    param_distributions=param_space,
                    n_iter=50,
                    cv=self.cv_folds,
                    scoring=self.scoring,
                    n_jobs=self.n_jobs,
                    random_state=42
                )
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰
            searcher.fit(X_train, y_train)
            
            # ê²°ê³¼ ì €ì¥
            self.results[algo_name] = {
                'best_estimator': searcher.best_estimator_,
                'best_params': searcher.best_params_,
                'best_score': searcher.best_score_,
                'cv_results': searcher.cv_results_
            }
            
            print(f"    âœ… ìµœê³  CV ì ìˆ˜: {searcher.best_score_:.4f}")
    
    def evaluate_on_test(self, X_test, y_test):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… ì„±ëŠ¥ í‰ê°€"""
        print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€:")
        print(f"{'ì•Œê³ ë¦¬ì¦˜':<20} {'CV ì ìˆ˜':<12} {'í…ŒìŠ¤íŠ¸ ì ìˆ˜':<12} {'ì°¨ì´'}")
        print("-" * 60)
        
        test_results = {}
        for algo_name, result in self.results.items():
            best_model = result['best_estimator']
            cv_score = result['best_score']
            test_score = best_model.score(X_test, y_test)
            
            test_results[algo_name] = {
                'cv_score': cv_score,
                'test_score': test_score,
                'generalization_gap': cv_score - test_score
            }
            
            print(f"{algo_name:<20} {cv_score:<12.4f} {test_score:<12.4f} "
                  f"{cv_score - test_score:+.4f}")
        
        return test_results
    
    def get_best_model(self):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë°˜í™˜"""
        best_algo = max(self.results.items(), key=lambda x: x[1]['best_score'])
        return best_algo[0], best_algo[1]['best_estimator']
    
    def plot_comparison(self):
        """ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹œê°í™”"""
        algo_names = list(self.results.keys())
        cv_scores = [self.results[name]['best_score'] for name in algo_names]
        
        plt.figure(figsize=(12, 6))
        
        bars = plt.bar(algo_names, cv_scores, 
                      color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'],
                      alpha=0.7)
        
        plt.ylabel('êµì°¨ ê²€ì¦ ì ìˆ˜')
        plt.title('ì•Œê³ ë¦¬ì¦˜ë³„ ìµœì í™”ëœ ì„±ëŠ¥ ë¹„êµ')
        plt.xticks(rotation=45)
        
        # ë§‰ëŒ€ ìœ„ì— ì ìˆ˜ í‘œì‹œ
        for bar, score in zip(bars, cv_scores):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                     f'{score:.4f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()

# ìë™í™” ì‹œìŠ¤í…œ ì‹¤í–‰
automl = AutoMLTuner(cv_folds=5, scoring='accuracy')

# ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ íŠœë‹
automl.tune_all_algorithms(X_train, y_train, method='grid')

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… í‰ê°€
test_results = automl.evaluate_on_test(X_test, y_test)

# ìµœê³  ëª¨ë¸ í™•ì¸
best_algo_name, best_model = automl.get_best_model()
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ì•Œê³ ë¦¬ì¦˜: {best_algo_name}")
print(f"   ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {automl.results[best_algo_name]['best_params']}")

# ê²°ê³¼ ì‹œê°í™”
automl.plot_comparison()
```

**ìë™í™” ì‹œìŠ¤í…œì˜ ì¥ì **
- **ì¼ê´€ì„±**: ëª¨ë“  ì•Œê³ ë¦¬ì¦˜ì— ë™ì¼í•œ í‰ê°€ ê¸°ì¤€ ì ìš©
- **íš¨ìœ¨ì„±**: í•œ ë²ˆì˜ ì‹¤í–‰ìœ¼ë¡œ ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ
- **ì¬í˜„ì„±**: ì„¤ì •ì´ ì €ì¥ë˜ì–´ ì¬ì‹¤í–‰ ê°€ëŠ¥
- **í™•ì¥ì„±**: ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥

#### ğŸ› ï¸ ì‹¤ìŠµ í”„ë¡œì íŠ¸: ì¢…í•© ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ

ì´ì œ ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì™„ì „í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤.

```python
# ì‹¤ì œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì¢…í•© ìµœì í™” í”„ë¡œì íŠ¸
print("ğŸ› ï¸ ì¢…í•© ì„±ëŠ¥ ìµœì í™” í”„ë¡œì íŠ¸")
print("=" * 50)

# ì†ê¸€ì”¨ ìˆ«ì ë°ì´í„° ë¡œë“œ (ë” ì‹¤ìš©ì ì¸ ì˜ˆì œ)
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

print(f"ğŸ“Š í”„ë¡œì íŠ¸ ë°ì´í„°ì…‹ ì •ë³´:")
print(f"  ë°ì´í„° í¬ê¸°: {X_digits.shape}")
print(f"  í´ë˜ìŠ¤ ìˆ˜: {len(np.unique(y_digits))}")
print(f"  í´ë˜ìŠ¤ë³„ ë¶„í¬: {np.bincount(y_digits)}")

# ë°ì´í„° ë¶„í• 
X_train_proj, X_test_proj, y_train_proj, y_test_proj = train_test_split(
    X_digits, y_digits, test_size=0.2, random_state=42, stratify=y_digits
)

print(f"\nğŸ”„ í”„ë¡œì íŠ¸ ì›Œí¬í”Œë¡œìš°:")
workflow_steps = [
    "1. ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •",
    "2. ë‹¨ì¼ ì•Œê³ ë¦¬ì¦˜ ìµœì í™”",
    "3. ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ìµœì í™”",
    "4. ì•™ìƒë¸” ìµœì í™”",
    "5. ìµœì¢… ì„±ëŠ¥ í‰ê°€ ë° í•´ì„"
]

for step in workflow_steps:
    print(f"  {step}")

# 1ë‹¨ê³„: ê¸°ë³¸ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ì¸¡ì •
print(f"\n1ï¸âƒ£ ê¸°ë³¸ ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •")
print("-" * 30)

baseline_models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

baseline_scores = {}
for name, model in baseline_models.items():
    scores = cross_val_score(model, X_train_proj, y_train_proj, cv=5)
    baseline_scores[name] = np.mean(scores)
    print(f"  {name}: {np.mean(scores):.4f} (Â±{np.std(scores):.4f})")

# 2ë‹¨ê³„: Random Forest ì‹¬í™” ìµœì í™”
print(f"\n2ï¸âƒ£ Random Forest ì‹¬í™” ìµœì í™”")
print("-" * 30)

# ë‹¨ê³„ì  ìµœì í™” (Coarse-to-Fine)
# 1ì°¨: ë„“ì€ ë²”ìœ„ íƒìƒ‰
coarse_param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [5, 10, 20, None],
    'min_samples_split': [2, 5, 10]
}

print("  ğŸ” 1ì°¨ íƒìƒ‰ (ë„“ì€ ë²”ìœ„)...")
coarse_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    coarse_param_grid,
    cv=5,
    n_jobs=-1
)
coarse_search.fit(X_train_proj, y_train_proj)

print(f"    ìµœê³  ì ìˆ˜: {coarse_search.best_score_:.4f}")
print(f"    ìµœì  íŒŒë¼ë¯¸í„°: {coarse_search.best_params_}")

# 2ì°¨: ìµœì  ê°’ ì£¼ë³€ ì •ë°€ íƒìƒ‰
best_n_est = coarse_search.best_params_['n_estimators']
best_depth = coarse_search.best_params_['max_depth']

fine_param_grid = {
    'n_estimators': [max(50, best_n_est-50), best_n_est, best_n_est+50],
    'max_depth': [best_depth] if best_depth else [None],
    'min_samples_split': [2, 3, 4, 5],
    'min_samples_leaf': [1, 2, 3]
}

print("  ğŸ¯ 2ì°¨ íƒìƒ‰ (ì •ë°€ ì¡°ì •)...")
fine_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    fine_param_grid,
    cv=5,
    n_jobs=-1
)
fine_search.fit(X_train_proj, y_train_proj)

print(f"    ìµœê³  ì ìˆ˜: {fine_search.best_score_:.4f}")
print(f"    ìµœì  íŒŒë¼ë¯¸í„°: {fine_search.best_params_}")

# 3ë‹¨ê³„: ì•™ìƒë¸” ìµœì í™”
print(f"\n3ï¸âƒ£ ì•™ìƒë¸” ìµœì í™”")
print("-" * 30)

from sklearn.ensemble import VotingClassifier

# ê°œë³„ ìµœì í™”ëœ ëª¨ë¸ë“¤ë¡œ ì•™ìƒë¸” êµ¬ì„±
optimized_rf = fine_search.best_estimator_
optimized_svm = SVC(C=10, gamma='scale', probability=True, random_state=42)  # ê°„ë‹¨íˆ ì„¤ì •
optimized_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# ì†Œí”„íŠ¸ íˆ¬í‘œ ì•™ìƒë¸”
ensemble = VotingClassifier(
    estimators=[
        ('rf', optimized_rf),
        ('svm', optimized_svm),
        ('gb', optimized_gb)
    ],
    voting='soft'
)

# ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
ensemble_scores = cross_val_score(ensemble, X_train_proj, y_train_proj, cv=5)
print(f"  ì•™ìƒë¸” CV ì ìˆ˜: {np.mean(ensemble_scores):.4f} (Â±{np.std(ensemble_scores):.4f})")

# 4ë‹¨ê³„: ìµœì¢… ì„±ëŠ¥ ë¹„êµ
print(f"\n4ï¸âƒ£ ìµœì¢… ì„±ëŠ¥ ë¹„êµ")
print("-" * 30)

final_models = {
    'ê¸°ë³¸ Random Forest': RandomForestClassifier(random_state=42),
    'ìµœì í™”ëœ Random Forest': fine_search.best_estimator_,
    'ì•™ìƒë¸” ëª¨ë¸': ensemble
}

print(f"{'ëª¨ë¸':<25} {'CV ì ìˆ˜':<12} {'í…ŒìŠ¤íŠ¸ ì ìˆ˜':<12} {'ê°œì„ í­'}")
print("-" * 65)

final_results = {}
for name, model in final_models.items():
    # êµì°¨ ê²€ì¦ ì ìˆ˜
    cv_scores = cross_val_score(model, X_train_proj, y_train_proj, cv=5)
    cv_mean = np.mean(cv_scores)
    
    # í…ŒìŠ¤íŠ¸ ì ìˆ˜
    model.fit(X_train_proj, y_train_proj)
    test_score = model.score(X_test_proj, y_test_proj)
    
    # ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ í­
    baseline_score = baseline_scores['Random Forest']
    improvement = test_score - baseline_score
    
    final_results[name] = {
        'cv_score': cv_mean,
        'test_score': test_score,
        'improvement': improvement
    }
    
    print(f"{name:<25} {cv_mean:<12.4f} {test_score:<12.4f} {improvement:+.4f}")

# ìµœì¢… ê¶Œê³ ì‚¬í•­
print(f"\nğŸ“‹ ìµœì¢… ë¶„ì„ ë° ê¶Œê³ ì‚¬í•­:")
best_model_name = max(final_results.keys(), key=lambda x: final_results[x]['test_score'])
best_score = final_results[best_model_name]['test_score']
total_improvement = final_results[best_model_name]['improvement']

print(f"  ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name}")
print(f"  ğŸ“ˆ ìµœì¢… í…ŒìŠ¤íŠ¸ ì ìˆ˜: {best_score:.4f}")
print(f"  ğŸš€ ì´ ì„±ëŠ¥ í–¥ìƒ: {total_improvement:+.4f} ({total_improvement*100:+.2f}%p)")
print(f"  ğŸ’¡ ê¶Œê³ ì‚¬í•­:")
print(f"    - ë‹¨ê³„ì  ìµœì í™”ê°€ {final_results['ìµœì í™”ëœ Random Forest']['improvement']:+.4f} ê°œì„  ë‹¬ì„±")
print(f"    - ì•™ìƒë¸”ì´ ì¶”ê°€ë¡œ {final_results['ì•™ìƒë¸” ëª¨ë¸']['improvement'] - final_results['ìµœì í™”ëœ Random Forest']['improvement']:+.4f} ê°œì„ ")
print(f"    - ì‹¤ë¬´ì—ì„œëŠ” ë³µì¡ë„ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì„ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ ì„ íƒ")
```

**í”„ë¡œì íŠ¸ì—ì„œ ë°°ìš´ í•µì‹¬ êµí›ˆ**
- **ë‹¨ê³„ì  ì ‘ê·¼**: Coarse-to-Fine ìµœì í™”ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
- **ì„±ëŠ¥ ê°œì„ ì˜ í•œê³„**: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ë§Œìœ¼ë¡œëŠ” í•œê³„ê°€ ìˆìŒ
- **ì•™ìƒë¸”ì˜ í˜**: ê°œë³„ ëª¨ë¸ ìµœì í™” + ì•™ìƒë¸”ë¡œ ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ
- **ì‹¤ë¬´ ê´€ì **: ì„±ëŠ¥ í–¥ìƒê³¼ ë³µì¡ë„ ì¦ê°€ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤

---

### ğŸ’ª ì§ì ‘ í•´ë³´ê¸° - ì—°ìŠµ ë¬¸ì œ

#### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 1: ë©€í‹°í´ë˜ìŠ¤ ë¶„ë¥˜ ìµœì í™”
ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ì™€ì¸ ë°ì´í„°ì…‹ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ ìµœì í™”í•´ë³´ì„¸ìš”.

```python
# TODO: ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”
from sklearn.datasets import load_wine

# ì™€ì¸ ë°ì´í„°ì…‹ ë¡œë“œ
wine = load_wine()
X_wine, y_wine = wine.data, wine.target

# TODO: ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•˜ì„¸ìš”
# 1. ë°ì´í„°ë¥¼ í›ˆë ¨/í…ŒìŠ¤íŠ¸ë¡œ ë¶„í•  (stratify ì ìš©)
# 2. 3ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
# 3. ê° ì•Œê³ ë¦¬ì¦˜ë³„ë¡œ GridSearchCV ìˆ˜í–‰
# 4. ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ ì • ë° í…ŒìŠ¤íŠ¸ í‰ê°€
# 5. ê²°ê³¼ë¥¼ í‘œì™€ ê·¸ë˜í”„ë¡œ ì‹œê°í™”

# íŒíŠ¸: ì™€ì¸ ë°ì´í„°ëŠ” 3ê°œ í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ë©€í‹°í´ë˜ìŠ¤ ë¬¸ì œì…ë‹ˆë‹¤
```

#### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 2: ì‹œê°„ ì œì•½ ì¡°ê±´í•˜ì˜ ìµœì í™”
ì œí•œëœ ì‹œê°„ ë‚´ì—ì„œ ìµœì ì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ì „ëµì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
# TODO: ì‹œê°„ ì œì•½ ì¡°ê±´í•˜ì˜ ìµœì í™” ì „ëµ
import time

# ì‹œê°„ ì œí•œ: 5ë¶„ (300ì´ˆ)
TIME_LIMIT = 300

# TODO: ë‹¤ìŒ ì „ëµì„ êµ¬í˜„í•˜ì„¸ìš”
# 1. ë¹ ë¥¸ ì•Œê³ ë¦¬ì¦˜ë¶€í„° ì‹œì‘í•˜ì—¬ ê¸°ë³¸ ì„±ëŠ¥ í™•ì¸
# 2. ë‚¨ì€ ì‹œê°„ì— ë”°ë¼ RandomizedSearchCV ë°˜ë³µ íšŸìˆ˜ ì¡°ì •
# 3. ì‹œê°„ì´ ë¶€ì¡±í•˜ë©´ ê°€ì¥ ìœ ë§í•œ ì•Œê³ ë¦¬ì¦˜ì—ë§Œ ì§‘ì¤‘
# 4. ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¶”ì  ë° ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ êµ¬í˜„

def time_constrained_optimization(X, y, time_limit):
    start_time = time.time()
    # TODO: êµ¬í˜„
    pass

# ì‹¤í–‰ ë° ê²°ê³¼ ë¶„ì„
```

#### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 3: ì»¤ìŠ¤í…€ ìµœì í™” ì „ëµ
ìì‹ ë§Œì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì „ëµì„ ì„¤ê³„í•´ë³´ì„¸ìš”.

```python
# TODO: ì°½ì˜ì ì¸ ìµœì í™” ì „ëµ êµ¬í˜„
class CreativeOptimizer:
    def __init__(self):
        # TODO: ì´ˆê¸°í™”
        pass
    
    def smart_search(self, X, y):
        """
        ë‹¤ìŒ ì•„ì´ë””ì–´ ì¤‘ ì¼ë¶€ë¥¼ êµ¬í˜„í•´ë³´ì„¸ìš”:
        1. ì´ì „ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì ì‘ì  ê²€ìƒ‰ ê³µê°„ ì¡°ì •
        2. ì„±ëŠ¥ì´ ì¢‹ì€ íŒŒë¼ë¯¸í„° ì¡°í•© ì£¼ë³€ì˜ ì§‘ì¤‘ íƒìƒ‰
        3. ì—¬ëŸ¬ ìµœì í™” ë°©ë²•ì˜ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
        4. ì•™ìƒë¸” ê°€ì¤‘ì¹˜ì˜ ë™ì  ì¡°ì •
        5. ë©”íƒ€ ëŸ¬ë‹ì„ í†µí•œ ì´ˆê¸° íŒŒë¼ë¯¸í„° ì¶”ì²œ
        """
        # TODO: êµ¬í˜„
        pass

# ì°½ì˜ì  ìµœì í™”ê¸° í…ŒìŠ¤íŠ¸
optimizer = CreativeOptimizer()
# TODO: í…ŒìŠ¤íŠ¸ ë° í‰ê°€
```

---

### ğŸ“š í•µì‹¬ ì •ë¦¬

#### âœ¨ ì´ë²ˆ íŒŒíŠ¸ì—ì„œ ë°°ìš´ ë‚´ìš©

**1. í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ì´í•´**
- í•˜ì´í¼íŒŒë¼ë¯¸í„° vs íŒŒë¼ë¯¸í„°ì˜ ì°¨ì´
- ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ê²°ì •ì  ì˜í–¥
- ì•Œê³ ë¦¬ì¦˜ë³„ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠ¹ì„±

**2. ê·¸ë¦¬ë“œ ì„œì¹˜ (Grid Search)**
- ì²´ê³„ì ì¸ ì „ìˆ˜ ì¡°ì‚¬ ë°©ë²•
- ëª¨ë“  ì¡°í•©ì„ ë¹ ì§ì—†ì´ íƒìƒ‰
- ê³„ì‚° ë¹„ìš©ì´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìˆ˜ì— ë”°ë¼ ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€

**3. ëœë¤ ì„œì¹˜ (Random Search)**
- í™•ë¥ ì  ìƒ˜í”Œë§ì„ í†µí•œ íš¨ìœ¨ì  íƒìƒ‰
- ì—°ì†í˜• í•˜ì´í¼íŒŒë¼ë¯¸í„° ì²˜ë¦¬ ìš°ìˆ˜
- ì œí•œëœ ì‹œê°„ ë‚´ì—ì„œ ë„“ì€ ê³µê°„ íƒìƒ‰ ê°€ëŠ¥

**4. ë² ì´ì§€ì•ˆ ìµœì í™” (Bayesian Optimization)**
- ì´ì „ ê²°ê³¼ë¥¼ í•™ìŠµí•˜ì—¬ ë‹¤ìŒ íƒìƒ‰ ì§€ì  ì§€ëŠ¥ì  ì„ íƒ
- ëŒ€ë¦¬ ëª¨ë¸ê³¼ íšë“ í•¨ìˆ˜ë¥¼ í†µí•œ íš¨ìœ¨ì  íƒìƒ‰
- ë¹„ìš©ì´ ë†’ì€ ëª¨ë¸ ìµœì í™”ì— íŠ¹íˆ ìœ ìš©

**5. êµì°¨ ê²€ì¦ê³¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í‰ê°€**
- ë‹¤ì–‘í•œ CV ì „ëµì˜ íŠ¹ì„±ê³¼ ì„ íƒ ê¸°ì¤€
- í•™ìŠµ ê³¡ì„ ê³¼ ê²€ì¦ ê³¡ì„ ì„ í†µí•œ ëª¨ë¸ ì§„ë‹¨
- ì¼ë°˜í™” ì„±ëŠ¥ì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶”ì •

**6. ìë™í™”ëœ íŠœë‹ ì‹œìŠ¤í…œ**
- ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‹œ ìµœì í™”
- ë‹¨ê³„ì  ìµœì í™” (Coarse-to-Fine) ì „ëµ
- ì•™ìƒë¸”ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ì˜ ê²°í•©

#### ğŸ¯ ì‹¤ë¬´ ì ìš© ê°€ì´ë“œë¼ì¸

**ìµœì í™” ë°©ë²• ì„ íƒ ê²°ì • íŠ¸ë¦¬**
```
í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ 3ê°œ ì´í•˜?
â”œâ”€ Yes â†’ ì‹œê°„ì´ ì¶©ë¶„? 
â”‚   â”œâ”€ Yes â†’ ê·¸ë¦¬ë“œ ì„œì¹˜
â”‚   â””â”€ No â†’ ëœë¤ ì„œì¹˜
â””â”€ No â†’ ì •í™•ë„ê°€ ë§¤ìš° ì¤‘ìš”?
    â”œâ”€ Yes â†’ ë² ì´ì§€ì•ˆ ìµœì í™”
    â””â”€ No â†’ ëœë¤ ì„œì¹˜
```

**íš¨ìœ¨ì ì¸ ìµœì í™” ì›Œí¬í”Œë¡œìš°**
1. **ê¸°ë³¸ ì„±ëŠ¥ ì¸¡ì •**: ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ í™•ë¦½
2. **ë¹ ë¥¸ íƒìƒ‰**: ëœë¤ ì„œì¹˜ë¡œ ìœ ë§í•œ ì˜ì—­ íŒŒì•…
3. **ì§‘ì¤‘ íƒìƒ‰**: ê·¸ë¦¬ë“œ ì„œì¹˜ë¡œ ìµœì ê°’ ì£¼ë³€ ì •ë°€ ì¡°ì •
4. **ì•™ìƒë¸” í™œìš©**: ê°œë³„ ìµœì í™” + ì•™ìƒë¸”ë¡œ ì„±ëŠ¥ ê·¹ëŒ€í™”
5. **ì‹¤ë¬´ ê²€ì¦**: ì‹¤ì œ í™˜ê²½ì—ì„œì˜ ì•ˆì •ì„± í™•ì¸

**ì£¼ì˜ì‚¬í•­ê³¼ í•¨ì •**
- **ë°ì´í„° ëˆ„ìˆ˜**: ì „ì²´ ë°ì´í„°ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ í›„ ê°™ì€ ë°ì´í„°ë¡œ í‰ê°€
- **ê³¼ìµœì í™”**: ê²€ì¦ ì„¸íŠ¸ì— ê³¼ë„í•˜ê²Œ ë§ì¶”ì–´ ì¼ë°˜í™” ì„±ëŠ¥ ì €í•˜
- **ê³„ì‚° ë¹„ìš©**: ì„±ëŠ¥ í–¥ìƒê³¼ ê³„ì‚° ì‹œê°„ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤
- **ì¬í˜„ì„±**: random_state ì„¤ì •ìœ¼ë¡œ ê²°ê³¼ ì¬í˜„ ê°€ëŠ¥ì„± í™•ë³´

#### ğŸ’¡ ê³ ê¸‰ íŒê³¼ ì‹¤ë¬´ ì§€í˜œ

**ì„±ëŠ¥ í–¥ìƒì˜ ìš°ì„ ìˆœìœ„**
1. **ë°ì´í„° í’ˆì§ˆ**: ì¢‹ì€ ë°ì´í„°ê°€ ìµœê³ ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³´ë‹¤ ì¤‘ìš”
2. **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§**: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± ìƒì„±
3. **ì•Œê³ ë¦¬ì¦˜ ì„ íƒ**: ë¬¸ì œì— ì í•©í•œ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ
4. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**: ë§ˆì§€ë§‰ ì„±ëŠ¥ ëŒì–´ì˜¬ë¦¬ê¸°

**ì‹¤ë¬´ì—ì„œì˜ ê· í˜•ì **
- **ì„±ëŠ¥ vs í•´ì„ì„±**: ë³µì¡í•œ ëª¨ë¸ì¼ìˆ˜ë¡ í•´ì„ ì–´ë ¤ì›€
- **ì •í™•ë„ vs ì†ë„**: ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ì„œëŠ” ì‘ë‹µ ì‹œê°„ ì¤‘ìš”
- **ë³µì¡ë„ vs ìœ ì§€ë³´ìˆ˜**: ê°„ë‹¨í•œ ëª¨ë¸ì´ ìš´ì˜í•˜ê¸° ì‰¬ì›€

---

### ğŸ”® ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

ë‹¤ìŒ Part 4ì—ì„œëŠ” **AIì™€ í˜‘ì—…ì„ í†µí•œ ëª¨ë¸ ê°œì„ **ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤:

- ğŸ¤ **AI ìƒì„± ì½”ë“œ ê²€ì¦**: ìë™ ìƒì„±ëœ ëª¨ë¸ë§ ì½”ë“œì˜ í’ˆì§ˆ í‰ê°€
- ğŸ” **ëª¨ë¸ í•´ì„ì„± í–¥ìƒ**: AIê°€ ìƒì„±í•œ ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì • ê³¼ì • ì´í•´
- âš–ï¸ **ì¸ê°„-AI í˜‘ì—…**: ì „ë¬¸ê°€ ì§€ì‹ê³¼ AI ìµœì í™”ì˜ ê· í˜•
- ğŸ›¡ï¸ **ëª¨ë¸ ì•ˆì •ì„± ê²€ì¦**: AI ìµœì í™” ëª¨ë¸ì˜ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸
- ğŸš€ **ì‹¤ìŠµ**: ChatGPT/Claudeì™€ í˜‘ì—…í•˜ëŠ” ì§€ëŠ¥ì  ëª¨ë¸ë§ ì‹œìŠ¤í…œ

í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í–ˆë‹¤ë©´, ì´ì œ AIì™€ í˜‘ì—…í•˜ì—¬ ë”ìš± ìŠ¤ë§ˆíŠ¸í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤!

---

*"ìµœì í™”ëŠ” ëì´ ì•„ë‹ˆë¼ ì‹œì‘ì´ë‹¤. ì§„ì •í•œ ê°€ì¹˜ëŠ” ìµœì í™”ëœ ëª¨ë¸ì„ í˜„ì‹¤ì— ì ìš©í•  ë•Œ ë‚˜íƒ€ë‚œë‹¤." - ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ì˜ ì² í•™*