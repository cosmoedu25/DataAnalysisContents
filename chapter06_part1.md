# 6ì¥ Part 1: ì•™ìƒë¸” í•™ìŠµê³¼ íˆ¬í‘œ ë°©ì‹
## ì§‘ë‹¨ ì§€í˜œë¡œ ë” ë‚˜ì€ ì˜ˆì¸¡í•˜ê¸°

### í•™ìŠµ ëª©í‘œ
ì´ë²ˆ íŒŒíŠ¸ë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ëŠ¥ë ¥ì„ ê°–ê²Œ ë©ë‹ˆë‹¤:
- ì•™ìƒë¸” í•™ìŠµì˜ í•µì‹¬ ê°œë…ê³¼ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë°°ê¹…(Bagging)ê³¼ ë¶€ìŠ¤íŒ…(Boosting)ì˜ ì°¨ì´ì ì„ êµ¬ë¶„í•˜ê³  ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ë‹¤ì–‘í•œ íˆ¬í‘œ ë°©ì‹(í•˜ë“œ íˆ¬í‘œ, ì†Œí”„íŠ¸ íˆ¬í‘œ)ì„ êµ¬í˜„í•˜ê³  í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìŠ¤íƒœí‚¹ ì•™ìƒë¸”ì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ ë¬¸ì œì— ì•™ìƒë¸” ê¸°ë²•ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

---

### 6.1 ì•™ìƒë¸” í•™ìŠµì´ë€? - ì§‘ë‹¨ ì§€í˜œì˜ í˜

#### ğŸ¯ ì‹¤ìƒí™œ ì† ì•™ìƒë¸”ì˜ ì˜ˆ
ë‹¹ì‹ ì´ ì–´ë ¤ìš´ ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ê³  ìˆë‹¤ê³  ìƒìƒí•´ë³´ì„¸ìš”. í˜¼ìì„œëŠ” í™•ì‹ ì´ ì„œì§€ ì•Šì•„ ì¹œêµ¬ 3ëª…ì—ê²Œ ë„ì›€ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. í•œ ì¹œêµ¬ëŠ” ê¸°í•˜ë¥¼ ì˜í•˜ê³ , ë‹¤ë¥¸ ì¹œêµ¬ëŠ” ëŒ€ìˆ˜ë¥¼ ì˜í•˜ë©°, ë§ˆì§€ë§‰ ì¹œêµ¬ëŠ” ì „ì²´ì ì¸ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. 

ì´ë•Œ ì„¸ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë‹µì„ ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
1. **ë‹¤ìˆ˜ê²° íˆ¬í‘œ**: ì„¸ ëª… ì¤‘ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ë‹µì„ ì„ íƒ
2. **ê°€ì¤‘ íˆ¬í‘œ**: ê° ì¹œêµ¬ì˜ ì‹¤ë ¥ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ ê²°ì •
3. **ë‹¨ê³„ë³„ í™œìš©**: ì²« ë²ˆì§¸ ì¹œêµ¬ë“¤ì´ í›„ë³´ ë‹µì•ˆì„ ë§Œë“¤ê³ , ë§ˆì§€ë§‰ ì¹œêµ¬ê°€ ìµœì¢… ê²°ì •

**ì•™ìƒë¸” í•™ìŠµ**ë„ ì´ì™€ ê°™ì€ ì›ë¦¬ì…ë‹ˆë‹¤! ì—¬ëŸ¬ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸(í•™ìŠµì)ì„ ê²°í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

#### ğŸ” ì•™ìƒë¸” í•™ìŠµì˜ í•µì‹¬ ì›ë¦¬

**1. ë‹¤ì–‘ì„±(Diversity)ì˜ ì¤‘ìš”ì„±**
```python
# ì•™ìƒë¸”ì´ íš¨ê³¼ì ì¸ ì´ìœ ë¥¼ ê°„ë‹¨í•œ ì˜ˆë¡œ ì‚´í´ë³´ê¸°
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# ì‹œê°í™”ë¥¼ ìœ„í•œ í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ìƒì„± (ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ì™€ ìœ ì‚¬í•œ íŠ¹ì„±)
X, y = make_classification(
    n_samples=1000,      # 1000ê°œì˜ ìƒ˜í”Œ
    n_features=20,       # 20ê°œì˜ íŠ¹ì„±
    n_informative=10,    # ì‹¤ì œë¡œ ìœ ìš©í•œ íŠ¹ì„± 10ê°œ
    n_redundant=10,      # ì¤‘ë³µëœ íŠ¹ì„± 10ê°œ  
    n_clusters_per_class=1,  # í´ë˜ìŠ¤ë‹¹ í´ëŸ¬ìŠ¤í„° ìˆ˜
    random_state=42
)

# ë°ì´í„°ë¥¼ í›ˆë ¨ìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print("ğŸ¯ ì•™ìƒë¸” í•™ìŠµì˜ íš¨ê³¼ ì‹¤í—˜")
print("=" * 50)
print(f"í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]}ê°œ ìƒ˜í”Œ")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ ìƒ˜í”Œ")
print(f"íŠ¹ì„± ìˆ˜: {X_train.shape[1]}ê°œ")
```

**ì™œ ì´ ì½”ë“œê°€ ì¤‘ìš”í•œê°€?**
- `make_classification`: ì‹¤ì œ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ì™€ ë¹„ìŠ·í•œ íŠ¹ì„±ì„ ê°€ì§„ ê°€ìƒ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤
- `n_informative=10`: ì‹¤ì œë¡œ ìœ ìš©í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆëŠ” íŠ¹ì„±ì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤
- `n_redundant=10`: ì¤‘ë³µëœ ì •ë³´ë¥¼ ê°€ì§„ íŠ¹ì„±ë“¤ë„ í¬í•¨í•˜ì—¬ í˜„ì‹¤ì ì¸ ë°ì´í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤

**2. ê°œë³„ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ë¹„êµ**
```python
# ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ì„¸ ê°œì˜ ëª¨ë¸ ìƒì„±
models = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Logistic Regression': LogisticRegression(random_state=42),
    'SVM': SVC(random_state=42, probability=True)  # probability=Trueë¡œ í™•ë¥  ì˜ˆì¸¡ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
}

# ê° ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ í›ˆë ¨í•˜ê³  ì„±ëŠ¥ í‰ê°€
individual_scores = {}
for name, model in models.items():
    # ëª¨ë¸ í›ˆë ¨
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    individual_scores[name] = accuracy
    
    print(f"{name}: {accuracy:.4f}")

print(f"\nê°œë³„ ëª¨ë¸ í‰ê·  ì„±ëŠ¥: {np.mean(list(individual_scores.values())):.4f}")
```

**ì™œ ì´ ì½”ë“œê°€ ì¤‘ìš”í•œê°€?**
- **ì˜ì‚¬ê²°ì •ë‚˜ë¬´**: ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ë¶„ë¥˜í•˜ë©°, ë¹„ì„ í˜• ê´€ê³„ë¥¼ ì˜ í¬ì°©í•©ë‹ˆë‹¤
- **ë¡œì§€ìŠ¤í‹± íšŒê·€**: ì„ í˜• ê´€ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, í•´ì„ì´ ìš©ì´í•©ë‹ˆë‹¤  
- **SVM**: ë³µì¡í•œ ê²½ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©°, ê³ ì°¨ì› ë°ì´í„°ì— ê°•í•©ë‹ˆë‹¤
- `probability=True`: SVMì—ì„œ í™•ë¥  ì˜ˆì¸¡ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬ ì†Œí”„íŠ¸ íˆ¬í‘œì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

---

### 6.2 íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸” - ë¯¼ì£¼ì£¼ì˜ ì›ë¦¬ë¥¼ AIì— ì ìš©

#### ğŸ—³ï¸ í•˜ë“œ íˆ¬í‘œ (Hard Voting) - ë‹¤ìˆ˜ê²°ì˜ ì›ë¦¬

í•˜ë“œ íˆ¬í‘œëŠ” ê°€ì¥ ì§ê´€ì ì¸ ì•™ìƒë¸” ë°©ë²•ì…ë‹ˆë‹¤. ê° ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ ì¤‘ì—ì„œ ê°€ì¥ ë§ì´ ì„ íƒëœ í´ë˜ìŠ¤ë¥¼ ìµœì¢… ì˜ˆì¸¡ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.

```python
# í•˜ë“œ íˆ¬í‘œ ë¶„ë¥˜ê¸° ìƒì„± ë° í›ˆë ¨
hard_voting_clf = VotingClassifier(
    estimators=[
        ('dt', DecisionTreeClassifier(random_state=42)),
        ('lr', LogisticRegression(random_state=42)), 
        ('svm', SVC(random_state=42))
    ],
    voting='hard'  # í•˜ë“œ íˆ¬í‘œ ë°©ì‹ ì„ íƒ
)

# ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
hard_voting_clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
y_pred_hard = hard_voting_clf.predict(X_test)
hard_voting_accuracy = accuracy_score(y_test, y_pred_hard)

print("ğŸ—³ï¸ í•˜ë“œ íˆ¬í‘œ ê²°ê³¼")
print("=" * 30)
print(f"í•˜ë“œ íˆ¬í‘œ ì•™ìƒë¸” ì„±ëŠ¥: {hard_voting_accuracy:.4f}")
print(f"ê°œë³„ ëª¨ë¸ í‰ê·  ì„±ëŠ¥: {np.mean(list(individual_scores.values())):.4f}")
print(f"ì„±ëŠ¥ í–¥ìƒ: {hard_voting_accuracy - np.mean(list(individual_scores.values())):.4f}")
```

**ì™œ ì´ ë°©ì‹ì´ íš¨ê³¼ì ì¸ê°€?**
- ê° ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ì‹¤ìˆ˜ë¥¼ í•˜ë”ë¼ë„, ë‹¤ìˆ˜ê²°ë¡œ ì˜¬ë°”ë¥¸ ë‹µì„ ì°¾ì„ í™•ë¥ ì´ ë†’ì•„ì§‘ë‹ˆë‹¤
- íŠ¹íˆ ê°œë³„ ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì´ 50% ì´ìƒì´ê³  ì„œë¡œ ë…ë¦½ì ì¸ ì‹¤ìˆ˜ë¥¼ í•  ë•Œ ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤

#### ğŸ¯ ì†Œí”„íŠ¸ íˆ¬í‘œ (Soft Voting) - í™•ì‹ ì˜ ì •ë„ë¥¼ ê³ ë ¤

ì†Œí”„íŠ¸ íˆ¬í‘œëŠ” ë‹¨ìˆœíˆ í´ë˜ìŠ¤ë§Œ ë³´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê° ëª¨ë¸ì´ ì˜ˆì¸¡ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ëŠ”ì§€ë„ ê³ ë ¤í•©ë‹ˆë‹¤.

```python
# ì†Œí”„íŠ¸ íˆ¬í‘œ ë¶„ë¥˜ê¸° ìƒì„± ë° í›ˆë ¨
soft_voting_clf = VotingClassifier(
    estimators=[
        ('dt', DecisionTreeClassifier(random_state=42)),
        ('lr', LogisticRegression(random_state=42)),
        ('svm', SVC(random_state=42, probability=True))  # SVMì— í™•ë¥  ì˜ˆì¸¡ í™œì„±í™”
    ],
    voting='soft'  # ì†Œí”„íŠ¸ íˆ¬í‘œ ë°©ì‹ ì„ íƒ
)

# ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨
soft_voting_clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
y_pred_soft = soft_voting_clf.predict(X_test)
soft_voting_accuracy = accuracy_score(y_test, y_pred_soft)

print("\nğŸ¯ ì†Œí”„íŠ¸ íˆ¬í‘œ ê²°ê³¼")
print("=" * 30)
print(f"ì†Œí”„íŠ¸ íˆ¬í‘œ ì•™ìƒë¸” ì„±ëŠ¥: {soft_voting_accuracy:.4f}")
print(f"í•˜ë“œ íˆ¬í‘œ ì•™ìƒë¸” ì„±ëŠ¥: {hard_voting_accuracy:.4f}")
print(f"ê°œë³„ ëª¨ë¸ í‰ê·  ì„±ëŠ¥: {np.mean(list(individual_scores.values())):.4f}")
```

**ì†Œí”„íŠ¸ íˆ¬í‘œì˜ ì¥ì **
- í™•ì‹ ì´ ë†’ì€ ëª¨ë¸ì˜ ì˜ê²¬ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ì¤ë‹ˆë‹¤
- ë¯¸ë¬˜í•œ ì°¨ì´ë¥¼ ë” ì˜ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì¼ë°˜ì ìœ¼ë¡œ í•˜ë“œ íˆ¬í‘œë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤

#### ğŸ“Š íˆ¬í‘œ ê³¼ì • ì‹œê°í™”í•˜ê¸°

```python
# ê°œë³„ ì˜ˆì¸¡ í™•ë¥  í™•ì¸ (ì²˜ìŒ 5ê°œ ìƒ˜í”Œ)
sample_idx = range(5)
X_sample = X_test[sample_idx]
y_sample = y_test[sample_idx]

print("\nğŸ“Š ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê³¼ì • ë¶„ì„")
print("=" * 50)

for i, (name, model) in enumerate(models.items()):
    model.fit(X_train, y_train)  # ëª¨ë¸ ì¬í›ˆë ¨
    
    if hasattr(model, 'predict_proba'):
        proba = model.predict_proba(X_sample)
        pred = model.predict(X_sample)
        
        print(f"\n{name} ì˜ˆì¸¡:")
        for j in range(len(sample_idx)):
            print(f"  ìƒ˜í”Œ {j+1}: í´ë˜ìŠ¤ {pred[j]} (í™•ë¥ : {proba[j][pred[j]]:.3f})")
    else:
        pred = model.predict(X_sample)
        print(f"\n{name} ì˜ˆì¸¡:")
        for j in range(len(sample_idx)):
            print(f"  ìƒ˜í”Œ {j+1}: í´ë˜ìŠ¤ {pred[j]}")

# ì‹¤ì œ ì •ë‹µ
print(f"\nì‹¤ì œ ì •ë‹µ: {y_sample}")
```

**ì™œ ì´ëŸ° ë¶„ì„ì´ ì¤‘ìš”í•œê°€?**
- ê° ëª¨ë¸ì´ ì–´ë–¤ ê·¼ê±°ë¡œ ì˜ˆì¸¡í•˜ëŠ”ì§€ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ëª¨ë¸ë“¤ ê°„ì˜ ì˜ê²¬ ì°¨ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ì•™ìƒë¸”ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

---

### 6.3 ë°°ê¹…(Bagging) - ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ ë‹¤ì–‘ì„± ë§Œë“¤ê¸°

#### ğŸ’ ë°°ê¹…ì˜ í•µì‹¬ ì•„ì´ë””ì–´

**ë°°ê¹…**ì€ "Bootstrap Aggregating"ì˜ ì¤„ì„ë§ì…ë‹ˆë‹¤. ë§ˆì¹˜ ê°™ì€ ë°˜ í•™ìƒë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì œì§‘ìœ¼ë¡œ ê³µë¶€í•œ í›„ ì‹œí—˜ì„ ë³´ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

# ë°°ê¹… ë¶„ë¥˜ê¸° ìƒì„±
bagging_clf = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(random_state=42),  # ê¸°ë³¸ ëª¨ë¸ë¡œ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì‚¬ìš©
    n_estimators=100,    # 100ê°œì˜ ëª¨ë¸ì„ ìƒì„±
    max_samples=0.8,     # ê° ëª¨ë¸ì€ ì „ì²´ ë°ì´í„°ì˜ 80%ë§Œ ì‚¬ìš©
    max_features=0.8,    # ê° ëª¨ë¸ì€ ì „ì²´ íŠ¹ì„±ì˜ 80%ë§Œ ì‚¬ìš©
    bootstrap=True,      # ë³µì› ì¶”ì¶œ ë°©ì‹ ì‚¬ìš©
    random_state=42
)

# ëª¨ë¸ í›ˆë ¨
bagging_clf.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
y_pred_bagging = bagging_clf.predict(X_test)
bagging_accuracy = accuracy_score(y_test, y_pred_bagging)

print("ğŸ’ ë°°ê¹… ë¶„ë¥˜ê¸° ê²°ê³¼")
print("=" * 30)
print(f"ë°°ê¹… ë¶„ë¥˜ê¸° ì„±ëŠ¥: {bagging_accuracy:.4f}")
print(f"ë‹¨ì¼ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì„±ëŠ¥: {individual_scores['Decision Tree']:.4f}")
print(f"ì„±ëŠ¥ í–¥ìƒ: {bagging_accuracy - individual_scores['Decision Tree']:.4f}")
```

**ë°°ê¹…ì´ íš¨ê³¼ì ì¸ ì´ìœ **
- **ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§**: ê° ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ë¡œ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤
- **ë¶„ì‚° ê°ì†Œ**: ì—¬ëŸ¬ ëª¨ë¸ì˜ í‰ê· ì„ ë‚´ë©´ ê°œë³„ ëª¨ë¸ì˜ ë¶ˆì•ˆì •ì„±ì´ ì¤„ì–´ë“­ë‹ˆë‹¤
- **ê³¼ì í•© ë°©ì§€**: ê° ëª¨ë¸ì´ ì „ì²´ ë°ì´í„°ì˜ ì¼ë¶€ë§Œ ë³´ë¯€ë¡œ ê³¼ì í•©ì´ ì¤„ì–´ë“­ë‹ˆë‹¤

#### ğŸŒ³ ëœë¤ í¬ë ˆìŠ¤íŠ¸ - ë°°ê¹…ì˜ ë°œì „ëœ í˜•íƒœ

```python
from sklearn.ensemble import RandomForestClassifier

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° ìƒì„±
rf_clf = RandomForestClassifier(
    n_estimators=100,     # 100ê°œì˜ íŠ¸ë¦¬
    max_depth=10,         # ê° íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
    min_samples_split=5,  # ë…¸ë“œ ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    min_samples_leaf=2,   # ë¦¬í”„ ë…¸ë“œì˜ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
    random_state=42
)

# ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
rf_clf.fit(X_train, y_train)
y_pred_rf = rf_clf.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)

print("\nğŸŒ³ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ê²°ê³¼")
print("=" * 30)
print(f"ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì„±ëŠ¥: {rf_accuracy:.4f}")
print(f"ë°°ê¹… ë¶„ë¥˜ê¸° ì„±ëŠ¥: {bagging_accuracy:.4f}")
print(f"ë‹¨ì¼ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì„±ëŠ¥: {individual_scores['Decision Tree']:.4f}")

# íŠ¹ì„± ì¤‘ìš”ë„ í™•ì¸
feature_importance = rf_clf.feature_importances_
print(f"\nê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„± ìƒìœ„ 5ê°œ:")
for i in np.argsort(feature_importance)[-5:][::-1]:
    print(f"  íŠ¹ì„± {i}: {feature_importance[i]:.4f}")
```

**ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ì¶”ê°€ ì¥ì **
- **íŠ¹ì„± ë¬´ì‘ìœ„ì„±**: ê° ë…¸ë“œì—ì„œ ì¼ë¶€ íŠ¹ì„±ë§Œ ê³ ë ¤í•˜ì—¬ ë”ìš± ë‹¤ì–‘í•œ íŠ¸ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤
- **íŠ¹ì„± ì¤‘ìš”ë„**: ì–´ë–¤ íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ì¤‘ìš”í•œì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤
- **ë¹ ë¥¸ í›ˆë ¨**: ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ì—¬ í›ˆë ¨ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤

---

### 6.4 ë¶€ìŠ¤íŒ…(Boosting) - ì•½í•œ í•™ìŠµìë“¤ì˜ í˜‘ë ¥

#### ğŸš€ ë¶€ìŠ¤íŒ…ì˜ í•µì‹¬ ì•„ì´ë””ì–´

ë¶€ìŠ¤íŒ…ì€ "ì•½í•œ í•™ìŠµì"ë“¤ì´ ì„œë¡œ í˜‘ë ¥í•˜ì—¬ "ê°•í•œ í•™ìŠµì"ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì¹˜ ìš´ë™ ì„ ìˆ˜ê°€ ì½”ì¹˜ì˜ ì§€ì ì„ ë°›ì•„ê°€ë©° ì ì  ì‹¤ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

```python
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier

# AdaBoost ë¶„ë¥˜ê¸° ìƒì„±
ada_clf = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),  # ì•½í•œ í•™ìŠµì (ìŠ¤í…€í”„)
    n_estimators=100,     # 100ê°œì˜ ì•½í•œ í•™ìŠµì
    learning_rate=1.0,    # í•™ìŠµë¥ 
    random_state=42
)

# ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
ada_clf.fit(X_train, y_train)
y_pred_ada = ada_clf.predict(X_test)
ada_accuracy = accuracy_score(y_test, y_pred_ada)

print("ğŸš€ AdaBoost ê²°ê³¼")
print("=" * 30)
print(f"AdaBoost ì„±ëŠ¥: {ada_accuracy:.4f}")

# Gradient Boosting ë¶„ë¥˜ê¸° ìƒì„±
gb_clf = GradientBoostingClassifier(
    n_estimators=100,     # 100ê°œì˜ íŠ¸ë¦¬
    learning_rate=0.1,    # í•™ìŠµë¥  (ì‘ì„ìˆ˜ë¡ ì•ˆì •ì )
    max_depth=3,          # ê° íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´
    random_state=42
)

# ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
gb_clf.fit(X_train, y_train)
y_pred_gb = gb_clf.predict(X_test)
gb_accuracy = accuracy_score(y_test, y_pred_gb)

print(f"Gradient Boosting ì„±ëŠ¥: {gb_accuracy:.4f}")
```

**ë¶€ìŠ¤íŒ…ì˜ ì‘ë™ ì›ë¦¬**
- **ìˆœì°¨ì  í•™ìŠµ**: ì´ì „ ëª¨ë¸ì˜ ì‹¤ìˆ˜ë¥¼ ë‹¤ìŒ ëª¨ë¸ì´ ì§‘ì¤‘ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤
- **ê°€ì¤‘ì¹˜ ì¡°ì •**: í‹€ë¦° ìƒ˜í”Œì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤  
- **ì ì§„ì  ê°œì„ **: ë§¤ ë‹¨ê³„ë§ˆë‹¤ ì¡°ê¸ˆì”© ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤

#### âš¡ XGBoost - ë¶€ìŠ¤íŒ…ì˜ ìµœê³ ë´‰

```python
# XGBoostëŠ” ë³„ë„ ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install xgboost
try:
    import xgboost as xgb
    
    # XGBoost ë¶„ë¥˜ê¸° ìƒì„±
    xgb_clf = xgb.XGBClassifier(
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        random_state=42,
        eval_metric='logloss'  # ê²½ê³  ë©”ì‹œì§€ ë°©ì§€
    )
    
    # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
    xgb_clf.fit(X_train, y_train)
    y_pred_xgb = xgb_clf.predict(X_test)
    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
    
    print(f"\nâš¡ XGBoost ê²°ê³¼")
    print("=" * 30)
    print(f"XGBoost ì„±ëŠ¥: {xgb_accuracy:.4f}")
    
except ImportError:
    print("XGBoostê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install xgboost'ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    xgb_accuracy = 0
```

**XGBoostì˜ íŠ¹ì§•**
- **ê³ ì„±ëŠ¥**: ë§ì€ ë¨¸ì‹ ëŸ¬ë‹ ëŒ€íšŒì—ì„œ ìš°ìŠ¹ì„ ì°¨ì§€í•œ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤
- **ì •ê·œí™”**: ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ë²•ì´ ë‚´ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤
- **ì†ë„**: C++ë¡œ êµ¬í˜„ë˜ì–´ ë§¤ìš° ë¹ ë¥¸ ì‹¤í–‰ ì†ë„ë¥¼ ìë‘í•©ë‹ˆë‹¤

---

### 6.5 ìŠ¤íƒœí‚¹(Stacking) - ë©”íƒ€ ëŸ¬ë‹ì˜ í˜

#### ğŸ—ï¸ ìŠ¤íƒœí‚¹ì˜ ê°œë…

ìŠ¤íƒœí‚¹ì€ 1ì°¨ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ 2ì°¨ ëª¨ë¸ì´ ìµœì¢… ì˜ˆì¸¡ì„ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë§ˆì¹˜ ì—¬ëŸ¬ ì „ë¬¸ê°€ì˜ ì˜ê²¬ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ì •ì„ ë‚´ë¦¬ëŠ” ì¤‘ì¬ì ê°™ì€ ì—­í• ì…ë‹ˆë‹¤.

```python
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

# 1ì°¨ í•™ìŠµìë“¤ ì •ì˜
base_learners = [
    ('dt', DecisionTreeClassifier(random_state=42)),
    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),
    ('svm', SVC(probability=True, random_state=42))
]

# 2ì°¨ í•™ìŠµì (ë©”íƒ€ í•™ìŠµì) ì •ì˜
meta_learner = LogisticRegression(random_state=42)

# ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸° ìƒì„±
stacking_clf = StackingClassifier(
    estimators=base_learners,
    final_estimator=meta_learner,
    cv=5,  # 5-fold êµì°¨ ê²€ì¦ ì‚¬ìš©
    passthrough=False  # ì›ë³¸ íŠ¹ì„±ì„ ë©”íƒ€ í•™ìŠµìì—ê²Œ ì „ë‹¬í•˜ì§€ ì•ŠìŒ
)

# ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
stacking_clf.fit(X_train, y_train)
y_pred_stacking = stacking_clf.predict(X_test)
stacking_accuracy = accuracy_score(y_test, y_pred_stacking)

print("ğŸ—ï¸ ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸° ê²°ê³¼")
print("=" * 30)
print(f"ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸° ì„±ëŠ¥: {stacking_accuracy:.4f}")
```

**ìŠ¤íƒœí‚¹ì˜ ì¥ì **
- **ë†’ì€ ì„±ëŠ¥**: ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ë¥¸ ì•™ìƒë¸” ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤
- **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ëª¨ë¸ì„ ì¡°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- **ê³¼ì í•© ë°©ì§€**: êµì°¨ ê²€ì¦ì„ í†µí•´ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤

---

### 6.6 ì¢…í•© ì„±ëŠ¥ ë¹„êµ ë° ë¶„ì„

#### ğŸ“Š ëª¨ë“  ì•™ìƒë¸” ë°©ë²• ì„±ëŠ¥ ë¹„êµ

```python
# ëª¨ë“  ë°©ë²•ì˜ ì„±ëŠ¥ì„ ì •ë¦¬
results = {
    'ë‹¨ì¼ ì˜ì‚¬ê²°ì •ë‚˜ë¬´': individual_scores['Decision Tree'],
    'ë‹¨ì¼ ë¡œì§€ìŠ¤í‹± íšŒê·€': individual_scores['Logistic Regression'], 
    'ë‹¨ì¼ SVM': individual_scores['SVM'],
    'í•˜ë“œ íˆ¬í‘œ': hard_voting_accuracy,
    'ì†Œí”„íŠ¸ íˆ¬í‘œ': soft_voting_accuracy,
    'ë°°ê¹…': bagging_accuracy,
    'ëœë¤ í¬ë ˆìŠ¤íŠ¸': rf_accuracy,
    'AdaBoost': ada_accuracy,
    'Gradient Boosting': gb_accuracy,
    'ìŠ¤íƒœí‚¹': stacking_accuracy
}

if xgb_accuracy > 0:
    results['XGBoost'] = xgb_accuracy

# ê²°ê³¼ë¥¼ ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
sorted_results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))

print("ğŸ“Š ì „ì²´ ì„±ëŠ¥ ë¹„êµ (ì •í™•ë„ ê¸°ì¤€)")
print("=" * 50)
for method, accuracy in sorted_results.items():
    print(f"{method:20}: {accuracy:.4f}")

# ìµœê³  ì„±ëŠ¥ê³¼ ìµœì € ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°
best_score = max(sorted_results.values())
worst_score = min(sorted_results.values())
improvement = best_score - worst_score

print(f"\nğŸ“ˆ ì„±ëŠ¥ ê°œì„  íš¨ê³¼")
print(f"ìµœê³  ì„±ëŠ¥: {best_score:.4f}")
print(f"ìµœì € ì„±ëŠ¥: {worst_score:.4f}")
print(f"ê°œì„  í­: {improvement:.4f} ({improvement*100:.2f}%p)")

#### ğŸ¨ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”

```python
# ì„±ëŠ¥ ë¹„êµ ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±
plt.figure(figsize=(12, 8))

methods = list(sorted_results.keys())
accuracies = list(sorted_results.values())

# ë§‰ëŒ€ ìƒ‰ê¹” ì„¤ì • (ì•™ìƒë¸” ë°©ë²•ì€ ë‹¤ë¥¸ ìƒ‰ìœ¼ë¡œ)
colors = []
for method in methods:
    if 'ë‹¨ì¼' in method:
        colors.append('#ff7f7f')  # ì—°í•œ ë¹¨ê°• (ë‹¨ì¼ ëª¨ë¸)
    else:
        colors.append('#7fbf7f')  # ì—°í•œ ì´ˆë¡ (ì•™ìƒë¸” ëª¨ë¸)

bars = plt.bar(range(len(methods)), accuracies, color=colors)

# ê·¸ë˜í”„ ê¾¸ë¯¸ê¸°
plt.title('ì•™ìƒë¸” ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')
plt.xlabel('ë°©ë²•', fontsize=12)
plt.ylabel('ì •í™•ë„', fontsize=12)
plt.xticks(range(len(methods)), methods, rotation=45, ha='right')
plt.ylim(min(accuracies) - 0.01, max(accuracies) + 0.01)

# ê° ë§‰ëŒ€ ìœ„ì— ì •í™•ë„ ê°’ í‘œì‹œ
for i, bar in enumerate(bars):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,
             f'{height:.3f}', ha='center', va='bottom', fontsize=10)

# ë²”ë¡€ ì¶”ê°€
from matplotlib.patches import Patch
legend_elements = [Patch(facecolor='#ff7f7f', label='ë‹¨ì¼ ëª¨ë¸'),
                   Patch(facecolor='#7fbf7f', label='ì•™ìƒë¸” ëª¨ë¸')]
plt.legend(handles=legend_elements, loc='upper right')

plt.tight_layout()
plt.show()
```

**ì‹œê°í™”ì—ì„œ ì£¼ëª©í•  ì **
- ì•™ìƒë¸” ë°©ë²•ë“¤ì´ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤
- íŠ¹íˆ ìŠ¤íƒœí‚¹ê³¼ ë¶€ìŠ¤íŒ… ê³„ì—´ì´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤
- ë‹¤ì–‘í•œ ë°©ë²• ì¤‘ì—ì„œ ë¬¸ì œì— ë§ëŠ” ìµœì ì˜ ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤

---

### ğŸ› ï¸ ì‹¤ìŠµ í”„ë¡œì íŠ¸: ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ ì•™ìƒë¸” ì‹œìŠ¤í…œ

ì´ì œ ì‹¤ì œ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì•™ìƒë¸” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

#### ğŸ“Š ë°ì´í„° ì¤€ë¹„

```python
# ì‹¤ì œ Credit Card Fraud Detection ë°ì´í„°ì…‹ ë¡œë“œ
# (Kaggleì—ì„œ ë‹¤ìš´ë¡œë“œ: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)

# ë°ì´í„°ì…‹ì´ ì—†ëŠ” ê²½ìš° ìœ ì‚¬í•œ ë°ì´í„° ìƒì„±
from sklearn.datasets import make_classification

# ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ì™€ ìœ ì‚¬í•œ ë¶ˆê· í˜• ë°ì´í„°ì…‹ ìƒì„±
X_fraud, y_fraud = make_classification(
    n_samples=10000,      # 10,000ê°œ ê±°ë˜
    n_features=30,        # 30ê°œ íŠ¹ì„± (PCA ë³€í™˜ëœ íŠ¹ì„±ë“¤)
    n_informative=20,     # ì‹¤ì œë¡œ ìœ ìš©í•œ íŠ¹ì„± 20ê°œ
    n_redundant=10,       # ì¤‘ë³µ íŠ¹ì„± 10ê°œ
    n_clusters_per_class=1,
    weights=[0.999, 0.001],  # ì‚¬ê¸° ê±°ë˜ëŠ” 0.1% (ë§¤ìš° ë¶ˆê· í˜•)
    flip_y=0.01,          # 1% ë…¸ì´ì¦ˆ ì¶”ê°€
    random_state=42
)

print("ğŸ’³ ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ ë°ì´í„°")
print("=" * 40)
print(f"ì „ì²´ ê±°ë˜ ìˆ˜: {X_fraud.shape[0]:,}ê°œ")
print(f"íŠ¹ì„± ìˆ˜: {X_fraud.shape[1]}ê°œ")
print(f"ì •ìƒ ê±°ë˜: {np.sum(y_fraud == 0):,}ê°œ ({np.sum(y_fraud == 0)/len(y_fraud)*100:.1f}%)")
print(f"ì‚¬ê¸° ê±°ë˜: {np.sum(y_fraud == 1):,}ê°œ ({np.sum(y_fraud == 1)/len(y_fraud)*100:.1f}%)")
```

#### ğŸ”§ ë¶ˆê· í˜• ë°ì´í„°ë¥¼ ìœ„í•œ ì•™ìƒë¸” ì „ëµ

```python
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

# ë¶ˆê· í˜• ë°ì´í„°ì— íŠ¹í™”ëœ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(
    X_fraud, y_fraud, test_size=0.3, random_state=42, stratify=y_fraud
)

# 1. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì•™ìƒë¸” ëª¨ë¸ë“¤
models_fraud = {
    'Random Forest': RandomForestClassifier(
        n_estimators=100,
        class_weight='balanced',  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ìë™ ì¡°ì •
        random_state=42
    ),
    'XGBoost': None,  # XGBoost ì„¤ì¹˜ ì‹œì—ë§Œ ì‚¬ìš©
    'Balanced Voting': VotingClassifier(
        estimators=[
            ('dt', DecisionTreeClassifier(class_weight='balanced', random_state=42)),
            ('lr', LogisticRegression(class_weight='balanced', random_state=42)),
        ],
        voting='soft'
    )
}

# XGBoostê°€ ì„¤ì¹˜ëœ ê²½ìš°ì—ë§Œ ì¶”ê°€
try:
    import xgboost as xgb
    models_fraud['XGBoost'] = xgb.XGBClassifier(
        n_estimators=100,
        scale_pos_weight=np.sum(y_fraud == 0) / np.sum(y_fraud == 1),  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì¡°ì •
        random_state=42,
        eval_metric='logloss'
    )
except ImportError:
    del models_fraud['XGBoost']

# ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
fraud_results = {}
for name, model in models_fraud.items():
    if model is not None:
        print(f"\nğŸ”§ {name} í›ˆë ¨ ì¤‘...")
        model.fit(X_train_fraud, y_train_fraud)
        y_pred = model.predict(X_test_fraud)
        y_pred_proba = model.predict_proba(X_test_fraud)[:, 1]
        
        # ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ ê³„ì‚°
        precision = precision_score(y_test_fraud, y_pred)
        recall = recall_score(y_test_fraud, y_pred)
        f1 = f1_score(y_test_fraud, y_pred)
        auc = roc_auc_score(y_test_fraud, y_pred_proba)
        
        fraud_results[name] = {
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1,
            'AUC': auc
        }
        
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print(f"  F1-Score: {f1:.4f}")
        print(f"  AUC: {auc:.4f}")
```

**ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ í‰ê°€ ì§€í‘œë“¤**
- **Precision**: ì‚¬ê¸°ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì‚¬ê¸°ì¸ ë¹„ìœ¨ (ê±°ì§“ ê²½ë³´ ë°©ì§€)
- **Recall**: ì‹¤ì œ ì‚¬ê¸° ì¤‘ ì˜¬ë°”ë¥´ê²Œ íƒì§€í•œ ë¹„ìœ¨ (ì‚¬ê¸° ë†“ì¹˜ì§€ ì•Šê¸°)
- **F1-Score**: Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· 
- **AUC**: ROC ê³¡ì„  ì•„ë˜ ë©´ì  (ì „ì²´ì ì¸ ë¶„ë¥˜ ì„±ëŠ¥)

#### ğŸ“ˆ ROC ê³¡ì„ ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

```python
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))

# ê° ëª¨ë¸ì˜ ROC ê³¡ì„  ê·¸ë¦¬ê¸°
for name, model in models_fraud.items():
    if model is not None:
        y_pred_proba = model.predict_proba(X_test_fraud)[:, 1]
        fpr, tpr, _ = roc_curve(y_test_fraud, y_pred_proba)
        auc_score = roc_auc_score(y_test_fraud, y_pred_proba)
        
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})', linewidth=2)

# ëŒ€ê°ì„  (ëœë¤ ë¶„ë¥˜ê¸°) ê·¸ë¦¬ê¸°
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', alpha=0.5)

# ê·¸ë˜í”„ ê¾¸ë¯¸ê¸°
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (ê±°ì§“ ì–‘ì„±ë¥ )', fontsize=12)
plt.ylabel('True Positive Rate (ì°¸ ì–‘ì„±ë¥ )', fontsize=12)
plt.title('ROC ê³¡ì„  ë¹„êµ - ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€', fontsize=14, fontweight='bold')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()
```

**ROC ê³¡ì„  í•´ì„**
- ê³¡ì„ ì´ ì™¼ìª½ ìœ„ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ìŠµë‹ˆë‹¤
- AUCê°€ 0.5ì— ê°€ê¹Œìš°ë©´ ëœë¤ ë¶„ë¥˜ê¸°ì™€ ë¹„ìŠ·í•œ ì„±ëŠ¥ì…ë‹ˆë‹¤
- AUCê°€ 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì™„ë²½í•œ ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤

---

### ğŸ’ª ì§ì ‘ í•´ë³´ê¸° - ì—°ìŠµ ë¬¸ì œ

#### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 1: ì•™ìƒë¸” ë°©ë²• ë¹„êµ
ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ 3ê°€ì§€ ì•™ìƒë¸” ë°©ë²•ì˜ ì„±ëŠ¥ì„ ë¹„êµí•´ë³´ì„¸ìš”.

```python
# TODO: ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression

# ë°ì´í„° ì¤€ë¹„ (ë¶“ê½ƒ ë°ì´í„° ì‚¬ìš©)
from sklearn.datasets import load_iris
iris = load_iris()
X_iris, y_iris = iris.data, iris.target

# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(
    X_iris, y_iris, test_size=0.3, random_state=42
)

# 1. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸° ìƒì„± ë° í›ˆë ¨
rf_clf = RandomForestClassifier(n_estimators=50, random_state=42)
# TODO: ëª¨ë¸ í›ˆë ¨
# TODO: ì˜ˆì¸¡ ë° ì •í™•ë„ ê³„ì‚°

# 2. AdaBoost ë¶„ë¥˜ê¸° ìƒì„± ë° í›ˆë ¨
ada_clf = AdaBoostClassifier(n_estimators=50, random_state=42)
# TODO: ëª¨ë¸ í›ˆë ¨
# TODO: ì˜ˆì¸¡ ë° ì •í™•ë„ ê³„ì‚°

# 3. íˆ¬í‘œ ë¶„ë¥˜ê¸° ìƒì„± ë° í›ˆë ¨
voting_clf = VotingClassifier(
    estimators=[
        ('dt', DecisionTreeClassifier(random_state=42)),
        ('lr', LogisticRegression(random_state=42))
    ],
    voting='soft'
)
# TODO: ëª¨ë¸ í›ˆë ¨
# TODO: ì˜ˆì¸¡ ë° ì •í™•ë„ ê³„ì‚°

# TODO: ê²°ê³¼ ë¹„êµ ì¶œë ¥
```

#### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 2: ìŠ¤íƒœí‚¹ ì•™ìƒë¸” êµ¬í˜„
```python
# TODO: ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸°ë¥¼ êµ¬í˜„í•˜ì„¸ìš”
from sklearn.ensemble import StackingClassifier

# 1ì°¨ í•™ìŠµìë“¤ ì •ì˜
base_learners = [
    # TODO: ì„¸ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì •ì˜í•˜ì„¸ìš”
]

# 2ì°¨ í•™ìŠµì ì •ì˜
meta_learner = # TODO: ë©”íƒ€ í•™ìŠµìë¥¼ ì •ì˜í•˜ì„¸ìš”

# ìŠ¤íƒœí‚¹ ë¶„ë¥˜ê¸° ìƒì„±
stacking_clf = StackingClassifier(
    # TODO: ë§¤ê°œë³€ìˆ˜ë“¤ì„ ì„¤ì •í•˜ì„¸ìš”
)

# TODO: ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ í‰ê°€
```

#### ğŸ¯ ì—°ìŠµ ë¬¸ì œ 3: ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
ë¶ˆê· í˜•í•œ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ìœ„í•œ ì•™ìƒë¸” ì „ëµì„ êµ¬í˜„í•´ë³´ì„¸ìš”.

```python
# ë¶ˆê· í˜• ë°ì´í„° ìƒì„±
X_imbal, y_imbal = make_classification(
    n_samples=1000,
    n_features=10,
    weights=[0.9, 0.1],  # 90:10 ë¹„ìœ¨
    random_state=42
)

# TODO: ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±í•˜ëŠ” ì•™ìƒë¸” ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ì„¸ìš”
# 1. í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ëœë¤ í¬ë ˆìŠ¤íŠ¸
# 2. SMOTEë¥¼ ì ìš©í•œ í›„ ì¼ë°˜ ëª¨ë¸ í›ˆë ¨
# 3. ì„ê³„ê°’ ì¡°ì •ì„ í†µí•œ ì„±ëŠ¥ ìµœì í™”
# 4. F1-Score ê¸°ì¤€ìœ¼ë¡œ ì„±ëŠ¥ ë¹„êµ
```

---

### ğŸ“š í•µì‹¬ ì •ë¦¬

#### âœ¨ ì´ë²ˆ íŒŒíŠ¸ì—ì„œ ë°°ìš´ ë‚´ìš©

**1. ì•™ìƒë¸” í•™ìŠµì˜ ê¸°ë³¸ ì›ë¦¬**
- ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•˜ì—¬ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±
- ë‹¤ì–‘ì„±(Diversity)ì´ ì•™ìƒë¸” ì„±ëŠ¥ì˜ í•µì‹¬
- í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ ê°œì„ 

**2. íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”**
- **í•˜ë“œ íˆ¬í‘œ**: ë‹¤ìˆ˜ê²° ì›ë¦¬ë¡œ í´ë˜ìŠ¤ ê²°ì •
- **ì†Œí”„íŠ¸ íˆ¬í‘œ**: ì˜ˆì¸¡ í™•ë¥ ì„ ê³ ë ¤í•œ ê°€ì¤‘ íˆ¬í‘œ
- ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì„±ì„ ê°€ì§„ ëª¨ë¸ë“¤ì˜ ì¡°í•©ì´ íš¨ê³¼ì 

**3. ë°°ê¹…(Bagging)**
- ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§ìœ¼ë¡œ ë‹¤ì–‘í•œ í›ˆë ¨ ë°ì´í„° ìƒì„±
- ë¶„ì‚°ì„ ì¤„ì—¬ ê³¼ì í•© ë°©ì§€
- ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ë°°ê¹…ì˜ ëŒ€í‘œì ì¸ ì˜ˆ

**4. ë¶€ìŠ¤íŒ…(Boosting)**
- ìˆœì°¨ì ìœ¼ë¡œ ì•½í•œ í•™ìŠµìë“¤ì„ ê²°í•©
- ì´ì „ ëª¨ë¸ì˜ ì‹¤ìˆ˜ë¥¼ ë‹¤ìŒ ëª¨ë¸ì´ ë³´ì™„
- AdaBoost, Gradient Boosting, XGBoost ë“±

**5. ìŠ¤íƒœí‚¹(Stacking)**
- 1ì°¨ ëª¨ë¸ë“¤ì˜ ì˜ˆì¸¡ì„ 2ì°¨ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
- ë©”íƒ€ ëŸ¬ë‹ì„ í†µí•œ ê³ ì„±ëŠ¥ ë‹¬ì„±
- êµì°¨ ê²€ì¦ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€

#### ğŸ¯ ì‹¤ë¬´ ì ìš© ê°€ì´ë“œë¼ì¸

**ì–¸ì œ ì–´ë–¤ ì•™ìƒë¸” ë°©ë²•ì„ ì‚¬ìš©í• ê¹Œ?**

- **ì†Œê·œëª¨ ë°ì´í„° + ë¹ ë¥¸ ê²°ê³¼**: íˆ¬í‘œ ê¸°ë°˜ ì•™ìƒë¸”
- **ì•ˆì •ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°**: ë°°ê¹…, ëœë¤ í¬ë ˆìŠ¤íŠ¸
- **ë†’ì€ ì„±ëŠ¥ì´ í•„ìš”í•œ ê²½ìš°**: ë¶€ìŠ¤íŒ…, ìŠ¤íƒœí‚¹
- **ë¶ˆê· í˜• ë°ì´í„°**: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ + ì•™ìƒë¸”
- **í•´ì„ ê°€ëŠ¥ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°**: ëœë¤ í¬ë ˆìŠ¤íŠ¸ (íŠ¹ì„± ì¤‘ìš”ë„)

**ì£¼ì˜ì‚¬í•­**
- ê³¼ì í•© ìœ„í—˜: íŠ¹íˆ ìŠ¤íƒœí‚¹ì—ì„œ ì£¼ì˜ í•„ìš”
- ê³„ì‚° ë¹„ìš©: ëª¨ë¸ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ í›ˆë ¨/ì˜ˆì¸¡ ì‹œê°„ ì¦ê°€
- í•´ì„ ì–´ë ¤ì›€: ì•™ìƒë¸”ì€ ë¸”ë™ë°•ìŠ¤ íŠ¹ì„±ì´ ê°•í•¨

---

### ğŸ”® ë‹¤ìŒ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°

ë‹¤ìŒ Part 2ì—ì„œëŠ” **ì°¨ì› ì¶•ì†Œì™€ êµ°ì§‘í™”**ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤:

- ğŸ¯ **ì£¼ì„±ë¶„ ë¶„ì„(PCA)**: ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•
- ğŸ” **t-SNE**: ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•˜ëŠ” ë¹„ì„ í˜• ì°¨ì› ì¶•ì†Œ
- ğŸ“Š **K-í‰ê·  êµ°ì§‘í™”**: ë¹„ìŠ·í•œ ë°ì´í„°ë“¤ì„ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ê¸°
- ğŸŒ³ **ê³„ì¸µì  êµ°ì§‘í™”**: ë´ë“œë¡œê·¸ë¨ìœ¼ë¡œ êµ°ì§‘ êµ¬ì¡° íŒŒì•…
- ğŸš€ **ì‹¤ìŠµ**: ê³ ê° ì„¸ë¶„í™”ë¥¼ ìœ„í•œ ì°¨ì› ì¶•ì†Œ + êµ°ì§‘í™” í”„ë¡œì íŠ¸

ì•™ìƒë¸” í•™ìŠµìœ¼ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë†’ì˜€ë‹¤ë©´, ì´ì œ ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ê³¼ êµ¬ì¡°ë¥¼ ë°œê²¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤!

---

*"ê°œë³„ ë‚˜ë¬´ë¥¼ ë³´ì§€ ë§ê³  ìˆ² ì „ì²´ë¥¼ ë³´ë¼. ì•™ìƒë¸”ì˜ í˜ì€ ë‹¤ì–‘ì„±ì—ì„œ ë‚˜ì˜¨ë‹¤." - ë°ì´í„° ê³¼í•™ìì˜ ì§€í˜œ*
```