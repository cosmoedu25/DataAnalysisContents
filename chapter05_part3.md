# 5ì¥ Part 3: íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì˜ ì´í•´ì™€ êµ¬í˜„

## í•™ìŠµ ëª©í‘œ
ì´ë²ˆ íŒŒíŠ¸ë¥¼ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- íšŒê·€ ë¬¸ì œì˜ ê°œë…ì„ ì´í•´í•˜ê³  ë¶„ë¥˜ ë¬¸ì œì™€ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤
- ì„ í˜• íšŒê·€ì™€ ë‹¤í•­ íšŒê·€ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ê³  êµ¬í˜„í•  ìˆ˜ ìˆë‹¤
- Ridgeì™€ Lasso ì •ê·œí™” ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤
- MSE, RMSE, MAE, RÂ² ë“± íšŒê·€ ì„±ëŠ¥ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤
- ì”ì°¨ ë¶„ì„ì„ í†µí•´ ëª¨ë¸ì˜ ì í•©ì„±ì„ ì§„ë‹¨í•  ìˆ˜ ìˆë‹¤
- ì‹¤ì œ ë¶€ë™ì‚° ê°€ê²© ì˜ˆì¸¡ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤

## ì´ë²ˆ íŒŒíŠ¸ ë¯¸ë¦¬ë³´ê¸°
ì§€ê¸ˆê¹Œì§€ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ 'ë²”ì£¼'ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” **ì—°ì†ì ì¸ ìˆ˜ì¹˜**ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì§‘ê°’ì´ë‚˜ ì˜¨ë„, ë§¤ì¶œì•¡ì²˜ëŸ¼ ìˆ«ìë¡œ ëœ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ íšŒê·€ ë¬¸ì œì…ë‹ˆë‹¤. ì„ í˜• íšŒê·€ë¶€í„° ì‹œì‘í•´ ì •ê·œí™” ê¸°ë²•ê¹Œì§€ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•˜ë©°, ì‹¤ì œ ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡œì íŠ¸ë¡œ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

---

## 5.3.1 íšŒê·€ ë¬¸ì œì˜ ì´í•´

### ì¼ìƒ ì† íšŒê·€ ë¬¸ì œ

íšŒê·€(Regression)ëŠ” ìš°ë¦¬ ì¼ìƒì—ì„œ ìˆ˜ì—†ì´ ë§ˆì£¼ì¹˜ëŠ” ì˜ˆì¸¡ ë¬¸ì œì…ë‹ˆë‹¤:

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.datasets import fetch_california_housing
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# íšŒê·€ ë¬¸ì œ ì˜ˆì‹œ
regression_examples = {
    "ğŸ  ë¶€ë™ì‚°": ["ì§‘ê°’ ì˜ˆì¸¡", "ì›”ì„¸ ì˜ˆì¸¡", "ì „ì„¸ê°€ ì˜ˆì¸¡"],
    "ğŸ“ˆ ê²½ì œ": ["ì£¼ê°€ ì˜ˆì¸¡", "ë§¤ì¶œì•¡ ì˜ˆì¸¡", "GDP ì„±ì¥ë¥  ì˜ˆì¸¡"],
    "ğŸŒ¡ï¸ ë‚ ì”¨": ["ê¸°ì˜¨ ì˜ˆì¸¡", "ê°•ìˆ˜ëŸ‰ ì˜ˆì¸¡", "ë¯¸ì„¸ë¨¼ì§€ ë†ë„ ì˜ˆì¸¡"],
    "ğŸ“ êµìœ¡": ["ì‹œí—˜ ì ìˆ˜ ì˜ˆì¸¡", "ëŒ€í•™ ì…í•™ë¥  ì˜ˆì¸¡", "í•™ìŠµ ì‹œê°„ ì˜ˆì¸¡"],
    "ğŸ¥ ì˜ë£Œ": ["í˜ˆì•• ì˜ˆì¸¡", "ì¹˜ë£Œ ë¹„ìš© ì˜ˆì¸¡", "íšŒë³µ ê¸°ê°„ ì˜ˆì¸¡"]
}

print("=== ì¼ìƒ ì† íšŒê·€ ë¬¸ì œë“¤ ===\n")
for category, examples in regression_examples.items():
    print(f"{category}")
    for example in examples:
        print(f"  â€¢ {example}")
    print()
```

### íšŒê·€ì™€ ë¶„ë¥˜ì˜ ì°¨ì´ì 

íšŒê·€ì™€ ë¶„ë¥˜ì˜ ê°€ì¥ í° ì°¨ì´ëŠ” **ì˜ˆì¸¡ê°’ì˜ ì„±ê²©**ì…ë‹ˆë‹¤:

```python
# íšŒê·€ vs ë¶„ë¥˜ ë¹„êµ ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# ë¶„ë¥˜ ì˜ˆì‹œ: ì´ì‚°ì  ì¶œë ¥
np.random.seed(42)
x_class = np.random.randn(100, 2)
y_class = (x_class[:, 0] + x_class[:, 1] > 0).astype(int)

scatter1 = ax1.scatter(x_class[:, 0], x_class[:, 1], c=y_class, 
                      cmap='RdBu', s=50, alpha=0.7, edgecolor='k')
ax1.set_title('ë¶„ë¥˜: ì´ì‚°ì  ì¶œë ¥ (0 ë˜ëŠ” 1)', fontsize=14, fontweight='bold')
ax1.set_xlabel('íŠ¹ì„± 1')
ax1.set_ylabel('íŠ¹ì„± 2')
ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)
ax1.axvline(x=0, color='k', linestyle='--', alpha=0.3)

# ë²”ë¡€ ì¶”ê°€
handles = [plt.Line2D([0], [0], marker='o', color='w', 
                     markerfacecolor=c, markersize=10) 
          for c in ['#d62728', '#1f77b4']]
ax1.legend(handles, ['í´ë˜ìŠ¤ 0', 'í´ë˜ìŠ¤ 1'])

# íšŒê·€ ì˜ˆì‹œ: ì—°ì†ì  ì¶œë ¥
x_reg = np.linspace(0, 10, 100)
y_reg = 2 * x_reg + 3 + np.random.normal(0, 2, 100)

scatter2 = ax2.scatter(x_reg, y_reg, c=y_reg, cmap='viridis', 
                      s=50, alpha=0.7, edgecolor='k')
ax2.plot(x_reg, 2 * x_reg + 3, 'r-', linewidth=2, label='íšŒê·€ì„ ')
ax2.set_title('íšŒê·€: ì—°ì†ì  ì¶œë ¥ (ì‹¤ìˆ˜ê°’)', fontsize=14, fontweight='bold')
ax2.set_xlabel('íŠ¹ì„±')
ax2.set_ylabel('ì˜ˆì¸¡ê°’')
ax2.legend()

# ì»¬ëŸ¬ë°” ì¶”ê°€
cbar = plt.colorbar(scatter2, ax=ax2)
cbar.set_label('ì˜ˆì¸¡ê°’')

plt.tight_layout()
plt.show()

# ì°¨ì´ì  ì •ë¦¬
print("\n=== íšŒê·€ vs ë¶„ë¥˜ í•µì‹¬ ì°¨ì´ì  ===\n")
comparison_df = pd.DataFrame({
    'êµ¬ë¶„': ['ì˜ˆì¸¡ ëŒ€ìƒ', 'ì¶œë ¥ê°’ ìœ í˜•', 'ì˜ˆì‹œ', 'í‰ê°€ ì§€í‘œ'],
    'ë¶„ë¥˜': ['ë²”ì£¼(Category)', 'ì´ì‚°ì (Discrete)', 'ìŠ¤íŒ¸/ì •ìƒ, ê°œ/ê³ ì–‘ì´', 'ì •í™•ë„, F1-ì ìˆ˜'],
    'íšŒê·€': ['ìˆ˜ì¹˜(Number)', 'ì—°ì†ì (Continuous)', 'ì§‘ê°’, ì˜¨ë„, ì ìˆ˜', 'MSE, RÂ²']
})
print(comparison_df.to_string(index=False))
```

## 5.3.2 ì„ í˜• íšŒê·€ (Linear Regression)

### ì„ í˜• íšŒê·€ì˜ ì§ê´€ì  ì´í•´

ì„ í˜• íšŒê·€ëŠ” **ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” ì§ì„ **ì„ ì°¾ëŠ” ê³¼ì •ì…ë‹ˆë‹¤:

```python
# ê°„ë‹¨í•œ ì„ í˜• ê´€ê³„ ì‹œë®¬ë ˆì´ì…˜
np.random.seed(42)
study_hours = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
test_scores = 5 * study_hours + 30 + np.random.normal(0, 3, 10)

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.scatter(study_hours, test_scores, s=100, alpha=0.7, 
           edgecolor='k', label='ì‹¤ì œ ë°ì´í„°')

# ì„ í˜• íšŒê·€ ì í•©
model = LinearRegression()
X = study_hours.reshape(-1, 1)
model.fit(X, test_scores)
predictions = model.predict(X)

plt.plot(study_hours, predictions, 'r-', linewidth=2, 
        label=f'íšŒê·€ì„ : y = {model.coef_[0]:.1f}x + {model.intercept_:.1f}')

# ì˜ˆì¸¡ ì˜¤ì°¨ í‘œì‹œ
for i in range(len(study_hours)):
    plt.plot([study_hours[i], study_hours[i]], 
            [test_scores[i], predictions[i]], 
            'gray', linestyle='--', alpha=0.5)

plt.xlabel('ê³µë¶€ ì‹œê°„ (ì‹œê°„)', fontsize=12)
plt.ylabel('ì‹œí—˜ ì ìˆ˜', fontsize=12)
plt.title('ì„ í˜• íšŒê·€: ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” ì§ì„  ì°¾ê¸°', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

print(f"ğŸ¯ íšŒê·€ ë°©ì •ì‹: ì ìˆ˜ = {model.coef_[0]:.1f} Ã— ê³µë¶€ì‹œê°„ + {model.intercept_:.1f}")
print(f"   â†’ ê³µë¶€ë¥¼ 1ì‹œê°„ ë” í•˜ë©´ ì ìˆ˜ê°€ ì•½ {model.coef_[0]:.1f}ì  ì˜¤ë¦…ë‹ˆë‹¤!")
```

### ìµœì†Œì œê³±ë²•ì˜ ì›ë¦¬

ì„ í˜• íšŒê·€ëŠ” **ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´(ì”ì°¨)ì˜ ì œê³±í•©ì„ ìµœì†Œí™”**í•˜ëŠ” ì§ì„ ì„ ì°¾ìŠµë‹ˆë‹¤:

```python
# ìµœì†Œì œê³±ë²• ì‹œê°í™”
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# ì™¼ìª½: ë‚˜ìœ íšŒê·€ì„ 
bad_slope = 3
bad_intercept = 40
bad_predictions = bad_slope * study_hours + bad_intercept

ax1.scatter(study_hours, test_scores, s=100, alpha=0.7, edgecolor='k')
ax1.plot(study_hours, bad_predictions, 'b-', linewidth=2, label='ë‚˜ìœ íšŒê·€ì„ ')

# ì”ì°¨ í‘œì‹œ
for i in range(len(study_hours)):
    ax1.plot([study_hours[i], study_hours[i]], 
            [test_scores[i], bad_predictions[i]], 
            'red', linewidth=2, alpha=0.7)
    
bad_sse = np.sum((test_scores - bad_predictions) ** 2)
ax1.set_title(f'ì˜ëª»ëœ íšŒê·€ì„ \nì”ì°¨ ì œê³±í•©: {bad_sse:.1f}', fontsize=12)
ax1.set_xlabel('ê³µë¶€ ì‹œê°„')
ax1.set_ylabel('ì‹œí—˜ ì ìˆ˜')
ax1.legend()
ax1.grid(True, alpha=0.3)

# ì˜¤ë¥¸ìª½: ìµœì  íšŒê·€ì„ 
ax2.scatter(study_hours, test_scores, s=100, alpha=0.7, edgecolor='k')
ax2.plot(study_hours, predictions, 'g-', linewidth=2, label='ìµœì  íšŒê·€ì„ ')

# ì”ì°¨ í‘œì‹œ
for i in range(len(study_hours)):
    ax2.plot([study_hours[i], study_hours[i]], 
            [test_scores[i], predictions[i]], 
            'green', linewidth=2, alpha=0.7)

good_sse = np.sum((test_scores - predictions) ** 2)
ax2.set_title(f'ìµœì  íšŒê·€ì„  (ìµœì†Œì œê³±ë²•)\nì”ì°¨ ì œê³±í•©: {good_sse:.1f}', fontsize=12)
ax2.set_xlabel('ê³µë¶€ ì‹œê°„')
ax2.set_ylabel('ì‹œí—˜ ì ìˆ˜')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nğŸ’¡ ìµœì†Œì œê³±ë²•ì˜ í•µì‹¬:")
print(f"   â€¢ ë‚˜ìœ íšŒê·€ì„ ì˜ ì˜¤ì°¨: {bad_sse:.1f}")
print(f"   â€¢ ìµœì  íšŒê·€ì„ ì˜ ì˜¤ì°¨: {good_sse:.1f}")
print(f"   â€¢ ì˜¤ì°¨ ê°ì†Œ: {bad_sse - good_sse:.1f} ({(bad_sse - good_sse)/bad_sse*100:.1f}%)")
```

### ë‹¤ë³€ëŸ‰ ì„ í˜• íšŒê·€

ì‹¤ì œ ë¬¸ì œì—ì„œëŠ” ì—¬ëŸ¬ íŠ¹ì„±ì„ ë™ì‹œì— ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:

```python
# ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ë°ì´í„°ì…‹ ë¡œë“œ
california = fetch_california_housing()
X_cal = pd.DataFrame(california.data, columns=california.feature_names)
y_cal = california.target

print("=== ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ë°ì´í„°ì…‹ ===")
print(f"ë°ì´í„° í¬ê¸°: {X_cal.shape}")
print(f"\níŠ¹ì„± ì„¤ëª…:")
for i, (feature, description) in enumerate(zip(california.feature_names, 
    ['í‰ê·  ì†Œë“', 'ì§‘ ë‚˜ì´', 'í‰ê·  ë°© ìˆ˜', 'í‰ê·  ì¹¨ì‹¤ ìˆ˜', 
     'ì¸êµ¬', 'í‰ê·  ê°€êµ¬ì› ìˆ˜', 'ìœ„ë„', 'ê²½ë„'])):
    print(f"  â€¢ {feature}: {description}")

# ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
plt.figure(figsize=(10, 8))
correlation_matrix = X_cal.copy()
correlation_matrix['Price'] = y_cal
corr = correlation_matrix.corr()

mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', 
            cmap='coolwarm', center=0, vmin=-1, vmax=1,
            square=True, linewidths=1, cbar_kws={"shrink": .8})
plt.title('ì£¼íƒ ê°€ê²©ê³¼ íŠ¹ì„±ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# ë‹¤ë³€ëŸ‰ ì„ í˜• íšŒê·€ ì í•©
X_train, X_test, y_train, y_test = train_test_split(
    X_cal, y_cal, test_size=0.2, random_state=42
)

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ëª¨ë¸ í•™ìŠµ
multi_model = LinearRegression()
multi_model.fit(X_train_scaled, y_train)

# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
feature_importance = pd.DataFrame({
    'feature': california.feature_names,
    'coefficient': multi_model.coef_
}).sort_values('coefficient', key=abs, ascending=False)

plt.figure(figsize=(10, 6))
colors = ['green' if x > 0 else 'red' for x in feature_importance['coefficient']]
plt.barh(feature_importance['feature'], feature_importance['coefficient'], 
         color=colors, alpha=0.7, edgecolor='black')
plt.xlabel('íšŒê·€ ê³„ìˆ˜', fontsize=12)
plt.title('ë‹¤ë³€ëŸ‰ ì„ í˜• íšŒê·€: ê° íŠ¹ì„±ì˜ ì˜í–¥ë ¥', fontsize=14, fontweight='bold')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

print("\nğŸ” í•´ì„:")
print("   â€¢ ì–‘(+)ì˜ ê³„ìˆ˜: í•´ë‹¹ íŠ¹ì„±ì´ ì¦ê°€í•˜ë©´ ì§‘ê°’ë„ ì¦ê°€")
print("   â€¢ ìŒ(-)ì˜ ê³„ìˆ˜: í•´ë‹¹ íŠ¹ì„±ì´ ì¦ê°€í•˜ë©´ ì§‘ê°’ì€ ê°ì†Œ")
print(f"   â€¢ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±: {feature_importance.iloc[0]['feature']}")
```

## 5.3.3 ë‹¤í•­ íšŒê·€ (Polynomial Regression)

### ë¹„ì„ í˜• ê´€ê³„ ëª¨ë¸ë§

ë°ì´í„°ê°€ ì§ì„ ì´ ì•„ë‹Œ **ê³¡ì„ ** íŒ¨í„´ì„ ë³´ì¼ ë•ŒëŠ” ë‹¤í•­ íšŒê·€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
# ë¹„ì„ í˜• ë°ì´í„° ìƒì„±
np.random.seed(42)
X_nonlinear = np.linspace(-3, 3, 100)
y_nonlinear = 0.5 * X_nonlinear**3 + X_nonlinear**2 - 2*X_nonlinear + 1 + np.random.normal(0, 1, 100)

# ì„ í˜• vs ë‹¤í•­ íšŒê·€ ë¹„êµ
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1ì°¨ (ì„ í˜•)
ax = axes[0, 0]
linear_model = LinearRegression()
X_reshape = X_nonlinear.reshape(-1, 1)
linear_model.fit(X_reshape, y_nonlinear)
y_linear_pred = linear_model.predict(X_reshape)

ax.scatter(X_nonlinear, y_nonlinear, alpha=0.5, s=30, label='ì‹¤ì œ ë°ì´í„°')
ax.plot(X_nonlinear, y_linear_pred, 'r-', linewidth=2, label='1ì°¨ (ì„ í˜•)')
ax.set_title('1ì°¨ ë‹¤í•­ì‹ (ì„ í˜• íšŒê·€)', fontsize=12)
ax.legend()
ax.grid(True, alpha=0.3)

# 2ì°¨, 3ì°¨, 5ì°¨ ë‹¤í•­ íšŒê·€
degrees = [2, 3, 5]
positions = [(0, 1), (1, 0), (1, 1)]

for degree, (i, j) in zip(degrees, positions):
    ax = axes[i, j]
    
    # ë‹¤í•­ íŠ¹ì„± ìƒì„±
    poly_features = PolynomialFeatures(degree=degree, include_bias=False)
    X_poly = poly_features.fit_transform(X_reshape)
    
    # ëª¨ë¸ í•™ìŠµ
    poly_model = LinearRegression()
    poly_model.fit(X_poly, y_nonlinear)
    
    # ì˜ˆì¸¡
    X_plot = np.linspace(-3, 3, 300).reshape(-1, 1)
    X_plot_poly = poly_features.transform(X_plot)
    y_poly_pred = poly_model.predict(X_plot_poly)
    
    ax.scatter(X_nonlinear, y_nonlinear, alpha=0.5, s=30, label='ì‹¤ì œ ë°ì´í„°')
    ax.plot(X_plot, y_poly_pred, 'g-', linewidth=2, label=f'{degree}ì°¨ ë‹¤í•­ì‹')
    ax.set_title(f'{degree}ì°¨ ë‹¤í•­ íšŒê·€', fontsize=12)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # RÂ² ì ìˆ˜ í‘œì‹œ
    y_pred_train = poly_model.predict(X_poly)
    r2 = r2_score(y_nonlinear, y_pred_train)
    ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax.transAxes, 
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.show()

print("\nğŸ’¡ ë‹¤í•­ íšŒê·€ì˜ í•µì‹¬:")
print("   â€¢ ì°¨ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë³µì¡í•œ íŒ¨í„´ì„ í¬ì°©í•  ìˆ˜ ìˆìŒ")
print("   â€¢ í•˜ì§€ë§Œ ë„ˆë¬´ ë†’ì€ ì°¨ìˆ˜ëŠ” ê³¼ì í•©ì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆìŒ")
print("   â€¢ ì ì ˆí•œ ì°¨ìˆ˜ ì„ íƒì´ ì¤‘ìš”!")
```

## 5.3.4 ì •ê·œí™” ê¸°ë²•: Ridgeì™€ Lasso

### ê³¼ì í•© ë¬¸ì œì™€ ì •ê·œí™”

ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ **íšŒê·€ ê³„ìˆ˜ì— í˜ë„í‹°**ë¥¼ ë¶€ì—¬í•˜ëŠ” ì •ê·œí™” ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
# ê³¼ì í•© ì‹œì—°ì„ ìœ„í•œ ë°ì´í„°
np.random.seed(42)
n_samples = 30
X_overfit = np.sort(np.random.uniform(0, 4, n_samples))
y_true = np.sin(X_overfit) + X_overfit/2
y_overfit = y_true + np.random.normal(0, 0.3, n_samples)

# ê³ ì°¨ ë‹¤í•­ì‹ìœ¼ë¡œ ê³¼ì í•© ìœ ë„
X_overfit_reshape = X_overfit.reshape(-1, 1)
poly_features = PolynomialFeatures(degree=15)
X_poly = poly_features.fit_transform(X_overfit_reshape)

# ì¼ë°˜ ì„ í˜• íšŒê·€, Ridge, Lasso ë¹„êµ
models = {
    'Linear Regression': LinearRegression(),
    'Ridge (Î±=0.1)': Ridge(alpha=0.1),
    'Ridge (Î±=1.0)': Ridge(alpha=1.0),
    'Lasso (Î±=0.01)': Lasso(alpha=0.01),
    'Lasso (Î±=0.1)': Lasso(alpha=0.1)
}

fig, axes = plt.subplots(2, 3, figsize=(15, 10))
axes = axes.ravel()

for idx, (name, model) in enumerate(models.items()):
    ax = axes[idx]
    
    # ëª¨ë¸ í•™ìŠµ
    model.fit(X_poly, y_overfit)
    
    # ì˜ˆì¸¡ì„ ìœ„í•œ ì„¸ë°€í•œ X ê°’
    X_plot = np.linspace(0, 4, 300).reshape(-1, 1)
    X_plot_poly = poly_features.transform(X_plot)
    y_pred = model.predict(X_plot_poly)
    
    # ì‹œê°í™”
    ax.scatter(X_overfit, y_overfit, s=50, alpha=0.7, 
              edgecolor='k', label='í•™ìŠµ ë°ì´í„°')
    ax.plot(X_overfit, y_true, 'g--', linewidth=2, 
           label='ì‹¤ì œ í•¨ìˆ˜', alpha=0.7)
    ax.plot(X_plot, y_pred, 'r-', linewidth=2, label='ì˜ˆì¸¡')
    
    ax.set_title(name, fontsize=12, fontweight='bold')
    ax.set_xlabel('X')
    ax.set_ylabel('y')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim(-2, 4)
    
    # ê³„ìˆ˜ì˜ í¬ê¸° í‘œì‹œ
    coef_norm = np.linalg.norm(model.coef_)
    ax.text(0.05, 0.95, f'||w|| = {coef_norm:.1f}', 
           transform=ax.transAxes, 
           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

# ë¹ˆ subplot ìˆ¨ê¸°ê¸°
axes[-1].set_visible(False)

plt.tight_layout()
plt.show()

# ì •ê·œí™” ë°©ë²• ë¹„êµ
print("\n=== Ridge vs Lasso ì •ê·œí™” ë¹„êµ ===")
comparison_df = pd.DataFrame({
    'íŠ¹ì§•': ['í˜ë„í‹° ìœ í˜•', 'ê³„ìˆ˜ ì²˜ë¦¬', 'íŠ¹ì„± ì„ íƒ', 'ì‚¬ìš© ì‹œê¸°'],
    'Ridge (L2)': ['ê³„ìˆ˜ ì œê³±í•©', 'ì‘ê²Œ ë§Œë“¦', 'ëª¨ë“  íŠ¹ì„± ìœ ì§€', 'ëª¨ë“  íŠ¹ì„±ì´ ì¤‘ìš”í•  ë•Œ'],
    'Lasso (L1)': ['ê³„ìˆ˜ ì ˆëŒ“ê°’ í•©', '0ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆìŒ', 'ìë™ íŠ¹ì„± ì„ íƒ', 'ì¼ë¶€ íŠ¹ì„±ë§Œ ì¤‘ìš”í•  ë•Œ']
})
print(comparison_df.to_string(index=False))
```

### ì •ê·œí™” ê°•ë„(Î±) ì„ íƒí•˜ê¸°

```python
# ë‹¤ì–‘í•œ ì•ŒíŒŒ ê°’ì— ëŒ€í•œ ì„±ëŠ¥ ë¹„êµ
alphas = np.logspace(-4, 2, 50)
ridge_scores = []
lasso_scores = []

# ë°ì´í„° ì¤€ë¹„
X_train, X_test, y_train, y_test = train_test_split(
    X_cal, y_cal, test_size=0.2, random_state=42
)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ê° ì•ŒíŒŒ ê°’ì— ëŒ€í•´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
for alpha in alphas:
    # Ridge
    ridge = Ridge(alpha=alpha)
    ridge.fit(X_train_scaled, y_train)
    ridge_scores.append(ridge.score(X_test_scaled, y_test))
    
    # Lasso
    lasso = Lasso(alpha=alpha, max_iter=1000)
    lasso.fit(X_train_scaled, y_train)
    lasso_scores.append(lasso.score(X_test_scaled, y_test))

# ì‹œê°í™”
plt.figure(figsize=(10, 6))
plt.semilogx(alphas, ridge_scores, 'b-', linewidth=2, label='Ridge')
plt.semilogx(alphas, lasso_scores, 'r-', linewidth=2, label='Lasso')
plt.xlabel('ì •ê·œí™” ê°•ë„ (Î±)', fontsize=12)
plt.ylabel('RÂ² ì ìˆ˜', fontsize=12)
plt.title('ì •ê·œí™” ê°•ë„ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥', fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)

# ìµœì  ì•ŒíŒŒ í‘œì‹œ
best_ridge_alpha = alphas[np.argmax(ridge_scores)]
best_lasso_alpha = alphas[np.argmax(lasso_scores)]
plt.axvline(x=best_ridge_alpha, color='blue', linestyle='--', alpha=0.5)
plt.axvline(x=best_lasso_alpha, color='red', linestyle='--', alpha=0.5)

plt.show()

print(f"\nğŸ¯ ìµœì  ì •ê·œí™” ê°•ë„:")
print(f"   â€¢ Ridge: Î± = {best_ridge_alpha:.4f} (RÂ² = {max(ridge_scores):.3f})")
print(f"   â€¢ Lasso: Î± = {best_lasso_alpha:.4f} (RÂ² = {max(lasso_scores):.3f})")
```

## 5.3.5 íšŒê·€ í‰ê°€ ì§€í‘œ

### ì£¼ìš” í‰ê°€ ì§€í‘œ ì´í•´í•˜ê¸°

íšŒê·€ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë‹¤ì–‘í•œ ì§€í‘œë“¤ì„ ì•Œì•„ë´…ì‹œë‹¤:

```python
# ì˜ˆì¸¡ê°’ ìƒì„± (Ridge ëª¨ë¸ ì‚¬ìš©)
ridge_best = Ridge(alpha=best_ridge_alpha)
ridge_best.fit(X_train_scaled, y_train)
y_pred = ridge_best.predict(X_test_scaled)

# í‰ê°€ ì§€í‘œ ê³„ì‚°
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# ì‹œê°í™”
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# 1. ì˜ˆì¸¡ vs ì‹¤ì œ ì‚°ì ë„
ax = axes[0, 0]
ax.scatter(y_test, y_pred, alpha=0.5, s=30)
ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
        'r--', linewidth=2)
ax.set_xlabel('ì‹¤ì œê°’', fontsize=12)
ax.set_ylabel('ì˜ˆì¸¡ê°’', fontsize=12)
ax.set_title('ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’', fontsize=12, fontweight='bold')
ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax.transAxes,
        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

# 2. ì”ì°¨ í”Œë¡¯
ax = axes[0, 1]
residuals = y_test - y_pred
ax.scatter(y_pred, residuals, alpha=0.5, s=30)
ax.axhline(y=0, color='r', linestyle='--', linewidth=2)
ax.set_xlabel('ì˜ˆì¸¡ê°’', fontsize=12)
ax.set_ylabel('ì”ì°¨ (ì‹¤ì œ - ì˜ˆì¸¡)', fontsize=12)
ax.set_title('ì”ì°¨ í”Œë¡¯', fontsize=12, fontweight='bold')

# 3. ì”ì°¨ íˆìŠ¤í† ê·¸ë¨
ax = axes[1, 0]
ax.hist(residuals, bins=30, alpha=0.7, edgecolor='black')
ax.axvline(x=0, color='r', linestyle='--', linewidth=2)
ax.set_xlabel('ì”ì°¨', fontsize=12)
ax.set_ylabel('ë¹ˆë„', fontsize=12)
ax.set_title('ì”ì°¨ ë¶„í¬', fontsize=12, fontweight='bold')

# 4. í‰ê°€ ì§€í‘œ ìš”ì•½
ax = axes[1, 1]
ax.axis('off')
metrics_text = f"""
ğŸ“Š íšŒê·€ í‰ê°€ ì§€í‘œ

MSE (Mean Squared Error): {mse:.3f}
  â€¢ ì˜¤ì°¨ì˜ ì œê³± í‰ê· 
  â€¢ í° ì˜¤ì°¨ì— ë” í° í˜ë„í‹°

RMSE (Root MSE): {rmse:.3f}
  â€¢ MSEì˜ ì œê³±ê·¼
  â€¢ ì›ë˜ ë‹¨ìœ„ì™€ ë™ì¼

MAE (Mean Absolute Error): {mae:.3f}
  â€¢ ì˜¤ì°¨ì˜ ì ˆëŒ“ê°’ í‰ê· 
  â€¢ ëª¨ë“  ì˜¤ì°¨ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬

RÂ² (ê²°ì •ê³„ìˆ˜): {r2:.3f}
  â€¢ ëª¨ë¸ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚°ì˜ ë¹„ìœ¨
  â€¢ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ
"""
ax.text(0.1, 0.5, metrics_text, transform=ax.transAxes, 
        fontsize=12, verticalalignment='center',
        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))

plt.tight_layout()
plt.show()

# í‰ê°€ ì§€í‘œ í•´ì„
print("\nğŸ” í‰ê°€ ì§€í‘œ í•´ì„:")
print(f"   â€¢ RMSE = {rmse:.3f}: í‰ê· ì ìœ¼ë¡œ ì˜ˆì¸¡ì´ ì‹¤ì œê°’ì—ì„œ Â±{rmse:.3f} ì •ë„ ë²—ì–´ë‚¨")
print(f"   â€¢ MAE = {mae:.3f}: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ê°€ {mae:.3f}")
print(f"   â€¢ RÂ² = {r2:.3f}: ëª¨ë¸ì´ ë°ì´í„° ë³€ë™ì„±ì˜ {r2*100:.1f}%ë¥¼ ì„¤ëª…í•¨")
```

## 5.3.6 ë¯¸ë‹ˆ í”„ë¡œì íŠ¸: ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡

ì´ì œ ë°°ìš´ ëª¨ë“  ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ì‹¤ì œ ì£¼íƒ ê°€ê²©ì„ ì˜ˆì¸¡í•´ë´…ì‹œë‹¤:

```python
# í”„ë¡œì íŠ¸ ê°œìš”
print("=== ğŸ  ìº˜ë¦¬í¬ë‹ˆì•„ ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ í”„ë¡œì íŠ¸ ===\n")
print("ëª©í‘œ: ì£¼íƒì˜ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ êµ¬ì¶•")
print("ë°ì´í„°: ìº˜ë¦¬í¬ë‹ˆì•„ 20,640ê°œ ì§€ì—­ì˜ ì£¼íƒ ì •ë³´")
print("í‰ê°€: ì—¬ëŸ¬ íšŒê·€ ëª¨ë¸ì„ ë¹„êµí•˜ì—¬ ìµœì  ëª¨ë¸ ì„ íƒ\n")

# 1. ë°ì´í„° íƒìƒ‰
print("1ï¸âƒ£ ë°ì´í„° íƒìƒ‰")
print("-" * 50)
print(f"ë°ì´í„° í¬ê¸°: {X_cal.shape}")
print(f"íƒ€ê²Ÿ í†µê³„: í‰ê·  ${y_cal.mean():.2f}ë§Œ, í‘œì¤€í¸ì°¨ ${y_cal.std():.2f}ë§Œ")

# ì£¼ìš” íŠ¹ì„±ê³¼ ê°€ê²©ì˜ ê´€ê³„ ì‹œê°í™”
fig, axes = plt.subplots(2, 4, figsize=(16, 8))
axes = axes.ravel()

for idx, col in enumerate(X_cal.columns):
    ax = axes[idx]
    scatter = ax.scatter(X_cal[col], y_cal, alpha=0.3, s=10, 
                        c=y_cal, cmap='viridis')
    ax.set_xlabel(col)
    ax.set_ylabel('House Price')
    ax.set_title(f'Price vs {col}')

plt.tight_layout()
plt.show()

# 2. ë°ì´í„° ì „ì²˜ë¦¬
print("\n2ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬")
print("-" * 50)

# íŠ¹ì„± ê³µí•™: ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±
X_enhanced = X_cal.copy()
X_enhanced['rooms_per_household'] = X_cal['AveRooms'] / X_cal['AveOccup']
X_enhanced['bedrooms_per_room'] = X_cal['AveBedrms'] / X_cal['AveRooms']
X_enhanced['population_per_household'] = X_cal['Population'] / X_cal['HouseAge']

print("âœ… ìƒˆë¡œìš´ íŠ¹ì„± ìƒì„±:")
print("   â€¢ rooms_per_household: ê°€êµ¬ë‹¹ ë°© ìˆ˜")
print("   â€¢ bedrooms_per_room: ë°©ë‹¹ ì¹¨ì‹¤ ë¹„ìœ¨")
print("   â€¢ population_per_household: ê°€êµ¬ë‹¹ ì¸êµ¬")

# ë°ì´í„° ë¶„í•  ë° ìŠ¤ì¼€ì¼ë§
X_train, X_test, y_train, y_test = train_test_split(
    X_enhanced, y_cal, test_size=0.2, random_state=42
)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nâœ… í•™ìŠµ ë°ì´í„°: {X_train.shape[0]}ê°œ")
print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]}ê°œ")

# 3. ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ
print("\n3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ ë° ë¹„êµ")
print("-" * 50)

models = {
    'Linear Regression': LinearRegression(),
    'Ridge (Î±=1.0)': Ridge(alpha=1.0),
    'Lasso (Î±=0.01)': Lasso(alpha=0.01),
    'Polynomial (degree=2)': 'poly'
}

results = []

for name, model in models.items():
    if name == 'Polynomial (degree=2)':
        # ë‹¤í•­ íŠ¹ì„± ìƒì„±
        poly = PolynomialFeatures(degree=2, include_bias=False)
        X_train_poly = poly.fit_transform(X_train_scaled)
        X_test_poly = poly.transform(X_test_scaled)
        
        model = Ridge(alpha=1.0)  # ë‹¤í•­ íšŒê·€ì—ëŠ” Ridge ì ìš©
        model.fit(X_train_poly, y_train)
        
        train_pred = model.predict(X_train_poly)
        test_pred = model.predict(X_test_poly)
    else:
        model.fit(X_train_scaled, y_train)
        train_pred = model.predict(X_train_scaled)
        test_pred = model.predict(X_test_scaled)
    
    # í‰ê°€
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    test_mae = mean_absolute_error(y_test, test_pred)
    
    results.append({
        'Model': name,
        'Train RÂ²': train_r2,
        'Test RÂ²': test_r2,
        'Test RMSE': test_rmse,
        'Test MAE': test_mae
    })
    
    print(f"\n{name}:")
    print(f"   Train RÂ²: {train_r2:.3f}")
    print(f"   Test RÂ²:  {test_r2:.3f}")
    print(f"   Test RMSE: ${test_rmse:.3f}ë§Œ")

# ê²°ê³¼ ì‹œê°í™”
results_df = pd.DataFrame(results)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

# RÂ² ë¹„êµ
x = np.arange(len(results_df))
width = 0.35

ax1.bar(x - width/2, results_df['Train RÂ²'], width, label='Train RÂ²', alpha=0.8)
ax1.bar(x + width/2, results_df['Test RÂ²'], width, label='Test RÂ²', alpha=0.8)
ax1.set_xlabel('Model')
ax1.set_ylabel('RÂ² Score')
ax1.set_title('ëª¨ë¸ë³„ RÂ² ì ìˆ˜ ë¹„êµ', fontsize=14, fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax1.legend()
ax1.grid(True, alpha=0.3, axis='y')

# RMSEì™€ MAE ë¹„êµ
ax2.bar(x - width/2, results_df['Test RMSE'], width, label='RMSE', alpha=0.8)
ax2.bar(x + width/2, results_df['Test MAE'], width, label='MAE', alpha=0.8)
ax2.set_xlabel('Model')
ax2.set_ylabel('Error ($10,000)')
ax2.set_title('ëª¨ë¸ë³„ ì˜ˆì¸¡ ì˜¤ì°¨ ë¹„êµ', fontsize=14, fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(results_df['Model'], rotation=45, ha='right')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# 4. ìµœì¢… ëª¨ë¸ ì„ íƒ ë° ë¶„ì„
print("\n4ï¸âƒ£ ìµœì¢… ëª¨ë¸ ë¶„ì„")
print("-" * 50)

# Ridge ëª¨ë¸ì„ ìµœì¢… ëª¨ë¸ë¡œ ì„ íƒ
final_model = Ridge(alpha=1.0)
final_model.fit(X_train_scaled, y_train)
final_pred = final_model.predict(X_test_scaled)

# ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„ì„
errors = y_test - final_pred
error_percentages = (errors / y_test) * 100

print("\nğŸ“Š ìµœì¢… ëª¨ë¸ (Ridge) ìƒì„¸ ë¶„ì„:")
print(f"   â€¢ í‰ê·  ì˜¤ì°¨ìœ¨: {np.mean(np.abs(error_percentages)):.1f}%")
print(f"   â€¢ 10% ì´ë‚´ ì •í™•ë„: {np.sum(np.abs(error_percentages) <= 10) / len(error_percentages) * 100:.1f}%")
print(f"   â€¢ 20% ì´ë‚´ ì •í™•ë„: {np.sum(np.abs(error_percentages) <= 20) / len(error_percentages) * 100:.1f}%")

# íŠ¹ì„± ì¤‘ìš”ë„
feature_importance = pd.DataFrame({
    'Feature': X_enhanced.columns,
    'Coefficient': final_model.coef_
}).sort_values('Coefficient', key=abs, ascending=False)

print("\nğŸ¯ ì£¼íƒ ê°€ê²©ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸:")
for i in range(5):
    feature = feature_importance.iloc[i]
    direction = "ì¦ê°€" if feature['Coefficient'] > 0 else "ê°ì†Œ"
    print(f"   {i+1}. {feature['Feature']}: ê³„ìˆ˜ {feature['Coefficient']:.3f} ({direction})")
```

## ğŸ¯ ì§ì ‘ í•´ë³´ê¸°

### ì—°ìŠµ ë¬¸ì œ 1: ê°„ë‹¨í•œ ì„ í˜• íšŒê·€
```python
# í•™ìƒë“¤ì˜ ìˆ˜ë©´ ì‹œê°„ê³¼ ì§‘ì¤‘ë ¥ ë°ì´í„°
sleep_hours = np.array([4, 5, 6, 6.5, 7, 7.5, 8, 8.5, 9, 10])
concentration = np.array([40, 55, 65, 70, 75, 80, 85, 82, 78, 70])

# TODO: ì„ í˜• íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì‹œê°í™”í•˜ì„¸ìš”
# 1. LinearRegression ëª¨ë¸ ìƒì„±
# 2. ë°ì´í„° reshape ë° í•™ìŠµ
# 3. ì˜ˆì¸¡ì„  ê·¸ë¦¬ê¸°
# 4. 7.3ì‹œê°„ ìˆ˜ë©´ ì‹œ ì§‘ì¤‘ë ¥ ì˜ˆì¸¡

# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

### ì—°ìŠµ ë¬¸ì œ 2: ë‹¤í•­ íšŒê·€ì™€ ì •ê·œí™”
```python
# ì˜¨ë„ì™€ ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ ë°ì´í„°
temperature = np.array([15, 18, 20, 22, 25, 28, 30, 32, 35, 38])
sales = np.array([20, 35, 50, 80, 120, 150, 160, 155, 140, 100])

# TODO: 2ì°¨ ë‹¤í•­ íšŒê·€ì™€ Ridge íšŒê·€ë¥¼ ë¹„êµí•˜ì„¸ìš”
# 1. PolynomialFeaturesë¡œ 2ì°¨ íŠ¹ì„± ìƒì„±
# 2. ì¼ë°˜ ì„ í˜• íšŒê·€ì™€ Ridge íšŒê·€ í•™ìŠµ
# 3. ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê³¡ì„  ë¹„êµ
# 4. ì–´ëŠ ëª¨ë¸ì´ ë” ì ì ˆí•œì§€ íŒë‹¨

# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

### ì—°ìŠµ ë¬¸ì œ 3: í‰ê°€ ì§€í‘œ ê³„ì‚°
```python
# ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’
y_actual = np.array([100, 120, 140, 160, 180, 200])
y_predicted = np.array([110, 115, 145, 155, 190, 195])

# TODO: ë‹¤ìŒ í‰ê°€ ì§€í‘œë¥¼ ì§ì ‘ ê³„ì‚°í•˜ì„¸ìš”
# 1. MSE (Mean Squared Error)
# 2. RMSE (Root Mean Squared Error)
# 3. MAE (Mean Absolute Error)
# 4. RÂ² (ê²°ì •ê³„ìˆ˜)

# íŒíŠ¸: numpy í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì„¸ìš”
# MSE = np.mean((y_actual - y_predicted) ** 2)

# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
```

## ğŸ“š í•µì‹¬ ì •ë¦¬

### âœ… íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ ê°œë…
1. **íšŒê·€ vs ë¶„ë¥˜**
   - íšŒê·€: ì—°ì†ì ì¸ ìˆ˜ì¹˜ ì˜ˆì¸¡ (ì§‘ê°’, ì˜¨ë„, ì ìˆ˜)
   - ë¶„ë¥˜: ì´ì‚°ì ì¸ ë²”ì£¼ ì˜ˆì¸¡ (í•©ê²©/ë¶ˆí•©ê²©, ìŠ¤íŒ¸/ì •ìƒ)

2. **ì„ í˜• íšŒê·€**
   - ìµœì†Œì œê³±ë²•ìœ¼ë¡œ ë°ì´í„°ì— ê°€ì¥ ì˜ ë§ëŠ” ì§ì„  ì°¾ê¸°
   - ë‹¤ë³€ëŸ‰ íšŒê·€ë¡œ ì—¬ëŸ¬ íŠ¹ì„± ë™ì‹œ ê³ ë ¤

3. **ë‹¤í•­ íšŒê·€**
   - ë¹„ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•  ë•Œ ì‚¬ìš©
   - ì°¨ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ë³µì¡í•œ íŒ¨í„´ í¬ì°© (ê³¼ì í•© ì£¼ì˜)

4. **ì •ê·œí™” ê¸°ë²•**
   - Ridge (L2): ëª¨ë“  ê³„ìˆ˜ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ê³¼ì í•© ë°©ì§€
   - Lasso (L1): ì¼ë¶€ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ íŠ¹ì„± ì„ íƒ

5. **í‰ê°€ ì§€í‘œ**
   - MSE/RMSE: í° ì˜¤ì°¨ì— ë¯¼ê°
   - MAE: ëª¨ë“  ì˜¤ì°¨ë¥¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
   - RÂ²: ëª¨ë¸ì˜ ì„¤ëª…ë ¥ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)

### ğŸ’¡ ì‹¤ë¬´ íŒ
- ë°ì´í„°ê°€ ì„ í˜• ê´€ê³„ë¥¼ ë³´ì´ë©´ ë‹¨ìˆœí•œ ì„ í˜• íšŒê·€ë¶€í„° ì‹œì‘
- ë¹„ì„ í˜• íŒ¨í„´ì´ ìˆìœ¼ë©´ ë‹¤í•­ íšŒê·€ë‚˜ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ê³ ë ¤
- íŠ¹ì„±ì´ ë§ìœ¼ë©´ ì •ê·œí™” ê¸°ë²• í•„ìˆ˜
- ì—¬ëŸ¬ í‰ê°€ ì§€í‘œë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ ì„ íƒ

---

## ğŸš€ ë‹¤ìŒ íŒŒíŠ¸ì—ì„œëŠ”

**ëª¨ë¸ í‰ê°€ì™€ ê²€ì¦ ë°©ë²•**ì„ í•™ìŠµí•©ë‹ˆë‹¤:
- êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥ í‰ê°€
- ê³¼ì í•©ê³¼ ê³¼ì†Œì í•© ì§„ë‹¨ ë°©ë²•
- í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ ì´í•´
- ìµœì ì˜ ëª¨ë¸ ì„ íƒ ì „ëµ

ë¨¸ì‹ ëŸ¬ë‹ì˜ í•µì‹¬ì¸ **"ì¢‹ì€ ëª¨ë¸"**ì„ ì„ íƒí•˜ëŠ” ì²´ê³„ì ì¸ ë°©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤!
